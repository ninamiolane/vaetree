{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import importlib\n",
    "import toylosses\n",
    "importlib.reload(toylosses)\n",
    "import toynn\n",
    "importlib.reload(toynn)\n",
    "import toyvis\n",
    "importlib.reload(toyvis)\n",
    "\n",
    "import torch\n",
    "sns.set()\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "OUTPUT = '/scratch/users/nmiolane/toyoutput'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on experiment's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-5.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[ 10.,   6.],\n",
      "        [ -9., -10.]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([-100., -100.], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[10.,  0.],\n",
      "        [ 0., -5.]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([2., 0.], device='cuda:0') \n",
      "\n",
      "layers.3.weight tensor([[-100., -100.],\n",
      "        [-100., -100.]], device='cuda:0') \n",
      "\n",
      "layers.3.bias tensor([-100., -100.], device='cuda:0') \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.970026209909856, 23.6077650040794, -125.80271160134153, 5.992195243919658)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_DIM = 2\n",
    "LATENT_DIM = 1\n",
    "N_DECODER_LAYERS = 3\n",
    "NONLINEARITY = True\n",
    "N_SAMPLES = 10000\n",
    "WITH_BIASX = True\n",
    "WITH_LOGVARX = True\n",
    "\n",
    "W_TRUE = {}\n",
    "B_TRUE = {}\n",
    "\n",
    "W_TRUE[0] = [[.6], [-5.7]]\n",
    "B_TRUE[0] = [0., -0.1]\n",
    "\n",
    "W_TRUE[1] = [[10., 6.], [-9., -10.]]\n",
    "B_TRUE[1] = [-100., -100.]\n",
    "\n",
    "# For the reconstruction\n",
    "W_TRUE[2] = [[10., 0.], [0., -5.]]\n",
    "B_TRUE[2] = [2., 0.]\n",
    "\n",
    "# For the logvarx\n",
    "W_TRUE[3] = [[-100., -100.], [-100., -100.]]\n",
    "B_TRUE[3] = [-100., -100.]\n",
    "\n",
    "if WITH_LOGVARX:\n",
    "    assert len(W_TRUE) == N_DECODER_LAYERS + 1, len(W_TRUE)\n",
    "else:\n",
    "    assert len(W_TRUE) == N_DECODER_LAYERS\n",
    "\n",
    "WITH_BIASZ = True\n",
    "WITH_LOGVARZ = True\n",
    "\n",
    "decoder_true = toynn.make_decoder_true(\n",
    "    w_true=W_TRUE, b_true=B_TRUE, latent_dim=LATENT_DIM, \n",
    "    data_dim=DATA_DIM, n_layers=N_DECODER_LAYERS,\n",
    "    nonlinearity=NONLINEARITY, with_biasx=WITH_BIASX, with_logvarx=WITH_LOGVARX)\n",
    "\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, N_SAMPLES)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = toyvis.plot_data(generated_true_x, color='green', label='from decoder true', ax=ax)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect generation of synthetic data from decoder_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[10.,  0.],\n",
      "        [ 0., -5.]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([2., 0.], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0., 0.], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(nina): Add a comparison to a FA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results from standard VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- True values of parameters\n",
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[10.,  0.],\n",
      "        [ 0., -5.]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([2., 0.], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0., 0.], device='cuda:0') \n",
      "\n",
      "\n",
      "-- Learnt values of parameters\n",
      "layers.0.weight tensor([[ 0.6173],\n",
      "        [-0.6216]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 1.3559, -1.6938], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[ 2.2002, -0.3396],\n",
      "        [-0.4842,  0.5791]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([ 1.3150, -0.4553], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[ 1.1083,  0.1278],\n",
      "        [ 0.5776, -0.0298]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0.5741, 0.6079], device='cuda:0') \n",
      "\n",
      "Last losses:\n",
      "[0.1497068026661873, 0.14915091699361802, 0.1498452787399292, 0.14956466609239577, 0.14914862048625946]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "decoder_path = glob.glob(f'{OUTPUT}/train_vae/models/decoder.pth')[0]\n",
    "decoder = torch.load(decoder_path, map_location=DEVICE)\n",
    "\n",
    "print('-- True values of parameters')\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "print('\\n-- Learnt values of parameters')\n",
    "for name, param in decoder.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "    \n",
    "losses_vae_path = glob.glob(f'{OUTPUT}/train_vae/train_losses.pkl')[0]\n",
    "train_losses_all_epochs = pickle.load(open(losses_vae_path, 'rb'))\n",
    "\n",
    "train_losses_total = [loss['total'] for loss in train_losses_all_epochs]\n",
    "n_epochs = len(train_losses_total)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(range(n_epochs), train_losses_total)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(range(90, n_epochs), train_losses_total[90:])\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(range(160, n_epochs), train_losses_total[160:])\n",
    "\n",
    "print('Last losses:')\n",
    "print(train_losses_total[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.039029157504746, 23.082172768232716, -16.66602428389796, 6.311862572128717)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, n_samples)\n",
    "generated_x = toynn.generate_from_decoder(decoder, n_samples)\n",
    "\n",
    "# For 1D\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(16, 4))\n",
    "axis_side = 20\n",
    "\n",
    "ax = axes[0]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[1]\n",
    "toyvis.plot_data(generated_x, color='red', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[2]\n",
    "toyvis.plot_data(generated_x, color='red', ax=ax)\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "\n",
    "\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results from VEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-ba8de477aa4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_true_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecoder_vem_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{OUTPUT}/train_vem/models/decoder.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdecoder_vem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "decoder_vem_path = glob.glob(f'{OUTPUT}/train_vem/models/decoder.pth')[0]\n",
    "decoder_vem = torch.load(decoder_path, map_location=DEVICE)\n",
    "\n",
    "print('-- True values of parameters')\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "print('\\n-- Learnt values of parameters')\n",
    "for name, param in decoder_vem.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "    \n",
    "losses_vae_path = glob.glob(f'{OUTPUT}/train_vem/train_losses.pkl')[0]\n",
    "train_losses_all_epochs = pickle.load(open(losses_vae_path, 'rb'))\n",
    "\n",
    "plt.figure()\n",
    "train_losses_total = [loss['total'] for loss in train_losses_all_epochs]\n",
    "n_epochs = len(train_losses_total)\n",
    "plt.plot(range(n_epochs), train_losses_total)\n",
    "print('Last losses:')\n",
    "print(train_losses_total[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_vem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-2242d775a246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgenerated_true_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoynn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgenerated_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoynn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_vem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# For 1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_vem' is not defined"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, n_samples)\n",
    "generated_x = toynn.generate_from_decoder(decoder_vem, n_samples)\n",
    "\n",
    "# For 1D\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(16, 4))\n",
    "axis_side = 20\n",
    "\n",
    "ax = axes[0]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[1]\n",
    "toyvis.plot_data(generated_x, color='orange', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[2]\n",
    "toyvis.plot_data(generated_x, color='orange', ax=ax)\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print pipeline logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 log files.\n",
      "\n",
      "-- Log file: logs2019-04-10 01:18:28.176590.txt\n",
      "\n",
      "2019-04-10 01:18:28,176 root         INFO     start\n",
      "2019-04-10 01:18:28,191 luigi        INFO     logging configured by default settings\n",
      "2019-04-10 01:18:28,215 luigi-interface DEBUG    Checking if RunAll() is complete\n",
      "2019-04-10 01:18:28,216 luigi-interface DEBUG    Checking if TrainVAE() is complete\n",
      "2019-04-10 01:18:28,217 luigi-interface DEBUG    Checking if TrainVEM() is complete\n",
      "2019-04-10 01:18:28,217 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   PENDING\n",
      "2019-04-10 01:18:28,217 luigi-interface DEBUG    Checking if MakeDataSet() is complete\n",
      "2019-04-10 01:18:28,218 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   PENDING\n",
      "2019-04-10 01:18:28,218 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   DONE\n",
      "2019-04-10 01:18:28,218 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   DONE\n",
      "2019-04-10 01:18:28,218 luigi-interface INFO     Done scheduling tasks\n",
      "2019-04-10 01:18:28,218 luigi-interface INFO     Running Worker with 1 processes\n",
      "2019-04-10 01:18:28,218 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:18:28,219 luigi-interface DEBUG    Pending tasks: 2\n",
      "2019-04-10 01:18:28,219 luigi-interface INFO     [pid 7676] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=7676) running   TrainVEM()\n",
      "2019-04-10 01:18:28,234 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-10 01:18:28,235 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-10 01:18:32,026 root         INFO     Values of VEM's decoder parameters before training:\n",
      "2019-04-10 01:18:32,026 root         INFO     layers.0.weight\n",
      "2019-04-10 01:18:32,026 root         INFO     tensor([[-0.2651],\n",
      "        [ 0.1353]], device='cuda:0')\n",
      "2019-04-10 01:18:32,046 root         INFO     layers.0.bias\n",
      "2019-04-10 01:18:32,046 root         INFO     tensor([-0.3248, -0.5761], device='cuda:0')\n",
      "2019-04-10 01:18:32,047 root         INFO     layers.1.weight\n",
      "2019-04-10 01:18:32,047 root         INFO     tensor([[-0.0574,  0.4460],\n",
      "        [ 0.5879, -0.3491]], device='cuda:0')\n",
      "2019-04-10 01:18:32,048 root         INFO     layers.1.bias\n",
      "2019-04-10 01:18:32,048 root         INFO     tensor([-0.4054, -0.0326], device='cuda:0')\n",
      "2019-04-10 01:18:32,049 root         INFO     layers.2.weight\n",
      "2019-04-10 01:18:32,049 root         INFO     tensor([[ 0.3113,  0.3165],\n",
      "        [-0.2632,  0.2449]], device='cuda:0')\n",
      "2019-04-10 01:18:32,050 root         INFO     layers.2.bias\n",
      "2019-04-10 01:18:32,050 root         INFO     tensor([-0.1202, -0.0849], device='cuda:0')\n",
      "2019-04-10 01:18:32,119 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 1.240236\n",
      "Reconstruction: 0.825683, Regularization: 0.323541, Discriminator: 0.070181; Generator: 0.020831,\n",
      "D(x): 0.247, D(G(z)): 0.513\n",
      "2019-04-10 01:18:32,225 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: 1.239790\n",
      "Reconstruction: 0.842878, Regularization: 0.305360, Discriminator: 0.070822; Generator: 0.020730,\n",
      "D(x): 0.233, D(G(z)): 0.515\n",
      "2019-04-10 01:18:32,325 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: 1.021821\n",
      "Reconstruction: 0.679426, Regularization: 0.256641, Discriminator: 0.065139; Generator: 0.020614,\n",
      "D(x): 0.284, D(G(z)): 0.517\n",
      "2019-04-10 01:18:32,423 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: 1.778689\n",
      "Reconstruction: 1.231620, Regularization: 0.452976, Discriminator: 0.073330; Generator: 0.020762,\n",
      "D(x): 0.234, D(G(z)): 0.515\n",
      "2019-04-10 01:18:32,520 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: 0.939055\n",
      "Reconstruction: 0.657796, Regularization: 0.195480, Discriminator: 0.065017; Generator: 0.020763,\n",
      "D(x): 0.272, D(G(z)): 0.515\n",
      "2019-04-10 01:18:32,618 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: 2.127374\n",
      "Reconstruction: 1.592639, Regularization: 0.441684, Discriminator: 0.072162; Generator: 0.020888,\n",
      "D(x): 0.246, D(G(z)): 0.513\n",
      "2019-04-10 01:18:32,716 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: 1.078177\n",
      "Reconstruction: 0.740503, Regularization: 0.252725, Discriminator: 0.064017; Generator: 0.020932,\n",
      "D(x): 0.286, D(G(z)): 0.512\n",
      "2019-04-10 01:18:32,814 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: 2.187440\n",
      "Reconstruction: 1.839278, Regularization: 0.260440, Discriminator: 0.066787; Generator: 0.020935,\n",
      "D(x): 0.264, D(G(z)): 0.512\n",
      "2019-04-10 01:18:32,912 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: 1.242403\n",
      "Reconstruction: 0.842456, Regularization: 0.315186, Discriminator: 0.063870; Generator: 0.020892,\n",
      "D(x): 0.290, D(G(z)): 0.512\n",
      "2019-04-10 01:18:33,010 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: 2.416463\n",
      "Reconstruction: 2.030133, Regularization: 0.300113, Discriminator: 0.065186; Generator: 0.021031,\n",
      "D(x): 0.274, D(G(z)): 0.510\n",
      "2019-04-10 01:18:33,109 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: 1.329998\n",
      "Reconstruction: 0.960461, Regularization: 0.286090, Discriminator: 0.062490; Generator: 0.020956,\n",
      "D(x): 0.297, D(G(z)): 0.511\n",
      "2019-04-10 01:18:33,210 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: 1.324883\n",
      "Reconstruction: 0.913758, Regularization: 0.326135, Discriminator: 0.063887; Generator: 0.021103,\n",
      "D(x): 0.282, D(G(z)): 0.509\n",
      "2019-04-10 01:18:33,311 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: 1.109596\n",
      "Reconstruction: 0.760048, Regularization: 0.268086, Discriminator: 0.060388; Generator: 0.021075,\n",
      "D(x): 0.317, D(G(z)): 0.509\n",
      "2019-04-10 01:18:33,412 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: 1.456439\n",
      "Reconstruction: 0.998793, Regularization: 0.374804, Discriminator: 0.061738; Generator: 0.021105,\n",
      "D(x): 0.299, D(G(z)): 0.509\n",
      "2019-04-10 01:18:33,514 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: 0.804567\n",
      "Reconstruction: 0.535674, Regularization: 0.192320, Discriminator: 0.055461; Generator: 0.021113,\n",
      "D(x): 0.356, D(G(z)): 0.509\n",
      "2019-04-10 01:18:33,616 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: 2.071696\n",
      "Reconstruction: 1.463964, Regularization: 0.525638, Discriminator: 0.060954; Generator: 0.021139,\n",
      "D(x): 0.311, D(G(z)): 0.508\n",
      "2019-04-10 01:18:33,692 root         INFO     ====> Epoch: 0 Average loss: 98.0984\n",
      "2019-04-10 01:18:33,718 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: 1.831397\n",
      "Reconstruction: 1.396828, Regularization: 0.353135, Discriminator: 0.060184; Generator: 0.021249,\n",
      "D(x): 0.307, D(G(z)): 0.507\n",
      "2019-04-10 01:18:33,816 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: 3.085501\n",
      "Reconstruction: 2.595633, Regularization: 0.410621, Discriminator: 0.057995; Generator: 0.021252,\n",
      "D(x): 0.333, D(G(z)): 0.507\n",
      "2019-04-10 01:18:33,914 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: 1.434940\n",
      "Reconstruction: 1.089744, Regularization: 0.266281, Discriminator: 0.057705; Generator: 0.021210,\n",
      "D(x): 0.330, D(G(z)): 0.507\n",
      "2019-04-10 01:18:34,012 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: 1.327989\n",
      "Reconstruction: 0.952665, Regularization: 0.295853, Discriminator: 0.058167; Generator: 0.021304,\n",
      "D(x): 0.324, D(G(z)): 0.506\n",
      "2019-04-10 01:18:34,109 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: 0.836673\n",
      "Reconstruction: 0.554335, Regularization: 0.207006, Discriminator: 0.053968; Generator: 0.021364,\n",
      "D(x): 0.366, D(G(z)): 0.505\n",
      "2019-04-10 01:18:34,206 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: 26.691908\n",
      "Reconstruction: 26.206915, Regularization: 0.406671, Discriminator: 0.056922; Generator: 0.021400,\n",
      "D(x): 0.342, D(G(z)): 0.504\n",
      "2019-04-10 01:18:34,304 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: 3.997540\n",
      "Reconstruction: 3.531264, Regularization: 0.388708, Discriminator: 0.056109; Generator: 0.021459,\n",
      "D(x): 0.345, D(G(z)): 0.503\n",
      "2019-04-10 01:18:34,402 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: 1.765122\n",
      "Reconstruction: 1.213915, Regularization: 0.474855, Discriminator: 0.054818; Generator: 0.021534,\n",
      "D(x): 0.360, D(G(z)): 0.502\n",
      "2019-04-10 01:18:34,500 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: 1.595762\n",
      "Reconstruction: 1.147574, Regularization: 0.372328, Discriminator: 0.054285; Generator: 0.021575,\n",
      "D(x): 0.364, D(G(z)): 0.501\n",
      "2019-04-10 01:18:34,597 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: 1.149634\n",
      "Reconstruction: 0.810493, Regularization: 0.265647, Discriminator: 0.051942; Generator: 0.021552,\n",
      "D(x): 0.387, D(G(z)): 0.502\n",
      "2019-04-10 01:18:34,695 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: 0.978686\n",
      "Reconstruction: 0.668696, Regularization: 0.236887, Discriminator: 0.051456; Generator: 0.021647,\n",
      "D(x): 0.391, D(G(z)): 0.500\n",
      "2019-04-10 01:18:34,795 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: 1.320639\n",
      "Reconstruction: 0.946791, Regularization: 0.301332, Discriminator: 0.050798; Generator: 0.021717,\n",
      "D(x): 0.399, D(G(z)): 0.499\n",
      "2019-04-10 01:18:34,894 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: 13.628688\n",
      "Reconstruction: 13.264177, Regularization: 0.291900, Discriminator: 0.050853; Generator: 0.021757,\n",
      "D(x): 0.396, D(G(z)): 0.498\n",
      "2019-04-10 01:18:34,993 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: 1.085780\n",
      "Reconstruction: 0.743976, Regularization: 0.269682, Discriminator: 0.050250; Generator: 0.021872,\n",
      "D(x): 0.401, D(G(z)): 0.497\n",
      "2019-04-10 01:18:35,092 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: 1.301242\n",
      "Reconstruction: 0.921563, Regularization: 0.308486, Discriminator: 0.049296; Generator: 0.021897,\n",
      "D(x): 0.413, D(G(z)): 0.496\n",
      "2019-04-10 01:18:35,191 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: 1.023412\n",
      "Reconstruction: 0.719267, Regularization: 0.233450, Discriminator: 0.048741; Generator: 0.021954,\n",
      "D(x): 0.418, D(G(z)): 0.495\n",
      "2019-04-10 01:18:35,265 root         INFO     ====> Epoch: 1 Average loss: 8489.8136\n",
      "2019-04-10 01:18:35,292 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: 1.411595\n",
      "Reconstruction: 0.967773, Regularization: 0.372547, Discriminator: 0.049271; Generator: 0.022003,\n",
      "D(x): 0.411, D(G(z)): 0.495\n",
      "2019-04-10 01:18:35,395 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: 1.297546\n",
      "Reconstruction: 0.900749, Regularization: 0.326150, Discriminator: 0.048628; Generator: 0.022020,\n",
      "D(x): 0.419, D(G(z)): 0.494\n",
      "2019-04-10 01:18:35,497 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: 0.866098\n",
      "Reconstruction: 0.596365, Regularization: 0.200347, Discriminator: 0.047281; Generator: 0.022105,\n",
      "D(x): 0.435, D(G(z)): 0.493\n",
      "2019-04-10 01:18:35,599 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: 0.932121\n",
      "Reconstruction: 0.653450, Regularization: 0.209566, Discriminator: 0.046907; Generator: 0.022197,\n",
      "D(x): 0.440, D(G(z)): 0.491\n",
      "2019-04-10 01:18:35,699 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: 1.144450\n",
      "Reconstruction: 0.777954, Regularization: 0.297586, Discriminator: 0.046679; Generator: 0.022232,\n",
      "D(x): 0.442, D(G(z)): 0.491\n",
      "2019-04-10 01:18:35,800 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: 1.240424\n",
      "Reconstruction: 0.877793, Regularization: 0.294173, Discriminator: 0.046158; Generator: 0.022300,\n",
      "D(x): 0.448, D(G(z)): 0.490\n",
      "2019-04-10 01:18:35,902 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: 1.086685\n",
      "Reconstruction: 0.737984, Regularization: 0.280804, Discriminator: 0.045502; Generator: 0.022395,\n",
      "D(x): 0.456, D(G(z)): 0.488\n",
      "2019-04-10 01:18:36,004 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: 1.527417\n",
      "Reconstruction: 1.057544, Regularization: 0.402192, Discriminator: 0.045257; Generator: 0.022424,\n",
      "D(x): 0.459, D(G(z)): 0.488\n",
      "2019-04-10 01:18:36,105 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: 1.539137\n",
      "Reconstruction: 1.202822, Regularization: 0.269178, Discriminator: 0.044611; Generator: 0.022527,\n",
      "D(x): 0.467, D(G(z)): 0.486\n",
      "2019-04-10 01:18:36,206 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: 2.368226\n",
      "Reconstruction: 1.925551, Regularization: 0.375833, Discriminator: 0.044248; Generator: 0.022594,\n",
      "D(x): 0.472, D(G(z)): 0.485\n",
      "2019-04-10 01:18:36,307 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: 1.065204\n",
      "Reconstruction: 0.739537, Regularization: 0.259295, Discriminator: 0.043693; Generator: 0.022679,\n",
      "D(x): 0.479, D(G(z)): 0.484\n",
      "2019-04-10 01:18:36,408 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: 18.885899\n",
      "Reconstruction: 18.619724, Regularization: 0.200168, Discriminator: 0.043254; Generator: 0.022751,\n",
      "D(x): 0.484, D(G(z)): 0.483\n",
      "2019-04-10 01:18:36,510 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: 1.000440\n",
      "Reconstruction: 0.708831, Regularization: 0.225983, Discriminator: 0.042799; Generator: 0.022827,\n",
      "D(x): 0.490, D(G(z)): 0.482\n",
      "2019-04-10 01:18:36,611 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: 1.177518\n",
      "Reconstruction: 0.807389, Regularization: 0.304851, Discriminator: 0.042377; Generator: 0.022901,\n",
      "D(x): 0.496, D(G(z)): 0.481\n",
      "2019-04-10 01:18:36,712 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: 1.074061\n",
      "Reconstruction: 0.766152, Regularization: 0.242915, Discriminator: 0.042006; Generator: 0.022989,\n",
      "D(x): 0.501, D(G(z)): 0.479\n",
      "2019-04-10 01:18:36,812 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: 1.324746\n",
      "Reconstruction: 0.906811, Regularization: 0.353337, Discriminator: 0.041511; Generator: 0.023088,\n",
      "D(x): 0.508, D(G(z)): 0.478\n",
      "2019-04-10 01:18:36,887 root         INFO     ====> Epoch: 2 Average loss: 2.6292\n",
      "2019-04-10 01:18:36,914 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: 1.601814\n",
      "Reconstruction: 1.098141, Regularization: 0.439496, Discriminator: 0.041043; Generator: 0.023133,\n",
      "D(x): 0.515, D(G(z)): 0.477\n",
      "2019-04-10 01:18:37,015 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: 1.495831\n",
      "Reconstruction: 1.135534, Regularization: 0.296197, Discriminator: 0.040863; Generator: 0.023237,\n",
      "D(x): 0.516, D(G(z)): 0.475\n",
      "2019-04-10 01:18:37,115 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: 1.176608\n",
      "Reconstruction: 0.811967, Regularization: 0.301029, Discriminator: 0.040281; Generator: 0.023331,\n",
      "D(x): 0.525, D(G(z)): 0.474\n",
      "2019-04-10 01:18:37,216 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: 1.300568\n",
      "Reconstruction: 0.892742, Regularization: 0.344352, Discriminator: 0.040087; Generator: 0.023387,\n",
      "D(x): 0.528, D(G(z)): 0.473\n",
      "2019-04-10 01:18:37,316 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: 1.156665\n",
      "Reconstruction: 0.803449, Regularization: 0.290018, Discriminator: 0.039714; Generator: 0.023485,\n",
      "D(x): 0.533, D(G(z)): 0.472\n",
      "2019-04-10 01:18:37,416 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: 1.060785\n",
      "Reconstruction: 0.732878, Regularization: 0.265177, Discriminator: 0.039075; Generator: 0.023656,\n",
      "D(x): 0.541, D(G(z)): 0.469\n",
      "2019-04-10 01:18:37,516 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: 0.830600\n",
      "Reconstruction: 0.574911, Regularization: 0.192856, Discriminator: 0.039146; Generator: 0.023687,\n",
      "D(x): 0.539, D(G(z)): 0.469\n",
      "2019-04-10 01:18:37,617 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: 1.069700\n",
      "Reconstruction: 0.742523, Regularization: 0.264759, Discriminator: 0.038597; Generator: 0.023821,\n",
      "D(x): 0.547, D(G(z)): 0.467\n",
      "2019-04-10 01:18:37,717 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: 20.440357\n",
      "Reconstruction: 19.991108, Regularization: 0.387512, Discriminator: 0.037866; Generator: 0.023870,\n",
      "D(x): 0.561, D(G(z)): 0.466\n",
      "2019-04-10 01:18:37,817 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: 1.007058\n",
      "Reconstruction: 0.704863, Regularization: 0.240162, Discriminator: 0.038029; Generator: 0.024004,\n",
      "D(x): 0.555, D(G(z)): 0.464\n",
      "2019-04-10 01:18:37,917 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: 1.306960\n",
      "Reconstruction: 0.943207, Regularization: 0.302442, Discriminator: 0.037185; Generator: 0.024126,\n",
      "D(x): 0.568, D(G(z)): 0.462\n",
      "2019-04-10 01:18:38,017 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: 1.285677\n",
      "Reconstruction: 0.936111, Regularization: 0.288574, Discriminator: 0.036816; Generator: 0.024175,\n",
      "D(x): 0.575, D(G(z)): 0.461\n",
      "2019-04-10 01:18:38,116 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: 0.954054\n",
      "Reconstruction: 0.655535, Regularization: 0.237246, Discriminator: 0.037038; Generator: 0.024235,\n",
      "D(x): 0.569, D(G(z)): 0.460\n",
      "2019-04-10 01:18:38,216 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: 1.308909\n",
      "Reconstruction: 0.906241, Regularization: 0.342582, Discriminator: 0.035759; Generator: 0.024327,\n",
      "D(x): 0.593, D(G(z)): 0.459\n",
      "2019-04-10 01:18:38,316 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: 1.167728\n",
      "Reconstruction: 0.812143, Regularization: 0.294839, Discriminator: 0.036264; Generator: 0.024482,\n",
      "D(x): 0.580, D(G(z)): 0.457\n",
      "2019-04-10 01:18:38,416 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: 1.137099\n",
      "Reconstruction: 0.786277, Regularization: 0.289686, Discriminator: 0.036581; Generator: 0.024554,\n",
      "D(x): 0.575, D(G(z)): 0.456\n",
      "2019-04-10 01:18:38,492 root         INFO     ====> Epoch: 3 Average loss: 6.3073\n",
      "2019-04-10 01:18:38,518 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: 0.937208\n",
      "Reconstruction: 0.623150, Regularization: 0.253176, Discriminator: 0.036235; Generator: 0.024648,\n",
      "D(x): 0.580, D(G(z)): 0.454\n",
      "2019-04-10 01:18:38,621 root         INFO     Train Epoch: 4 [512/8000 (6%)]\tTotal Loss: 3.196442\n",
      "Reconstruction: 2.831219, Regularization: 0.306286, Discriminator: 0.034062; Generator: 0.024875,\n",
      "D(x): 0.617, D(G(z)): 0.451\n",
      "2019-04-10 01:18:38,722 root         INFO     Train Epoch: 4 [1024/8000 (13%)]\tTotal Loss: 0.907382\n",
      "Reconstruction: 0.648171, Regularization: 0.199093, Discriminator: 0.035153; Generator: 0.024966,\n",
      "D(x): 0.594, D(G(z)): 0.450\n",
      "2019-04-10 01:18:38,823 root         INFO     Train Epoch: 4 [1536/8000 (19%)]\tTotal Loss: 0.693378\n",
      "Reconstruction: 0.482335, Regularization: 0.150868, Discriminator: 0.035148; Generator: 0.025026,\n",
      "D(x): 0.593, D(G(z)): 0.449\n",
      "2019-04-10 01:18:38,923 root         INFO     Train Epoch: 4 [2048/8000 (26%)]\tTotal Loss: 1.670522\n",
      "Reconstruction: 1.191924, Regularization: 0.420256, Discriminator: 0.033237; Generator: 0.025106,\n",
      "D(x): 0.634, D(G(z)): 0.448\n",
      "2019-04-10 01:18:39,024 root         INFO     Train Epoch: 4 [2560/8000 (32%)]\tTotal Loss: 1.612128\n",
      "Reconstruction: 1.072439, Regularization: 0.481055, Discriminator: 0.033342; Generator: 0.025293,\n",
      "D(x): 0.629, D(G(z)): 0.445\n",
      "2019-04-10 01:18:39,123 root         INFO     Train Epoch: 4 [3072/8000 (38%)]\tTotal Loss: 1.016667\n",
      "Reconstruction: 0.723380, Regularization: 0.234538, Discriminator: 0.033355; Generator: 0.025394,\n",
      "D(x): 0.623, D(G(z)): 0.444\n",
      "2019-04-10 01:18:39,222 root         INFO     Train Epoch: 4 [3584/8000 (45%)]\tTotal Loss: 1.404170\n",
      "Reconstruction: 0.983760, Regularization: 0.362623, Discriminator: 0.032267; Generator: 0.025520,\n",
      "D(x): 0.644, D(G(z)): 0.442\n",
      "2019-04-10 01:18:39,322 root         INFO     Train Epoch: 4 [4096/8000 (51%)]\tTotal Loss: 1.746529\n",
      "Reconstruction: 1.294560, Regularization: 0.394338, Discriminator: 0.031953; Generator: 0.025678,\n",
      "D(x): 0.652, D(G(z)): 0.440\n",
      "2019-04-10 01:18:39,425 root         INFO     Train Epoch: 4 [4608/8000 (58%)]\tTotal Loss: 1.523996\n",
      "Reconstruction: 1.157005, Regularization: 0.308527, Discriminator: 0.032642; Generator: 0.025822,\n",
      "D(x): 0.635, D(G(z)): 0.438\n",
      "2019-04-10 01:18:39,526 root         INFO     Train Epoch: 4 [5120/8000 (64%)]\tTotal Loss: 0.977752\n",
      "Reconstruction: 0.695422, Regularization: 0.224629, Discriminator: 0.031727; Generator: 0.025974,\n",
      "D(x): 0.649, D(G(z)): 0.436\n",
      "2019-04-10 01:18:39,628 root         INFO     Train Epoch: 4 [5632/8000 (70%)]\tTotal Loss: 1.011065\n",
      "Reconstruction: 0.710573, Regularization: 0.241941, Discriminator: 0.032412; Generator: 0.026140,\n",
      "D(x): 0.635, D(G(z)): 0.433\n",
      "2019-04-10 01:18:39,727 root         INFO     Train Epoch: 4 [6144/8000 (77%)]\tTotal Loss: 1.375300\n",
      "Reconstruction: 0.971316, Regularization: 0.346224, Discriminator: 0.031560; Generator: 0.026200,\n",
      "D(x): 0.653, D(G(z)): 0.432\n",
      "2019-04-10 01:18:39,827 root         INFO     Train Epoch: 4 [6656/8000 (83%)]\tTotal Loss: 2.255714\n",
      "Reconstruction: 1.892012, Regularization: 0.305329, Discriminator: 0.032022; Generator: 0.026351,\n",
      "D(x): 0.645, D(G(z)): 0.430\n",
      "2019-04-10 01:18:39,927 root         INFO     Train Epoch: 4 [7168/8000 (90%)]\tTotal Loss: 1.380960\n",
      "Reconstruction: 0.973404, Regularization: 0.350743, Discriminator: 0.030399; Generator: 0.026414,\n",
      "D(x): 0.672, D(G(z)): 0.429\n",
      "2019-04-10 01:18:40,026 root         INFO     Train Epoch: 4 [7680/8000 (96%)]\tTotal Loss: 1.222313\n",
      "Reconstruction: 0.842346, Regularization: 0.322724, Discriminator: 0.030560; Generator: 0.026684,\n",
      "D(x): 0.665, D(G(z)): 0.426\n",
      "2019-04-10 01:18:40,099 root         INFO     ====> Epoch: 4 Average loss: 1.4416\n",
      "2019-04-10 01:18:40,125 root         INFO     Train Epoch: 5 [0/8000 (0%)]\tTotal Loss: 1.024622\n",
      "Reconstruction: 0.705901, Regularization: 0.259951, Discriminator: 0.032116; Generator: 0.026654,\n",
      "D(x): 0.640, D(G(z)): 0.426\n",
      "2019-04-10 01:18:40,227 root         INFO     Train Epoch: 5 [512/8000 (6%)]\tTotal Loss: 1.007952\n",
      "Reconstruction: 0.706357, Regularization: 0.245181, Discriminator: 0.029634; Generator: 0.026781,\n",
      "D(x): 0.679, D(G(z)): 0.424\n",
      "2019-04-10 01:18:40,327 root         INFO     Train Epoch: 5 [1024/8000 (13%)]\tTotal Loss: 1.296358\n",
      "Reconstruction: 0.883975, Regularization: 0.354962, Discriminator: 0.030468; Generator: 0.026954,\n",
      "D(x): 0.665, D(G(z)): 0.422\n",
      "2019-04-10 01:18:40,427 root         INFO     Train Epoch: 5 [1536/8000 (19%)]\tTotal Loss: 1.448897\n",
      "Reconstruction: 1.047377, Regularization: 0.344446, Discriminator: 0.029898; Generator: 0.027176,\n",
      "D(x): 0.674, D(G(z)): 0.419\n",
      "2019-04-10 01:18:40,527 root         INFO     Train Epoch: 5 [2048/8000 (26%)]\tTotal Loss: 0.982673\n",
      "Reconstruction: 0.674870, Regularization: 0.250387, Discriminator: 0.030091; Generator: 0.027324,\n",
      "D(x): 0.670, D(G(z)): 0.417\n",
      "2019-04-10 01:18:40,627 root         INFO     Train Epoch: 5 [2560/8000 (32%)]\tTotal Loss: 1.162508\n",
      "Reconstruction: 0.811276, Regularization: 0.295384, Discriminator: 0.028143; Generator: 0.027704,\n",
      "D(x): 0.703, D(G(z)): 0.412\n",
      "2019-04-10 01:18:40,728 root         INFO     Train Epoch: 5 [3072/8000 (38%)]\tTotal Loss: 1.871221\n",
      "Reconstruction: 1.321824, Regularization: 0.494538, Discriminator: 0.027002; Generator: 0.027856,\n",
      "D(x): 0.733, D(G(z)): 0.410\n",
      "2019-04-10 01:18:40,828 root         INFO     Train Epoch: 5 [3584/8000 (45%)]\tTotal Loss: 1.748112\n",
      "Reconstruction: 1.197631, Regularization: 0.495319, Discriminator: 0.027517; Generator: 0.027646,\n",
      "D(x): 0.720, D(G(z)): 0.413\n",
      "2019-04-10 01:18:40,928 root         INFO     Train Epoch: 5 [4096/8000 (51%)]\tTotal Loss: 0.907105\n",
      "Reconstruction: 0.616240, Regularization: 0.232898, Discriminator: 0.030195; Generator: 0.027771,\n",
      "D(x): 0.659, D(G(z)): 0.411\n",
      "2019-04-10 01:18:41,027 root         INFO     Train Epoch: 5 [4608/8000 (58%)]\tTotal Loss: 1.755842\n",
      "Reconstruction: 1.197518, Regularization: 0.503425, Discriminator: 0.026967; Generator: 0.027932,\n",
      "D(x): 0.728, D(G(z)): 0.409\n",
      "2019-04-10 01:18:41,126 root         INFO     Train Epoch: 5 [5120/8000 (64%)]\tTotal Loss: 1.624251\n",
      "Reconstruction: 1.180698, Regularization: 0.387967, Discriminator: 0.027391; Generator: 0.028195,\n",
      "D(x): 0.715, D(G(z)): 0.406\n",
      "2019-04-10 01:18:41,226 root         INFO     Train Epoch: 5 [5632/8000 (70%)]\tTotal Loss: 0.972270\n",
      "Reconstruction: 0.707390, Regularization: 0.207899, Discriminator: 0.028718; Generator: 0.028264,\n",
      "D(x): 0.687, D(G(z)): 0.405\n",
      "2019-04-10 01:18:41,326 root         INFO     Train Epoch: 5 [6144/8000 (77%)]\tTotal Loss: 1.693688\n",
      "Reconstruction: 1.328704, Regularization: 0.308655, Discriminator: 0.027818; Generator: 0.028511,\n",
      "D(x): 0.701, D(G(z)): 0.402\n",
      "2019-04-10 01:18:41,426 root         INFO     Train Epoch: 5 [6656/8000 (83%)]\tTotal Loss: 1.073691\n",
      "Reconstruction: 0.742507, Regularization: 0.275691, Discriminator: 0.026889; Generator: 0.028604,\n",
      "D(x): 0.718, D(G(z)): 0.400\n",
      "2019-04-10 01:18:41,526 root         INFO     Train Epoch: 5 [7168/8000 (90%)]\tTotal Loss: 1.140414\n",
      "Reconstruction: 0.773713, Regularization: 0.310387, Discriminator: 0.027604; Generator: 0.028710,\n",
      "D(x): 0.705, D(G(z)): 0.399\n",
      "2019-04-10 01:18:41,625 root         INFO     Train Epoch: 5 [7680/8000 (96%)]\tTotal Loss: 0.808547\n",
      "Reconstruction: 0.536811, Regularization: 0.212672, Discriminator: 0.030144; Generator: 0.028919,\n",
      "D(x): 0.647, D(G(z)): 0.396\n",
      "2019-04-10 01:18:41,700 root         INFO     ====> Epoch: 5 Average loss: 4.8133\n",
      "2019-04-10 01:18:41,727 root         INFO     Train Epoch: 6 [0/8000 (0%)]\tTotal Loss: 0.865134\n",
      "Reconstruction: 0.586426, Regularization: 0.221213, Discriminator: 0.028482; Generator: 0.029013,\n",
      "D(x): 0.680, D(G(z)): 0.395\n",
      "2019-04-10 01:18:41,828 root         INFO     Train Epoch: 6 [512/8000 (6%)]\tTotal Loss: 1.285907\n",
      "Reconstruction: 0.886393, Regularization: 0.344869, Discriminator: 0.025529; Generator: 0.029116,\n",
      "D(x): 0.746, D(G(z)): 0.394\n",
      "2019-04-10 01:18:41,928 root         INFO     Train Epoch: 6 [1024/8000 (13%)]\tTotal Loss: 3.066985\n",
      "Reconstruction: 2.756131, Regularization: 0.253952, Discriminator: 0.027484; Generator: 0.029418,\n",
      "D(x): 0.697, D(G(z)): 0.390\n",
      "2019-04-10 01:18:42,030 root         INFO     Train Epoch: 6 [1536/8000 (19%)]\tTotal Loss: 1.496509\n",
      "Reconstruction: 1.124009, Regularization: 0.318804, Discriminator: 0.024178; Generator: 0.029518,\n",
      "D(x): 0.766, D(G(z)): 0.389\n",
      "2019-04-10 01:18:42,132 root         INFO     Train Epoch: 6 [2048/8000 (26%)]\tTotal Loss: 0.891107\n",
      "Reconstruction: 0.624683, Regularization: 0.209864, Discriminator: 0.026875; Generator: 0.029685,\n",
      "D(x): 0.704, D(G(z)): 0.387\n",
      "2019-04-10 01:18:42,232 root         INFO     Train Epoch: 6 [2560/8000 (32%)]\tTotal Loss: 0.895660\n",
      "Reconstruction: 0.616522, Regularization: 0.224027, Discriminator: 0.025339; Generator: 0.029773,\n",
      "D(x): 0.732, D(G(z)): 0.386\n",
      "2019-04-10 01:18:42,331 root         INFO     Train Epoch: 6 [3072/8000 (38%)]\tTotal Loss: 1.603836\n",
      "Reconstruction: 1.121571, Regularization: 0.427404, Discriminator: 0.024836; Generator: 0.030025,\n",
      "D(x): 0.759, D(G(z)): 0.383\n",
      "2019-04-10 01:18:42,431 root         INFO     Train Epoch: 6 [3584/8000 (45%)]\tTotal Loss: 1.009812\n",
      "Reconstruction: 0.705026, Regularization: 0.248362, Discriminator: 0.026018; Generator: 0.030406,\n",
      "D(x): 0.719, D(G(z)): 0.378\n",
      "2019-04-10 01:18:42,530 root         INFO     Train Epoch: 6 [4096/8000 (51%)]\tTotal Loss: 1.311502\n",
      "Reconstruction: 0.895375, Regularization: 0.360916, Discriminator: 0.024990; Generator: 0.030221,\n",
      "D(x): 0.744, D(G(z)): 0.380\n",
      "2019-04-10 01:18:42,629 root         INFO     Train Epoch: 6 [4608/8000 (58%)]\tTotal Loss: 0.956266\n",
      "Reconstruction: 0.654409, Regularization: 0.247273, Discriminator: 0.024238; Generator: 0.030346,\n",
      "D(x): 0.753, D(G(z)): 0.379\n",
      "2019-04-10 01:18:42,729 root         INFO     Train Epoch: 6 [5120/8000 (64%)]\tTotal Loss: 1.178624\n",
      "Reconstruction: 0.874549, Regularization: 0.249988, Discriminator: 0.023285; Generator: 0.030801,\n",
      "D(x): 0.772, D(G(z)): 0.373\n",
      "2019-04-10 01:18:42,828 root         INFO     Train Epoch: 6 [5632/8000 (70%)]\tTotal Loss: 1.504367\n",
      "Reconstruction: 1.038239, Regularization: 0.410263, Discriminator: 0.025145; Generator: 0.030720,\n",
      "D(x): 0.738, D(G(z)): 0.374\n",
      "2019-04-10 01:18:42,927 root         INFO     Train Epoch: 6 [6144/8000 (77%)]\tTotal Loss: 1.132548\n",
      "Reconstruction: 0.786033, Regularization: 0.290629, Discriminator: 0.024846; Generator: 0.031041,\n",
      "D(x): 0.743, D(G(z)): 0.370\n",
      "2019-04-10 01:18:43,027 root         INFO     Train Epoch: 6 [6656/8000 (83%)]\tTotal Loss: 1.032983\n",
      "Reconstruction: 0.721385, Regularization: 0.256883, Discriminator: 0.023646; Generator: 0.031069,\n",
      "D(x): 0.762, D(G(z)): 0.370\n",
      "2019-04-10 01:18:43,127 root         INFO     Train Epoch: 6 [7168/8000 (90%)]\tTotal Loss: 3.220982\n",
      "Reconstruction: 2.767209, Regularization: 0.400018, Discriminator: 0.022452; Generator: 0.031303,\n",
      "D(x): 0.787, D(G(z)): 0.367\n",
      "2019-04-10 01:18:43,226 root         INFO     Train Epoch: 6 [7680/8000 (96%)]\tTotal Loss: 1.221644\n",
      "Reconstruction: 0.850341, Regularization: 0.318051, Discriminator: 0.021827; Generator: 0.031424,\n",
      "D(x): 0.798, D(G(z)): 0.366\n",
      "2019-04-10 01:18:43,300 root         INFO     ====> Epoch: 6 Average loss: 1.9906\n",
      "2019-04-10 01:18:43,326 root         INFO     Train Epoch: 7 [0/8000 (0%)]\tTotal Loss: 1.349194\n",
      "Reconstruction: 0.923560, Regularization: 0.369402, Discriminator: 0.024803; Generator: 0.031429,\n",
      "D(x): 0.731, D(G(z)): 0.366\n",
      "2019-04-10 01:18:43,425 root         INFO     Train Epoch: 7 [512/8000 (6%)]\tTotal Loss: 0.980320\n",
      "Reconstruction: 0.683638, Regularization: 0.240701, Discriminator: 0.024292; Generator: 0.031688,\n",
      "D(x): 0.741, D(G(z)): 0.363\n",
      "2019-04-10 01:18:43,523 root         INFO     Train Epoch: 7 [1024/8000 (13%)]\tTotal Loss: 1.823291\n",
      "Reconstruction: 1.256665, Regularization: 0.513073, Discriminator: 0.021815; Generator: 0.031738,\n",
      "D(x): 0.806, D(G(z)): 0.362\n",
      "2019-04-10 01:18:43,621 root         INFO     Train Epoch: 7 [1536/8000 (19%)]\tTotal Loss: 11.605702\n",
      "Reconstruction: 11.167234, Regularization: 0.383414, Discriminator: 0.022955; Generator: 0.032099,\n",
      "D(x): 0.774, D(G(z)): 0.358\n",
      "2019-04-10 01:18:43,720 root         INFO     Train Epoch: 7 [2048/8000 (26%)]\tTotal Loss: 1.063978\n",
      "Reconstruction: 0.727307, Regularization: 0.280195, Discriminator: 0.024424; Generator: 0.032052,\n",
      "D(x): 0.740, D(G(z)): 0.359\n",
      "2019-04-10 01:18:43,818 root         INFO     Train Epoch: 7 [2560/8000 (32%)]\tTotal Loss: 1.115025\n",
      "Reconstruction: 0.783282, Regularization: 0.278274, Discriminator: 0.021031; Generator: 0.032439,\n",
      "D(x): 0.808, D(G(z)): 0.354\n",
      "2019-04-10 01:18:43,917 root         INFO     Train Epoch: 7 [3072/8000 (38%)]\tTotal Loss: 1.798403\n",
      "Reconstruction: 1.262095, Regularization: 0.483871, Discriminator: 0.019998; Generator: 0.032438,\n",
      "D(x): 0.837, D(G(z)): 0.354\n",
      "2019-04-10 01:18:44,015 root         INFO     Train Epoch: 7 [3584/8000 (45%)]\tTotal Loss: 1.103072\n",
      "Reconstruction: 0.768944, Regularization: 0.281192, Discriminator: 0.020014; Generator: 0.032923,\n",
      "D(x): 0.819, D(G(z)): 0.349\n",
      "2019-04-10 01:18:44,113 root         INFO     Train Epoch: 7 [4096/8000 (51%)]\tTotal Loss: 0.867863\n",
      "Reconstruction: 0.600848, Regularization: 0.210268, Discriminator: 0.024000; Generator: 0.032746,\n",
      "D(x): 0.737, D(G(z)): 0.351\n",
      "2019-04-10 01:18:44,211 root         INFO     Train Epoch: 7 [4608/8000 (58%)]\tTotal Loss: 0.707230\n",
      "Reconstruction: 0.477517, Regularization: 0.173572, Discriminator: 0.023191; Generator: 0.032951,\n",
      "D(x): 0.752, D(G(z)): 0.348\n",
      "2019-04-10 01:18:44,310 root         INFO     Train Epoch: 7 [5120/8000 (64%)]\tTotal Loss: 0.523370\n",
      "Reconstruction: 0.347693, Regularization: 0.117315, Discriminator: 0.025252; Generator: 0.033111,\n",
      "D(x): 0.708, D(G(z)): 0.347\n",
      "2019-04-10 01:18:44,408 root         INFO     Train Epoch: 7 [5632/8000 (70%)]\tTotal Loss: 0.917030\n",
      "Reconstruction: 0.636172, Regularization: 0.225974, Discriminator: 0.021686; Generator: 0.033198,\n",
      "D(x): 0.779, D(G(z)): 0.346\n",
      "2019-04-10 01:18:44,508 root         INFO     Train Epoch: 7 [6144/8000 (77%)]\tTotal Loss: 1.209761\n",
      "Reconstruction: 0.823263, Regularization: 0.331505, Discriminator: 0.021622; Generator: 0.033372,\n",
      "D(x): 0.789, D(G(z)): 0.344\n",
      "2019-04-10 01:18:44,606 root         INFO     Train Epoch: 7 [6656/8000 (83%)]\tTotal Loss: 1.031720\n",
      "Reconstruction: 0.730770, Regularization: 0.244327, Discriminator: 0.023066; Generator: 0.033558,\n",
      "D(x): 0.752, D(G(z)): 0.342\n",
      "2019-04-10 01:18:44,704 root         INFO     Train Epoch: 7 [7168/8000 (90%)]\tTotal Loss: 1.191391\n",
      "Reconstruction: 0.809989, Regularization: 0.325682, Discriminator: 0.021885; Generator: 0.033835,\n",
      "D(x): 0.777, D(G(z)): 0.339\n",
      "2019-04-10 01:18:44,802 root         INFO     Train Epoch: 7 [7680/8000 (96%)]\tTotal Loss: 1.248740\n",
      "Reconstruction: 0.859523, Regularization: 0.332826, Discriminator: 0.022451; Generator: 0.033940,\n",
      "D(x): 0.762, D(G(z)): 0.338\n",
      "2019-04-10 01:18:44,876 root         INFO     ====> Epoch: 7 Average loss: 1.7318\n",
      "2019-04-10 01:18:44,902 root         INFO     Train Epoch: 8 [0/8000 (0%)]\tTotal Loss: 1.170876\n",
      "Reconstruction: 0.807315, Regularization: 0.309306, Discriminator: 0.020179; Generator: 0.034076,\n",
      "D(x): 0.812, D(G(z)): 0.336\n",
      "2019-04-10 01:18:45,000 root         INFO     Train Epoch: 8 [512/8000 (6%)]\tTotal Loss: 0.993719\n",
      "Reconstruction: 0.671127, Regularization: 0.266676, Discriminator: 0.021743; Generator: 0.034173,\n",
      "D(x): 0.775, D(G(z)): 0.335\n",
      "2019-04-10 01:18:45,100 root         INFO     Train Epoch: 8 [1024/8000 (13%)]\tTotal Loss: 1.771234\n",
      "Reconstruction: 1.255251, Regularization: 0.460538, Discriminator: 0.021018; Generator: 0.034427,\n",
      "D(x): 0.787, D(G(z)): 0.332\n",
      "2019-04-10 01:18:45,198 root         INFO     Train Epoch: 8 [1536/8000 (19%)]\tTotal Loss: 0.877573\n",
      "Reconstruction: 0.582897, Regularization: 0.234925, Discriminator: 0.025253; Generator: 0.034497,\n",
      "D(x): 0.708, D(G(z)): 0.332\n",
      "2019-04-10 01:18:45,296 root         INFO     Train Epoch: 8 [2048/8000 (26%)]\tTotal Loss: 1.255566\n",
      "Reconstruction: 0.860729, Regularization: 0.340281, Discriminator: 0.019939; Generator: 0.034617,\n",
      "D(x): 0.813, D(G(z)): 0.330\n",
      "2019-04-10 01:18:45,394 root         INFO     Train Epoch: 8 [2560/8000 (32%)]\tTotal Loss: 1.116268\n",
      "Reconstruction: 0.758027, Regularization: 0.303771, Discriminator: 0.019701; Generator: 0.034769,\n",
      "D(x): 0.812, D(G(z)): 0.329\n",
      "2019-04-10 01:18:45,492 root         INFO     Train Epoch: 8 [3072/8000 (38%)]\tTotal Loss: 0.873715\n",
      "Reconstruction: 0.610822, Regularization: 0.206011, Discriminator: 0.021775; Generator: 0.035106,\n",
      "D(x): 0.780, D(G(z)): 0.325\n",
      "2019-04-10 01:18:45,591 root         INFO     Train Epoch: 8 [3584/8000 (45%)]\tTotal Loss: 1.002241\n",
      "Reconstruction: 0.708400, Regularization: 0.238416, Discriminator: 0.020228; Generator: 0.035197,\n",
      "D(x): 0.800, D(G(z)): 0.324\n",
      "2019-04-10 01:18:45,689 root         INFO     Train Epoch: 8 [4096/8000 (51%)]\tTotal Loss: 0.978752\n",
      "Reconstruction: 0.670528, Regularization: 0.251423, Discriminator: 0.021410; Generator: 0.035392,\n",
      "D(x): 0.782, D(G(z)): 0.322\n",
      "2019-04-10 01:18:45,788 root         INFO     Train Epoch: 8 [4608/8000 (58%)]\tTotal Loss: 1.417793\n",
      "Reconstruction: 0.956519, Regularization: 0.408101, Discriminator: 0.017708; Generator: 0.035465,\n",
      "D(x): 0.852, D(G(z)): 0.321\n",
      "2019-04-10 01:18:45,886 root         INFO     Train Epoch: 8 [5120/8000 (64%)]\tTotal Loss: 0.629942\n",
      "Reconstruction: 0.424579, Regularization: 0.148589, Discriminator: 0.021083; Generator: 0.035690,\n",
      "D(x): 0.768, D(G(z)): 0.319\n",
      "2019-04-10 01:18:45,984 root         INFO     Train Epoch: 8 [5632/8000 (70%)]\tTotal Loss: 1.084375\n",
      "Reconstruction: 0.743897, Regularization: 0.285448, Discriminator: 0.019252; Generator: 0.035778,\n",
      "D(x): 0.817, D(G(z)): 0.318\n",
      "2019-04-10 01:18:46,083 root         INFO     Train Epoch: 8 [6144/8000 (77%)]\tTotal Loss: 1.223756\n",
      "Reconstruction: 0.832898, Regularization: 0.336840, Discriminator: 0.017605; Generator: 0.036413,\n",
      "D(x): 0.846, D(G(z)): 0.312\n",
      "2019-04-10 01:18:46,181 root         INFO     Train Epoch: 8 [6656/8000 (83%)]\tTotal Loss: 1.083913\n",
      "Reconstruction: 0.743940, Regularization: 0.285535, Discriminator: 0.018360; Generator: 0.036078,\n",
      "D(x): 0.834, D(G(z)): 0.315\n",
      "2019-04-10 01:18:46,279 root         INFO     Train Epoch: 8 [7168/8000 (90%)]\tTotal Loss: 0.879784\n",
      "Reconstruction: 0.604803, Regularization: 0.221023, Discriminator: 0.017640; Generator: 0.036317,\n",
      "D(x): 0.845, D(G(z)): 0.313\n",
      "2019-04-10 01:18:46,377 root         INFO     Train Epoch: 8 [7680/8000 (96%)]\tTotal Loss: 1.385931\n",
      "Reconstruction: 0.944366, Regularization: 0.386375, Discriminator: 0.018784; Generator: 0.036406,\n",
      "D(x): 0.822, D(G(z)): 0.312\n",
      "2019-04-10 01:18:46,451 root         INFO     ====> Epoch: 8 Average loss: 1.5937\n",
      "2019-04-10 01:18:46,477 root         INFO     Train Epoch: 9 [0/8000 (0%)]\tTotal Loss: 0.690860\n",
      "Reconstruction: 0.465206, Regularization: 0.168494, Discriminator: 0.020537; Generator: 0.036623,\n",
      "D(x): 0.778, D(G(z)): 0.310\n",
      "2019-04-10 01:18:46,577 root         INFO     Train Epoch: 9 [512/8000 (6%)]\tTotal Loss: 3.276295\n",
      "Reconstruction: 2.690705, Regularization: 0.531085, Discriminator: 0.017777; Generator: 0.036729,\n",
      "D(x): 0.842, D(G(z)): 0.309\n",
      "2019-04-10 01:18:46,676 root         INFO     Train Epoch: 9 [1024/8000 (13%)]\tTotal Loss: 1.320807\n",
      "Reconstruction: 0.928315, Regularization: 0.336918, Discriminator: 0.018605; Generator: 0.036969,\n",
      "D(x): 0.822, D(G(z)): 0.306\n",
      "2019-04-10 01:18:46,775 root         INFO     Train Epoch: 9 [1536/8000 (19%)]\tTotal Loss: 1.200333\n",
      "Reconstruction: 0.834544, Regularization: 0.311937, Discriminator: 0.016858; Generator: 0.036993,\n",
      "D(x): 0.863, D(G(z)): 0.306\n",
      "2019-04-10 01:18:46,874 root         INFO     Train Epoch: 9 [2048/8000 (26%)]\tTotal Loss: 1.688764\n",
      "Reconstruction: 1.152692, Regularization: 0.482752, Discriminator: 0.016150; Generator: 0.037170,\n",
      "D(x): 0.880, D(G(z)): 0.304\n",
      "2019-04-10 01:18:46,973 root         INFO     Train Epoch: 9 [2560/8000 (32%)]\tTotal Loss: 1.366793\n",
      "Reconstruction: 0.930127, Regularization: 0.382012, Discriminator: 0.017328; Generator: 0.037326,\n",
      "D(x): 0.846, D(G(z)): 0.303\n",
      "2019-04-10 01:18:47,073 root         INFO     Train Epoch: 9 [3072/8000 (38%)]\tTotal Loss: 1.009181\n",
      "Reconstruction: 0.673140, Regularization: 0.279270, Discriminator: 0.019294; Generator: 0.037477,\n",
      "D(x): 0.799, D(G(z)): 0.301\n",
      "2019-04-10 01:18:47,171 root         INFO     Train Epoch: 9 [3584/8000 (45%)]\tTotal Loss: 1.878406\n",
      "Reconstruction: 1.343347, Regularization: 0.480508, Discriminator: 0.016915; Generator: 0.037636,\n",
      "D(x): 0.853, D(G(z)): 0.300\n",
      "2019-04-10 01:18:47,269 root         INFO     Train Epoch: 9 [4096/8000 (51%)]\tTotal Loss: 1.400323\n",
      "Reconstruction: 0.985167, Regularization: 0.359344, Discriminator: 0.017690; Generator: 0.038122,\n",
      "D(x): 0.840, D(G(z)): 0.295\n",
      "2019-04-10 01:18:47,367 root         INFO     Train Epoch: 9 [4608/8000 (58%)]\tTotal Loss: 1.226590\n",
      "Reconstruction: 0.845240, Regularization: 0.326612, Discriminator: 0.016788; Generator: 0.037950,\n",
      "D(x): 0.872, D(G(z)): 0.297\n",
      "2019-04-10 01:18:47,465 root         INFO     Train Epoch: 9 [5120/8000 (64%)]\tTotal Loss: 0.721421\n",
      "Reconstruction: 0.501180, Regularization: 0.162455, Discriminator: 0.019647; Generator: 0.038139,\n",
      "D(x): 0.797, D(G(z)): 0.295\n",
      "2019-04-10 01:18:47,564 root         INFO     Train Epoch: 9 [5632/8000 (70%)]\tTotal Loss: 1.373217\n",
      "Reconstruction: 0.920668, Regularization: 0.396427, Discriminator: 0.017722; Generator: 0.038401,\n",
      "D(x): 0.845, D(G(z)): 0.293\n",
      "2019-04-10 01:18:47,662 root         INFO     Train Epoch: 9 [6144/8000 (77%)]\tTotal Loss: 0.834539\n",
      "Reconstruction: 0.565697, Regularization: 0.213032, Discriminator: 0.017367; Generator: 0.038444,\n",
      "D(x): 0.827, D(G(z)): 0.292\n",
      "2019-04-10 01:18:47,760 root         INFO     Train Epoch: 9 [6656/8000 (83%)]\tTotal Loss: 1.267825\n",
      "Reconstruction: 0.865743, Regularization: 0.347685, Discriminator: 0.015767; Generator: 0.038629,\n",
      "D(x): 0.873, D(G(z)): 0.291\n",
      "2019-04-10 01:18:47,858 root         INFO     Train Epoch: 9 [7168/8000 (90%)]\tTotal Loss: 0.998230\n",
      "Reconstruction: 0.674918, Regularization: 0.268215, Discriminator: 0.016313; Generator: 0.038783,\n",
      "D(x): 0.864, D(G(z)): 0.289\n",
      "2019-04-10 01:18:47,956 root         INFO     Train Epoch: 9 [7680/8000 (96%)]\tTotal Loss: 1.431638\n",
      "Reconstruction: 0.983309, Regularization: 0.394628, Discriminator: 0.014748; Generator: 0.038954,\n",
      "D(x): 0.890, D(G(z)): 0.288\n",
      "2019-04-10 01:18:48,030 root         INFO     ====> Epoch: 9 Average loss: 1.2378\n",
      "2019-04-10 01:18:48,056 root         INFO     Train Epoch: 10 [0/8000 (0%)]\tTotal Loss: 1.062376\n",
      "Reconstruction: 0.730807, Regularization: 0.275052, Discriminator: 0.017454; Generator: 0.039062,\n",
      "D(x): 0.837, D(G(z)): 0.287\n",
      "2019-04-10 01:18:48,156 root         INFO     Train Epoch: 10 [512/8000 (6%)]\tTotal Loss: 0.994958\n",
      "Reconstruction: 0.675825, Regularization: 0.265438, Discriminator: 0.014471; Generator: 0.039224,\n",
      "D(x): 0.891, D(G(z)): 0.285\n",
      "2019-04-10 01:18:48,254 root         INFO     Train Epoch: 10 [1024/8000 (13%)]\tTotal Loss: 1.467660\n",
      "Reconstruction: 1.010222, Regularization: 0.402260, Discriminator: 0.015780; Generator: 0.039398,\n",
      "D(x): 0.869, D(G(z)): 0.283\n",
      "2019-04-10 01:18:48,353 root         INFO     Train Epoch: 10 [1536/8000 (19%)]\tTotal Loss: 0.943374\n",
      "Reconstruction: 0.643032, Regularization: 0.244055, Discriminator: 0.016728; Generator: 0.039559,\n",
      "D(x): 0.850, D(G(z)): 0.282\n",
      "2019-04-10 01:18:48,451 root         INFO     Train Epoch: 10 [2048/8000 (26%)]\tTotal Loss: 1.828224\n",
      "Reconstruction: 1.244695, Regularization: 0.526459, Discriminator: 0.017087; Generator: 0.039983,\n",
      "D(x): 0.838, D(G(z)): 0.278\n",
      "2019-04-10 01:18:48,548 root         INFO     Train Epoch: 10 [2560/8000 (32%)]\tTotal Loss: 0.991794\n",
      "Reconstruction: 0.682468, Regularization: 0.254941, Discriminator: 0.014505; Generator: 0.039879,\n",
      "D(x): 0.881, D(G(z)): 0.279\n",
      "2019-04-10 01:18:48,646 root         INFO     Train Epoch: 10 [3072/8000 (38%)]\tTotal Loss: 1.191775\n",
      "Reconstruction: 0.826486, Regularization: 0.307336, Discriminator: 0.017903; Generator: 0.040050,\n",
      "D(x): 0.831, D(G(z)): 0.278\n",
      "2019-04-10 01:18:48,742 root         INFO     Train Epoch: 10 [3584/8000 (45%)]\tTotal Loss: 1.698130\n",
      "Reconstruction: 1.328080, Regularization: 0.311548, Discriminator: 0.018277; Generator: 0.040225,\n",
      "D(x): 0.806, D(G(z)): 0.276\n",
      "2019-04-10 01:18:48,839 root         INFO     Train Epoch: 10 [4096/8000 (51%)]\tTotal Loss: 1.398257\n",
      "Reconstruction: 1.034288, Regularization: 0.307548, Discriminator: 0.016035; Generator: 0.040385,\n",
      "D(x): 0.848, D(G(z)): 0.275\n",
      "2019-04-10 01:18:48,936 root         INFO     Train Epoch: 10 [4608/8000 (58%)]\tTotal Loss: 1.036230\n",
      "Reconstruction: 0.686240, Regularization: 0.292134, Discriminator: 0.017299; Generator: 0.040557,\n",
      "D(x): 0.827, D(G(z)): 0.273\n",
      "2019-04-10 01:18:49,034 root         INFO     Train Epoch: 10 [5120/8000 (64%)]\tTotal Loss: 0.879009\n",
      "Reconstruction: 0.601500, Regularization: 0.223106, Discriminator: 0.013691; Generator: 0.040712,\n",
      "D(x): 0.898, D(G(z)): 0.272\n",
      "2019-04-10 01:18:49,133 root         INFO     Train Epoch: 10 [5632/8000 (70%)]\tTotal Loss: 0.979648\n",
      "Reconstruction: 0.653868, Regularization: 0.267161, Discriminator: 0.017738; Generator: 0.040881,\n",
      "D(x): 0.822, D(G(z)): 0.270\n",
      "2019-04-10 01:18:49,231 root         INFO     Train Epoch: 10 [6144/8000 (77%)]\tTotal Loss: 1.314709\n",
      "Reconstruction: 0.890351, Regularization: 0.368616, Discriminator: 0.014717; Generator: 0.041026,\n",
      "D(x): 0.873, D(G(z)): 0.269\n",
      "2019-04-10 01:18:49,330 root         INFO     Train Epoch: 10 [6656/8000 (83%)]\tTotal Loss: 1.107538\n",
      "Reconstruction: 0.763278, Regularization: 0.285298, Discriminator: 0.017634; Generator: 0.041328,\n",
      "D(x): 0.836, D(G(z)): 0.267\n",
      "2019-04-10 01:18:49,429 root         INFO     Train Epoch: 10 [7168/8000 (90%)]\tTotal Loss: 0.812346\n",
      "Reconstruction: 0.546511, Regularization: 0.207743, Discriminator: 0.016741; Generator: 0.041350,\n",
      "D(x): 0.831, D(G(z)): 0.266\n",
      "2019-04-10 01:18:49,527 root         INFO     Train Epoch: 10 [7680/8000 (96%)]\tTotal Loss: 1.133591\n",
      "Reconstruction: 0.769327, Regularization: 0.307641, Discriminator: 0.015105; Generator: 0.041517,\n",
      "D(x): 0.861, D(G(z)): 0.265\n",
      "2019-04-10 01:18:49,601 root         INFO     ====> Epoch: 10 Average loss: 1.2005\n",
      "2019-04-10 01:18:49,628 root         INFO     Train Epoch: 11 [0/8000 (0%)]\tTotal Loss: 1.676009\n",
      "Reconstruction: 1.228891, Regularization: 0.392266, Discriminator: 0.013135; Generator: 0.041717,\n",
      "D(x): 0.907, D(G(z)): 0.263\n",
      "2019-04-10 01:18:49,729 root         INFO     Train Epoch: 11 [512/8000 (6%)]\tTotal Loss: 0.702478\n",
      "Reconstruction: 0.476407, Regularization: 0.169113, Discriminator: 0.015060; Generator: 0.041897,\n",
      "D(x): 0.858, D(G(z)): 0.262\n",
      "2019-04-10 01:18:49,829 root         INFO     Train Epoch: 11 [1024/8000 (13%)]\tTotal Loss: 0.979892\n",
      "Reconstruction: 0.664241, Regularization: 0.256545, Discriminator: 0.017166; Generator: 0.041941,\n",
      "D(x): 0.828, D(G(z)): 0.261\n",
      "2019-04-10 01:18:49,929 root         INFO     Train Epoch: 11 [1536/8000 (19%)]\tTotal Loss: 0.852346\n",
      "Reconstruction: 0.580534, Regularization: 0.213369, Discriminator: 0.016325; Generator: 0.042119,\n",
      "D(x): 0.836, D(G(z)): 0.260\n",
      "2019-04-10 01:18:50,027 root         INFO     Train Epoch: 11 [2048/8000 (26%)]\tTotal Loss: 1.004718\n",
      "Reconstruction: 0.676432, Regularization: 0.269877, Discriminator: 0.016135; Generator: 0.042274,\n",
      "D(x): 0.831, D(G(z)): 0.259\n",
      "2019-04-10 01:18:50,124 root         INFO     Train Epoch: 11 [2560/8000 (32%)]\tTotal Loss: 0.803919\n",
      "Reconstruction: 0.547261, Regularization: 0.199187, Discriminator: 0.015033; Generator: 0.042438,\n",
      "D(x): 0.859, D(G(z)): 0.257\n",
      "2019-04-10 01:18:50,221 root         INFO     Train Epoch: 11 [3072/8000 (38%)]\tTotal Loss: 1.032100\n",
      "Reconstruction: 0.696932, Regularization: 0.274571, Discriminator: 0.017782; Generator: 0.042814,\n",
      "D(x): 0.812, D(G(z)): 0.254\n",
      "2019-04-10 01:18:50,318 root         INFO     Train Epoch: 11 [3584/8000 (45%)]\tTotal Loss: 0.900719\n",
      "Reconstruction: 0.601526, Regularization: 0.239273, Discriminator: 0.017175; Generator: 0.042745,\n",
      "D(x): 0.837, D(G(z)): 0.255\n",
      "2019-04-10 01:18:50,414 root         INFO     Train Epoch: 11 [4096/8000 (51%)]\tTotal Loss: 0.918103\n",
      "Reconstruction: 0.618161, Regularization: 0.241031, Discriminator: 0.015990; Generator: 0.042921,\n",
      "D(x): 0.849, D(G(z)): 0.253\n",
      "2019-04-10 01:18:50,511 root         INFO     Train Epoch: 11 [4608/8000 (58%)]\tTotal Loss: 1.154511\n",
      "Reconstruction: 0.779007, Regularization: 0.316135, Discriminator: 0.016288; Generator: 0.043081,\n",
      "D(x): 0.841, D(G(z)): 0.252\n",
      "2019-04-10 01:18:50,608 root         INFO     Train Epoch: 11 [5120/8000 (64%)]\tTotal Loss: 1.036146\n",
      "Reconstruction: 0.702852, Regularization: 0.274960, Discriminator: 0.015105; Generator: 0.043228,\n",
      "D(x): 0.852, D(G(z)): 0.251\n",
      "2019-04-10 01:18:50,704 root         INFO     Train Epoch: 11 [5632/8000 (70%)]\tTotal Loss: 1.411248\n",
      "Reconstruction: 1.038814, Regularization: 0.316331, Discriminator: 0.012570; Generator: 0.043533,\n",
      "D(x): 0.903, D(G(z)): 0.248\n",
      "2019-04-10 01:18:50,804 root         INFO     Train Epoch: 11 [6144/8000 (77%)]\tTotal Loss: 0.744124\n",
      "Reconstruction: 0.494407, Regularization: 0.187510, Discriminator: 0.018640; Generator: 0.043567,\n",
      "D(x): 0.787, D(G(z)): 0.248\n",
      "2019-04-10 01:18:50,904 root         INFO     Train Epoch: 11 [6656/8000 (83%)]\tTotal Loss: 0.920977\n",
      "Reconstruction: 0.616840, Regularization: 0.246122, Discriminator: 0.014276; Generator: 0.043739,\n",
      "D(x): 0.875, D(G(z)): 0.247\n",
      "2019-04-10 01:18:51,004 root         INFO     Train Epoch: 11 [7168/8000 (90%)]\tTotal Loss: 1.109731\n",
      "Reconstruction: 0.753538, Regularization: 0.301002, Discriminator: 0.011265; Generator: 0.043925,\n",
      "D(x): 0.931, D(G(z)): 0.245\n",
      "2019-04-10 01:18:51,105 root         INFO     Train Epoch: 11 [7680/8000 (96%)]\tTotal Loss: 1.368596\n",
      "Reconstruction: 0.930565, Regularization: 0.380727, Discriminator: 0.013223; Generator: 0.044080,\n",
      "D(x): 0.896, D(G(z)): 0.244\n",
      "2019-04-10 01:18:51,180 root         INFO     ====> Epoch: 11 Average loss: 1.2380\n",
      "2019-04-10 01:18:51,207 root         INFO     Train Epoch: 12 [0/8000 (0%)]\tTotal Loss: 1.245130\n",
      "Reconstruction: 0.853037, Regularization: 0.335555, Discriminator: 0.012311; Generator: 0.044228,\n",
      "D(x): 0.908, D(G(z)): 0.243\n",
      "2019-04-10 01:18:51,308 root         INFO     Train Epoch: 12 [512/8000 (6%)]\tTotal Loss: 1.403999\n",
      "Reconstruction: 0.937880, Regularization: 0.407897, Discriminator: 0.013852; Generator: 0.044371,\n",
      "D(x): 0.880, D(G(z)): 0.242\n",
      "2019-04-10 01:18:51,408 root         INFO     Train Epoch: 12 [1024/8000 (13%)]\tTotal Loss: 1.716976\n",
      "Reconstruction: 1.150645, Regularization: 0.504401, Discriminator: 0.017390; Generator: 0.044541,\n",
      "D(x): 0.866, D(G(z)): 0.240\n",
      "2019-04-10 01:18:51,508 root         INFO     Train Epoch: 12 [1536/8000 (19%)]\tTotal Loss: 0.965352\n",
      "Reconstruction: 0.632135, Regularization: 0.272012, Discriminator: 0.016491; Generator: 0.044713,\n",
      "D(x): 0.831, D(G(z)): 0.239\n",
      "2019-04-10 01:18:51,609 root         INFO     Train Epoch: 12 [2048/8000 (26%)]\tTotal Loss: 1.542310\n",
      "Reconstruction: 1.052159, Regularization: 0.434901, Discriminator: 0.010364; Generator: 0.044886,\n",
      "D(x): 0.952, D(G(z)): 0.238\n",
      "2019-04-10 01:18:51,709 root         INFO     Train Epoch: 12 [2560/8000 (32%)]\tTotal Loss: 1.015949\n",
      "Reconstruction: 0.676967, Regularization: 0.280391, Discriminator: 0.013533; Generator: 0.045058,\n",
      "D(x): 0.878, D(G(z)): 0.236\n",
      "2019-04-10 01:18:51,809 root         INFO     Train Epoch: 12 [3072/8000 (38%)]\tTotal Loss: 1.475358\n",
      "Reconstruction: 0.990042, Regularization: 0.427174, Discriminator: 0.012913; Generator: 0.045229,\n",
      "D(x): 0.884, D(G(z)): 0.235\n",
      "2019-04-10 01:18:51,910 root         INFO     Train Epoch: 12 [3584/8000 (45%)]\tTotal Loss: 0.891005\n",
      "Reconstruction: 0.593762, Regularization: 0.234495, Discriminator: 0.017369; Generator: 0.045379,\n",
      "D(x): 0.819, D(G(z)): 0.234\n",
      "2019-04-10 01:18:52,010 root         INFO     Train Epoch: 12 [4096/8000 (51%)]\tTotal Loss: 1.293556\n",
      "Reconstruction: 0.876039, Regularization: 0.359580, Discriminator: 0.012395; Generator: 0.045542,\n",
      "D(x): 0.897, D(G(z)): 0.233\n",
      "2019-04-10 01:18:52,110 root         INFO     Train Epoch: 12 [4608/8000 (58%)]\tTotal Loss: 0.577276\n",
      "Reconstruction: 0.376134, Regularization: 0.140546, Discriminator: 0.014883; Generator: 0.045714,\n",
      "D(x): 0.827, D(G(z)): 0.232\n",
      "2019-04-10 01:18:52,210 root         INFO     Train Epoch: 12 [5120/8000 (64%)]\tTotal Loss: 1.438487\n",
      "Reconstruction: 0.968924, Regularization: 0.407473, Discriminator: 0.016202; Generator: 0.045887,\n",
      "D(x): 0.839, D(G(z)): 0.230\n",
      "2019-04-10 01:18:52,310 root         INFO     Train Epoch: 12 [5632/8000 (70%)]\tTotal Loss: 0.875446\n",
      "Reconstruction: 0.584383, Regularization: 0.232734, Discriminator: 0.012288; Generator: 0.046042,\n",
      "D(x): 0.890, D(G(z)): 0.229\n",
      "2019-04-10 01:18:52,410 root         INFO     Train Epoch: 12 [6144/8000 (77%)]\tTotal Loss: 0.910612\n",
      "Reconstruction: 0.611032, Regularization: 0.241321, Discriminator: 0.012055; Generator: 0.046204,\n",
      "D(x): 0.894, D(G(z)): 0.228\n",
      "2019-04-10 01:18:52,510 root         INFO     Train Epoch: 12 [6656/8000 (83%)]\tTotal Loss: 0.573621\n",
      "Reconstruction: 0.366808, Regularization: 0.146238, Discriminator: 0.014224; Generator: 0.046351,\n",
      "D(x): 0.856, D(G(z)): 0.227\n",
      "2019-04-10 01:18:52,610 root         INFO     Train Epoch: 12 [7168/8000 (90%)]\tTotal Loss: 1.176108\n",
      "Reconstruction: 0.785750, Regularization: 0.329312, Discriminator: 0.014525; Generator: 0.046521,\n",
      "D(x): 0.864, D(G(z)): 0.226\n",
      "2019-04-10 01:18:52,710 root         INFO     Train Epoch: 12 [7680/8000 (96%)]\tTotal Loss: 1.050892\n",
      "Reconstruction: 0.710718, Regularization: 0.280389, Discriminator: 0.013118; Generator: 0.046666,\n",
      "D(x): 0.893, D(G(z)): 0.225\n",
      "2019-04-10 01:18:52,785 root         INFO     ====> Epoch: 12 Average loss: 1.4128\n",
      "2019-04-10 01:18:52,812 root         INFO     Train Epoch: 13 [0/8000 (0%)]\tTotal Loss: 1.107308\n",
      "Reconstruction: 0.735206, Regularization: 0.311117, Discriminator: 0.014236; Generator: 0.046749,\n",
      "D(x): 0.850, D(G(z)): 0.224\n",
      "2019-04-10 01:18:52,915 root         INFO     Train Epoch: 13 [512/8000 (6%)]\tTotal Loss: 1.157361\n",
      "Reconstruction: 0.779997, Regularization: 0.317902, Discriminator: 0.012576; Generator: 0.046886,\n",
      "D(x): 0.883, D(G(z)): 0.223\n",
      "2019-04-10 01:18:53,016 root         INFO     Train Epoch: 13 [1024/8000 (13%)]\tTotal Loss: 1.302923\n",
      "Reconstruction: 0.874185, Regularization: 0.365991, Discriminator: 0.015678; Generator: 0.047068,\n",
      "D(x): 0.864, D(G(z)): 0.222\n",
      "2019-04-10 01:18:53,117 root         INFO     Train Epoch: 13 [1536/8000 (19%)]\tTotal Loss: 1.018665\n",
      "Reconstruction: 0.674040, Regularization: 0.281217, Discriminator: 0.016211; Generator: 0.047198,\n",
      "D(x): 0.835, D(G(z)): 0.221\n",
      "2019-04-10 01:18:53,219 root         INFO     Train Epoch: 13 [2048/8000 (26%)]\tTotal Loss: 0.934298\n",
      "Reconstruction: 0.615829, Regularization: 0.257021, Discriminator: 0.014092; Generator: 0.047356,\n",
      "D(x): 0.872, D(G(z)): 0.220\n",
      "2019-04-10 01:18:53,320 root         INFO     Train Epoch: 13 [2560/8000 (32%)]\tTotal Loss: 1.355608\n",
      "Reconstruction: 0.939612, Regularization: 0.354272, Discriminator: 0.014202; Generator: 0.047521,\n",
      "D(x): 0.855, D(G(z)): 0.219\n",
      "2019-04-10 01:18:53,422 root         INFO     Train Epoch: 13 [3072/8000 (38%)]\tTotal Loss: 1.197647\n",
      "Reconstruction: 0.826019, Regularization: 0.310387, Discriminator: 0.013549; Generator: 0.047692,\n",
      "D(x): 0.865, D(G(z)): 0.217\n",
      "2019-04-10 01:18:53,523 root         INFO     Train Epoch: 13 [3584/8000 (45%)]\tTotal Loss: 1.238415\n",
      "Reconstruction: 0.833196, Regularization: 0.344898, Discriminator: 0.012436; Generator: 0.047885,\n",
      "D(x): 0.897, D(G(z)): 0.216\n",
      "2019-04-10 01:18:53,622 root         INFO     Train Epoch: 13 [4096/8000 (51%)]\tTotal Loss: 1.361671\n",
      "Reconstruction: 0.912421, Regularization: 0.389480, Discriminator: 0.011701; Generator: 0.048069,\n",
      "D(x): 0.918, D(G(z)): 0.215\n",
      "2019-04-10 01:18:53,722 root         INFO     Train Epoch: 13 [4608/8000 (58%)]\tTotal Loss: 0.958328\n",
      "Reconstruction: 0.629190, Regularization: 0.263728, Discriminator: 0.017184; Generator: 0.048226,\n",
      "D(x): 0.809, D(G(z)): 0.214\n",
      "2019-04-10 01:18:53,821 root         INFO     Train Epoch: 13 [5120/8000 (64%)]\tTotal Loss: 1.337976\n",
      "Reconstruction: 0.899441, Regularization: 0.379774, Discriminator: 0.010313; Generator: 0.048448,\n",
      "D(x): 0.924, D(G(z)): 0.212\n",
      "2019-04-10 01:18:53,921 root         INFO     Train Epoch: 13 [5632/8000 (70%)]\tTotal Loss: 1.002674\n",
      "Reconstruction: 0.710996, Regularization: 0.231931, Discriminator: 0.011166; Generator: 0.048580,\n",
      "D(x): 0.901, D(G(z)): 0.211\n",
      "2019-04-10 01:18:54,024 root         INFO     Train Epoch: 13 [6144/8000 (77%)]\tTotal Loss: 0.882490\n",
      "Reconstruction: 0.584155, Regularization: 0.233317, Discriminator: 0.016279; Generator: 0.048738,\n",
      "D(x): 0.824, D(G(z)): 0.210\n",
      "2019-04-10 01:18:54,124 root         INFO     Train Epoch: 13 [6656/8000 (83%)]\tTotal Loss: 1.242705\n",
      "Reconstruction: 0.819227, Regularization: 0.359832, Discriminator: 0.014746; Generator: 0.048899,\n",
      "D(x): 0.872, D(G(z)): 0.209\n",
      "2019-04-10 01:18:54,226 root         INFO     Train Epoch: 13 [7168/8000 (90%)]\tTotal Loss: 0.723320\n",
      "Reconstruction: 0.490440, Regularization: 0.170417, Discriminator: 0.013415; Generator: 0.049048,\n",
      "D(x): 0.877, D(G(z)): 0.208\n",
      "2019-04-10 01:18:54,326 root         INFO     Train Epoch: 13 [7680/8000 (96%)]\tTotal Loss: 0.754620\n",
      "Reconstruction: 0.495439, Regularization: 0.195915, Discriminator: 0.014057; Generator: 0.049210,\n",
      "D(x): 0.852, D(G(z)): 0.207\n",
      "2019-04-10 01:18:54,402 root         INFO     ====> Epoch: 13 Average loss: 1.1282\n",
      "2019-04-10 01:18:54,428 root         INFO     Train Epoch: 14 [0/8000 (0%)]\tTotal Loss: 1.594430\n",
      "Reconstruction: 1.103423, Regularization: 0.431444, Discriminator: 0.010246; Generator: 0.049317,\n",
      "D(x): 0.934, D(G(z)): 0.206\n",
      "2019-04-10 01:18:54,531 root         INFO     Train Epoch: 14 [512/8000 (6%)]\tTotal Loss: 0.993308\n",
      "Reconstruction: 0.667425, Regularization: 0.263562, Discriminator: 0.012813; Generator: 0.049508,\n",
      "D(x): 0.866, D(G(z)): 0.205\n",
      "2019-04-10 01:18:54,633 root         INFO     Train Epoch: 14 [1024/8000 (13%)]\tTotal Loss: 1.139711\n",
      "Reconstruction: 0.769857, Regularization: 0.308012, Discriminator: 0.012168; Generator: 0.049673,\n",
      "D(x): 0.875, D(G(z)): 0.204\n",
      "2019-04-10 01:18:54,736 root         INFO     Train Epoch: 14 [1536/8000 (19%)]\tTotal Loss: 1.409664\n",
      "Reconstruction: 0.946104, Regularization: 0.402571, Discriminator: 0.011144; Generator: 0.049845,\n",
      "D(x): 0.918, D(G(z)): 0.203\n",
      "2019-04-10 01:18:54,838 root         INFO     Train Epoch: 14 [2048/8000 (26%)]\tTotal Loss: 1.974285\n",
      "Reconstruction: 1.576019, Regularization: 0.335073, Discriminator: 0.013206; Generator: 0.049987,\n",
      "D(x): 0.871, D(G(z)): 0.202\n",
      "2019-04-10 01:18:54,941 root         INFO     Train Epoch: 14 [2560/8000 (32%)]\tTotal Loss: 0.749105\n",
      "Reconstruction: 0.491592, Regularization: 0.194534, Discriminator: 0.012815; Generator: 0.050165,\n",
      "D(x): 0.861, D(G(z)): 0.201\n",
      "2019-04-10 01:18:55,043 root         INFO     Train Epoch: 14 [3072/8000 (38%)]\tTotal Loss: 0.823376\n",
      "Reconstruction: 0.539932, Regularization: 0.221015, Discriminator: 0.011737; Generator: 0.050692,\n",
      "D(x): 0.886, D(G(z)): 0.198\n",
      "2019-04-10 01:18:55,145 root         INFO     Train Epoch: 14 [3584/8000 (45%)]\tTotal Loss: 1.090001\n",
      "Reconstruction: 0.718284, Regularization: 0.306900, Discriminator: 0.014164; Generator: 0.050654,\n",
      "D(x): 0.851, D(G(z)): 0.198\n",
      "2019-04-10 01:18:55,248 root         INFO     Train Epoch: 14 [4096/8000 (51%)]\tTotal Loss: 1.156830\n",
      "Reconstruction: 0.765326, Regularization: 0.327420, Discriminator: 0.013475; Generator: 0.050609,\n",
      "D(x): 0.855, D(G(z)): 0.198\n",
      "2019-04-10 01:18:55,350 root         INFO     Train Epoch: 14 [4608/8000 (58%)]\tTotal Loss: 0.961565\n",
      "Reconstruction: 0.640750, Regularization: 0.258304, Discriminator: 0.011729; Generator: 0.050781,\n",
      "D(x): 0.897, D(G(z)): 0.197\n",
      "2019-04-10 01:18:55,453 root         INFO     Train Epoch: 14 [5120/8000 (64%)]\tTotal Loss: 1.415719\n",
      "Reconstruction: 0.934881, Regularization: 0.412312, Discriminator: 0.017425; Generator: 0.051101,\n",
      "D(x): 0.849, D(G(z)): 0.195\n",
      "2019-04-10 01:18:55,555 root         INFO     Train Epoch: 14 [5632/8000 (70%)]\tTotal Loss: 1.216512\n",
      "Reconstruction: 0.810617, Regularization: 0.343610, Discriminator: 0.011189; Generator: 0.051095,\n",
      "D(x): 0.893, D(G(z)): 0.195\n",
      "2019-04-10 01:18:55,658 root         INFO     Train Epoch: 14 [6144/8000 (77%)]\tTotal Loss: 1.189496\n",
      "Reconstruction: 0.789133, Regularization: 0.337663, Discriminator: 0.011459; Generator: 0.051240,\n",
      "D(x): 0.891, D(G(z)): 0.194\n",
      "2019-04-10 01:18:55,760 root         INFO     Train Epoch: 14 [6656/8000 (83%)]\tTotal Loss: 1.562811\n",
      "Reconstruction: 1.017090, Regularization: 0.479483, Discriminator: 0.014838; Generator: 0.051399,\n",
      "D(x): 0.861, D(G(z)): 0.193\n",
      "2019-04-10 01:18:55,859 root         INFO     Train Epoch: 14 [7168/8000 (90%)]\tTotal Loss: 1.118834\n",
      "Reconstruction: 0.740279, Regularization: 0.315206, Discriminator: 0.011825; Generator: 0.051524,\n",
      "D(x): 0.911, D(G(z)): 0.192\n",
      "2019-04-10 01:18:55,959 root         INFO     Train Epoch: 14 [7680/8000 (96%)]\tTotal Loss: 0.920881\n",
      "Reconstruction: 0.609913, Regularization: 0.249928, Discriminator: 0.009330; Generator: 0.051710,\n",
      "D(x): 0.930, D(G(z)): 0.191\n",
      "2019-04-10 01:18:56,033 root         INFO     ====> Epoch: 14 Average loss: 1.2041\n",
      "2019-04-10 01:18:56,060 root         INFO     Train Epoch: 15 [0/8000 (0%)]\tTotal Loss: 0.975129\n",
      "Reconstruction: 0.636544, Regularization: 0.272548, Discriminator: 0.014224; Generator: 0.051813,\n",
      "D(x): 0.856, D(G(z)): 0.191\n",
      "2019-04-10 01:18:56,159 root         INFO     Train Epoch: 15 [512/8000 (6%)]\tTotal Loss: 1.349913\n",
      "Reconstruction: 0.885151, Regularization: 0.400905, Discriminator: 0.011884; Generator: 0.051973,\n",
      "D(x): 0.886, D(G(z)): 0.190\n",
      "2019-04-10 01:18:56,257 root         INFO     Train Epoch: 15 [1024/8000 (13%)]\tTotal Loss: 0.968710\n",
      "Reconstruction: 0.640812, Regularization: 0.261077, Discriminator: 0.014708; Generator: 0.052113,\n",
      "D(x): 0.872, D(G(z)): 0.189\n",
      "2019-04-10 01:18:56,357 root         INFO     Train Epoch: 15 [1536/8000 (19%)]\tTotal Loss: 0.893479\n",
      "Reconstruction: 0.584983, Regularization: 0.241778, Discriminator: 0.014251; Generator: 0.052467,\n",
      "D(x): 0.829, D(G(z)): 0.187\n",
      "2019-04-10 01:18:56,455 root         INFO     Train Epoch: 15 [2048/8000 (26%)]\tTotal Loss: 1.159306\n",
      "Reconstruction: 0.782683, Regularization: 0.306659, Discriminator: 0.017523; Generator: 0.052441,\n",
      "D(x): 0.818, D(G(z)): 0.187\n",
      "2019-04-10 01:18:56,554 root         INFO     Train Epoch: 15 [2560/8000 (32%)]\tTotal Loss: 1.992815\n",
      "Reconstruction: 1.323601, Regularization: 0.597870, Discriminator: 0.018754; Generator: 0.052591,\n",
      "D(x): 0.837, D(G(z)): 0.186\n",
      "2019-04-10 01:18:56,654 root         INFO     Train Epoch: 15 [3072/8000 (38%)]\tTotal Loss: 0.892832\n",
      "Reconstruction: 0.564944, Regularization: 0.263173, Discriminator: 0.012002; Generator: 0.052713,\n",
      "D(x): 0.883, D(G(z)): 0.185\n",
      "2019-04-10 01:18:56,752 root         INFO     Train Epoch: 15 [3584/8000 (45%)]\tTotal Loss: 1.054349\n",
      "Reconstruction: 0.695973, Regularization: 0.291709, Discriminator: 0.013792; Generator: 0.052875,\n",
      "D(x): 0.878, D(G(z)): 0.184\n",
      "2019-04-10 01:18:56,851 root         INFO     Train Epoch: 15 [4096/8000 (51%)]\tTotal Loss: 0.968390\n",
      "Reconstruction: 0.641179, Regularization: 0.265027, Discriminator: 0.009138; Generator: 0.053047,\n",
      "D(x): 0.931, D(G(z)): 0.183\n",
      "2019-04-10 01:18:56,950 root         INFO     Train Epoch: 15 [4608/8000 (58%)]\tTotal Loss: 1.239004\n",
      "Reconstruction: 0.814684, Regularization: 0.353651, Discriminator: 0.017492; Generator: 0.053177,\n",
      "D(x): 0.843, D(G(z)): 0.182\n",
      "2019-04-10 01:18:57,052 root         INFO     Train Epoch: 15 [5120/8000 (64%)]\tTotal Loss: 1.131229\n",
      "Reconstruction: 0.737023, Regularization: 0.331443, Discriminator: 0.009442; Generator: 0.053320,\n",
      "D(x): 0.918, D(G(z)): 0.182\n",
      "2019-04-10 01:18:57,153 root         INFO     Train Epoch: 15 [5632/8000 (70%)]\tTotal Loss: 1.167974\n",
      "Reconstruction: 0.794008, Regularization: 0.309113, Discriminator: 0.011367; Generator: 0.053485,\n",
      "D(x): 0.917, D(G(z)): 0.181\n",
      "2019-04-10 01:18:57,255 root         INFO     Train Epoch: 15 [6144/8000 (77%)]\tTotal Loss: 0.999711\n",
      "Reconstruction: 0.650647, Regularization: 0.284489, Discriminator: 0.010967; Generator: 0.053608,\n",
      "D(x): 0.912, D(G(z)): 0.180\n",
      "2019-04-10 01:18:57,356 root         INFO     Train Epoch: 15 [6656/8000 (83%)]\tTotal Loss: 0.963115\n",
      "Reconstruction: 0.642352, Regularization: 0.259848, Discriminator: 0.007133; Generator: 0.053782,\n",
      "D(x): 0.971, D(G(z)): 0.179\n",
      "2019-04-10 01:18:57,457 root         INFO     Train Epoch: 15 [7168/8000 (90%)]\tTotal Loss: 1.323630\n",
      "Reconstruction: 0.885384, Regularization: 0.374677, Discriminator: 0.009610; Generator: 0.053958,\n",
      "D(x): 0.923, D(G(z)): 0.178\n",
      "2019-04-10 01:18:57,558 root         INFO     Train Epoch: 15 [7680/8000 (96%)]\tTotal Loss: 36.014675\n",
      "Reconstruction: 35.596992, Regularization: 0.348519, Discriminator: 0.015044; Generator: 0.054121,\n",
      "D(x): 0.865, D(G(z)): 0.177\n",
      "2019-04-10 01:18:57,634 root         INFO     ====> Epoch: 15 Average loss: 1.3144\n",
      "2019-04-10 01:18:57,661 root         INFO     Train Epoch: 16 [0/8000 (0%)]\tTotal Loss: 0.988662\n",
      "Reconstruction: 0.642179, Regularization: 0.279358, Discriminator: 0.012889; Generator: 0.054236,\n",
      "D(x): 0.854, D(G(z)): 0.176\n",
      "2019-04-10 01:18:57,764 root         INFO     Train Epoch: 16 [512/8000 (6%)]\tTotal Loss: 1.101518\n",
      "Reconstruction: 0.728134, Regularization: 0.308089, Discriminator: 0.010884; Generator: 0.054411,\n",
      "D(x): 0.886, D(G(z)): 0.175\n",
      "2019-04-10 01:18:57,867 root         INFO     Train Epoch: 16 [1024/8000 (13%)]\tTotal Loss: 4.184468\n",
      "Reconstruction: 3.711768, Regularization: 0.408301, Discriminator: 0.009837; Generator: 0.054563,\n",
      "D(x): 0.906, D(G(z)): 0.174\n",
      "2019-04-10 01:18:57,970 root         INFO     Train Epoch: 16 [1536/8000 (19%)]\tTotal Loss: 1.110600\n",
      "Reconstruction: 0.733079, Regularization: 0.313633, Discriminator: 0.009211; Generator: 0.054678,\n",
      "D(x): 0.925, D(G(z)): 0.174\n",
      "2019-04-10 01:18:58,073 root         INFO     Train Epoch: 16 [2048/8000 (26%)]\tTotal Loss: 1.216230\n",
      "Reconstruction: 0.805235, Regularization: 0.346521, Discriminator: 0.009611; Generator: 0.054863,\n",
      "D(x): 0.926, D(G(z)): 0.173\n",
      "2019-04-10 01:18:58,176 root         INFO     Train Epoch: 16 [2560/8000 (32%)]\tTotal Loss: 0.812423\n",
      "Reconstruction: 0.528974, Regularization: 0.214808, Discriminator: 0.013591; Generator: 0.055051,\n",
      "D(x): 0.867, D(G(z)): 0.172\n",
      "2019-04-10 01:18:58,279 root         INFO     Train Epoch: 16 [3072/8000 (38%)]\tTotal Loss: 1.261292\n",
      "Reconstruction: 0.886213, Regularization: 0.310443, Discriminator: 0.009461; Generator: 0.055175,\n",
      "D(x): 0.917, D(G(z)): 0.171\n",
      "2019-04-10 01:18:58,380 root         INFO     Train Epoch: 16 [3584/8000 (45%)]\tTotal Loss: 1.019830\n",
      "Reconstruction: 0.674708, Regularization: 0.281290, Discriminator: 0.008425; Generator: 0.055406,\n",
      "D(x): 0.939, D(G(z)): 0.170\n",
      "2019-04-10 01:18:58,482 root         INFO     Train Epoch: 16 [4096/8000 (51%)]\tTotal Loss: 0.841126\n",
      "Reconstruction: 0.556114, Regularization: 0.219180, Discriminator: 0.010385; Generator: 0.055448,\n",
      "D(x): 0.925, D(G(z)): 0.170\n",
      "2019-04-10 01:18:58,584 root         INFO     Train Epoch: 16 [4608/8000 (58%)]\tTotal Loss: 1.065178\n",
      "Reconstruction: 0.709502, Regularization: 0.290352, Discriminator: 0.009765; Generator: 0.055560,\n",
      "D(x): 0.905, D(G(z)): 0.169\n",
      "2019-04-10 01:18:58,685 root         INFO     Train Epoch: 16 [5120/8000 (64%)]\tTotal Loss: 0.849501\n",
      "Reconstruction: 0.551142, Regularization: 0.228538, Discriminator: 0.014072; Generator: 0.055749,\n",
      "D(x): 0.853, D(G(z)): 0.168\n",
      "2019-04-10 01:18:58,785 root         INFO     Train Epoch: 16 [5632/8000 (70%)]\tTotal Loss: 1.381606\n",
      "Reconstruction: 0.922393, Regularization: 0.390999, Discriminator: 0.012367; Generator: 0.055846,\n",
      "D(x): 0.923, D(G(z)): 0.167\n",
      "2019-04-10 01:18:58,884 root         INFO     Train Epoch: 16 [6144/8000 (77%)]\tTotal Loss: 0.980989\n",
      "Reconstruction: 0.642919, Regularization: 0.271131, Discriminator: 0.010885; Generator: 0.056054,\n",
      "D(x): 0.920, D(G(z)): 0.166\n",
      "2019-04-10 01:18:58,984 root         INFO     Train Epoch: 16 [6656/8000 (83%)]\tTotal Loss: 0.968138\n",
      "Reconstruction: 0.633015, Regularization: 0.267620, Discriminator: 0.011326; Generator: 0.056177,\n",
      "D(x): 0.876, D(G(z)): 0.166\n",
      "2019-04-10 01:18:59,083 root         INFO     Train Epoch: 16 [7168/8000 (90%)]\tTotal Loss: 1.015630\n",
      "Reconstruction: 0.660490, Regularization: 0.289158, Discriminator: 0.009691; Generator: 0.056291,\n",
      "D(x): 0.904, D(G(z)): 0.165\n",
      "2019-04-10 01:18:59,183 root         INFO     Train Epoch: 16 [7680/8000 (96%)]\tTotal Loss: 1.499729\n",
      "Reconstruction: 1.021722, Regularization: 0.412883, Discriminator: 0.008693; Generator: 0.056430,\n",
      "D(x): 0.947, D(G(z)): 0.164\n",
      "2019-04-10 01:18:59,257 root         INFO     ====> Epoch: 16 Average loss: 1.1082\n",
      "2019-04-10 01:18:59,284 root         INFO     Train Epoch: 17 [0/8000 (0%)]\tTotal Loss: 0.950535\n",
      "Reconstruction: 0.611529, Regularization: 0.271487, Discriminator: 0.010991; Generator: 0.056528,\n",
      "D(x): 0.873, D(G(z)): 0.164\n",
      "2019-04-10 01:18:59,384 root         INFO     Train Epoch: 17 [512/8000 (6%)]\tTotal Loss: 0.868563\n",
      "Reconstruction: 0.565486, Regularization: 0.233033, Discriminator: 0.013347; Generator: 0.056698,\n",
      "D(x): 0.883, D(G(z)): 0.163\n",
      "2019-04-10 01:18:59,482 root         INFO     Train Epoch: 17 [1024/8000 (13%)]\tTotal Loss: 0.720297\n",
      "Reconstruction: 0.425708, Regularization: 0.223509, Discriminator: 0.014216; Generator: 0.056864,\n",
      "D(x): 0.828, D(G(z)): 0.162\n",
      "2019-04-10 01:18:59,582 root         INFO     Train Epoch: 17 [1536/8000 (19%)]\tTotal Loss: 1.066051\n",
      "Reconstruction: 0.696387, Regularization: 0.302553, Discriminator: 0.010031; Generator: 0.057079,\n",
      "D(x): 0.935, D(G(z)): 0.161\n",
      "2019-04-10 01:18:59,681 root         INFO     Train Epoch: 17 [2048/8000 (26%)]\tTotal Loss: 1.140370\n",
      "Reconstruction: 0.740843, Regularization: 0.333731, Discriminator: 0.008487; Generator: 0.057309,\n",
      "D(x): 0.927, D(G(z)): 0.160\n",
      "2019-04-10 01:18:59,780 root         INFO     Train Epoch: 17 [2560/8000 (32%)]\tTotal Loss: 0.907552\n",
      "Reconstruction: 0.581586, Regularization: 0.251538, Discriminator: 0.017035; Generator: 0.057394,\n",
      "D(x): 0.799, D(G(z)): 0.159\n",
      "2019-04-10 01:18:59,878 root         INFO     Train Epoch: 17 [3072/8000 (38%)]\tTotal Loss: 1.017145\n",
      "Reconstruction: 0.661627, Regularization: 0.287311, Discriminator: 0.010667; Generator: 0.057539,\n",
      "D(x): 0.903, D(G(z)): 0.159\n",
      "2019-04-10 01:18:59,977 root         INFO     Train Epoch: 17 [3584/8000 (45%)]\tTotal Loss: 1.073621\n",
      "Reconstruction: 0.710936, Regularization: 0.298027, Discriminator: 0.007012; Generator: 0.057646,\n",
      "D(x): 0.953, D(G(z)): 0.158\n",
      "2019-04-10 01:19:00,075 root         INFO     Train Epoch: 17 [4096/8000 (51%)]\tTotal Loss: 0.819176\n",
      "Reconstruction: 0.531823, Regularization: 0.216166, Discriminator: 0.013430; Generator: 0.057757,\n",
      "D(x): 0.869, D(G(z)): 0.158\n",
      "2019-04-10 01:19:00,174 root         INFO     Train Epoch: 17 [4608/8000 (58%)]\tTotal Loss: 1.957674\n",
      "Reconstruction: 1.546330, Regularization: 0.342077, Discriminator: 0.011396; Generator: 0.057871,\n",
      "D(x): 0.880, D(G(z)): 0.157\n",
      "2019-04-10 01:19:00,273 root         INFO     Train Epoch: 17 [5120/8000 (64%)]\tTotal Loss: 1.040291\n",
      "Reconstruction: 0.683482, Regularization: 0.286702, Discriminator: 0.012121; Generator: 0.057986,\n",
      "D(x): 0.901, D(G(z)): 0.156\n",
      "2019-04-10 01:19:00,371 root         INFO     Train Epoch: 17 [5632/8000 (70%)]\tTotal Loss: 0.729607\n",
      "Reconstruction: 0.467493, Regularization: 0.192647, Discriminator: 0.011374; Generator: 0.058093,\n",
      "D(x): 0.878, D(G(z)): 0.156\n",
      "2019-04-10 01:19:00,470 root         INFO     Train Epoch: 17 [6144/8000 (77%)]\tTotal Loss: 1.043481\n",
      "Reconstruction: 0.677951, Regularization: 0.293696, Discriminator: 0.013580; Generator: 0.058254,\n",
      "D(x): 0.858, D(G(z)): 0.155\n",
      "2019-04-10 01:19:00,569 root         INFO     Train Epoch: 17 [6656/8000 (83%)]\tTotal Loss: 1.233651\n",
      "Reconstruction: 0.804834, Regularization: 0.364568, Discriminator: 0.005945; Generator: 0.058303,\n",
      "D(x): 0.979, D(G(z)): 0.155\n",
      "2019-04-10 01:19:00,668 root         INFO     Train Epoch: 17 [7168/8000 (90%)]\tTotal Loss: 1.140005\n",
      "Reconstruction: 0.750451, Regularization: 0.322814, Discriminator: 0.008282; Generator: 0.058459,\n",
      "D(x): 0.926, D(G(z)): 0.154\n",
      "2019-04-10 01:19:00,766 root         INFO     Train Epoch: 17 [7680/8000 (96%)]\tTotal Loss: 1.227170\n",
      "Reconstruction: 0.810795, Regularization: 0.348552, Discriminator: 0.009231; Generator: 0.058592,\n",
      "D(x): 0.924, D(G(z)): 0.153\n",
      "2019-04-10 01:19:00,842 root         INFO     ====> Epoch: 17 Average loss: 1.1014\n",
      "2019-04-10 01:19:00,868 root         INFO     Train Epoch: 18 [0/8000 (0%)]\tTotal Loss: 1.164029\n",
      "Reconstruction: 0.764279, Regularization: 0.332544, Discriminator: 0.008526; Generator: 0.058680,\n",
      "D(x): 0.927, D(G(z)): 0.153\n",
      "2019-04-10 01:19:00,969 root         INFO     Train Epoch: 18 [512/8000 (6%)]\tTotal Loss: 0.878299\n",
      "Reconstruction: 0.577659, Regularization: 0.231293, Discriminator: 0.010546; Generator: 0.058801,\n",
      "D(x): 0.921, D(G(z)): 0.152\n",
      "2019-04-10 01:19:01,069 root         INFO     Train Epoch: 18 [1024/8000 (13%)]\tTotal Loss: 0.882764\n",
      "Reconstruction: 0.593520, Regularization: 0.223750, Discriminator: 0.006560; Generator: 0.058934,\n",
      "D(x): 0.959, D(G(z)): 0.152\n",
      "2019-04-10 01:19:01,170 root         INFO     Train Epoch: 18 [1536/8000 (19%)]\tTotal Loss: 0.717750\n",
      "Reconstruction: 0.462796, Regularization: 0.185598, Discriminator: 0.010299; Generator: 0.059057,\n",
      "D(x): 0.907, D(G(z)): 0.151\n",
      "2019-04-10 01:19:01,270 root         INFO     Train Epoch: 18 [2048/8000 (26%)]\tTotal Loss: 0.820941\n",
      "Reconstruction: 0.539087, Regularization: 0.215702, Discriminator: 0.007028; Generator: 0.059124,\n",
      "D(x): 0.951, D(G(z)): 0.151\n",
      "2019-04-10 01:19:01,370 root         INFO     Train Epoch: 18 [2560/8000 (32%)]\tTotal Loss: 0.905414\n",
      "Reconstruction: 0.591207, Regularization: 0.244782, Discriminator: 0.010169; Generator: 0.059256,\n",
      "D(x): 0.915, D(G(z)): 0.150\n",
      "2019-04-10 01:19:01,470 root         INFO     Train Epoch: 18 [3072/8000 (38%)]\tTotal Loss: 1.115326\n",
      "Reconstruction: 0.723009, Regularization: 0.320201, Discriminator: 0.012749; Generator: 0.059367,\n",
      "D(x): 0.882, D(G(z)): 0.150\n",
      "2019-04-10 01:19:01,571 root         INFO     Train Epoch: 18 [3584/8000 (45%)]\tTotal Loss: 1.183102\n",
      "Reconstruction: 0.772298, Regularization: 0.340616, Discriminator: 0.010699; Generator: 0.059488,\n",
      "D(x): 0.905, D(G(z)): 0.149\n",
      "2019-04-10 01:19:01,671 root         INFO     Train Epoch: 18 [4096/8000 (51%)]\tTotal Loss: 0.887707\n",
      "Reconstruction: 0.568507, Regularization: 0.251065, Discriminator: 0.008481; Generator: 0.059654,\n",
      "D(x): 0.916, D(G(z)): 0.148\n",
      "2019-04-10 01:19:01,771 root         INFO     Train Epoch: 18 [4608/8000 (58%)]\tTotal Loss: 1.236731\n",
      "Reconstruction: 0.809910, Regularization: 0.352465, Discriminator: 0.014560; Generator: 0.059795,\n",
      "D(x): 0.891, D(G(z)): 0.148\n",
      "2019-04-10 01:19:01,869 root         INFO     Train Epoch: 18 [5120/8000 (64%)]\tTotal Loss: 1.241969\n",
      "Reconstruction: 0.826067, Regularization: 0.350384, Discriminator: 0.005602; Generator: 0.059916,\n",
      "D(x): 0.982, D(G(z)): 0.147\n",
      "2019-04-10 01:19:01,967 root         INFO     Train Epoch: 18 [5632/8000 (70%)]\tTotal Loss: 0.921009\n",
      "Reconstruction: 0.589062, Regularization: 0.260593, Discriminator: 0.011230; Generator: 0.060125,\n",
      "D(x): 0.891, D(G(z)): 0.146\n",
      "2019-04-10 01:19:02,066 root         INFO     Train Epoch: 18 [6144/8000 (77%)]\tTotal Loss: 0.956945\n",
      "Reconstruction: 0.617605, Regularization: 0.260359, Discriminator: 0.018691; Generator: 0.060290,\n",
      "D(x): 0.841, D(G(z)): 0.145\n",
      "2019-04-10 01:19:02,164 root         INFO     Train Epoch: 18 [6656/8000 (83%)]\tTotal Loss: 1.175682\n",
      "Reconstruction: 0.802026, Regularization: 0.305738, Discriminator: 0.007527; Generator: 0.060392,\n",
      "D(x): 0.939, D(G(z)): 0.145\n",
      "2019-04-10 01:19:02,265 root         INFO     Train Epoch: 18 [7168/8000 (90%)]\tTotal Loss: 1.075862\n",
      "Reconstruction: 0.704756, Regularization: 0.302287, Discriminator: 0.008114; Generator: 0.060706,\n",
      "D(x): 0.940, D(G(z)): 0.143\n",
      "2019-04-10 01:19:02,365 root         INFO     Train Epoch: 18 [7680/8000 (96%)]\tTotal Loss: 1.268932\n",
      "Reconstruction: 0.837349, Regularization: 0.363337, Discriminator: 0.007620; Generator: 0.060626,\n",
      "D(x): 0.949, D(G(z)): 0.144\n",
      "2019-04-10 01:19:02,440 root         INFO     ====> Epoch: 18 Average loss: 1.2096\n",
      "2019-04-10 01:19:02,466 root         INFO     Train Epoch: 19 [0/8000 (0%)]\tTotal Loss: 0.959267\n",
      "Reconstruction: 0.623815, Regularization: 0.266318, Discriminator: 0.008395; Generator: 0.060739,\n",
      "D(x): 0.934, D(G(z)): 0.143\n",
      "2019-04-10 01:19:02,568 root         INFO     Train Epoch: 19 [512/8000 (6%)]\tTotal Loss: 1.058896\n",
      "Reconstruction: 0.684116, Regularization: 0.305224, Discriminator: 0.008657; Generator: 0.060900,\n",
      "D(x): 0.917, D(G(z)): 0.142\n",
      "2019-04-10 01:19:02,665 root         INFO     Train Epoch: 19 [1024/8000 (13%)]\tTotal Loss: 1.268202\n",
      "Reconstruction: 0.851194, Regularization: 0.348947, Discriminator: 0.007137; Generator: 0.060924,\n",
      "D(x): 0.952, D(G(z)): 0.142\n",
      "2019-04-10 01:19:02,761 root         INFO     Train Epoch: 19 [1536/8000 (19%)]\tTotal Loss: 0.861169\n",
      "Reconstruction: 0.551074, Regularization: 0.236380, Discriminator: 0.012660; Generator: 0.061056,\n",
      "D(x): 0.891, D(G(z)): 0.142\n",
      "2019-04-10 01:19:02,858 root         INFO     Train Epoch: 19 [2048/8000 (26%)]\tTotal Loss: 1.478677\n",
      "Reconstruction: 0.973944, Regularization: 0.436771, Discriminator: 0.006735; Generator: 0.061227,\n",
      "D(x): 0.956, D(G(z)): 0.141\n",
      "2019-04-10 01:19:02,954 root         INFO     Train Epoch: 19 [2560/8000 (32%)]\tTotal Loss: 0.804985\n",
      "Reconstruction: 0.510566, Regularization: 0.222058, Discriminator: 0.011077; Generator: 0.061284,\n",
      "D(x): 0.917, D(G(z)): 0.141\n",
      "2019-04-10 01:19:03,052 root         INFO     Train Epoch: 19 [3072/8000 (38%)]\tTotal Loss: 0.723801\n",
      "Reconstruction: 0.460443, Regularization: 0.194683, Discriminator: 0.007260; Generator: 0.061416,\n",
      "D(x): 0.943, D(G(z)): 0.140\n",
      "2019-04-10 01:19:03,148 root         INFO     Train Epoch: 19 [3584/8000 (45%)]\tTotal Loss: 1.309699\n",
      "Reconstruction: 0.843362, Regularization: 0.393926, Discriminator: 0.010932; Generator: 0.061479,\n",
      "D(x): 0.895, D(G(z)): 0.140\n",
      "2019-04-10 01:19:03,245 root         INFO     Train Epoch: 19 [4096/8000 (51%)]\tTotal Loss: 1.370588\n",
      "Reconstruction: 0.894528, Regularization: 0.400019, Discriminator: 0.014394; Generator: 0.061648,\n",
      "D(x): 0.886, D(G(z)): 0.139\n",
      "2019-04-10 01:19:03,342 root         INFO     Train Epoch: 19 [4608/8000 (58%)]\tTotal Loss: 0.962730\n",
      "Reconstruction: 0.655400, Regularization: 0.236646, Discriminator: 0.008953; Generator: 0.061731,\n",
      "D(x): 0.923, D(G(z)): 0.139\n",
      "2019-04-10 01:19:03,439 root         INFO     Train Epoch: 19 [5120/8000 (64%)]\tTotal Loss: 1.099828\n",
      "Reconstruction: 0.696619, Regularization: 0.327508, Discriminator: 0.013823; Generator: 0.061878,\n",
      "D(x): 0.882, D(G(z)): 0.138\n",
      "2019-04-10 01:19:03,536 root         INFO     Train Epoch: 19 [5632/8000 (70%)]\tTotal Loss: 0.867004\n",
      "Reconstruction: 0.557403, Regularization: 0.235501, Discriminator: 0.012083; Generator: 0.062016,\n",
      "D(x): 0.901, D(G(z)): 0.137\n",
      "2019-04-10 01:19:03,632 root         INFO     Train Epoch: 19 [6144/8000 (77%)]\tTotal Loss: 0.931462\n",
      "Reconstruction: 0.656135, Regularization: 0.195118, Discriminator: 0.018090; Generator: 0.062119,\n",
      "D(x): 0.864, D(G(z)): 0.137\n",
      "2019-04-10 01:19:03,729 root         INFO     Train Epoch: 19 [6656/8000 (83%)]\tTotal Loss: 1.137688\n",
      "Reconstruction: 0.741534, Regularization: 0.327234, Discriminator: 0.006669; Generator: 0.062251,\n",
      "D(x): 0.945, D(G(z)): 0.136\n",
      "2019-04-10 01:19:03,825 root         INFO     Train Epoch: 19 [7168/8000 (90%)]\tTotal Loss: 0.907951\n",
      "Reconstruction: 0.593704, Regularization: 0.240176, Discriminator: 0.011651; Generator: 0.062420,\n",
      "D(x): 0.945, D(G(z)): 0.136\n",
      "2019-04-10 01:19:03,924 root         INFO     Train Epoch: 19 [7680/8000 (96%)]\tTotal Loss: 1.152303\n",
      "Reconstruction: 0.745473, Regularization: 0.336884, Discriminator: 0.007470; Generator: 0.062476,\n",
      "D(x): 0.939, D(G(z)): 0.135\n",
      "2019-04-10 01:19:03,998 root         INFO     ====> Epoch: 19 Average loss: 1.0900\n",
      "2019-04-10 01:19:04,024 root         INFO     Train Epoch: 20 [0/8000 (0%)]\tTotal Loss: 2.247113\n",
      "Reconstruction: 1.549919, Regularization: 0.621036, Discriminator: 0.013536; Generator: 0.062622,\n",
      "D(x): 0.886, D(G(z)): 0.135\n",
      "2019-04-10 01:19:04,123 root         INFO     Train Epoch: 20 [512/8000 (6%)]\tTotal Loss: 1.428811\n",
      "Reconstruction: 0.984049, Regularization: 0.365402, Discriminator: 0.016600; Generator: 0.062760,\n",
      "D(x): 0.902, D(G(z)): 0.134\n",
      "2019-04-10 01:19:04,222 root         INFO     Train Epoch: 20 [1024/8000 (13%)]\tTotal Loss: 0.685433\n",
      "Reconstruction: 0.439451, Regularization: 0.177182, Discriminator: 0.005848; Generator: 0.062951,\n",
      "D(x): 0.962, D(G(z)): 0.133\n",
      "2019-04-10 01:19:04,321 root         INFO     Train Epoch: 20 [1536/8000 (19%)]\tTotal Loss: 1.351683\n",
      "Reconstruction: 0.889771, Regularization: 0.391902, Discriminator: 0.006881; Generator: 0.063129,\n",
      "D(x): 0.951, D(G(z)): 0.133\n",
      "2019-04-10 01:19:04,420 root         INFO     Train Epoch: 20 [2048/8000 (26%)]\tTotal Loss: 0.979013\n",
      "Reconstruction: 0.620056, Regularization: 0.288243, Discriminator: 0.007529; Generator: 0.063184,\n",
      "D(x): 0.921, D(G(z)): 0.132\n",
      "2019-04-10 01:19:04,519 root         INFO     Train Epoch: 20 [2560/8000 (32%)]\tTotal Loss: 1.255874\n",
      "Reconstruction: 0.852024, Regularization: 0.331717, Discriminator: 0.008801; Generator: 0.063333,\n",
      "D(x): 0.920, D(G(z)): 0.132\n",
      "2019-04-10 01:19:04,618 root         INFO     Train Epoch: 20 [3072/8000 (38%)]\tTotal Loss: 1.080847\n",
      "Reconstruction: 0.765225, Regularization: 0.243489, Discriminator: 0.008661; Generator: 0.063472,\n",
      "D(x): 0.932, D(G(z)): 0.131\n",
      "2019-04-10 01:19:04,717 root         INFO     Train Epoch: 20 [3584/8000 (45%)]\tTotal Loss: 0.823131\n",
      "Reconstruction: 0.532380, Regularization: 0.218434, Discriminator: 0.008767; Generator: 0.063550,\n",
      "D(x): 0.922, D(G(z)): 0.131\n",
      "2019-04-10 01:19:04,814 root         INFO     Train Epoch: 20 [4096/8000 (51%)]\tTotal Loss: 0.769522\n",
      "Reconstruction: 0.487564, Regularization: 0.212357, Discriminator: 0.005984; Generator: 0.063617,\n",
      "D(x): 0.957, D(G(z)): 0.131\n",
      "2019-04-10 01:19:04,912 root         INFO     Train Epoch: 20 [4608/8000 (58%)]\tTotal Loss: 1.238188\n",
      "Reconstruction: 0.812220, Regularization: 0.356990, Discriminator: 0.005167; Generator: 0.063811,\n",
      "D(x): 0.977, D(G(z)): 0.130\n",
      "2019-04-10 01:19:05,010 root         INFO     Train Epoch: 20 [5120/8000 (64%)]\tTotal Loss: 1.311797\n",
      "Reconstruction: 0.851363, Regularization: 0.387734, Discriminator: 0.008761; Generator: 0.063939,\n",
      "D(x): 0.929, D(G(z)): 0.129\n",
      "2019-04-10 01:19:05,108 root         INFO     Train Epoch: 20 [5632/8000 (70%)]\tTotal Loss: 1.259019\n",
      "Reconstruction: 0.815201, Regularization: 0.366740, Discriminator: 0.013046; Generator: 0.064033,\n",
      "D(x): 0.877, D(G(z)): 0.129\n",
      "2019-04-10 01:19:05,206 root         INFO     Train Epoch: 20 [6144/8000 (77%)]\tTotal Loss: 1.153691\n",
      "Reconstruction: 0.760604, Regularization: 0.316092, Discriminator: 0.012885; Generator: 0.064109,\n",
      "D(x): 0.887, D(G(z)): 0.129\n",
      "2019-04-10 01:19:05,304 root         INFO     Train Epoch: 20 [6656/8000 (83%)]\tTotal Loss: 1.564174\n",
      "Reconstruction: 1.019106, Regularization: 0.471301, Discriminator: 0.009689; Generator: 0.064079,\n",
      "D(x): 0.909, D(G(z)): 0.129\n",
      "2019-04-10 01:19:05,402 root         INFO     Train Epoch: 20 [7168/8000 (90%)]\tTotal Loss: 0.776273\n",
      "Reconstruction: 0.481256, Regularization: 0.220806, Discriminator: 0.010082; Generator: 0.064129,\n",
      "D(x): 0.944, D(G(z)): 0.128\n",
      "2019-04-10 01:19:05,499 root         INFO     Train Epoch: 20 [7680/8000 (96%)]\tTotal Loss: 1.094084\n",
      "Reconstruction: 0.707735, Regularization: 0.312858, Discriminator: 0.009269; Generator: 0.064222,\n",
      "D(x): 0.909, D(G(z)): 0.128\n",
      "2019-04-10 01:19:05,573 root         INFO     ====> Epoch: 20 Average loss: 1.1292\n",
      "2019-04-10 01:19:05,599 root         INFO     Train Epoch: 21 [0/8000 (0%)]\tTotal Loss: 0.605651\n",
      "Reconstruction: 0.368155, Regularization: 0.159826, Discriminator: 0.013437; Generator: 0.064234,\n",
      "D(x): 0.839, D(G(z)): 0.128\n",
      "2019-04-10 01:19:05,698 root         INFO     Train Epoch: 21 [512/8000 (6%)]\tTotal Loss: 0.860808\n",
      "Reconstruction: 0.560103, Regularization: 0.227285, Discriminator: 0.009080; Generator: 0.064340,\n",
      "D(x): 0.895, D(G(z)): 0.128\n",
      "2019-04-10 01:19:05,796 root         INFO     Train Epoch: 21 [1024/8000 (13%)]\tTotal Loss: 0.810984\n",
      "Reconstruction: 0.517035, Regularization: 0.218381, Discriminator: 0.011155; Generator: 0.064413,\n",
      "D(x): 0.898, D(G(z)): 0.127\n",
      "2019-04-10 01:19:05,895 root         INFO     Train Epoch: 21 [1536/8000 (19%)]\tTotal Loss: 1.364765\n",
      "Reconstruction: 0.893103, Regularization: 0.398137, Discriminator: 0.008951; Generator: 0.064574,\n",
      "D(x): 0.954, D(G(z)): 0.127\n",
      "2019-04-10 01:19:05,993 root         INFO     Train Epoch: 21 [2048/8000 (26%)]\tTotal Loss: 1.194295\n",
      "Reconstruction: 0.774709, Regularization: 0.346495, Discriminator: 0.008441; Generator: 0.064651,\n",
      "D(x): 0.919, D(G(z)): 0.126\n",
      "2019-04-10 01:19:06,091 root         INFO     Train Epoch: 21 [2560/8000 (32%)]\tTotal Loss: 0.775425\n",
      "Reconstruction: 0.490928, Regularization: 0.209613, Discriminator: 0.010084; Generator: 0.064800,\n",
      "D(x): 0.896, D(G(z)): 0.126\n",
      "2019-04-10 01:19:06,190 root         INFO     Train Epoch: 21 [3072/8000 (38%)]\tTotal Loss: 1.121012\n",
      "Reconstruction: 0.724117, Regularization: 0.322767, Discriminator: 0.009184; Generator: 0.064944,\n",
      "D(x): 0.934, D(G(z)): 0.125\n",
      "2019-04-10 01:19:06,289 root         INFO     Train Epoch: 21 [3584/8000 (45%)]\tTotal Loss: 1.309557\n",
      "Reconstruction: 0.845177, Regularization: 0.390976, Discriminator: 0.008345; Generator: 0.065059,\n",
      "D(x): 0.923, D(G(z)): 0.125\n",
      "2019-04-10 01:19:06,387 root         INFO     Train Epoch: 21 [4096/8000 (51%)]\tTotal Loss: 1.848768\n",
      "Reconstruction: 1.375239, Regularization: 0.398825, Discriminator: 0.009558; Generator: 0.065145,\n",
      "D(x): 0.917, D(G(z)): 0.124\n",
      "2019-04-10 01:19:06,485 root         INFO     Train Epoch: 21 [4608/8000 (58%)]\tTotal Loss: 1.103715\n",
      "Reconstruction: 0.716210, Regularization: 0.312059, Discriminator: 0.010218; Generator: 0.065229,\n",
      "D(x): 0.904, D(G(z)): 0.124\n",
      "2019-04-10 01:19:06,584 root         INFO     Train Epoch: 21 [5120/8000 (64%)]\tTotal Loss: 1.205219\n",
      "Reconstruction: 0.773893, Regularization: 0.352970, Discriminator: 0.013119; Generator: 0.065238,\n",
      "D(x): 0.916, D(G(z)): 0.124\n",
      "2019-04-10 01:19:06,682 root         INFO     Train Epoch: 21 [5632/8000 (70%)]\tTotal Loss: 0.650201\n",
      "Reconstruction: 0.407642, Regularization: 0.166532, Discriminator: 0.010730; Generator: 0.065297,\n",
      "D(x): 0.870, D(G(z)): 0.124\n",
      "2019-04-10 01:19:06,781 root         INFO     Train Epoch: 21 [6144/8000 (77%)]\tTotal Loss: 0.876088\n",
      "Reconstruction: 0.553611, Regularization: 0.248578, Discriminator: 0.008554; Generator: 0.065345,\n",
      "D(x): 0.901, D(G(z)): 0.124\n",
      "2019-04-10 01:19:06,879 root         INFO     Train Epoch: 21 [6656/8000 (83%)]\tTotal Loss: 0.988020\n",
      "Reconstruction: 0.636027, Regularization: 0.266557, Discriminator: 0.019941; Generator: 0.065495,\n",
      "D(x): 0.818, D(G(z)): 0.123\n",
      "2019-04-10 01:19:06,977 root         INFO     Train Epoch: 21 [7168/8000 (90%)]\tTotal Loss: 0.530775\n",
      "Reconstruction: 0.324414, Regularization: 0.132847, Discriminator: 0.007900; Generator: 0.065614,\n",
      "D(x): 0.922, D(G(z)): 0.122\n",
      "2019-04-10 01:19:07,075 root         INFO     Train Epoch: 21 [7680/8000 (96%)]\tTotal Loss: 1.546412\n",
      "Reconstruction: 1.008899, Regularization: 0.460632, Discriminator: 0.011181; Generator: 0.065700,\n",
      "D(x): 0.896, D(G(z)): 0.122\n",
      "2019-04-10 01:19:07,148 root         INFO     ====> Epoch: 21 Average loss: 1.2068\n",
      "2019-04-10 01:19:07,175 root         INFO     Train Epoch: 22 [0/8000 (0%)]\tTotal Loss: 0.982355\n",
      "Reconstruction: 0.624184, Regularization: 0.277299, Discriminator: 0.015163; Generator: 0.065709,\n",
      "D(x): 0.837, D(G(z)): 0.122\n",
      "2019-04-10 01:19:07,274 root         INFO     Train Epoch: 22 [512/8000 (6%)]\tTotal Loss: 0.930058\n",
      "Reconstruction: 0.590263, Regularization: 0.264292, Discriminator: 0.009684; Generator: 0.065819,\n",
      "D(x): 0.890, D(G(z)): 0.122\n",
      "2019-04-10 01:19:07,372 root         INFO     Train Epoch: 22 [1024/8000 (13%)]\tTotal Loss: 1.226058\n",
      "Reconstruction: 0.792285, Regularization: 0.357389, Discriminator: 0.010526; Generator: 0.065857,\n",
      "D(x): 0.895, D(G(z)): 0.122\n",
      "2019-04-10 01:19:07,471 root         INFO     Train Epoch: 22 [1536/8000 (19%)]\tTotal Loss: 1.075064\n",
      "Reconstruction: 0.692919, Regularization: 0.304321, Discriminator: 0.011941; Generator: 0.065883,\n",
      "D(x): 0.880, D(G(z)): 0.121\n",
      "2019-04-10 01:19:07,572 root         INFO     Train Epoch: 22 [2048/8000 (26%)]\tTotal Loss: 1.235174\n",
      "Reconstruction: 0.802818, Regularization: 0.356851, Discriminator: 0.009457; Generator: 0.066049,\n",
      "D(x): 0.917, D(G(z)): 0.121\n",
      "2019-04-10 01:19:07,672 root         INFO     Train Epoch: 22 [2560/8000 (32%)]\tTotal Loss: 0.818404\n",
      "Reconstruction: 0.505082, Regularization: 0.236043, Discriminator: 0.011123; Generator: 0.066156,\n",
      "D(x): 0.887, D(G(z)): 0.120\n",
      "2019-04-10 01:19:07,771 root         INFO     Train Epoch: 22 [3072/8000 (38%)]\tTotal Loss: 0.914431\n",
      "Reconstruction: 0.585363, Regularization: 0.256210, Discriminator: 0.006616; Generator: 0.066243,\n",
      "D(x): 0.940, D(G(z)): 0.120\n",
      "2019-04-10 01:19:07,870 root         INFO     Train Epoch: 22 [3584/8000 (45%)]\tTotal Loss: 1.011862\n",
      "Reconstruction: 0.649610, Regularization: 0.286686, Discriminator: 0.009237; Generator: 0.066329,\n",
      "D(x): 0.896, D(G(z)): 0.120\n",
      "2019-04-10 01:19:07,970 root         INFO     Train Epoch: 22 [4096/8000 (51%)]\tTotal Loss: 0.830354\n",
      "Reconstruction: 0.513199, Regularization: 0.241371, Discriminator: 0.009366; Generator: 0.066418,\n",
      "D(x): 0.876, D(G(z)): 0.119\n",
      "2019-04-10 01:19:08,069 root         INFO     Train Epoch: 22 [4608/8000 (58%)]\tTotal Loss: 0.974447\n",
      "Reconstruction: 0.625591, Regularization: 0.271990, Discriminator: 0.010340; Generator: 0.066526,\n",
      "D(x): 0.919, D(G(z)): 0.119\n",
      "2019-04-10 01:19:08,168 root         INFO     Train Epoch: 22 [5120/8000 (64%)]\tTotal Loss: 1.204836\n",
      "Reconstruction: 0.811121, Regularization: 0.320240, Discriminator: 0.006877; Generator: 0.066598,\n",
      "D(x): 0.933, D(G(z)): 0.119\n",
      "2019-04-10 01:19:08,268 root         INFO     Train Epoch: 22 [5632/8000 (70%)]\tTotal Loss: 0.962359\n",
      "Reconstruction: 0.617320, Regularization: 0.269892, Discriminator: 0.008362; Generator: 0.066785,\n",
      "D(x): 0.921, D(G(z)): 0.118\n",
      "2019-04-10 01:19:08,368 root         INFO     Train Epoch: 22 [6144/8000 (77%)]\tTotal Loss: 1.219470\n",
      "Reconstruction: 0.776655, Regularization: 0.366155, Discriminator: 0.009745; Generator: 0.066914,\n",
      "D(x): 0.932, D(G(z)): 0.118\n",
      "2019-04-10 01:19:08,468 root         INFO     Train Epoch: 22 [6656/8000 (83%)]\tTotal Loss: 1.407135\n",
      "Reconstruction: 0.913636, Regularization: 0.420401, Discriminator: 0.006148; Generator: 0.066951,\n",
      "D(x): 0.957, D(G(z)): 0.117\n",
      "2019-04-10 01:19:08,568 root         INFO     Train Epoch: 22 [7168/8000 (90%)]\tTotal Loss: 1.197137\n",
      "Reconstruction: 0.784550, Regularization: 0.340661, Discriminator: 0.004853; Generator: 0.067074,\n",
      "D(x): 0.973, D(G(z)): 0.117\n",
      "2019-04-10 01:19:08,669 root         INFO     Train Epoch: 22 [7680/8000 (96%)]\tTotal Loss: 1.581910\n",
      "Reconstruction: 1.038480, Regularization: 0.467961, Discriminator: 0.008424; Generator: 0.067045,\n",
      "D(x): 0.960, D(G(z)): 0.117\n",
      "2019-04-10 01:19:08,743 root         INFO     ====> Epoch: 22 Average loss: 1.0768\n",
      "2019-04-10 01:19:08,769 root         INFO     Train Epoch: 23 [0/8000 (0%)]\tTotal Loss: 0.923852\n",
      "Reconstruction: 0.588438, Regularization: 0.260933, Discriminator: 0.007394; Generator: 0.067087,\n",
      "D(x): 0.950, D(G(z)): 0.117\n",
      "2019-04-10 01:19:08,872 root         INFO     Train Epoch: 23 [512/8000 (6%)]\tTotal Loss: 0.962051\n",
      "Reconstruction: 0.619566, Regularization: 0.263421, Discriminator: 0.011813; Generator: 0.067250,\n",
      "D(x): 0.894, D(G(z)): 0.116\n",
      "2019-04-10 01:19:08,973 root         INFO     Train Epoch: 23 [1024/8000 (13%)]\tTotal Loss: 0.786804\n",
      "Reconstruction: 0.493236, Regularization: 0.218403, Discriminator: 0.007905; Generator: 0.067260,\n",
      "D(x): 0.918, D(G(z)): 0.116\n",
      "2019-04-10 01:19:09,071 root         INFO     Train Epoch: 23 [1536/8000 (19%)]\tTotal Loss: 0.967986\n",
      "Reconstruction: 0.613576, Regularization: 0.276076, Discriminator: 0.010896; Generator: 0.067438,\n",
      "D(x): 0.921, D(G(z)): 0.116\n",
      "2019-04-10 01:19:09,170 root         INFO     Train Epoch: 23 [2048/8000 (26%)]\tTotal Loss: 0.712629\n",
      "Reconstruction: 0.442684, Regularization: 0.189829, Discriminator: 0.012584; Generator: 0.067532,\n",
      "D(x): 0.903, D(G(z)): 0.115\n",
      "2019-04-10 01:19:09,269 root         INFO     Train Epoch: 23 [2560/8000 (32%)]\tTotal Loss: 1.221718\n",
      "Reconstruction: 0.796457, Regularization: 0.351563, Discriminator: 0.006187; Generator: 0.067511,\n",
      "D(x): 0.964, D(G(z)): 0.115\n",
      "2019-04-10 01:19:09,367 root         INFO     Train Epoch: 23 [3072/8000 (38%)]\tTotal Loss: 1.099467\n",
      "Reconstruction: 0.704828, Regularization: 0.318188, Discriminator: 0.008970; Generator: 0.067480,\n",
      "D(x): 0.921, D(G(z)): 0.115\n",
      "2019-04-10 01:19:09,465 root         INFO     Train Epoch: 23 [3584/8000 (45%)]\tTotal Loss: 0.842155\n",
      "Reconstruction: 0.538227, Regularization: 0.227434, Discriminator: 0.008894; Generator: 0.067600,\n",
      "D(x): 0.934, D(G(z)): 0.115\n",
      "2019-04-10 01:19:09,564 root         INFO     Train Epoch: 23 [4096/8000 (51%)]\tTotal Loss: 1.068451\n",
      "Reconstruction: 0.682264, Regularization: 0.303456, Discriminator: 0.015100; Generator: 0.067631,\n",
      "D(x): 0.855, D(G(z)): 0.115\n",
      "2019-04-10 01:19:09,663 root         INFO     Train Epoch: 23 [4608/8000 (58%)]\tTotal Loss: 1.745435\n",
      "Reconstruction: 1.146169, Regularization: 0.516212, Discriminator: 0.015381; Generator: 0.067674,\n",
      "D(x): 0.867, D(G(z)): 0.115\n",
      "2019-04-10 01:19:09,763 root         INFO     Train Epoch: 23 [5120/8000 (64%)]\tTotal Loss: 0.677861\n",
      "Reconstruction: 0.418083, Regularization: 0.175636, Discriminator: 0.016269; Generator: 0.067874,\n",
      "D(x): 0.862, D(G(z)): 0.114\n",
      "2019-04-10 01:19:09,862 root         INFO     Train Epoch: 23 [5632/8000 (70%)]\tTotal Loss: 0.790565\n",
      "Reconstruction: 0.490807, Regularization: 0.223043, Discriminator: 0.008779; Generator: 0.067936,\n",
      "D(x): 0.934, D(G(z)): 0.114\n",
      "2019-04-10 01:19:09,960 root         INFO     Train Epoch: 23 [6144/8000 (77%)]\tTotal Loss: 0.799959\n",
      "Reconstruction: 0.499003, Regularization: 0.223217, Discriminator: 0.009707; Generator: 0.068033,\n",
      "D(x): 0.904, D(G(z)): 0.113\n",
      "2019-04-10 01:19:10,059 root         INFO     Train Epoch: 23 [6656/8000 (83%)]\tTotal Loss: 0.837588\n",
      "Reconstruction: 0.518707, Regularization: 0.227659, Discriminator: 0.023050; Generator: 0.068172,\n",
      "D(x): 0.801, D(G(z)): 0.113\n",
      "2019-04-10 01:19:10,156 root         INFO     Train Epoch: 23 [7168/8000 (90%)]\tTotal Loss: 1.155360\n",
      "Reconstruction: 0.731923, Regularization: 0.342223, Discriminator: 0.012988; Generator: 0.068226,\n",
      "D(x): 0.897, D(G(z)): 0.113\n",
      "2019-04-10 01:19:10,255 root         INFO     Train Epoch: 23 [7680/8000 (96%)]\tTotal Loss: 0.711179\n",
      "Reconstruction: 0.435264, Regularization: 0.197124, Discriminator: 0.010551; Generator: 0.068241,\n",
      "D(x): 0.933, D(G(z)): 0.113\n",
      "2019-04-10 01:19:10,328 root         INFO     ====> Epoch: 23 Average loss: 1.2660\n",
      "2019-04-10 01:19:10,355 root         INFO     Train Epoch: 24 [0/8000 (0%)]\tTotal Loss: 1.786971\n",
      "Reconstruction: 1.166525, Regularization: 0.547980, Discriminator: 0.004221; Generator: 0.068245,\n",
      "D(x): 0.985, D(G(z)): 0.113\n",
      "2019-04-10 01:19:10,456 root         INFO     Train Epoch: 24 [512/8000 (6%)]\tTotal Loss: 1.028768\n",
      "Reconstruction: 0.629260, Regularization: 0.317134, Discriminator: 0.014032; Generator: 0.068342,\n",
      "D(x): 0.897, D(G(z)): 0.112\n",
      "2019-04-10 01:19:10,556 root         INFO     Train Epoch: 24 [1024/8000 (13%)]\tTotal Loss: 0.741970\n",
      "Reconstruction: 0.461332, Regularization: 0.204335, Discriminator: 0.007869; Generator: 0.068433,\n",
      "D(x): 0.917, D(G(z)): 0.112\n",
      "2019-04-10 01:19:10,656 root         INFO     Train Epoch: 24 [1536/8000 (19%)]\tTotal Loss: 1.443026\n",
      "Reconstruction: 0.909100, Regularization: 0.460069, Discriminator: 0.005312; Generator: 0.068545,\n",
      "D(x): 0.961, D(G(z)): 0.112\n",
      "2019-04-10 01:19:10,756 root         INFO     Train Epoch: 24 [2048/8000 (26%)]\tTotal Loss: 1.091030\n",
      "Reconstruction: 0.691550, Regularization: 0.317811, Discriminator: 0.013039; Generator: 0.068630,\n",
      "D(x): 0.883, D(G(z)): 0.111\n",
      "2019-04-10 01:19:10,854 root         INFO     Train Epoch: 24 [2560/8000 (32%)]\tTotal Loss: 0.797215\n",
      "Reconstruction: 0.496613, Regularization: 0.225710, Discriminator: 0.006220; Generator: 0.068672,\n",
      "D(x): 0.940, D(G(z)): 0.111\n",
      "2019-04-10 01:19:10,954 root         INFO     Train Epoch: 24 [3072/8000 (38%)]\tTotal Loss: 0.744888\n",
      "Reconstruction: 0.458044, Regularization: 0.207604, Discriminator: 0.010574; Generator: 0.068666,\n",
      "D(x): 0.899, D(G(z)): 0.111\n",
      "2019-04-10 01:19:11,052 root         INFO     Train Epoch: 24 [3584/8000 (45%)]\tTotal Loss: 0.876815\n",
      "Reconstruction: 0.546877, Regularization: 0.245615, Discriminator: 0.015570; Generator: 0.068752,\n",
      "D(x): 0.836, D(G(z)): 0.111\n",
      "2019-04-10 01:19:11,152 root         INFO     Train Epoch: 24 [4096/8000 (51%)]\tTotal Loss: 0.810292\n",
      "Reconstruction: 0.521993, Regularization: 0.213818, Discriminator: 0.005673; Generator: 0.068808,\n",
      "D(x): 0.955, D(G(z)): 0.111\n",
      "2019-04-10 01:19:11,250 root         INFO     Train Epoch: 24 [4608/8000 (58%)]\tTotal Loss: 0.832237\n",
      "Reconstruction: 0.515915, Regularization: 0.239418, Discriminator: 0.008036; Generator: 0.068868,\n",
      "D(x): 0.908, D(G(z)): 0.110\n",
      "2019-04-10 01:19:11,349 root         INFO     Train Epoch: 24 [5120/8000 (64%)]\tTotal Loss: 0.898273\n",
      "Reconstruction: 0.559383, Regularization: 0.256557, Discriminator: 0.013365; Generator: 0.068968,\n",
      "D(x): 0.920, D(G(z)): 0.110\n",
      "2019-04-10 01:19:11,448 root         INFO     Train Epoch: 24 [5632/8000 (70%)]\tTotal Loss: 1.156994\n",
      "Reconstruction: 0.737205, Regularization: 0.344558, Discriminator: 0.006365; Generator: 0.068867,\n",
      "D(x): 0.959, D(G(z)): 0.110\n",
      "2019-04-10 01:19:11,546 root         INFO     Train Epoch: 24 [6144/8000 (77%)]\tTotal Loss: 0.728461\n",
      "Reconstruction: 0.445314, Regularization: 0.204144, Discriminator: 0.010136; Generator: 0.068867,\n",
      "D(x): 0.907, D(G(z)): 0.110\n",
      "2019-04-10 01:19:11,645 root         INFO     Train Epoch: 24 [6656/8000 (83%)]\tTotal Loss: 0.770661\n",
      "Reconstruction: 0.480751, Regularization: 0.211884, Discriminator: 0.009224; Generator: 0.068802,\n",
      "D(x): 0.911, D(G(z)): 0.111\n",
      "2019-04-10 01:19:11,744 root         INFO     Train Epoch: 24 [7168/8000 (90%)]\tTotal Loss: 0.779571\n",
      "Reconstruction: 0.484848, Regularization: 0.219710, Discriminator: 0.006004; Generator: 0.069008,\n",
      "D(x): 0.967, D(G(z)): 0.110\n",
      "2019-04-10 01:19:11,841 root         INFO     Train Epoch: 24 [7680/8000 (96%)]\tTotal Loss: 1.458372\n",
      "Reconstruction: 0.941298, Regularization: 0.441019, Discriminator: 0.006960; Generator: 0.069096,\n",
      "D(x): 0.933, D(G(z)): 0.110\n",
      "2019-04-10 01:19:11,915 root         INFO     ====> Epoch: 24 Average loss: 1.0701\n",
      "2019-04-10 01:19:11,942 root         INFO     Train Epoch: 25 [0/8000 (0%)]\tTotal Loss: 1.400727\n",
      "Reconstruction: 0.907802, Regularization: 0.412542, Discriminator: 0.011162; Generator: 0.069221,\n",
      "D(x): 0.903, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,043 root         INFO     Train Epoch: 25 [512/8000 (6%)]\tTotal Loss: 1.079825\n",
      "Reconstruction: 0.684960, Regularization: 0.318335, Discriminator: 0.007322; Generator: 0.069208,\n",
      "D(x): 0.933, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,143 root         INFO     Train Epoch: 25 [1024/8000 (13%)]\tTotal Loss: 1.295592\n",
      "Reconstruction: 0.829685, Regularization: 0.387820, Discriminator: 0.008835; Generator: 0.069252,\n",
      "D(x): 0.961, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,243 root         INFO     Train Epoch: 25 [1536/8000 (19%)]\tTotal Loss: 0.948064\n",
      "Reconstruction: 0.606644, Regularization: 0.264006, Discriminator: 0.008169; Generator: 0.069245,\n",
      "D(x): 0.909, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,343 root         INFO     Train Epoch: 25 [2048/8000 (26%)]\tTotal Loss: 1.005567\n",
      "Reconstruction: 0.630082, Regularization: 0.285937, Discriminator: 0.020221; Generator: 0.069326,\n",
      "D(x): 0.878, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,443 root         INFO     Train Epoch: 25 [2560/8000 (32%)]\tTotal Loss: 1.246822\n",
      "Reconstruction: 0.777910, Regularization: 0.393237, Discriminator: 0.006444; Generator: 0.069231,\n",
      "D(x): 0.934, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,544 root         INFO     Train Epoch: 25 [3072/8000 (38%)]\tTotal Loss: 1.007394\n",
      "Reconstruction: 0.635098, Regularization: 0.298586, Discriminator: 0.004470; Generator: 0.069240,\n",
      "D(x): 0.976, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,644 root         INFO     Train Epoch: 25 [3584/8000 (45%)]\tTotal Loss: 1.670819\n",
      "Reconstruction: 1.048060, Regularization: 0.539437, Discriminator: 0.014003; Generator: 0.069319,\n",
      "D(x): 0.857, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,744 root         INFO     Train Epoch: 25 [4096/8000 (51%)]\tTotal Loss: 0.742691\n",
      "Reconstruction: 0.455847, Regularization: 0.205510, Discriminator: 0.011962; Generator: 0.069372,\n",
      "D(x): 0.901, D(G(z)): 0.109\n",
      "2019-04-10 01:19:12,844 root         INFO     Train Epoch: 25 [4608/8000 (58%)]\tTotal Loss: 1.118916\n",
      "Reconstruction: 0.705138, Regularization: 0.330737, Discriminator: 0.013539; Generator: 0.069503,\n",
      "D(x): 0.897, D(G(z)): 0.108\n",
      "2019-04-10 01:19:12,944 root         INFO     Train Epoch: 25 [5120/8000 (64%)]\tTotal Loss: 1.121123\n",
      "Reconstruction: 0.712847, Regularization: 0.321174, Discriminator: 0.017501; Generator: 0.069601,\n",
      "D(x): 0.848, D(G(z)): 0.108\n",
      "2019-04-10 01:19:13,044 root         INFO     Train Epoch: 25 [5632/8000 (70%)]\tTotal Loss: 1.258064\n",
      "Reconstruction: 0.800741, Regularization: 0.376823, Discriminator: 0.010839; Generator: 0.069660,\n",
      "D(x): 0.879, D(G(z)): 0.108\n",
      "2019-04-10 01:19:13,144 root         INFO     Train Epoch: 25 [6144/8000 (77%)]\tTotal Loss: 0.689687\n",
      "Reconstruction: 0.416994, Regularization: 0.193582, Discriminator: 0.009428; Generator: 0.069683,\n",
      "D(x): 0.927, D(G(z)): 0.108\n",
      "2019-04-10 01:19:13,244 root         INFO     Train Epoch: 25 [6656/8000 (83%)]\tTotal Loss: 1.345427\n",
      "Reconstruction: 0.908312, Regularization: 0.362748, Discriminator: 0.004516; Generator: 0.069851,\n",
      "D(x): 0.974, D(G(z)): 0.107\n",
      "2019-04-10 01:19:13,344 root         INFO     Train Epoch: 25 [7168/8000 (90%)]\tTotal Loss: 0.718814\n",
      "Reconstruction: 0.448158, Regularization: 0.193824, Discriminator: 0.006919; Generator: 0.069912,\n",
      "D(x): 0.940, D(G(z)): 0.107\n",
      "2019-04-10 01:19:13,444 root         INFO     Train Epoch: 25 [7680/8000 (96%)]\tTotal Loss: 0.827037\n",
      "Reconstruction: 0.520815, Regularization: 0.231627, Discriminator: 0.004617; Generator: 0.069977,\n",
      "D(x): 0.975, D(G(z)): 0.107\n",
      "2019-04-10 01:19:13,519 root         INFO     ====> Epoch: 25 Average loss: 1.0583\n",
      "2019-04-10 01:19:13,545 root         INFO     Train Epoch: 26 [0/8000 (0%)]\tTotal Loss: 0.901291\n",
      "Reconstruction: 0.555829, Regularization: 0.266814, Discriminator: 0.008567; Generator: 0.070081,\n",
      "D(x): 0.919, D(G(z)): 0.106\n",
      "2019-04-10 01:19:13,646 root         INFO     Train Epoch: 26 [512/8000 (6%)]\tTotal Loss: 1.146535\n",
      "Reconstruction: 0.722557, Regularization: 0.349601, Discriminator: 0.004163; Generator: 0.070214,\n",
      "D(x): 0.981, D(G(z)): 0.106\n",
      "2019-04-10 01:19:13,747 root         INFO     Train Epoch: 26 [1024/8000 (13%)]\tTotal Loss: 0.998591\n",
      "Reconstruction: 0.631409, Regularization: 0.290607, Discriminator: 0.006190; Generator: 0.070385,\n",
      "D(x): 0.945, D(G(z)): 0.105\n",
      "2019-04-10 01:19:13,848 root         INFO     Train Epoch: 26 [1536/8000 (19%)]\tTotal Loss: 0.912729\n",
      "Reconstruction: 0.582557, Regularization: 0.246407, Discriminator: 0.013439; Generator: 0.070326,\n",
      "D(x): 0.888, D(G(z)): 0.105\n",
      "2019-04-10 01:19:13,948 root         INFO     Train Epoch: 26 [2048/8000 (26%)]\tTotal Loss: 1.268676\n",
      "Reconstruction: 0.812563, Regularization: 0.378529, Discriminator: 0.007195; Generator: 0.070390,\n",
      "D(x): 0.949, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,049 root         INFO     Train Epoch: 26 [2560/8000 (32%)]\tTotal Loss: 1.016594\n",
      "Reconstruction: 0.630789, Regularization: 0.306358, Discriminator: 0.008952; Generator: 0.070495,\n",
      "D(x): 0.915, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,149 root         INFO     Train Epoch: 26 [3072/8000 (38%)]\tTotal Loss: 1.309671\n",
      "Reconstruction: 0.823032, Regularization: 0.406867, Discriminator: 0.009364; Generator: 0.070407,\n",
      "D(x): 0.895, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,249 root         INFO     Train Epoch: 26 [3584/8000 (45%)]\tTotal Loss: 0.996237\n",
      "Reconstruction: 0.633844, Regularization: 0.285995, Discriminator: 0.005888; Generator: 0.070510,\n",
      "D(x): 0.948, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,349 root         INFO     Train Epoch: 26 [4096/8000 (51%)]\tTotal Loss: 0.945093\n",
      "Reconstruction: 0.585766, Regularization: 0.270873, Discriminator: 0.017723; Generator: 0.070731,\n",
      "D(x): 0.832, D(G(z)): 0.104\n",
      "2019-04-10 01:19:14,449 root         INFO     Train Epoch: 26 [4608/8000 (58%)]\tTotal Loss: 1.229571\n",
      "Reconstruction: 0.808753, Regularization: 0.343034, Discriminator: 0.007267; Generator: 0.070516,\n",
      "D(x): 0.942, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,549 root         INFO     Train Epoch: 26 [5120/8000 (64%)]\tTotal Loss: 1.318976\n",
      "Reconstruction: 0.861161, Regularization: 0.371494, Discriminator: 0.015756; Generator: 0.070565,\n",
      "D(x): 0.900, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,646 root         INFO     Train Epoch: 26 [5632/8000 (70%)]\tTotal Loss: 0.848327\n",
      "Reconstruction: 0.523207, Regularization: 0.242478, Discriminator: 0.012088; Generator: 0.070554,\n",
      "D(x): 0.880, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,744 root         INFO     Train Epoch: 26 [6144/8000 (77%)]\tTotal Loss: 0.782255\n",
      "Reconstruction: 0.480007, Regularization: 0.212190, Discriminator: 0.019684; Generator: 0.070373,\n",
      "D(x): 0.873, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,840 root         INFO     Train Epoch: 26 [6656/8000 (83%)]\tTotal Loss: 1.315144\n",
      "Reconstruction: 0.832636, Regularization: 0.406910, Discriminator: 0.005137; Generator: 0.070462,\n",
      "D(x): 0.957, D(G(z)): 0.105\n",
      "2019-04-10 01:19:14,937 root         INFO     Train Epoch: 26 [7168/8000 (90%)]\tTotal Loss: 0.854519\n",
      "Reconstruction: 0.553138, Regularization: 0.219248, Discriminator: 0.011582; Generator: 0.070550,\n",
      "D(x): 0.860, D(G(z)): 0.105\n",
      "2019-04-10 01:19:15,034 root         INFO     Train Epoch: 26 [7680/8000 (96%)]\tTotal Loss: 0.840875\n",
      "Reconstruction: 0.510791, Regularization: 0.249186, Discriminator: 0.010359; Generator: 0.070538,\n",
      "D(x): 0.952, D(G(z)): 0.105\n",
      "2019-04-10 01:19:15,109 root         INFO     ====> Epoch: 26 Average loss: 1.0552\n",
      "2019-04-10 01:19:15,135 root         INFO     Train Epoch: 27 [0/8000 (0%)]\tTotal Loss: 0.970773\n",
      "Reconstruction: 0.599960, Regularization: 0.282145, Discriminator: 0.018111; Generator: 0.070558,\n",
      "D(x): 0.855, D(G(z)): 0.105\n",
      "2019-04-10 01:19:15,237 root         INFO     Train Epoch: 27 [512/8000 (6%)]\tTotal Loss: 0.795328\n",
      "Reconstruction: 0.494642, Regularization: 0.225196, Discriminator: 0.004857; Generator: 0.070634,\n",
      "D(x): 0.960, D(G(z)): 0.104\n",
      "2019-04-10 01:19:15,338 root         INFO     Train Epoch: 27 [1024/8000 (13%)]\tTotal Loss: 0.848767\n",
      "Reconstruction: 0.526878, Regularization: 0.242966, Discriminator: 0.008278; Generator: 0.070645,\n",
      "D(x): 0.932, D(G(z)): 0.104\n",
      "2019-04-10 01:19:15,438 root         INFO     Train Epoch: 27 [1536/8000 (19%)]\tTotal Loss: 1.410340\n",
      "Reconstruction: 0.941559, Regularization: 0.376473, Discriminator: 0.021690; Generator: 0.070618,\n",
      "D(x): 0.822, D(G(z)): 0.104\n",
      "2019-04-10 01:19:15,538 root         INFO     Train Epoch: 27 [2048/8000 (26%)]\tTotal Loss: 1.037853\n",
      "Reconstruction: 0.648725, Regularization: 0.302634, Discriminator: 0.015806; Generator: 0.070688,\n",
      "D(x): 0.905, D(G(z)): 0.104\n",
      "2019-04-10 01:19:15,639 root         INFO     Train Epoch: 27 [2560/8000 (32%)]\tTotal Loss: 1.834004\n",
      "Reconstruction: 1.172506, Regularization: 0.579234, Discriminator: 0.011448; Generator: 0.070815,\n",
      "D(x): 0.918, D(G(z)): 0.104\n",
      "2019-04-10 01:19:15,739 root         INFO     Train Epoch: 27 [3072/8000 (38%)]\tTotal Loss: 0.902162\n",
      "Reconstruction: 0.559103, Regularization: 0.267593, Discriminator: 0.004671; Generator: 0.070795,\n",
      "D(x): 0.974, D(G(z)): 0.104\n",
      "2019-04-10 01:19:15,839 root         INFO     Train Epoch: 27 [3584/8000 (45%)]\tTotal Loss: 1.401862\n",
      "Reconstruction: 0.903905, Regularization: 0.418186, Discriminator: 0.008724; Generator: 0.071047,\n",
      "D(x): 0.931, D(G(z)): 0.103\n",
      "2019-04-10 01:19:15,939 root         INFO     Train Epoch: 27 [4096/8000 (51%)]\tTotal Loss: 1.120067\n",
      "Reconstruction: 0.701545, Regularization: 0.336195, Discriminator: 0.011256; Generator: 0.071072,\n",
      "D(x): 0.896, D(G(z)): 0.103\n",
      "2019-04-10 01:19:16,040 root         INFO     Train Epoch: 27 [4608/8000 (58%)]\tTotal Loss: 1.098323\n",
      "Reconstruction: 0.692879, Regularization: 0.322239, Discriminator: 0.012341; Generator: 0.070865,\n",
      "D(x): 0.912, D(G(z)): 0.104\n",
      "2019-04-10 01:19:16,140 root         INFO     Train Epoch: 27 [5120/8000 (64%)]\tTotal Loss: 0.980902\n",
      "Reconstruction: 0.621231, Regularization: 0.279316, Discriminator: 0.009349; Generator: 0.071006,\n",
      "D(x): 0.941, D(G(z)): 0.103\n",
      "2019-04-10 01:19:16,240 root         INFO     Train Epoch: 27 [5632/8000 (70%)]\tTotal Loss: 0.985577\n",
      "Reconstruction: 0.615578, Regularization: 0.290416, Discriminator: 0.008545; Generator: 0.071039,\n",
      "D(x): 0.952, D(G(z)): 0.103\n",
      "2019-04-10 01:19:16,339 root         INFO     Train Epoch: 27 [6144/8000 (77%)]\tTotal Loss: 1.221774\n",
      "Reconstruction: 0.769214, Regularization: 0.371803, Discriminator: 0.009508; Generator: 0.071249,\n",
      "D(x): 0.923, D(G(z)): 0.102\n",
      "2019-04-10 01:19:16,438 root         INFO     Train Epoch: 27 [6656/8000 (83%)]\tTotal Loss: 0.805671\n",
      "Reconstruction: 0.492470, Regularization: 0.237393, Discriminator: 0.004464; Generator: 0.071344,\n",
      "D(x): 0.970, D(G(z)): 0.102\n",
      "2019-04-10 01:19:16,538 root         INFO     Train Epoch: 27 [7168/8000 (90%)]\tTotal Loss: 0.877032\n",
      "Reconstruction: 0.543167, Regularization: 0.252018, Discriminator: 0.010624; Generator: 0.071224,\n",
      "D(x): 0.893, D(G(z)): 0.102\n",
      "2019-04-10 01:19:16,638 root         INFO     Train Epoch: 27 [7680/8000 (96%)]\tTotal Loss: 0.732288\n",
      "Reconstruction: 0.449752, Regularization: 0.200839, Discriminator: 0.010630; Generator: 0.071066,\n",
      "D(x): 0.914, D(G(z)): 0.103\n",
      "2019-04-10 01:19:16,712 root         INFO     ====> Epoch: 27 Average loss: 1.0528\n",
      "2019-04-10 01:19:16,738 root         INFO     Train Epoch: 28 [0/8000 (0%)]\tTotal Loss: 1.263387\n",
      "Reconstruction: 0.805245, Regularization: 0.382246, Discriminator: 0.004930; Generator: 0.070965,\n",
      "D(x): 0.965, D(G(z)): 0.103\n",
      "2019-04-10 01:19:16,839 root         INFO     Train Epoch: 28 [512/8000 (6%)]\tTotal Loss: 1.023665\n",
      "Reconstruction: 0.641372, Regularization: 0.303341, Discriminator: 0.007893; Generator: 0.071058,\n",
      "D(x): 0.899, D(G(z)): 0.103\n",
      "2019-04-10 01:19:16,939 root         INFO     Train Epoch: 28 [1024/8000 (13%)]\tTotal Loss: 1.087885\n",
      "Reconstruction: 0.698228, Regularization: 0.312186, Discriminator: 0.006400; Generator: 0.071070,\n",
      "D(x): 0.958, D(G(z)): 0.103\n",
      "2019-04-10 01:19:17,039 root         INFO     Train Epoch: 28 [1536/8000 (19%)]\tTotal Loss: 0.936891\n",
      "Reconstruction: 0.582708, Regularization: 0.276588, Discriminator: 0.006535; Generator: 0.071060,\n",
      "D(x): 0.931, D(G(z)): 0.103\n",
      "2019-04-10 01:19:17,139 root         INFO     Train Epoch: 28 [2048/8000 (26%)]\tTotal Loss: 1.647467\n",
      "Reconstruction: 1.062791, Regularization: 0.506965, Discriminator: 0.006562; Generator: 0.071150,\n",
      "D(x): 0.935, D(G(z)): 0.103\n",
      "2019-04-10 01:19:17,239 root         INFO     Train Epoch: 28 [2560/8000 (32%)]\tTotal Loss: 1.423504\n",
      "Reconstruction: 0.910945, Regularization: 0.437053, Discriminator: 0.004281; Generator: 0.071224,\n",
      "D(x): 0.974, D(G(z)): 0.102\n",
      "2019-04-10 01:19:17,340 root         INFO     Train Epoch: 28 [3072/8000 (38%)]\tTotal Loss: 0.943625\n",
      "Reconstruction: 0.580772, Regularization: 0.271715, Discriminator: 0.019704; Generator: 0.071434,\n",
      "D(x): 0.858, D(G(z)): 0.102\n",
      "2019-04-10 01:19:17,440 root         INFO     Train Epoch: 28 [3584/8000 (45%)]\tTotal Loss: 0.828399\n",
      "Reconstruction: 0.517320, Regularization: 0.233997, Discriminator: 0.005575; Generator: 0.071507,\n",
      "D(x): 0.968, D(G(z)): 0.101\n",
      "2019-04-10 01:19:17,540 root         INFO     Train Epoch: 28 [4096/8000 (51%)]\tTotal Loss: 1.446192\n",
      "Reconstruction: 0.993494, Regularization: 0.374148, Discriminator: 0.007189; Generator: 0.071361,\n",
      "D(x): 0.929, D(G(z)): 0.102\n",
      "2019-04-10 01:19:17,640 root         INFO     Train Epoch: 28 [4608/8000 (58%)]\tTotal Loss: 1.170010\n",
      "Reconstruction: 0.665650, Regularization: 0.421252, Discriminator: 0.011656; Generator: 0.071451,\n",
      "D(x): 0.902, D(G(z)): 0.102\n",
      "2019-04-10 01:19:17,741 root         INFO     Train Epoch: 28 [5120/8000 (64%)]\tTotal Loss: 0.899076\n",
      "Reconstruction: 0.566809, Regularization: 0.255571, Discriminator: 0.005304; Generator: 0.071392,\n",
      "D(x): 0.969, D(G(z)): 0.102\n",
      "2019-04-10 01:19:17,841 root         INFO     Train Epoch: 28 [5632/8000 (70%)]\tTotal Loss: 1.052039\n",
      "Reconstruction: 0.649568, Regularization: 0.318727, Discriminator: 0.012099; Generator: 0.071644,\n",
      "D(x): 0.944, D(G(z)): 0.101\n",
      "2019-04-10 01:19:17,941 root         INFO     Train Epoch: 28 [6144/8000 (77%)]\tTotal Loss: 1.126984\n",
      "Reconstruction: 0.711606, Regularization: 0.338420, Discriminator: 0.005248; Generator: 0.071711,\n",
      "D(x): 0.965, D(G(z)): 0.101\n",
      "2019-04-10 01:19:18,041 root         INFO     Train Epoch: 28 [6656/8000 (83%)]\tTotal Loss: 1.133211\n",
      "Reconstruction: 0.725302, Regularization: 0.327825, Discriminator: 0.008137; Generator: 0.071948,\n",
      "D(x): 0.911, D(G(z)): 0.100\n",
      "2019-04-10 01:19:18,141 root         INFO     Train Epoch: 28 [7168/8000 (90%)]\tTotal Loss: 0.965475\n",
      "Reconstruction: 0.604945, Regularization: 0.284681, Discriminator: 0.004361; Generator: 0.071487,\n",
      "D(x): 0.977, D(G(z)): 0.102\n",
      "2019-04-10 01:19:18,242 root         INFO     Train Epoch: 28 [7680/8000 (96%)]\tTotal Loss: 1.030135\n",
      "Reconstruction: 0.639250, Regularization: 0.302535, Discriminator: 0.016879; Generator: 0.071471,\n",
      "D(x): 0.869, D(G(z)): 0.102\n",
      "2019-04-10 01:19:18,316 root         INFO     ====> Epoch: 28 Average loss: 1.0580\n",
      "2019-04-10 01:19:18,343 root         INFO     Train Epoch: 29 [0/8000 (0%)]\tTotal Loss: 1.192896\n",
      "Reconstruction: 0.753232, Regularization: 0.363570, Discriminator: 0.004850; Generator: 0.071244,\n",
      "D(x): 0.969, D(G(z)): 0.102\n",
      "2019-04-10 01:19:18,444 root         INFO     Train Epoch: 29 [512/8000 (6%)]\tTotal Loss: 1.654837\n",
      "Reconstruction: 1.062944, Regularization: 0.514285, Discriminator: 0.006609; Generator: 0.070999,\n",
      "D(x): 0.923, D(G(z)): 0.103\n",
      "2019-04-10 01:19:18,544 root         INFO     Train Epoch: 29 [1024/8000 (13%)]\tTotal Loss: 1.282173\n",
      "Reconstruction: 0.808520, Regularization: 0.389154, Discriminator: 0.013503; Generator: 0.070996,\n",
      "D(x): 0.881, D(G(z)): 0.103\n",
      "2019-04-10 01:19:18,644 root         INFO     Train Epoch: 29 [1536/8000 (19%)]\tTotal Loss: 1.223801\n",
      "Reconstruction: 0.777933, Regularization: 0.369049, Discriminator: 0.005817; Generator: 0.071002,\n",
      "D(x): 0.949, D(G(z)): 0.103\n",
      "2019-04-10 01:19:18,744 root         INFO     Train Epoch: 29 [2048/8000 (26%)]\tTotal Loss: 0.681582\n",
      "Reconstruction: 0.414237, Regularization: 0.190201, Discriminator: 0.005925; Generator: 0.071218,\n",
      "D(x): 0.948, D(G(z)): 0.102\n",
      "2019-04-10 01:19:18,844 root         INFO     Train Epoch: 29 [2560/8000 (32%)]\tTotal Loss: 1.308102\n",
      "Reconstruction: 0.816386, Regularization: 0.410076, Discriminator: 0.010377; Generator: 0.071264,\n",
      "D(x): 0.924, D(G(z)): 0.102\n",
      "2019-04-10 01:19:18,943 root         INFO     Train Epoch: 29 [3072/8000 (38%)]\tTotal Loss: 1.248825\n",
      "Reconstruction: 0.807508, Regularization: 0.362111, Discriminator: 0.008039; Generator: 0.071167,\n",
      "D(x): 0.921, D(G(z)): 0.103\n",
      "2019-04-10 01:19:19,043 root         INFO     Train Epoch: 29 [3584/8000 (45%)]\tTotal Loss: 0.634808\n",
      "Reconstruction: 0.383379, Regularization: 0.171703, Discriminator: 0.008497; Generator: 0.071230,\n",
      "D(x): 0.917, D(G(z)): 0.102\n",
      "2019-04-10 01:19:19,142 root         INFO     Train Epoch: 29 [4096/8000 (51%)]\tTotal Loss: 0.761229\n",
      "Reconstruction: 0.466205, Regularization: 0.211737, Discriminator: 0.011898; Generator: 0.071389,\n",
      "D(x): 0.933, D(G(z)): 0.102\n",
      "2019-04-10 01:19:19,241 root         INFO     Train Epoch: 29 [4608/8000 (58%)]\tTotal Loss: 2.219290\n",
      "Reconstruction: 1.874747, Regularization: 0.269733, Discriminator: 0.003601; Generator: 0.071209,\n",
      "D(x): 0.993, D(G(z)): 0.102\n",
      "2019-04-10 01:19:19,341 root         INFO     Train Epoch: 29 [5120/8000 (64%)]\tTotal Loss: 0.940980\n",
      "Reconstruction: 0.587365, Regularization: 0.277652, Discriminator: 0.004524; Generator: 0.071439,\n",
      "D(x): 0.968, D(G(z)): 0.102\n",
      "2019-04-10 01:19:19,440 root         INFO     Train Epoch: 29 [5632/8000 (70%)]\tTotal Loss: 1.256870\n",
      "Reconstruction: 0.784879, Regularization: 0.389491, Discriminator: 0.010904; Generator: 0.071596,\n",
      "D(x): 0.909, D(G(z)): 0.101\n",
      "2019-04-10 01:19:19,540 root         INFO     Train Epoch: 29 [6144/8000 (77%)]\tTotal Loss: 0.730728\n",
      "Reconstruction: 0.443528, Regularization: 0.202931, Discriminator: 0.012541; Generator: 0.071729,\n",
      "D(x): 0.891, D(G(z)): 0.101\n",
      "2019-04-10 01:19:19,639 root         INFO     Train Epoch: 29 [6656/8000 (83%)]\tTotal Loss: 0.933394\n",
      "Reconstruction: 0.590692, Regularization: 0.259976, Discriminator: 0.011037; Generator: 0.071689,\n",
      "D(x): 0.902, D(G(z)): 0.101\n",
      "2019-04-10 01:19:19,738 root         INFO     Train Epoch: 29 [7168/8000 (90%)]\tTotal Loss: 1.455393\n",
      "Reconstruction: 0.970631, Regularization: 0.409609, Discriminator: 0.003538; Generator: 0.071615,\n",
      "D(x): 0.994, D(G(z)): 0.101\n",
      "2019-04-10 01:19:19,838 root         INFO     Train Epoch: 29 [7680/8000 (96%)]\tTotal Loss: 0.963073\n",
      "Reconstruction: 0.602490, Regularization: 0.275574, Discriminator: 0.013507; Generator: 0.071503,\n",
      "D(x): 0.908, D(G(z)): 0.101\n",
      "2019-04-10 01:19:19,912 root         INFO     ====> Epoch: 29 Average loss: 1.0394\n",
      "2019-04-10 01:19:19,938 root         INFO     Train Epoch: 30 [0/8000 (0%)]\tTotal Loss: 1.271055\n",
      "Reconstruction: 0.785182, Regularization: 0.395849, Discriminator: 0.018443; Generator: 0.071582,\n",
      "D(x): 0.847, D(G(z)): 0.101\n",
      "2019-04-10 01:19:20,039 root         INFO     Train Epoch: 30 [512/8000 (6%)]\tTotal Loss: 1.020618\n",
      "Reconstruction: 0.633416, Regularization: 0.308347, Discriminator: 0.007396; Generator: 0.071460,\n",
      "D(x): 0.957, D(G(z)): 0.102\n",
      "2019-04-10 01:19:20,140 root         INFO     Train Epoch: 30 [1024/8000 (13%)]\tTotal Loss: 1.103830\n",
      "Reconstruction: 0.701884, Regularization: 0.325831, Discriminator: 0.004396; Generator: 0.071718,\n",
      "D(x): 0.972, D(G(z)): 0.101\n",
      "2019-04-10 01:19:20,238 root         INFO     Train Epoch: 30 [1536/8000 (19%)]\tTotal Loss: 0.984136\n",
      "Reconstruction: 0.604067, Regularization: 0.297310, Discriminator: 0.010950; Generator: 0.071808,\n",
      "D(x): 0.880, D(G(z)): 0.100\n",
      "2019-04-10 01:19:20,335 root         INFO     Train Epoch: 30 [2048/8000 (26%)]\tTotal Loss: 1.277612\n",
      "Reconstruction: 0.796704, Regularization: 0.391095, Discriminator: 0.018082; Generator: 0.071730,\n",
      "D(x): 0.921, D(G(z)): 0.101\n",
      "2019-04-10 01:19:20,434 root         INFO     Train Epoch: 30 [2560/8000 (32%)]\tTotal Loss: 1.011203\n",
      "Reconstruction: 0.618913, Regularization: 0.316226, Discriminator: 0.004328; Generator: 0.071736,\n",
      "D(x): 0.979, D(G(z)): 0.101\n",
      "2019-04-10 01:19:20,532 root         INFO     Train Epoch: 30 [3072/8000 (38%)]\tTotal Loss: 0.952909\n",
      "Reconstruction: 0.608060, Regularization: 0.266626, Discriminator: 0.006476; Generator: 0.071748,\n",
      "D(x): 0.941, D(G(z)): 0.101\n",
      "2019-04-10 01:19:20,633 root         INFO     Train Epoch: 30 [3584/8000 (45%)]\tTotal Loss: 1.189736\n",
      "Reconstruction: 0.750685, Regularization: 0.361065, Discriminator: 0.006157; Generator: 0.071830,\n",
      "D(x): 0.937, D(G(z)): 0.100\n",
      "2019-04-10 01:19:20,732 root         INFO     Train Epoch: 30 [4096/8000 (51%)]\tTotal Loss: 0.684774\n",
      "Reconstruction: 0.403587, Regularization: 0.194616, Discriminator: 0.014773; Generator: 0.071798,\n",
      "D(x): 0.890, D(G(z)): 0.101\n",
      "2019-04-10 01:19:20,833 root         INFO     Train Epoch: 30 [4608/8000 (58%)]\tTotal Loss: 0.813359\n",
      "Reconstruction: 0.497480, Regularization: 0.235147, Discriminator: 0.008834; Generator: 0.071898,\n",
      "D(x): 0.919, D(G(z)): 0.100\n",
      "2019-04-10 01:19:20,933 root         INFO     Train Epoch: 30 [5120/8000 (64%)]\tTotal Loss: 1.168267\n",
      "Reconstruction: 0.739730, Regularization: 0.352201, Discriminator: 0.004489; Generator: 0.071846,\n",
      "D(x): 0.968, D(G(z)): 0.100\n",
      "2019-04-10 01:19:21,032 root         INFO     Train Epoch: 30 [5632/8000 (70%)]\tTotal Loss: 0.764126\n",
      "Reconstruction: 0.454523, Regularization: 0.223347, Discriminator: 0.014474; Generator: 0.071782,\n",
      "D(x): 0.891, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,132 root         INFO     Train Epoch: 30 [6144/8000 (77%)]\tTotal Loss: 1.028837\n",
      "Reconstruction: 0.645873, Regularization: 0.304953, Discriminator: 0.006268; Generator: 0.071742,\n",
      "D(x): 0.926, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,232 root         INFO     Train Epoch: 30 [6656/8000 (83%)]\tTotal Loss: 0.968273\n",
      "Reconstruction: 0.601905, Regularization: 0.287235, Discriminator: 0.007332; Generator: 0.071801,\n",
      "D(x): 0.931, D(G(z)): 0.100\n",
      "2019-04-10 01:19:21,333 root         INFO     Train Epoch: 30 [7168/8000 (90%)]\tTotal Loss: 1.016205\n",
      "Reconstruction: 0.628437, Regularization: 0.295283, Discriminator: 0.020711; Generator: 0.071775,\n",
      "D(x): 0.820, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,433 root         INFO     Train Epoch: 30 [7680/8000 (96%)]\tTotal Loss: 1.275579\n",
      "Reconstruction: 0.803748, Regularization: 0.385981, Discriminator: 0.014221; Generator: 0.071629,\n",
      "D(x): 0.917, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,509 root         INFO     ====> Epoch: 30 Average loss: 1.0515\n",
      "2019-04-10 01:19:21,535 root         INFO     Train Epoch: 31 [0/8000 (0%)]\tTotal Loss: 1.259174\n",
      "Reconstruction: 0.812824, Regularization: 0.369687, Discriminator: 0.005162; Generator: 0.071502,\n",
      "D(x): 0.962, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,636 root         INFO     Train Epoch: 31 [512/8000 (6%)]\tTotal Loss: 1.291166\n",
      "Reconstruction: 0.833755, Regularization: 0.366074, Discriminator: 0.019663; Generator: 0.071674,\n",
      "D(x): 0.888, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,736 root         INFO     Train Epoch: 31 [1024/8000 (13%)]\tTotal Loss: 0.793235\n",
      "Reconstruction: 0.460845, Regularization: 0.240315, Discriminator: 0.020559; Generator: 0.071517,\n",
      "D(x): 0.862, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,836 root         INFO     Train Epoch: 31 [1536/8000 (19%)]\tTotal Loss: 0.980042\n",
      "Reconstruction: 0.608845, Regularization: 0.292276, Discriminator: 0.007412; Generator: 0.071509,\n",
      "D(x): 0.922, D(G(z)): 0.101\n",
      "2019-04-10 01:19:21,935 root         INFO     Train Epoch: 31 [2048/8000 (26%)]\tTotal Loss: 1.088509\n",
      "Reconstruction: 0.686048, Regularization: 0.322027, Discriminator: 0.008889; Generator: 0.071545,\n",
      "D(x): 0.931, D(G(z)): 0.101\n",
      "2019-04-10 01:19:22,035 root         INFO     Train Epoch: 31 [2560/8000 (32%)]\tTotal Loss: 0.887553\n",
      "Reconstruction: 0.543039, Regularization: 0.256276, Discriminator: 0.016829; Generator: 0.071410,\n",
      "D(x): 0.913, D(G(z)): 0.102\n",
      "2019-04-10 01:19:22,137 root         INFO     Train Epoch: 31 [3072/8000 (38%)]\tTotal Loss: 1.403457\n",
      "Reconstruction: 0.859589, Regularization: 0.466506, Discriminator: 0.005907; Generator: 0.071455,\n",
      "D(x): 0.952, D(G(z)): 0.102\n",
      "2019-04-10 01:19:22,239 root         INFO     Train Epoch: 31 [3584/8000 (45%)]\tTotal Loss: 0.870429\n",
      "Reconstruction: 0.527608, Regularization: 0.252598, Discriminator: 0.018634; Generator: 0.071589,\n",
      "D(x): 0.859, D(G(z)): 0.101\n",
      "2019-04-10 01:19:22,341 root         INFO     Train Epoch: 31 [4096/8000 (51%)]\tTotal Loss: 0.914564\n",
      "Reconstruction: 0.558611, Regularization: 0.275285, Discriminator: 0.009006; Generator: 0.071662,\n",
      "D(x): 0.898, D(G(z)): 0.101\n",
      "2019-04-10 01:19:22,442 root         INFO     Train Epoch: 31 [4608/8000 (58%)]\tTotal Loss: 1.330711\n",
      "Reconstruction: 0.835626, Regularization: 0.415808, Discriminator: 0.007666; Generator: 0.071611,\n",
      "D(x): 0.923, D(G(z)): 0.101\n",
      "2019-04-10 01:19:22,543 root         INFO     Train Epoch: 31 [5120/8000 (64%)]\tTotal Loss: 0.990238\n",
      "Reconstruction: 0.617969, Regularization: 0.296098, Discriminator: 0.004486; Generator: 0.071685,\n",
      "D(x): 0.967, D(G(z)): 0.101\n",
      "2019-04-10 01:19:22,643 root         INFO     Train Epoch: 31 [5632/8000 (70%)]\tTotal Loss: 0.842144\n",
      "Reconstruction: 0.521417, Regularization: 0.244570, Discriminator: 0.004578; Generator: 0.071579,\n",
      "D(x): 0.968, D(G(z)): 0.101\n",
      "2019-04-10 01:19:22,743 root         INFO     Train Epoch: 31 [6144/8000 (77%)]\tTotal Loss: 1.157046\n",
      "Reconstruction: 0.731326, Regularization: 0.330404, Discriminator: 0.023974; Generator: 0.071342,\n",
      "D(x): 0.843, D(G(z)): 0.102\n",
      "2019-04-10 01:19:22,842 root         INFO     Train Epoch: 31 [6656/8000 (83%)]\tTotal Loss: 1.171356\n",
      "Reconstruction: 0.763274, Regularization: 0.332478, Discriminator: 0.004516; Generator: 0.071087,\n",
      "D(x): 0.973, D(G(z)): 0.103\n",
      "2019-04-10 01:19:22,942 root         INFO     Train Epoch: 31 [7168/8000 (90%)]\tTotal Loss: 0.776919\n",
      "Reconstruction: 0.475052, Regularization: 0.221056, Discriminator: 0.009547; Generator: 0.071265,\n",
      "D(x): 0.920, D(G(z)): 0.102\n",
      "2019-04-10 01:19:23,040 root         INFO     Train Epoch: 31 [7680/8000 (96%)]\tTotal Loss: 1.049539\n",
      "Reconstruction: 0.647701, Regularization: 0.318411, Discriminator: 0.011972; Generator: 0.071455,\n",
      "D(x): 0.901, D(G(z)): 0.102\n",
      "2019-04-10 01:19:23,115 root         INFO     ====> Epoch: 31 Average loss: 4.3790\n",
      "2019-04-10 01:19:23,142 root         INFO     Train Epoch: 32 [0/8000 (0%)]\tTotal Loss: 1.523890\n",
      "Reconstruction: 0.965158, Regularization: 0.483427, Discriminator: 0.003833; Generator: 0.071472,\n",
      "D(x): 0.986, D(G(z)): 0.102\n",
      "2019-04-10 01:19:23,243 root         INFO     Train Epoch: 32 [512/8000 (6%)]\tTotal Loss: 0.795979\n",
      "Reconstruction: 0.475341, Regularization: 0.242708, Discriminator: 0.006414; Generator: 0.071517,\n",
      "D(x): 0.925, D(G(z)): 0.101\n",
      "2019-04-10 01:19:23,346 root         INFO     Train Epoch: 32 [1024/8000 (13%)]\tTotal Loss: 1.086137\n",
      "Reconstruction: 0.678878, Regularization: 0.324313, Discriminator: 0.011406; Generator: 0.071539,\n",
      "D(x): 0.907, D(G(z)): 0.101\n",
      "2019-04-10 01:19:23,447 root         INFO     Train Epoch: 32 [1536/8000 (19%)]\tTotal Loss: 0.805358\n",
      "Reconstruction: 0.483332, Regularization: 0.237134, Discriminator: 0.013331; Generator: 0.071562,\n",
      "D(x): 0.905, D(G(z)): 0.101\n",
      "2019-04-10 01:19:23,550 root         INFO     Train Epoch: 32 [2048/8000 (26%)]\tTotal Loss: 1.005897\n",
      "Reconstruction: 0.607963, Regularization: 0.308266, Discriminator: 0.018111; Generator: 0.071558,\n",
      "D(x): 0.872, D(G(z)): 0.101\n",
      "2019-04-10 01:19:23,653 root         INFO     Train Epoch: 32 [2560/8000 (32%)]\tTotal Loss: 0.656873\n",
      "Reconstruction: 0.388255, Regularization: 0.179018, Discriminator: 0.018057; Generator: 0.071542,\n",
      "D(x): 0.857, D(G(z)): 0.101\n",
      "2019-04-10 01:19:23,756 root         INFO     Train Epoch: 32 [3072/8000 (38%)]\tTotal Loss: 1.420718\n",
      "Reconstruction: 0.899600, Regularization: 0.443470, Discriminator: 0.006042; Generator: 0.071606,\n",
      "D(x): 0.943, D(G(z)): 0.101\n",
      "2019-04-10 01:19:23,858 root         INFO     Train Epoch: 32 [3584/8000 (45%)]\tTotal Loss: 1.154444\n",
      "Reconstruction: 0.724572, Regularization: 0.354087, Discriminator: 0.004442; Generator: 0.071343,\n",
      "D(x): 0.971, D(G(z)): 0.102\n",
      "2019-04-10 01:19:23,961 root         INFO     Train Epoch: 32 [4096/8000 (51%)]\tTotal Loss: 1.009312\n",
      "Reconstruction: 0.623990, Regularization: 0.304764, Discriminator: 0.009136; Generator: 0.071423,\n",
      "D(x): 0.903, D(G(z)): 0.102\n",
      "2019-04-10 01:19:24,062 root         INFO     Train Epoch: 32 [4608/8000 (58%)]\tTotal Loss: 1.104609\n",
      "Reconstruction: 0.706511, Regularization: 0.317359, Discriminator: 0.009314; Generator: 0.071425,\n",
      "D(x): 0.926, D(G(z)): 0.102\n",
      "2019-04-10 01:19:24,162 root         INFO     Train Epoch: 32 [5120/8000 (64%)]\tTotal Loss: 1.738090\n",
      "Reconstruction: 1.114276, Regularization: 0.547765, Discriminator: 0.004735; Generator: 0.071314,\n",
      "D(x): 0.969, D(G(z)): 0.102\n",
      "2019-04-10 01:19:24,262 root         INFO     Train Epoch: 32 [5632/8000 (70%)]\tTotal Loss: 1.152782\n",
      "Reconstruction: 0.718577, Regularization: 0.356804, Discriminator: 0.006103; Generator: 0.071298,\n",
      "D(x): 0.960, D(G(z)): 0.102\n",
      "2019-04-10 01:19:24,361 root         INFO     Train Epoch: 32 [6144/8000 (77%)]\tTotal Loss: 1.136029\n",
      "Reconstruction: 0.717474, Regularization: 0.332634, Discriminator: 0.014390; Generator: 0.071532,\n",
      "D(x): 0.869, D(G(z)): 0.101\n",
      "2019-04-10 01:19:24,462 root         INFO     Train Epoch: 32 [6656/8000 (83%)]\tTotal Loss: 0.871862\n",
      "Reconstruction: 0.533953, Regularization: 0.255911, Discriminator: 0.010629; Generator: 0.071369,\n",
      "D(x): 0.883, D(G(z)): 0.102\n",
      "2019-04-10 01:19:24,564 root         INFO     Train Epoch: 32 [7168/8000 (90%)]\tTotal Loss: 1.167480\n",
      "Reconstruction: 0.730533, Regularization: 0.346246, Discriminator: 0.019372; Generator: 0.071329,\n",
      "D(x): 0.857, D(G(z)): 0.102\n",
      "2019-04-10 01:19:24,666 root         INFO     Train Epoch: 32 [7680/8000 (96%)]\tTotal Loss: 0.949434\n",
      "Reconstruction: 0.577743, Regularization: 0.282384, Discriminator: 0.018089; Generator: 0.071218,\n",
      "D(x): 0.867, D(G(z)): 0.102\n",
      "2019-04-10 01:19:24,741 root         INFO     ====> Epoch: 32 Average loss: 1.0265\n",
      "2019-04-10 01:19:24,768 root         INFO     Train Epoch: 33 [0/8000 (0%)]\tTotal Loss: 0.989102\n",
      "Reconstruction: 0.630601, Regularization: 0.280032, Discriminator: 0.007314; Generator: 0.071154,\n",
      "D(x): 0.918, D(G(z)): 0.103\n",
      "2019-04-10 01:19:24,870 root         INFO     Train Epoch: 33 [512/8000 (6%)]\tTotal Loss: 1.020602\n",
      "Reconstruction: 0.624360, Regularization: 0.301721, Discriminator: 0.023379; Generator: 0.071142,\n",
      "D(x): 0.836, D(G(z)): 0.103\n",
      "2019-04-10 01:19:24,971 root         INFO     Train Epoch: 33 [1024/8000 (13%)]\tTotal Loss: 0.874879\n",
      "Reconstruction: 0.530204, Regularization: 0.267901, Discriminator: 0.005477; Generator: 0.071297,\n",
      "D(x): 0.965, D(G(z)): 0.102\n",
      "2019-04-10 01:19:25,072 root         INFO     Train Epoch: 33 [1536/8000 (19%)]\tTotal Loss: 1.284027\n",
      "Reconstruction: 0.802307, Regularization: 0.397091, Discriminator: 0.013065; Generator: 0.071564,\n",
      "D(x): 0.926, D(G(z)): 0.101\n",
      "2019-04-10 01:19:25,173 root         INFO     Train Epoch: 33 [2048/8000 (26%)]\tTotal Loss: 0.921344\n",
      "Reconstruction: 0.566087, Regularization: 0.274772, Discriminator: 0.008910; Generator: 0.071575,\n",
      "D(x): 0.967, D(G(z)): 0.101\n",
      "2019-04-10 01:19:25,275 root         INFO     Train Epoch: 33 [2560/8000 (32%)]\tTotal Loss: 0.576771\n",
      "Reconstruction: 0.329991, Regularization: 0.167813, Discriminator: 0.007408; Generator: 0.071560,\n",
      "D(x): 0.919, D(G(z)): 0.101\n",
      "2019-04-10 01:19:25,375 root         INFO     Train Epoch: 33 [3072/8000 (38%)]\tTotal Loss: 1.225960\n",
      "Reconstruction: 0.771521, Regularization: 0.371319, Discriminator: 0.011614; Generator: 0.071507,\n",
      "D(x): 0.920, D(G(z)): 0.101\n",
      "2019-04-10 01:19:25,475 root         INFO     Train Epoch: 33 [3584/8000 (45%)]\tTotal Loss: 0.814383\n",
      "Reconstruction: 0.498635, Regularization: 0.231015, Discriminator: 0.013382; Generator: 0.071352,\n",
      "D(x): 0.851, D(G(z)): 0.102\n",
      "2019-04-10 01:19:25,576 root         INFO     Train Epoch: 33 [4096/8000 (51%)]\tTotal Loss: 1.257772\n",
      "Reconstruction: 0.784364, Regularization: 0.386758, Discriminator: 0.015465; Generator: 0.071185,\n",
      "D(x): 0.854, D(G(z)): 0.102\n",
      "2019-04-10 01:19:25,676 root         INFO     Train Epoch: 33 [4608/8000 (58%)]\tTotal Loss: 2.733253\n",
      "Reconstruction: 2.297995, Regularization: 0.355042, Discriminator: 0.009233; Generator: 0.070983,\n",
      "D(x): 0.897, D(G(z)): 0.103\n",
      "2019-04-10 01:19:25,776 root         INFO     Train Epoch: 33 [5120/8000 (64%)]\tTotal Loss: 0.635577\n",
      "Reconstruction: 0.387090, Regularization: 0.167733, Discriminator: 0.009767; Generator: 0.070987,\n",
      "D(x): 0.918, D(G(z)): 0.103\n",
      "2019-04-10 01:19:25,877 root         INFO     Train Epoch: 33 [5632/8000 (70%)]\tTotal Loss: 0.837081\n",
      "Reconstruction: 0.607156, Regularization: 0.144693, Discriminator: 0.014323; Generator: 0.070909,\n",
      "D(x): 0.899, D(G(z)): 0.103\n",
      "2019-04-10 01:19:25,977 root         INFO     Train Epoch: 33 [6144/8000 (77%)]\tTotal Loss: 1.324014\n",
      "Reconstruction: 0.842411, Regularization: 0.395039, Discriminator: 0.015719; Generator: 0.070845,\n",
      "D(x): 0.910, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,078 root         INFO     Train Epoch: 33 [6656/8000 (83%)]\tTotal Loss: 0.744699\n",
      "Reconstruction: 0.447341, Regularization: 0.219832, Discriminator: 0.006769; Generator: 0.070756,\n",
      "D(x): 0.922, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,178 root         INFO     Train Epoch: 33 [7168/8000 (90%)]\tTotal Loss: 0.873340\n",
      "Reconstruction: 0.529989, Regularization: 0.258001, Discriminator: 0.014555; Generator: 0.070794,\n",
      "D(x): 0.870, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,278 root         INFO     Train Epoch: 33 [7680/8000 (96%)]\tTotal Loss: 0.588879\n",
      "Reconstruction: 0.344558, Regularization: 0.162676, Discriminator: 0.010944; Generator: 0.070700,\n",
      "D(x): 0.901, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,353 root         INFO     ====> Epoch: 33 Average loss: 1.0282\n",
      "2019-04-10 01:19:26,380 root         INFO     Train Epoch: 34 [0/8000 (0%)]\tTotal Loss: 1.549147\n",
      "Reconstruction: 0.982319, Regularization: 0.492379, Discriminator: 0.003820; Generator: 0.070629,\n",
      "D(x): 0.990, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,481 root         INFO     Train Epoch: 34 [512/8000 (6%)]\tTotal Loss: 1.321001\n",
      "Reconstruction: 0.829223, Regularization: 0.402244, Discriminator: 0.018847; Generator: 0.070688,\n",
      "D(x): 0.882, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,581 root         INFO     Train Epoch: 34 [1024/8000 (13%)]\tTotal Loss: 1.362894\n",
      "Reconstruction: 0.900764, Regularization: 0.384221, Discriminator: 0.007154; Generator: 0.070755,\n",
      "D(x): 0.968, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,681 root         INFO     Train Epoch: 34 [1536/8000 (19%)]\tTotal Loss: 1.378263\n",
      "Reconstruction: 0.866772, Regularization: 0.431900, Discriminator: 0.008832; Generator: 0.070758,\n",
      "D(x): 0.930, D(G(z)): 0.104\n",
      "2019-04-10 01:19:26,782 root         INFO     Train Epoch: 34 [2048/8000 (26%)]\tTotal Loss: 0.896006\n",
      "Reconstruction: 0.554580, Regularization: 0.266970, Discriminator: 0.003946; Generator: 0.070510,\n",
      "D(x): 0.987, D(G(z)): 0.105\n",
      "2019-04-10 01:19:26,882 root         INFO     Train Epoch: 34 [2560/8000 (32%)]\tTotal Loss: 0.815880\n",
      "Reconstruction: 0.486522, Regularization: 0.237769, Discriminator: 0.021203; Generator: 0.070386,\n",
      "D(x): 0.822, D(G(z)): 0.105\n",
      "2019-04-10 01:19:26,982 root         INFO     Train Epoch: 34 [3072/8000 (38%)]\tTotal Loss: 0.652746\n",
      "Reconstruction: 0.387686, Regularization: 0.183940, Discriminator: 0.010701; Generator: 0.070419,\n",
      "D(x): 0.905, D(G(z)): 0.105\n",
      "2019-04-10 01:19:27,083 root         INFO     Train Epoch: 34 [3584/8000 (45%)]\tTotal Loss: 1.044158\n",
      "Reconstruction: 0.636532, Regularization: 0.326089, Discriminator: 0.011408; Generator: 0.070129,\n",
      "D(x): 0.896, D(G(z)): 0.106\n",
      "2019-04-10 01:19:27,183 root         INFO     Train Epoch: 34 [4096/8000 (51%)]\tTotal Loss: 1.050288\n",
      "Reconstruction: 0.649730, Regularization: 0.317571, Discriminator: 0.012640; Generator: 0.070347,\n",
      "D(x): 0.912, D(G(z)): 0.105\n",
      "2019-04-10 01:19:27,283 root         INFO     Train Epoch: 34 [4608/8000 (58%)]\tTotal Loss: 1.008668\n",
      "Reconstruction: 0.628595, Regularization: 0.290302, Discriminator: 0.019528; Generator: 0.070243,\n",
      "D(x): 0.863, D(G(z)): 0.106\n",
      "2019-04-10 01:19:27,384 root         INFO     Train Epoch: 34 [5120/8000 (64%)]\tTotal Loss: 0.805862\n",
      "Reconstruction: 0.486006, Regularization: 0.244630, Discriminator: 0.005174; Generator: 0.070051,\n",
      "D(x): 0.957, D(G(z)): 0.106\n",
      "2019-04-10 01:19:27,484 root         INFO     Train Epoch: 34 [5632/8000 (70%)]\tTotal Loss: 0.425248\n",
      "Reconstruction: 0.228240, Regularization: 0.105540, Discriminator: 0.021248; Generator: 0.070222,\n",
      "D(x): 0.819, D(G(z)): 0.106\n",
      "2019-04-10 01:19:27,584 root         INFO     Train Epoch: 34 [6144/8000 (77%)]\tTotal Loss: 0.926893\n",
      "Reconstruction: 0.573468, Regularization: 0.274737, Discriminator: 0.008445; Generator: 0.070243,\n",
      "D(x): 0.907, D(G(z)): 0.106\n",
      "2019-04-10 01:19:27,685 root         INFO     Train Epoch: 34 [6656/8000 (83%)]\tTotal Loss: 1.275952\n",
      "Reconstruction: 0.803443, Regularization: 0.397620, Discriminator: 0.004573; Generator: 0.070316,\n",
      "D(x): 0.969, D(G(z)): 0.105\n",
      "2019-04-10 01:19:27,785 root         INFO     Train Epoch: 34 [7168/8000 (90%)]\tTotal Loss: 1.360618\n",
      "Reconstruction: 0.835911, Regularization: 0.433408, Discriminator: 0.020910; Generator: 0.070389,\n",
      "D(x): 0.872, D(G(z)): 0.105\n",
      "2019-04-10 01:19:27,885 root         INFO     Train Epoch: 34 [7680/8000 (96%)]\tTotal Loss: 1.270935\n",
      "Reconstruction: 0.785896, Regularization: 0.392819, Discriminator: 0.021661; Generator: 0.070559,\n",
      "D(x): 0.846, D(G(z)): 0.105\n",
      "2019-04-10 01:19:27,960 root         INFO     ====> Epoch: 34 Average loss: 1.0282\n",
      "2019-04-10 01:19:27,986 root         INFO     Train Epoch: 35 [0/8000 (0%)]\tTotal Loss: 1.073599\n",
      "Reconstruction: 0.673707, Regularization: 0.320284, Discriminator: 0.009177; Generator: 0.070431,\n",
      "D(x): 0.906, D(G(z)): 0.105\n",
      "2019-04-10 01:19:28,088 root         INFO     Train Epoch: 35 [512/8000 (6%)]\tTotal Loss: 0.986759\n",
      "Reconstruction: 0.616826, Regularization: 0.293453, Discriminator: 0.006198; Generator: 0.070282,\n",
      "D(x): 0.947, D(G(z)): 0.106\n",
      "2019-04-10 01:19:28,188 root         INFO     Train Epoch: 35 [1024/8000 (13%)]\tTotal Loss: 2.224433\n",
      "Reconstruction: 1.804533, Regularization: 0.344826, Discriminator: 0.004918; Generator: 0.070156,\n",
      "D(x): 0.965, D(G(z)): 0.106\n",
      "2019-04-10 01:19:28,290 root         INFO     Train Epoch: 35 [1536/8000 (19%)]\tTotal Loss: 0.851495\n",
      "Reconstruction: 0.518543, Regularization: 0.248712, Discriminator: 0.014100; Generator: 0.070140,\n",
      "D(x): 0.844, D(G(z)): 0.106\n",
      "2019-04-10 01:19:28,391 root         INFO     Train Epoch: 35 [2048/8000 (26%)]\tTotal Loss: 1.260460\n",
      "Reconstruction: 0.785807, Regularization: 0.386632, Discriminator: 0.018058; Generator: 0.069963,\n",
      "D(x): 0.868, D(G(z)): 0.107\n",
      "2019-04-10 01:19:28,492 root         INFO     Train Epoch: 35 [2560/8000 (32%)]\tTotal Loss: 0.986486\n",
      "Reconstruction: 0.609347, Regularization: 0.291833, Discriminator: 0.015284; Generator: 0.070023,\n",
      "D(x): 0.896, D(G(z)): 0.106\n",
      "2019-04-10 01:19:28,593 root         INFO     Train Epoch: 35 [3072/8000 (38%)]\tTotal Loss: 0.790412\n",
      "Reconstruction: 0.473621, Regularization: 0.222429, Discriminator: 0.024150; Generator: 0.070213,\n",
      "D(x): 0.828, D(G(z)): 0.106\n",
      "2019-04-10 01:19:28,693 root         INFO     Train Epoch: 35 [3584/8000 (45%)]\tTotal Loss: 1.033302\n",
      "Reconstruction: 0.690019, Regularization: 0.248207, Discriminator: 0.024739; Generator: 0.070337,\n",
      "D(x): 0.770, D(G(z)): 0.105\n",
      "2019-04-10 01:19:28,792 root         INFO     Train Epoch: 35 [4096/8000 (51%)]\tTotal Loss: 1.288059\n",
      "Reconstruction: 0.792204, Regularization: 0.402812, Discriminator: 0.023154; Generator: 0.069890,\n",
      "D(x): 0.854, D(G(z)): 0.107\n",
      "2019-04-10 01:19:28,892 root         INFO     Train Epoch: 35 [4608/8000 (58%)]\tTotal Loss: 1.211261\n",
      "Reconstruction: 0.751528, Regularization: 0.385596, Discriminator: 0.004362; Generator: 0.069775,\n",
      "D(x): 0.977, D(G(z)): 0.107\n",
      "2019-04-10 01:19:28,991 root         INFO     Train Epoch: 35 [5120/8000 (64%)]\tTotal Loss: 1.063627\n",
      "Reconstruction: 0.666358, Regularization: 0.320270, Discriminator: 0.007319; Generator: 0.069679,\n",
      "D(x): 0.933, D(G(z)): 0.108\n",
      "2019-04-10 01:19:29,090 root         INFO     Train Epoch: 35 [5632/8000 (70%)]\tTotal Loss: 0.965140\n",
      "Reconstruction: 0.589947, Regularization: 0.293365, Discriminator: 0.012281; Generator: 0.069547,\n",
      "D(x): 0.879, D(G(z)): 0.108\n",
      "2019-04-10 01:19:29,190 root         INFO     Train Epoch: 35 [6144/8000 (77%)]\tTotal Loss: 0.661031\n",
      "Reconstruction: 0.392240, Regularization: 0.191339, Discriminator: 0.007844; Generator: 0.069609,\n",
      "D(x): 0.910, D(G(z)): 0.108\n",
      "2019-04-10 01:19:29,290 root         INFO     Train Epoch: 35 [6656/8000 (83%)]\tTotal Loss: 1.220979\n",
      "Reconstruction: 0.767542, Regularization: 0.376473, Discriminator: 0.007003; Generator: 0.069961,\n",
      "D(x): 0.928, D(G(z)): 0.107\n",
      "2019-04-10 01:19:29,390 root         INFO     Train Epoch: 35 [7168/8000 (90%)]\tTotal Loss: 1.259976\n",
      "Reconstruction: 0.788790, Regularization: 0.392030, Discriminator: 0.009266; Generator: 0.069890,\n",
      "D(x): 0.905, D(G(z)): 0.107\n",
      "2019-04-10 01:19:29,491 root         INFO     Train Epoch: 35 [7680/8000 (96%)]\tTotal Loss: 0.427686\n",
      "Reconstruction: 0.232927, Regularization: 0.108867, Discriminator: 0.016129; Generator: 0.069762,\n",
      "D(x): 0.814, D(G(z)): 0.107\n",
      "2019-04-10 01:19:29,568 root         INFO     ====> Epoch: 35 Average loss: 1.0559\n",
      "2019-04-10 01:19:29,595 root         INFO     Train Epoch: 36 [0/8000 (0%)]\tTotal Loss: 0.632564\n",
      "Reconstruction: 0.362155, Regularization: 0.183775, Discriminator: 0.016815; Generator: 0.069819,\n",
      "D(x): 0.812, D(G(z)): 0.107\n",
      "2019-04-10 01:19:29,696 root         INFO     Train Epoch: 36 [512/8000 (6%)]\tTotal Loss: 0.977528\n",
      "Reconstruction: 0.605691, Regularization: 0.287189, Discriminator: 0.014892; Generator: 0.069756,\n",
      "D(x): 0.854, D(G(z)): 0.107\n",
      "2019-04-10 01:19:29,797 root         INFO     Train Epoch: 36 [1024/8000 (13%)]\tTotal Loss: 1.013693\n",
      "Reconstruction: 0.610038, Regularization: 0.314523, Discriminator: 0.019760; Generator: 0.069372,\n",
      "D(x): 0.852, D(G(z)): 0.109\n",
      "2019-04-10 01:19:29,898 root         INFO     Train Epoch: 36 [1536/8000 (19%)]\tTotal Loss: 1.019749\n",
      "Reconstruction: 0.626817, Regularization: 0.311112, Discriminator: 0.012722; Generator: 0.069097,\n",
      "D(x): 0.864, D(G(z)): 0.110\n",
      "2019-04-10 01:19:29,999 root         INFO     Train Epoch: 36 [2048/8000 (26%)]\tTotal Loss: 0.983685\n",
      "Reconstruction: 0.625041, Regularization: 0.279128, Discriminator: 0.010378; Generator: 0.069139,\n",
      "D(x): 0.949, D(G(z)): 0.109\n",
      "2019-04-10 01:19:30,100 root         INFO     Train Epoch: 36 [2560/8000 (32%)]\tTotal Loss: 0.594664\n",
      "Reconstruction: 0.351429, Regularization: 0.166087, Discriminator: 0.007979; Generator: 0.069169,\n",
      "D(x): 0.933, D(G(z)): 0.109\n",
      "2019-04-10 01:19:30,200 root         INFO     Train Epoch: 36 [3072/8000 (38%)]\tTotal Loss: 0.869622\n",
      "Reconstruction: 0.523519, Regularization: 0.259205, Discriminator: 0.017844; Generator: 0.069054,\n",
      "D(x): 0.882, D(G(z)): 0.110\n",
      "2019-04-10 01:19:30,301 root         INFO     Train Epoch: 36 [3584/8000 (45%)]\tTotal Loss: 1.291935\n",
      "Reconstruction: 0.812542, Regularization: 0.404764, Discriminator: 0.005660; Generator: 0.068970,\n",
      "D(x): 0.952, D(G(z)): 0.110\n",
      "2019-04-10 01:19:30,402 root         INFO     Train Epoch: 36 [4096/8000 (51%)]\tTotal Loss: 0.816092\n",
      "Reconstruction: 0.497738, Regularization: 0.238798, Discriminator: 0.010230; Generator: 0.069325,\n",
      "D(x): 0.930, D(G(z)): 0.109\n",
      "2019-04-10 01:19:30,503 root         INFO     Train Epoch: 36 [4608/8000 (58%)]\tTotal Loss: 0.787250\n",
      "Reconstruction: 0.479466, Regularization: 0.233744, Discriminator: 0.004709; Generator: 0.069330,\n",
      "D(x): 0.970, D(G(z)): 0.109\n",
      "2019-04-10 01:19:30,602 root         INFO     Train Epoch: 36 [5120/8000 (64%)]\tTotal Loss: 0.664211\n",
      "Reconstruction: 0.394477, Regularization: 0.188737, Discriminator: 0.011403; Generator: 0.069595,\n",
      "D(x): 0.916, D(G(z)): 0.108\n",
      "2019-04-10 01:19:30,701 root         INFO     Train Epoch: 36 [5632/8000 (70%)]\tTotal Loss: 0.996906\n",
      "Reconstruction: 0.613468, Regularization: 0.303294, Discriminator: 0.010266; Generator: 0.069879,\n",
      "D(x): 0.918, D(G(z)): 0.107\n",
      "2019-04-10 01:19:30,800 root         INFO     Train Epoch: 36 [6144/8000 (77%)]\tTotal Loss: 1.167364\n",
      "Reconstruction: 0.723504, Regularization: 0.351959, Discriminator: 0.022128; Generator: 0.069773,\n",
      "D(x): 0.811, D(G(z)): 0.107\n",
      "2019-04-10 01:19:30,900 root         INFO     Train Epoch: 36 [6656/8000 (83%)]\tTotal Loss: 0.672704\n",
      "Reconstruction: 0.393451, Regularization: 0.197751, Discriminator: 0.012145; Generator: 0.069358,\n",
      "D(x): 0.891, D(G(z)): 0.109\n",
      "2019-04-10 01:19:31,001 root         INFO     Train Epoch: 36 [7168/8000 (90%)]\tTotal Loss: 1.308566\n",
      "Reconstruction: 0.813743, Regularization: 0.411650, Discriminator: 0.014180; Generator: 0.068994,\n",
      "D(x): 0.910, D(G(z)): 0.110\n",
      "2019-04-10 01:19:31,100 root         INFO     Train Epoch: 36 [7680/8000 (96%)]\tTotal Loss: 0.688993\n",
      "Reconstruction: 0.404749, Regularization: 0.198759, Discriminator: 0.016411; Generator: 0.069074,\n",
      "D(x): 0.913, D(G(z)): 0.110\n",
      "2019-04-10 01:19:31,175 root         INFO     ====> Epoch: 36 Average loss: 1.0270\n",
      "2019-04-10 01:19:31,201 root         INFO     Train Epoch: 37 [0/8000 (0%)]\tTotal Loss: 0.920278\n",
      "Reconstruction: 0.565736, Regularization: 0.279916, Discriminator: 0.005401; Generator: 0.069225,\n",
      "D(x): 0.969, D(G(z)): 0.109\n",
      "2019-04-10 01:19:31,301 root         INFO     Train Epoch: 37 [512/8000 (6%)]\tTotal Loss: 1.081938\n",
      "Reconstruction: 0.665305, Regularization: 0.337795, Discriminator: 0.009617; Generator: 0.069221,\n",
      "D(x): 0.925, D(G(z)): 0.109\n",
      "2019-04-10 01:19:31,400 root         INFO     Train Epoch: 37 [1024/8000 (13%)]\tTotal Loss: 1.258664\n",
      "Reconstruction: 0.790645, Regularization: 0.379633, Discriminator: 0.019107; Generator: 0.069280,\n",
      "D(x): 0.873, D(G(z)): 0.109\n",
      "2019-04-10 01:19:31,499 root         INFO     Train Epoch: 37 [1536/8000 (19%)]\tTotal Loss: 0.696738\n",
      "Reconstruction: 0.409132, Regularization: 0.200417, Discriminator: 0.018056; Generator: 0.069133,\n",
      "D(x): 0.884, D(G(z)): 0.109\n",
      "2019-04-10 01:19:31,598 root         INFO     Train Epoch: 37 [2048/8000 (26%)]\tTotal Loss: 1.286461\n",
      "Reconstruction: 0.797459, Regularization: 0.396370, Discriminator: 0.023517; Generator: 0.069115,\n",
      "D(x): 0.844, D(G(z)): 0.110\n",
      "2019-04-10 01:19:31,698 root         INFO     Train Epoch: 37 [2560/8000 (32%)]\tTotal Loss: 0.855695\n",
      "Reconstruction: 0.508998, Regularization: 0.256123, Discriminator: 0.021702; Generator: 0.068871,\n",
      "D(x): 0.868, D(G(z)): 0.110\n",
      "2019-04-10 01:19:31,797 root         INFO     Train Epoch: 37 [3072/8000 (38%)]\tTotal Loss: 0.890202\n",
      "Reconstruction: 0.544967, Regularization: 0.271958, Discriminator: 0.004564; Generator: 0.068713,\n",
      "D(x): 0.976, D(G(z)): 0.111\n",
      "2019-04-10 01:19:31,896 root         INFO     Train Epoch: 37 [3584/8000 (45%)]\tTotal Loss: 0.719795\n",
      "Reconstruction: 0.422060, Regularization: 0.211483, Discriminator: 0.017424; Generator: 0.068827,\n",
      "D(x): 0.829, D(G(z)): 0.111\n",
      "2019-04-10 01:19:31,996 root         INFO     Train Epoch: 37 [4096/8000 (51%)]\tTotal Loss: 1.153967\n",
      "Reconstruction: 0.712117, Regularization: 0.358060, Discriminator: 0.015084; Generator: 0.068706,\n",
      "D(x): 0.945, D(G(z)): 0.111\n",
      "2019-04-10 01:19:32,095 root         INFO     Train Epoch: 37 [4608/8000 (58%)]\tTotal Loss: 0.442319\n",
      "Reconstruction: 0.244469, Regularization: 0.110589, Discriminator: 0.018664; Generator: 0.068597,\n",
      "D(x): 0.874, D(G(z)): 0.111\n",
      "2019-04-10 01:19:32,193 root         INFO     Train Epoch: 37 [5120/8000 (64%)]\tTotal Loss: 1.405676\n",
      "Reconstruction: 0.880730, Regularization: 0.448133, Discriminator: 0.008430; Generator: 0.068383,\n",
      "D(x): 0.923, D(G(z)): 0.112\n",
      "2019-04-10 01:19:32,293 root         INFO     Train Epoch: 37 [5632/8000 (70%)]\tTotal Loss: 1.542011\n",
      "Reconstruction: 0.975477, Regularization: 0.491044, Discriminator: 0.007121; Generator: 0.068368,\n",
      "D(x): 0.961, D(G(z)): 0.112\n",
      "2019-04-10 01:19:32,392 root         INFO     Train Epoch: 37 [6144/8000 (77%)]\tTotal Loss: 0.708993\n",
      "Reconstruction: 0.424759, Regularization: 0.205099, Discriminator: 0.010784; Generator: 0.068351,\n",
      "D(x): 0.900, D(G(z)): 0.112\n",
      "2019-04-10 01:19:32,491 root         INFO     Train Epoch: 37 [6656/8000 (83%)]\tTotal Loss: 0.817094\n",
      "Reconstruction: 0.504436, Regularization: 0.227747, Discriminator: 0.016773; Generator: 0.068139,\n",
      "D(x): 0.817, D(G(z)): 0.113\n",
      "2019-04-10 01:19:32,590 root         INFO     Train Epoch: 37 [7168/8000 (90%)]\tTotal Loss: 0.665715\n",
      "Reconstruction: 0.385802, Regularization: 0.192578, Discriminator: 0.019217; Generator: 0.068118,\n",
      "D(x): 0.905, D(G(z)): 0.113\n",
      "2019-04-10 01:19:32,689 root         INFO     Train Epoch: 37 [7680/8000 (96%)]\tTotal Loss: 0.693006\n",
      "Reconstruction: 0.408553, Regularization: 0.200038, Discriminator: 0.016177; Generator: 0.068238,\n",
      "D(x): 0.920, D(G(z)): 0.113\n",
      "2019-04-10 01:19:32,763 root         INFO     ====> Epoch: 37 Average loss: 1.2440\n",
      "2019-04-10 01:19:32,790 root         INFO     Train Epoch: 38 [0/8000 (0%)]\tTotal Loss: 0.692000\n",
      "Reconstruction: 0.408599, Regularization: 0.210200, Discriminator: 0.004725; Generator: 0.068476,\n",
      "D(x): 0.971, D(G(z)): 0.112\n",
      "2019-04-10 01:19:32,889 root         INFO     Train Epoch: 38 [512/8000 (6%)]\tTotal Loss: 0.704546\n",
      "Reconstruction: 0.423966, Regularization: 0.204114, Discriminator: 0.008210; Generator: 0.068257,\n",
      "D(x): 0.912, D(G(z)): 0.113\n",
      "2019-04-10 01:19:32,988 root         INFO     Train Epoch: 38 [1024/8000 (13%)]\tTotal Loss: 0.769604\n",
      "Reconstruction: 0.456867, Regularization: 0.229910, Discriminator: 0.014624; Generator: 0.068202,\n",
      "D(x): 0.898, D(G(z)): 0.113\n",
      "2019-04-10 01:19:33,088 root         INFO     Train Epoch: 38 [1536/8000 (19%)]\tTotal Loss: 1.017672\n",
      "Reconstruction: 0.643826, Regularization: 0.297098, Discriminator: 0.008259; Generator: 0.068489,\n",
      "D(x): 0.916, D(G(z)): 0.112\n",
      "2019-04-10 01:19:33,187 root         INFO     Train Epoch: 38 [2048/8000 (26%)]\tTotal Loss: 1.002593\n",
      "Reconstruction: 0.621369, Regularization: 0.302200, Discriminator: 0.010225; Generator: 0.068799,\n",
      "D(x): 0.920, D(G(z)): 0.111\n",
      "2019-04-10 01:19:33,287 root         INFO     Train Epoch: 38 [2560/8000 (32%)]\tTotal Loss: 0.784054\n",
      "Reconstruction: 0.461738, Regularization: 0.245303, Discriminator: 0.008192; Generator: 0.068821,\n",
      "D(x): 0.934, D(G(z)): 0.111\n",
      "2019-04-10 01:19:33,386 root         INFO     Train Epoch: 38 [3072/8000 (38%)]\tTotal Loss: 1.216452\n",
      "Reconstruction: 0.748049, Regularization: 0.381649, Discriminator: 0.017764; Generator: 0.068991,\n",
      "D(x): 0.835, D(G(z)): 0.110\n",
      "2019-04-10 01:19:33,486 root         INFO     Train Epoch: 38 [3584/8000 (45%)]\tTotal Loss: 0.834238\n",
      "Reconstruction: 0.499342, Regularization: 0.257105, Discriminator: 0.008972; Generator: 0.068819,\n",
      "D(x): 0.922, D(G(z)): 0.111\n",
      "2019-04-10 01:19:33,585 root         INFO     Train Epoch: 38 [4096/8000 (51%)]\tTotal Loss: 0.897752\n",
      "Reconstruction: 0.555014, Regularization: 0.260282, Discriminator: 0.013848; Generator: 0.068608,\n",
      "D(x): 0.867, D(G(z)): 0.111\n",
      "2019-04-10 01:19:33,684 root         INFO     Train Epoch: 38 [4608/8000 (58%)]\tTotal Loss: 0.763301\n",
      "Reconstruction: 0.462598, Regularization: 0.218385, Discriminator: 0.013935; Generator: 0.068383,\n",
      "D(x): 0.846, D(G(z)): 0.112\n",
      "2019-04-10 01:19:33,783 root         INFO     Train Epoch: 38 [5120/8000 (64%)]\tTotal Loss: 0.847996\n",
      "Reconstruction: 0.511924, Regularization: 0.257141, Discriminator: 0.010703; Generator: 0.068227,\n",
      "D(x): 0.911, D(G(z)): 0.113\n",
      "2019-04-10 01:19:33,883 root         INFO     Train Epoch: 38 [5632/8000 (70%)]\tTotal Loss: 0.734472\n",
      "Reconstruction: 0.422040, Regularization: 0.226902, Discriminator: 0.017660; Generator: 0.067870,\n",
      "D(x): 0.851, D(G(z)): 0.114\n",
      "2019-04-10 01:19:33,982 root         INFO     Train Epoch: 38 [6144/8000 (77%)]\tTotal Loss: 1.024909\n",
      "Reconstruction: 0.625088, Regularization: 0.321483, Discriminator: 0.010886; Generator: 0.067451,\n",
      "D(x): 0.929, D(G(z)): 0.116\n",
      "2019-04-10 01:19:34,081 root         INFO     Train Epoch: 38 [6656/8000 (83%)]\tTotal Loss: 0.929882\n",
      "Reconstruction: 0.560815, Regularization: 0.290897, Discriminator: 0.011065; Generator: 0.067106,\n",
      "D(x): 0.903, D(G(z)): 0.117\n",
      "2019-04-10 01:19:34,181 root         INFO     Train Epoch: 38 [7168/8000 (90%)]\tTotal Loss: 0.986343\n",
      "Reconstruction: 0.604155, Regularization: 0.308857, Discriminator: 0.006060; Generator: 0.067271,\n",
      "D(x): 0.956, D(G(z)): 0.116\n",
      "2019-04-10 01:19:34,280 root         INFO     Train Epoch: 38 [7680/8000 (96%)]\tTotal Loss: 0.575233\n",
      "Reconstruction: 0.331127, Regularization: 0.164289, Discriminator: 0.012315; Generator: 0.067502,\n",
      "D(x): 0.913, D(G(z)): 0.115\n",
      "2019-04-10 01:19:34,355 root         INFO     ====> Epoch: 38 Average loss: 1.0052\n",
      "2019-04-10 01:19:34,381 root         INFO     Train Epoch: 39 [0/8000 (0%)]\tTotal Loss: 1.082844\n",
      "Reconstruction: 0.663260, Regularization: 0.340645, Discriminator: 0.011814; Generator: 0.067125,\n",
      "D(x): 0.900, D(G(z)): 0.117\n",
      "2019-04-10 01:19:34,482 root         INFO     Train Epoch: 39 [512/8000 (6%)]\tTotal Loss: 1.042670\n",
      "Reconstruction: 0.638763, Regularization: 0.324535, Discriminator: 0.012266; Generator: 0.067106,\n",
      "D(x): 0.933, D(G(z)): 0.117\n",
      "2019-04-10 01:19:34,581 root         INFO     Train Epoch: 39 [1024/8000 (13%)]\tTotal Loss: 0.758388\n",
      "Reconstruction: 0.449214, Regularization: 0.228322, Discriminator: 0.013799; Generator: 0.067053,\n",
      "D(x): 0.900, D(G(z)): 0.117\n",
      "2019-04-10 01:19:34,681 root         INFO     Train Epoch: 39 [1536/8000 (19%)]\tTotal Loss: 0.736794\n",
      "Reconstruction: 0.436453, Regularization: 0.224307, Discriminator: 0.009064; Generator: 0.066970,\n",
      "D(x): 0.921, D(G(z)): 0.117\n",
      "2019-04-10 01:19:34,781 root         INFO     Train Epoch: 39 [2048/8000 (26%)]\tTotal Loss: 1.156040\n",
      "Reconstruction: 0.730675, Regularization: 0.347905, Discriminator: 0.010468; Generator: 0.066992,\n",
      "D(x): 0.923, D(G(z)): 0.117\n",
      "2019-04-10 01:19:34,881 root         INFO     Train Epoch: 39 [2560/8000 (32%)]\tTotal Loss: 0.661688\n",
      "Reconstruction: 0.392850, Regularization: 0.188179, Discriminator: 0.013664; Generator: 0.066994,\n",
      "D(x): 0.897, D(G(z)): 0.117\n",
      "2019-04-10 01:19:34,982 root         INFO     Train Epoch: 39 [3072/8000 (38%)]\tTotal Loss: 0.758331\n",
      "Reconstruction: 0.459642, Regularization: 0.220036, Discriminator: 0.011585; Generator: 0.067067,\n",
      "D(x): 0.914, D(G(z)): 0.117\n",
      "2019-04-10 01:19:35,082 root         INFO     Train Epoch: 39 [3584/8000 (45%)]\tTotal Loss: 0.872321\n",
      "Reconstruction: 0.548782, Regularization: 0.249194, Discriminator: 0.007419; Generator: 0.066926,\n",
      "D(x): 0.955, D(G(z)): 0.117\n",
      "2019-04-10 01:19:35,182 root         INFO     Train Epoch: 39 [4096/8000 (51%)]\tTotal Loss: 0.805467\n",
      "Reconstruction: 0.483822, Regularization: 0.246772, Discriminator: 0.007963; Generator: 0.066909,\n",
      "D(x): 0.936, D(G(z)): 0.118\n",
      "2019-04-10 01:19:35,282 root         INFO     Train Epoch: 39 [4608/8000 (58%)]\tTotal Loss: 1.161838\n",
      "Reconstruction: 0.678552, Regularization: 0.386506, Discriminator: 0.029867; Generator: 0.066913,\n",
      "D(x): 0.793, D(G(z)): 0.118\n",
      "2019-04-10 01:19:35,382 root         INFO     Train Epoch: 39 [5120/8000 (64%)]\tTotal Loss: 1.015402\n",
      "Reconstruction: 0.622136, Regularization: 0.302094, Discriminator: 0.024449; Generator: 0.066723,\n",
      "D(x): 0.853, D(G(z)): 0.118\n",
      "2019-04-10 01:19:35,481 root         INFO     Train Epoch: 39 [5632/8000 (70%)]\tTotal Loss: 1.049641\n",
      "Reconstruction: 0.650105, Regularization: 0.328099, Discriminator: 0.004971; Generator: 0.066467,\n",
      "D(x): 0.971, D(G(z)): 0.119\n",
      "2019-04-10 01:19:35,581 root         INFO     Train Epoch: 39 [6144/8000 (77%)]\tTotal Loss: 1.207193\n",
      "Reconstruction: 0.739641, Regularization: 0.381010, Discriminator: 0.020048; Generator: 0.066495,\n",
      "D(x): 0.866, D(G(z)): 0.119\n",
      "2019-04-10 01:19:35,681 root         INFO     Train Epoch: 39 [6656/8000 (83%)]\tTotal Loss: 0.845626\n",
      "Reconstruction: 0.504031, Regularization: 0.265632, Discriminator: 0.009427; Generator: 0.066536,\n",
      "D(x): 0.900, D(G(z)): 0.119\n",
      "2019-04-10 01:19:35,781 root         INFO     Train Epoch: 39 [7168/8000 (90%)]\tTotal Loss: 0.841742\n",
      "Reconstruction: 0.497380, Regularization: 0.262250, Discriminator: 0.015633; Generator: 0.066480,\n",
      "D(x): 0.899, D(G(z)): 0.119\n",
      "2019-04-10 01:19:35,881 root         INFO     Train Epoch: 39 [7680/8000 (96%)]\tTotal Loss: 1.027773\n",
      "Reconstruction: 0.620852, Regularization: 0.329364, Discriminator: 0.011370; Generator: 0.066189,\n",
      "D(x): 0.914, D(G(z)): 0.120\n",
      "2019-04-10 01:19:35,956 root         INFO     ====> Epoch: 39 Average loss: 1.0333\n",
      "2019-04-10 01:19:35,982 root         INFO     Train Epoch: 40 [0/8000 (0%)]\tTotal Loss: 0.851478\n",
      "Reconstruction: 0.516785, Regularization: 0.259279, Discriminator: 0.009159; Generator: 0.066256,\n",
      "D(x): 0.931, D(G(z)): 0.120\n",
      "2019-04-10 01:19:36,081 root         INFO     Train Epoch: 40 [512/8000 (6%)]\tTotal Loss: 0.905915\n",
      "Reconstruction: 0.546098, Regularization: 0.278169, Discriminator: 0.015201; Generator: 0.066448,\n",
      "D(x): 0.848, D(G(z)): 0.119\n",
      "2019-04-10 01:19:36,180 root         INFO     Train Epoch: 40 [1024/8000 (13%)]\tTotal Loss: 0.790010\n",
      "Reconstruction: 0.464467, Regularization: 0.241041, Discriminator: 0.018258; Generator: 0.066244,\n",
      "D(x): 0.835, D(G(z)): 0.120\n",
      "2019-04-10 01:19:36,280 root         INFO     Train Epoch: 40 [1536/8000 (19%)]\tTotal Loss: 1.108725\n",
      "Reconstruction: 0.681706, Regularization: 0.350345, Discriminator: 0.010449; Generator: 0.066226,\n",
      "D(x): 0.918, D(G(z)): 0.120\n",
      "2019-04-10 01:19:36,380 root         INFO     Train Epoch: 40 [2048/8000 (26%)]\tTotal Loss: 1.600883\n",
      "Reconstruction: 0.967977, Regularization: 0.551107, Discriminator: 0.015626; Generator: 0.066173,\n",
      "D(x): 0.855, D(G(z)): 0.120\n",
      "2019-04-10 01:19:36,479 root         INFO     Train Epoch: 40 [2560/8000 (32%)]\tTotal Loss: 0.876057\n",
      "Reconstruction: 0.524669, Regularization: 0.266594, Discriminator: 0.018475; Generator: 0.066319,\n",
      "D(x): 0.843, D(G(z)): 0.120\n",
      "2019-04-10 01:19:36,578 root         INFO     Train Epoch: 40 [3072/8000 (38%)]\tTotal Loss: 0.670614\n",
      "Reconstruction: 0.408541, Regularization: 0.188605, Discriminator: 0.007449; Generator: 0.066018,\n",
      "D(x): 0.917, D(G(z)): 0.121\n",
      "2019-04-10 01:19:36,678 root         INFO     Train Epoch: 40 [3584/8000 (45%)]\tTotal Loss: 1.419874\n",
      "Reconstruction: 1.000378, Regularization: 0.339437, Discriminator: 0.014300; Generator: 0.065759,\n",
      "D(x): 0.893, D(G(z)): 0.122\n",
      "2019-04-10 01:19:36,777 root         INFO     Train Epoch: 40 [4096/8000 (51%)]\tTotal Loss: 0.791621\n",
      "Reconstruction: 0.475239, Regularization: 0.236103, Discriminator: 0.014542; Generator: 0.065737,\n",
      "D(x): 0.890, D(G(z)): 0.122\n",
      "2019-04-10 01:19:36,877 root         INFO     Train Epoch: 40 [4608/8000 (58%)]\tTotal Loss: 0.652174\n",
      "Reconstruction: 0.377077, Regularization: 0.191044, Discriminator: 0.018336; Generator: 0.065717,\n",
      "D(x): 0.870, D(G(z)): 0.122\n",
      "2019-04-10 01:19:36,976 root         INFO     Train Epoch: 40 [5120/8000 (64%)]\tTotal Loss: 1.005231\n",
      "Reconstruction: 0.613982, Regularization: 0.308526, Discriminator: 0.016743; Generator: 0.065981,\n",
      "D(x): 0.896, D(G(z)): 0.121\n",
      "2019-04-10 01:19:37,076 root         INFO     Train Epoch: 40 [5632/8000 (70%)]\tTotal Loss: 1.229131\n",
      "Reconstruction: 0.762680, Regularization: 0.395781, Discriminator: 0.004758; Generator: 0.065912,\n",
      "D(x): 0.980, D(G(z)): 0.121\n",
      "2019-04-10 01:19:37,177 root         INFO     Train Epoch: 40 [6144/8000 (77%)]\tTotal Loss: 0.985862\n",
      "Reconstruction: 0.598653, Regularization: 0.308826, Discriminator: 0.012439; Generator: 0.065945,\n",
      "D(x): 0.881, D(G(z)): 0.121\n",
      "2019-04-10 01:19:37,279 root         INFO     Train Epoch: 40 [6656/8000 (83%)]\tTotal Loss: 0.706398\n",
      "Reconstruction: 0.412108, Regularization: 0.204163, Discriminator: 0.024337; Generator: 0.065790,\n",
      "D(x): 0.813, D(G(z)): 0.122\n",
      "2019-04-10 01:19:37,381 root         INFO     Train Epoch: 40 [7168/8000 (90%)]\tTotal Loss: 1.046834\n",
      "Reconstruction: 0.637971, Regularization: 0.325025, Discriminator: 0.018435; Generator: 0.065403,\n",
      "D(x): 0.887, D(G(z)): 0.123\n",
      "2019-04-10 01:19:37,483 root         INFO     Train Epoch: 40 [7680/8000 (96%)]\tTotal Loss: 0.860698\n",
      "Reconstruction: 0.521812, Regularization: 0.257880, Discriminator: 0.015648; Generator: 0.065358,\n",
      "D(x): 0.846, D(G(z)): 0.124\n",
      "2019-04-10 01:19:37,559 root         INFO     ====> Epoch: 40 Average loss: 0.9909\n",
      "2019-04-10 01:19:37,586 root         INFO     Train Epoch: 41 [0/8000 (0%)]\tTotal Loss: 1.049042\n",
      "Reconstruction: 0.650367, Regularization: 0.326052, Discriminator: 0.007262; Generator: 0.065361,\n",
      "D(x): 0.937, D(G(z)): 0.123\n",
      "2019-04-10 01:19:37,689 root         INFO     Train Epoch: 41 [512/8000 (6%)]\tTotal Loss: 1.299425\n",
      "Reconstruction: 0.803637, Regularization: 0.416106, Discriminator: 0.014353; Generator: 0.065329,\n",
      "D(x): 0.839, D(G(z)): 0.124\n",
      "2019-04-10 01:19:37,791 root         INFO     Train Epoch: 41 [1024/8000 (13%)]\tTotal Loss: 0.860091\n",
      "Reconstruction: 0.518790, Regularization: 0.267485, Discriminator: 0.008898; Generator: 0.064917,\n",
      "D(x): 0.928, D(G(z)): 0.125\n",
      "2019-04-10 01:19:37,892 root         INFO     Train Epoch: 41 [1536/8000 (19%)]\tTotal Loss: 0.985890\n",
      "Reconstruction: 0.614724, Regularization: 0.298809, Discriminator: 0.007388; Generator: 0.064969,\n",
      "D(x): 0.944, D(G(z)): 0.125\n",
      "2019-04-10 01:19:37,991 root         INFO     Train Epoch: 41 [2048/8000 (26%)]\tTotal Loss: 1.083987\n",
      "Reconstruction: 0.685869, Regularization: 0.327929, Discriminator: 0.005160; Generator: 0.065030,\n",
      "D(x): 0.973, D(G(z)): 0.125\n",
      "2019-04-10 01:19:38,089 root         INFO     Train Epoch: 41 [2560/8000 (32%)]\tTotal Loss: 1.265942\n",
      "Reconstruction: 0.802469, Regularization: 0.387646, Discriminator: 0.010930; Generator: 0.064897,\n",
      "D(x): 0.926, D(G(z)): 0.125\n",
      "2019-04-10 01:19:38,188 root         INFO     Train Epoch: 41 [3072/8000 (38%)]\tTotal Loss: 0.786772\n",
      "Reconstruction: 0.458988, Regularization: 0.245520, Discriminator: 0.017276; Generator: 0.064988,\n",
      "D(x): 0.855, D(G(z)): 0.125\n",
      "2019-04-10 01:19:38,290 root         INFO     Train Epoch: 41 [3584/8000 (45%)]\tTotal Loss: 0.961468\n",
      "Reconstruction: 0.588364, Regularization: 0.302825, Discriminator: 0.005477; Generator: 0.064802,\n",
      "D(x): 0.966, D(G(z)): 0.126\n",
      "2019-04-10 01:19:38,391 root         INFO     Train Epoch: 41 [4096/8000 (51%)]\tTotal Loss: 1.057267\n",
      "Reconstruction: 0.641243, Regularization: 0.333969, Discriminator: 0.017263; Generator: 0.064792,\n",
      "D(x): 0.876, D(G(z)): 0.126\n",
      "2019-04-10 01:19:38,492 root         INFO     Train Epoch: 41 [4608/8000 (58%)]\tTotal Loss: 0.785135\n",
      "Reconstruction: 0.472002, Regularization: 0.227640, Discriminator: 0.020579; Generator: 0.064913,\n",
      "D(x): 0.831, D(G(z)): 0.125\n",
      "2019-04-10 01:19:38,592 root         INFO     Train Epoch: 41 [5120/8000 (64%)]\tTotal Loss: 1.341238\n",
      "Reconstruction: 0.838178, Regularization: 0.422405, Discriminator: 0.015532; Generator: 0.065122,\n",
      "D(x): 0.882, D(G(z)): 0.124\n",
      "2019-04-10 01:19:38,692 root         INFO     Train Epoch: 41 [5632/8000 (70%)]\tTotal Loss: 0.992975\n",
      "Reconstruction: 0.595769, Regularization: 0.313289, Discriminator: 0.019219; Generator: 0.064698,\n",
      "D(x): 0.870, D(G(z)): 0.126\n",
      "2019-04-10 01:19:38,790 root         INFO     Train Epoch: 41 [6144/8000 (77%)]\tTotal Loss: 0.999156\n",
      "Reconstruction: 0.603776, Regularization: 0.319834, Discriminator: 0.010866; Generator: 0.064679,\n",
      "D(x): 0.924, D(G(z)): 0.126\n",
      "2019-04-10 01:19:38,888 root         INFO     Train Epoch: 41 [6656/8000 (83%)]\tTotal Loss: 0.943486\n",
      "Reconstruction: 0.578517, Regularization: 0.271917, Discriminator: 0.028395; Generator: 0.064656,\n",
      "D(x): 0.835, D(G(z)): 0.126\n",
      "2019-04-10 01:19:38,984 root         INFO     Train Epoch: 41 [7168/8000 (90%)]\tTotal Loss: 1.236550\n",
      "Reconstruction: 0.755672, Regularization: 0.405277, Discriminator: 0.010811; Generator: 0.064791,\n",
      "D(x): 0.893, D(G(z)): 0.126\n",
      "2019-04-10 01:19:39,082 root         INFO     Train Epoch: 41 [7680/8000 (96%)]\tTotal Loss: 1.076637\n",
      "Reconstruction: 0.654767, Regularization: 0.335693, Discriminator: 0.021530; Generator: 0.064647,\n",
      "D(x): 0.816, D(G(z)): 0.126\n",
      "2019-04-10 01:19:39,155 root         INFO     ====> Epoch: 41 Average loss: 0.9962\n",
      "2019-04-10 01:19:39,182 root         INFO     Train Epoch: 42 [0/8000 (0%)]\tTotal Loss: 0.978387\n",
      "Reconstruction: 0.614032, Regularization: 0.289461, Discriminator: 0.010435; Generator: 0.064459,\n",
      "D(x): 0.909, D(G(z)): 0.127\n",
      "2019-04-10 01:19:39,282 root         INFO     Train Epoch: 42 [512/8000 (6%)]\tTotal Loss: 1.165114\n",
      "Reconstruction: 0.715433, Regularization: 0.370245, Discriminator: 0.014968; Generator: 0.064469,\n",
      "D(x): 0.897, D(G(z)): 0.127\n",
      "2019-04-10 01:19:39,382 root         INFO     Train Epoch: 42 [1024/8000 (13%)]\tTotal Loss: 0.604723\n",
      "Reconstruction: 0.344606, Regularization: 0.183443, Discriminator: 0.011991; Generator: 0.064683,\n",
      "D(x): 0.897, D(G(z)): 0.126\n",
      "2019-04-10 01:19:39,483 root         INFO     Train Epoch: 42 [1536/8000 (19%)]\tTotal Loss: 1.368929\n",
      "Reconstruction: 0.840195, Regularization: 0.440265, Discriminator: 0.023943; Generator: 0.064525,\n",
      "D(x): 0.815, D(G(z)): 0.127\n",
      "2019-04-10 01:19:39,582 root         INFO     Train Epoch: 42 [2048/8000 (26%)]\tTotal Loss: 0.744905\n",
      "Reconstruction: 0.440570, Regularization: 0.223298, Discriminator: 0.016520; Generator: 0.064517,\n",
      "D(x): 0.859, D(G(z)): 0.127\n",
      "2019-04-10 01:19:39,679 root         INFO     Train Epoch: 42 [2560/8000 (32%)]\tTotal Loss: 0.684212\n",
      "Reconstruction: 0.400612, Regularization: 0.196367, Discriminator: 0.022923; Generator: 0.064311,\n",
      "D(x): 0.800, D(G(z)): 0.128\n",
      "2019-04-10 01:19:39,775 root         INFO     Train Epoch: 42 [3072/8000 (38%)]\tTotal Loss: 0.965020\n",
      "Reconstruction: 0.594641, Regularization: 0.300052, Discriminator: 0.006005; Generator: 0.064321,\n",
      "D(x): 0.962, D(G(z)): 0.128\n",
      "2019-04-10 01:19:39,871 root         INFO     Train Epoch: 42 [3584/8000 (45%)]\tTotal Loss: 0.748924\n",
      "Reconstruction: 0.436481, Regularization: 0.221902, Discriminator: 0.026223; Generator: 0.064318,\n",
      "D(x): 0.788, D(G(z)): 0.128\n",
      "2019-04-10 01:19:39,968 root         INFO     Train Epoch: 42 [4096/8000 (51%)]\tTotal Loss: 1.615142\n",
      "Reconstruction: 0.993268, Regularization: 0.530182, Discriminator: 0.027624; Generator: 0.064069,\n",
      "D(x): 0.823, D(G(z)): 0.129\n",
      "2019-04-10 01:19:40,064 root         INFO     Train Epoch: 42 [4608/8000 (58%)]\tTotal Loss: 0.679898\n",
      "Reconstruction: 0.391353, Regularization: 0.212648, Discriminator: 0.012285; Generator: 0.063611,\n",
      "D(x): 0.908, D(G(z)): 0.131\n",
      "2019-04-10 01:19:40,159 root         INFO     Train Epoch: 42 [5120/8000 (64%)]\tTotal Loss: 1.009841\n",
      "Reconstruction: 0.611544, Regularization: 0.318208, Discriminator: 0.016525; Generator: 0.063564,\n",
      "D(x): 0.839, D(G(z)): 0.131\n",
      "2019-04-10 01:19:40,256 root         INFO     Train Epoch: 42 [5632/8000 (70%)]\tTotal Loss: 0.736661\n",
      "Reconstruction: 0.429973, Regularization: 0.214676, Discriminator: 0.028664; Generator: 0.063348,\n",
      "D(x): 0.862, D(G(z)): 0.132\n",
      "2019-04-10 01:19:40,354 root         INFO     Train Epoch: 42 [6144/8000 (77%)]\tTotal Loss: 0.723192\n",
      "Reconstruction: 0.429746, Regularization: 0.221284, Discriminator: 0.008767; Generator: 0.063395,\n",
      "D(x): 0.912, D(G(z)): 0.132\n",
      "2019-04-10 01:19:40,452 root         INFO     Train Epoch: 42 [6656/8000 (83%)]\tTotal Loss: 0.650764\n",
      "Reconstruction: 0.383488, Regularization: 0.191240, Discriminator: 0.012684; Generator: 0.063351,\n",
      "D(x): 0.881, D(G(z)): 0.132\n",
      "2019-04-10 01:19:40,549 root         INFO     Train Epoch: 42 [7168/8000 (90%)]\tTotal Loss: 1.227414\n",
      "Reconstruction: 0.851131, Regularization: 0.296288, Discriminator: 0.016796; Generator: 0.063199,\n",
      "D(x): 0.862, D(G(z)): 0.132\n",
      "2019-04-10 01:19:40,646 root         INFO     Train Epoch: 42 [7680/8000 (96%)]\tTotal Loss: 1.055467\n",
      "Reconstruction: 0.653328, Regularization: 0.325875, Discriminator: 0.012946; Generator: 0.063319,\n",
      "D(x): 0.876, D(G(z)): 0.132\n",
      "2019-04-10 01:19:40,719 root         INFO     ====> Epoch: 42 Average loss: 1.0074\n",
      "2019-04-10 01:19:40,746 root         INFO     Train Epoch: 43 [0/8000 (0%)]\tTotal Loss: 0.669808\n",
      "Reconstruction: 0.399959, Regularization: 0.182950, Discriminator: 0.023598; Generator: 0.063301,\n",
      "D(x): 0.852, D(G(z)): 0.132\n",
      "2019-04-10 01:19:40,847 root         INFO     Train Epoch: 43 [512/8000 (6%)]\tTotal Loss: 0.774722\n",
      "Reconstruction: 0.459149, Regularization: 0.244107, Discriminator: 0.008014; Generator: 0.063453,\n",
      "D(x): 0.956, D(G(z)): 0.131\n",
      "2019-04-10 01:19:40,948 root         INFO     Train Epoch: 43 [1024/8000 (13%)]\tTotal Loss: 0.881769\n",
      "Reconstruction: 0.531373, Regularization: 0.266242, Discriminator: 0.020788; Generator: 0.063366,\n",
      "D(x): 0.852, D(G(z)): 0.132\n",
      "2019-04-10 01:19:41,049 root         INFO     Train Epoch: 43 [1536/8000 (19%)]\tTotal Loss: 0.801059\n",
      "Reconstruction: 0.501634, Regularization: 0.225492, Discriminator: 0.010554; Generator: 0.063380,\n",
      "D(x): 0.918, D(G(z)): 0.132\n",
      "2019-04-10 01:19:41,150 root         INFO     Train Epoch: 43 [2048/8000 (26%)]\tTotal Loss: 0.892636\n",
      "Reconstruction: 0.539131, Regularization: 0.268607, Discriminator: 0.021622; Generator: 0.063276,\n",
      "D(x): 0.857, D(G(z)): 0.132\n",
      "2019-04-10 01:19:41,250 root         INFO     Train Epoch: 43 [2560/8000 (32%)]\tTotal Loss: 0.640845\n",
      "Reconstruction: 0.366858, Regularization: 0.191188, Discriminator: 0.019656; Generator: 0.063143,\n",
      "D(x): 0.818, D(G(z)): 0.133\n",
      "2019-04-10 01:19:41,352 root         INFO     Train Epoch: 43 [3072/8000 (38%)]\tTotal Loss: 0.807919\n",
      "Reconstruction: 0.482381, Regularization: 0.242218, Discriminator: 0.020205; Generator: 0.063115,\n",
      "D(x): 0.817, D(G(z)): 0.133\n",
      "2019-04-10 01:19:41,454 root         INFO     Train Epoch: 43 [3584/8000 (45%)]\tTotal Loss: 1.627606\n",
      "Reconstruction: 0.983910, Regularization: 0.568500, Discriminator: 0.012278; Generator: 0.062917,\n",
      "D(x): 0.899, D(G(z)): 0.134\n",
      "2019-04-10 01:19:41,555 root         INFO     Train Epoch: 43 [4096/8000 (51%)]\tTotal Loss: 0.993939\n",
      "Reconstruction: 0.626293, Regularization: 0.284126, Discriminator: 0.020735; Generator: 0.062785,\n",
      "D(x): 0.826, D(G(z)): 0.134\n",
      "2019-04-10 01:19:41,657 root         INFO     Train Epoch: 43 [4608/8000 (58%)]\tTotal Loss: 1.411805\n",
      "Reconstruction: 0.882638, Regularization: 0.457231, Discriminator: 0.008807; Generator: 0.063130,\n",
      "D(x): 0.917, D(G(z)): 0.133\n",
      "2019-04-10 01:19:41,758 root         INFO     Train Epoch: 43 [5120/8000 (64%)]\tTotal Loss: 0.521078\n",
      "Reconstruction: 0.286020, Regularization: 0.143560, Discriminator: 0.028370; Generator: 0.063127,\n",
      "D(x): 0.748, D(G(z)): 0.133\n",
      "2019-04-10 01:19:41,860 root         INFO     Train Epoch: 43 [5632/8000 (70%)]\tTotal Loss: 0.891329\n",
      "Reconstruction: 0.541633, Regularization: 0.274303, Discriminator: 0.012410; Generator: 0.062982,\n",
      "D(x): 0.913, D(G(z)): 0.133\n",
      "2019-04-10 01:19:41,961 root         INFO     Train Epoch: 43 [6144/8000 (77%)]\tTotal Loss: 0.821853\n",
      "Reconstruction: 0.491673, Regularization: 0.255739, Discriminator: 0.011481; Generator: 0.062960,\n",
      "D(x): 0.887, D(G(z)): 0.133\n",
      "2019-04-10 01:19:42,062 root         INFO     Train Epoch: 43 [6656/8000 (83%)]\tTotal Loss: 0.772821\n",
      "Reconstruction: 0.445903, Regularization: 0.223468, Discriminator: 0.040733; Generator: 0.062717,\n",
      "D(x): 0.710, D(G(z)): 0.134\n",
      "2019-04-10 01:19:42,164 root         INFO     Train Epoch: 43 [7168/8000 (90%)]\tTotal Loss: 1.053956\n",
      "Reconstruction: 0.658410, Regularization: 0.318945, Discriminator: 0.014266; Generator: 0.062334,\n",
      "D(x): 0.882, D(G(z)): 0.136\n",
      "2019-04-10 01:19:42,266 root         INFO     Train Epoch: 43 [7680/8000 (96%)]\tTotal Loss: 0.890060\n",
      "Reconstruction: 0.563227, Regularization: 0.247356, Discriminator: 0.017339; Generator: 0.062137,\n",
      "D(x): 0.865, D(G(z)): 0.137\n",
      "2019-04-10 01:19:42,341 root         INFO     ====> Epoch: 43 Average loss: 2.3668\n",
      "2019-04-10 01:19:42,367 root         INFO     Train Epoch: 44 [0/8000 (0%)]\tTotal Loss: 0.808434\n",
      "Reconstruction: 0.475583, Regularization: 0.258908, Discriminator: 0.011716; Generator: 0.062227,\n",
      "D(x): 0.923, D(G(z)): 0.137\n",
      "2019-04-10 01:19:42,470 root         INFO     Train Epoch: 44 [512/8000 (6%)]\tTotal Loss: 1.229640\n",
      "Reconstruction: 0.754485, Regularization: 0.395394, Discriminator: 0.017423; Generator: 0.062338,\n",
      "D(x): 0.908, D(G(z)): 0.136\n",
      "2019-04-10 01:19:42,572 root         INFO     Train Epoch: 44 [1024/8000 (13%)]\tTotal Loss: 0.740820\n",
      "Reconstruction: 0.392843, Regularization: 0.269502, Discriminator: 0.016070; Generator: 0.062406,\n",
      "D(x): 0.833, D(G(z)): 0.136\n",
      "2019-04-10 01:19:42,674 root         INFO     Train Epoch: 44 [1536/8000 (19%)]\tTotal Loss: 1.136456\n",
      "Reconstruction: 0.700570, Regularization: 0.363654, Discriminator: 0.010051; Generator: 0.062182,\n",
      "D(x): 0.914, D(G(z)): 0.137\n",
      "2019-04-10 01:19:42,776 root         INFO     Train Epoch: 44 [2048/8000 (26%)]\tTotal Loss: 0.732340\n",
      "Reconstruction: 0.435890, Regularization: 0.219867, Discriminator: 0.014415; Generator: 0.062167,\n",
      "D(x): 0.863, D(G(z)): 0.137\n",
      "2019-04-10 01:19:42,878 root         INFO     Train Epoch: 44 [2560/8000 (32%)]\tTotal Loss: 2.100649\n",
      "Reconstruction: 1.720081, Regularization: 0.307784, Discriminator: 0.010812; Generator: 0.061971,\n",
      "D(x): 0.908, D(G(z)): 0.138\n",
      "2019-04-10 01:19:42,979 root         INFO     Train Epoch: 44 [3072/8000 (38%)]\tTotal Loss: 0.852539\n",
      "Reconstruction: 0.505068, Regularization: 0.268995, Discriminator: 0.016694; Generator: 0.061782,\n",
      "D(x): 0.864, D(G(z)): 0.139\n",
      "2019-04-10 01:19:43,081 root         INFO     Train Epoch: 44 [3584/8000 (45%)]\tTotal Loss: 0.829437\n",
      "Reconstruction: 0.501493, Regularization: 0.255593, Discriminator: 0.010774; Generator: 0.061577,\n",
      "D(x): 0.891, D(G(z)): 0.140\n",
      "2019-04-10 01:19:43,182 root         INFO     Train Epoch: 44 [4096/8000 (51%)]\tTotal Loss: 1.205869\n",
      "Reconstruction: 0.740191, Regularization: 0.393446, Discriminator: 0.010768; Generator: 0.061464,\n",
      "D(x): 0.889, D(G(z)): 0.140\n",
      "2019-04-10 01:19:43,284 root         INFO     Train Epoch: 44 [4608/8000 (58%)]\tTotal Loss: 1.206509\n",
      "Reconstruction: 0.808962, Regularization: 0.317647, Discriminator: 0.018372; Generator: 0.061528,\n",
      "D(x): 0.809, D(G(z)): 0.140\n",
      "2019-04-10 01:19:43,386 root         INFO     Train Epoch: 44 [5120/8000 (64%)]\tTotal Loss: 0.978724\n",
      "Reconstruction: 0.642538, Regularization: 0.259660, Discriminator: 0.015450; Generator: 0.061075,\n",
      "D(x): 0.865, D(G(z)): 0.142\n",
      "2019-04-10 01:19:43,487 root         INFO     Train Epoch: 44 [5632/8000 (70%)]\tTotal Loss: 1.109993\n",
      "Reconstruction: 0.727007, Regularization: 0.311352, Discriminator: 0.010546; Generator: 0.061089,\n",
      "D(x): 0.899, D(G(z)): 0.142\n",
      "2019-04-10 01:19:43,589 root         INFO     Train Epoch: 44 [6144/8000 (77%)]\tTotal Loss: 1.225217\n",
      "Reconstruction: 0.758546, Regularization: 0.386740, Discriminator: 0.018567; Generator: 0.061364,\n",
      "D(x): 0.866, D(G(z)): 0.140\n",
      "2019-04-10 01:19:43,689 root         INFO     Train Epoch: 44 [6656/8000 (83%)]\tTotal Loss: 1.004138\n",
      "Reconstruction: 0.646061, Regularization: 0.282362, Discriminator: 0.014259; Generator: 0.061457,\n",
      "D(x): 0.862, D(G(z)): 0.140\n",
      "2019-04-10 01:19:43,792 root         INFO     Train Epoch: 44 [7168/8000 (90%)]\tTotal Loss: 0.619778\n",
      "Reconstruction: 0.359800, Regularization: 0.175051, Discriminator: 0.023464; Generator: 0.061462,\n",
      "D(x): 0.820, D(G(z)): 0.140\n",
      "2019-04-10 01:19:43,893 root         INFO     Train Epoch: 44 [7680/8000 (96%)]\tTotal Loss: 1.027803\n",
      "Reconstruction: 0.631370, Regularization: 0.325685, Discriminator: 0.009431; Generator: 0.061317,\n",
      "D(x): 0.909, D(G(z)): 0.141\n",
      "2019-04-10 01:19:43,969 root         INFO     ====> Epoch: 44 Average loss: 1.0093\n",
      "2019-04-10 01:19:43,995 root         INFO     Train Epoch: 45 [0/8000 (0%)]\tTotal Loss: 0.781493\n",
      "Reconstruction: 0.463636, Regularization: 0.247003, Discriminator: 0.009565; Generator: 0.061290,\n",
      "D(x): 0.941, D(G(z)): 0.141\n",
      "2019-04-10 01:19:44,097 root         INFO     Train Epoch: 45 [512/8000 (6%)]\tTotal Loss: 0.685223\n",
      "Reconstruction: 0.405543, Regularization: 0.204164, Discriminator: 0.013985; Generator: 0.061531,\n",
      "D(x): 0.901, D(G(z)): 0.140\n",
      "2019-04-10 01:19:44,198 root         INFO     Train Epoch: 45 [1024/8000 (13%)]\tTotal Loss: 0.767308\n",
      "Reconstruction: 0.463104, Regularization: 0.235094, Discriminator: 0.007418; Generator: 0.061692,\n",
      "D(x): 0.939, D(G(z)): 0.139\n",
      "2019-04-10 01:19:44,298 root         INFO     Train Epoch: 45 [1536/8000 (19%)]\tTotal Loss: 0.932586\n",
      "Reconstruction: 0.576448, Regularization: 0.288099, Discriminator: 0.007354; Generator: 0.060684,\n",
      "D(x): 0.943, D(G(z)): 0.144\n",
      "2019-04-10 01:19:44,399 root         INFO     Train Epoch: 45 [2048/8000 (26%)]\tTotal Loss: 0.970686\n",
      "Reconstruction: 0.597017, Regularization: 0.306040, Discriminator: 0.006263; Generator: 0.061366,\n",
      "D(x): 0.959, D(G(z)): 0.140\n",
      "2019-04-10 01:19:44,499 root         INFO     Train Epoch: 45 [2560/8000 (32%)]\tTotal Loss: 1.642044\n",
      "Reconstruction: 1.022805, Regularization: 0.540718, Discriminator: 0.017218; Generator: 0.061303,\n",
      "D(x): 0.820, D(G(z)): 0.141\n",
      "2019-04-10 01:19:44,601 root         INFO     Train Epoch: 45 [3072/8000 (38%)]\tTotal Loss: 0.961006\n",
      "Reconstruction: 0.571187, Regularization: 0.317813, Discriminator: 0.010631; Generator: 0.061374,\n",
      "D(x): 0.907, D(G(z)): 0.140\n",
      "2019-04-10 01:19:44,702 root         INFO     Train Epoch: 45 [3584/8000 (45%)]\tTotal Loss: 0.780902\n",
      "Reconstruction: 0.459781, Regularization: 0.240863, Discriminator: 0.019137; Generator: 0.061121,\n",
      "D(x): 0.837, D(G(z)): 0.142\n",
      "2019-04-10 01:19:44,801 root         INFO     Train Epoch: 45 [4096/8000 (51%)]\tTotal Loss: 1.269362\n",
      "Reconstruction: 0.784650, Regularization: 0.392867, Discriminator: 0.031668; Generator: 0.060176,\n",
      "D(x): 0.828, D(G(z)): 0.147\n",
      "2019-04-10 01:19:44,899 root         INFO     Train Epoch: 45 [4608/8000 (58%)]\tTotal Loss: 0.925204\n",
      "Reconstruction: 0.557879, Regularization: 0.291041, Discriminator: 0.015315; Generator: 0.060969,\n",
      "D(x): 0.889, D(G(z)): 0.142\n",
      "2019-04-10 01:19:44,996 root         INFO     Train Epoch: 45 [5120/8000 (64%)]\tTotal Loss: 1.216644\n",
      "Reconstruction: 0.765848, Regularization: 0.372528, Discriminator: 0.017341; Generator: 0.060928,\n",
      "D(x): 0.845, D(G(z)): 0.142\n",
      "2019-04-10 01:19:45,094 root         INFO     Train Epoch: 45 [5632/8000 (70%)]\tTotal Loss: 1.028740\n",
      "Reconstruction: 0.638893, Regularization: 0.313029, Discriminator: 0.015920; Generator: 0.060898,\n",
      "D(x): 0.888, D(G(z)): 0.143\n",
      "2019-04-10 01:19:45,191 root         INFO     Train Epoch: 45 [6144/8000 (77%)]\tTotal Loss: 1.278594\n",
      "Reconstruction: 0.839029, Regularization: 0.355437, Discriminator: 0.023963; Generator: 0.060166,\n",
      "D(x): 0.833, D(G(z)): 0.146\n",
      "2019-04-10 01:19:45,288 root         INFO     Train Epoch: 45 [6656/8000 (83%)]\tTotal Loss: 0.629794\n",
      "Reconstruction: 0.363705, Regularization: 0.190873, Discriminator: 0.014762; Generator: 0.060455,\n",
      "D(x): 0.869, D(G(z)): 0.145\n",
      "2019-04-10 01:19:45,385 root         INFO     Train Epoch: 45 [7168/8000 (90%)]\tTotal Loss: 1.135802\n",
      "Reconstruction: 0.680972, Regularization: 0.373866, Discriminator: 0.021096; Generator: 0.059868,\n",
      "D(x): 0.820, D(G(z)): 0.149\n",
      "2019-04-10 01:19:45,484 root         INFO     Train Epoch: 45 [7680/8000 (96%)]\tTotal Loss: 1.063852\n",
      "Reconstruction: 0.644434, Regularization: 0.345423, Discriminator: 0.013669; Generator: 0.060327,\n",
      "D(x): 0.853, D(G(z)): 0.145\n",
      "2019-04-10 01:19:45,558 root         INFO     ====> Epoch: 45 Average loss: 1.0144\n",
      "2019-04-10 01:19:45,585 root         INFO     Train Epoch: 46 [0/8000 (0%)]\tTotal Loss: 0.626480\n",
      "Reconstruction: 0.369671, Regularization: 0.186168, Discriminator: 0.012189; Generator: 0.058453,\n",
      "D(x): 0.876, D(G(z)): 0.156\n",
      "2019-04-10 01:19:45,688 root         INFO     Train Epoch: 46 [512/8000 (6%)]\tTotal Loss: 8.254894\n",
      "Reconstruction: 7.645371, Regularization: 0.530058, Discriminator: 0.019429; Generator: 0.060036,\n",
      "D(x): 0.868, D(G(z)): 0.147\n",
      "2019-04-10 01:19:45,790 root         INFO     Train Epoch: 46 [1024/8000 (13%)]\tTotal Loss: 0.781838\n",
      "Reconstruction: 0.479056, Regularization: 0.229519, Discriminator: 0.013556; Generator: 0.059707,\n",
      "D(x): 0.855, D(G(z)): 0.148\n",
      "2019-04-10 01:19:45,892 root         INFO     Train Epoch: 46 [1536/8000 (19%)]\tTotal Loss: 1.037089\n",
      "Reconstruction: 0.636814, Regularization: 0.312405, Discriminator: 0.027721; Generator: 0.060149,\n",
      "D(x): 0.779, D(G(z)): 0.146\n",
      "2019-04-10 01:19:45,994 root         INFO     Train Epoch: 46 [2048/8000 (26%)]\tTotal Loss: 1.101002\n",
      "Reconstruction: 0.661126, Regularization: 0.366895, Discriminator: 0.013010; Generator: 0.059971,\n",
      "D(x): 0.907, D(G(z)): 0.147\n",
      "2019-04-10 01:19:46,095 root         INFO     Train Epoch: 46 [2560/8000 (32%)]\tTotal Loss: 1.478658\n",
      "Reconstruction: 0.929128, Regularization: 0.470737, Discriminator: 0.020060; Generator: 0.058734,\n",
      "D(x): 0.835, D(G(z)): 0.155\n",
      "2019-04-10 01:19:46,197 root         INFO     Train Epoch: 46 [3072/8000 (38%)]\tTotal Loss: 0.743777\n",
      "Reconstruction: 0.457060, Regularization: 0.199622, Discriminator: 0.027279; Generator: 0.059816,\n",
      "D(x): 0.782, D(G(z)): 0.148\n",
      "2019-04-10 01:19:46,298 root         INFO     Train Epoch: 46 [3584/8000 (45%)]\tTotal Loss: 1.040907\n",
      "Reconstruction: 0.634376, Regularization: 0.317916, Discriminator: 0.028972; Generator: 0.059642,\n",
      "D(x): 0.800, D(G(z)): 0.149\n",
      "2019-04-10 01:19:46,398 root         INFO     Train Epoch: 46 [4096/8000 (51%)]\tTotal Loss: 1.140448\n",
      "Reconstruction: 0.699723, Regularization: 0.360018, Discriminator: 0.021529; Generator: 0.059178,\n",
      "D(x): 0.880, D(G(z)): 0.152\n",
      "2019-04-10 01:19:46,500 root         INFO     Train Epoch: 46 [4608/8000 (58%)]\tTotal Loss: 0.914614\n",
      "Reconstruction: 0.539927, Regularization: 0.283735, Discriminator: 0.030684; Generator: 0.060269,\n",
      "D(x): 0.796, D(G(z)): 0.145\n",
      "2019-04-10 01:19:46,600 root         INFO     Train Epoch: 46 [5120/8000 (64%)]\tTotal Loss: 0.855707\n",
      "Reconstruction: 0.517657, Regularization: 0.260935, Discriminator: 0.016922; Generator: 0.060193,\n",
      "D(x): 0.874, D(G(z)): 0.146\n",
      "2019-04-10 01:19:46,700 root         INFO     Train Epoch: 46 [5632/8000 (70%)]\tTotal Loss: 0.701970\n",
      "Reconstruction: 0.409031, Regularization: 0.221683, Discriminator: 0.011119; Generator: 0.060136,\n",
      "D(x): 0.886, D(G(z)): 0.146\n",
      "2019-04-10 01:19:46,800 root         INFO     Train Epoch: 46 [6144/8000 (77%)]\tTotal Loss: 0.796939\n",
      "Reconstruction: 0.480915, Regularization: 0.232125, Discriminator: 0.023568; Generator: 0.060330,\n",
      "D(x): 0.757, D(G(z)): 0.145\n",
      "2019-04-10 01:19:46,899 root         INFO     Train Epoch: 46 [6656/8000 (83%)]\tTotal Loss: 0.805618\n",
      "Reconstruction: 0.488865, Regularization: 0.235512, Discriminator: 0.022210; Generator: 0.059030,\n",
      "D(x): 0.796, D(G(z)): 0.152\n",
      "2019-04-10 01:19:46,996 root         INFO     Train Epoch: 46 [7168/8000 (90%)]\tTotal Loss: 0.897155\n",
      "Reconstruction: 0.547059, Regularization: 0.275586, Discriminator: 0.015539; Generator: 0.058970,\n",
      "D(x): 0.830, D(G(z)): 0.153\n",
      "2019-04-10 01:19:47,094 root         INFO     Train Epoch: 46 [7680/8000 (96%)]\tTotal Loss: 0.726230\n",
      "Reconstruction: 0.433520, Regularization: 0.217110, Discriminator: 0.016470; Generator: 0.059131,\n",
      "D(x): 0.874, D(G(z)): 0.151\n",
      "2019-04-10 01:19:47,168 root         INFO     ====> Epoch: 46 Average loss: 3259432.2692\n",
      "2019-04-10 01:19:47,194 root         INFO     Train Epoch: 47 [0/8000 (0%)]\tTotal Loss: 0.647688\n",
      "Reconstruction: 0.372303, Regularization: 0.194525, Discriminator: 0.022234; Generator: 0.058626,\n",
      "D(x): 0.827, D(G(z)): 0.155\n",
      "2019-04-10 01:19:47,297 root         INFO     Train Epoch: 47 [512/8000 (6%)]\tTotal Loss: 1.035299\n",
      "Reconstruction: 0.630662, Regularization: 0.331405, Discriminator: 0.013993; Generator: 0.059238,\n",
      "D(x): 0.879, D(G(z)): 0.150\n",
      "2019-04-10 01:19:47,399 root         INFO     Train Epoch: 47 [1024/8000 (13%)]\tTotal Loss: 1.112314\n",
      "Reconstruction: 0.721976, Regularization: 0.299798, Discriminator: 0.030538; Generator: 0.060001,\n",
      "D(x): 0.812, D(G(z)): 0.147\n",
      "2019-04-10 01:19:47,502 root         INFO     Train Epoch: 47 [1536/8000 (19%)]\tTotal Loss: 0.905187\n",
      "Reconstruction: 0.552760, Regularization: 0.282968, Discriminator: 0.010308; Generator: 0.059151,\n",
      "D(x): 0.925, D(G(z)): 0.152\n",
      "2019-04-10 01:19:47,604 root         INFO     Train Epoch: 47 [2048/8000 (26%)]\tTotal Loss: 0.797310\n",
      "Reconstruction: 0.490638, Regularization: 0.227741, Discriminator: 0.019484; Generator: 0.059447,\n",
      "D(x): 0.836, D(G(z)): 0.150\n",
      "2019-04-10 01:19:47,705 root         INFO     Train Epoch: 47 [2560/8000 (32%)]\tTotal Loss: 0.914061\n",
      "Reconstruction: 0.545689, Regularization: 0.279981, Discriminator: 0.029514; Generator: 0.058878,\n",
      "D(x): 0.796, D(G(z)): 0.154\n",
      "2019-04-10 01:19:47,808 root         INFO     Train Epoch: 47 [3072/8000 (38%)]\tTotal Loss: 0.758304\n",
      "Reconstruction: 0.462065, Regularization: 0.220722, Discriminator: 0.016670; Generator: 0.058847,\n",
      "D(x): 0.816, D(G(z)): 0.153\n",
      "2019-04-10 01:19:47,910 root         INFO     Train Epoch: 47 [3584/8000 (45%)]\tTotal Loss: 1.202308\n",
      "Reconstruction: 0.862931, Regularization: 0.264573, Discriminator: 0.016139; Generator: 0.058665,\n",
      "D(x): 0.836, D(G(z)): 0.154\n",
      "2019-04-10 01:19:48,012 root         INFO     Train Epoch: 47 [4096/8000 (51%)]\tTotal Loss: 0.607100\n",
      "Reconstruction: 0.344972, Regularization: 0.181821, Discriminator: 0.023326; Generator: 0.056982,\n",
      "D(x): 0.757, D(G(z)): 0.166\n",
      "2019-04-10 01:19:48,114 root         INFO     Train Epoch: 47 [4608/8000 (58%)]\tTotal Loss: 0.955638\n",
      "Reconstruction: 0.585151, Regularization: 0.297749, Discriminator: 0.013408; Generator: 0.059330,\n",
      "D(x): 0.896, D(G(z)): 0.150\n",
      "2019-04-10 01:19:48,216 root         INFO     Train Epoch: 47 [5120/8000 (64%)]\tTotal Loss: 0.698456\n",
      "Reconstruction: 0.409030, Regularization: 0.209277, Discriminator: 0.021782; Generator: 0.058367,\n",
      "D(x): 0.816, D(G(z)): 0.157\n",
      "2019-04-10 01:19:48,317 root         INFO     Train Epoch: 47 [5632/8000 (70%)]\tTotal Loss: 0.968612\n",
      "Reconstruction: 0.610156, Regularization: 0.284890, Discriminator: 0.014177; Generator: 0.059389,\n",
      "D(x): 0.834, D(G(z)): 0.150\n",
      "2019-04-10 01:19:48,419 root         INFO     Train Epoch: 47 [6144/8000 (77%)]\tTotal Loss: 48.052338\n",
      "Reconstruction: 47.742432, Regularization: 0.236563, Discriminator: 0.014333; Generator: 0.059010,\n",
      "D(x): 0.842, D(G(z)): 0.152\n",
      "2019-04-10 01:19:48,521 root         INFO     Train Epoch: 47 [6656/8000 (83%)]\tTotal Loss: 0.788409\n",
      "Reconstruction: 0.470354, Regularization: 0.239669, Discriminator: 0.019827; Generator: 0.058560,\n",
      "D(x): 0.890, D(G(z)): 0.155\n",
      "2019-04-10 01:19:48,621 root         INFO     Train Epoch: 47 [7168/8000 (90%)]\tTotal Loss: 0.910623\n",
      "Reconstruction: 0.556432, Regularization: 0.277168, Discriminator: 0.018438; Generator: 0.058585,\n",
      "D(x): 0.810, D(G(z)): 0.155\n",
      "2019-04-10 01:19:48,722 root         INFO     Train Epoch: 47 [7680/8000 (96%)]\tTotal Loss: 1.473305\n",
      "Reconstruction: 0.921907, Regularization: 0.480875, Discriminator: 0.012472; Generator: 0.058051,\n",
      "D(x): 0.859, D(G(z)): 0.158\n",
      "2019-04-10 01:19:48,796 root         INFO     ====> Epoch: 47 Average loss: 109984.9945\n",
      "2019-04-10 01:19:48,823 root         INFO     Train Epoch: 48 [0/8000 (0%)]\tTotal Loss: 1.073377\n",
      "Reconstruction: 0.651004, Regularization: 0.343901, Discriminator: 0.020316; Generator: 0.058156,\n",
      "D(x): 0.857, D(G(z)): 0.157\n",
      "2019-04-10 01:19:48,924 root         INFO     Train Epoch: 48 [512/8000 (6%)]\tTotal Loss: 1.152903\n",
      "Reconstruction: 0.701139, Regularization: 0.365498, Discriminator: 0.029787; Generator: 0.056480,\n",
      "D(x): 0.785, D(G(z)): 0.169\n",
      "2019-04-10 01:19:49,024 root         INFO     Train Epoch: 48 [1024/8000 (13%)]\tTotal Loss: 0.919603\n",
      "Reconstruction: 0.552022, Regularization: 0.291463, Discriminator: 0.019128; Generator: 0.056990,\n",
      "D(x): 0.886, D(G(z)): 0.164\n",
      "2019-04-10 01:19:49,124 root         INFO     Train Epoch: 48 [1536/8000 (19%)]\tTotal Loss: 1.305748\n",
      "Reconstruction: 0.857786, Regularization: 0.379743, Discriminator: 0.013974; Generator: 0.054245,\n",
      "D(x): 0.883, D(G(z)): 0.192\n",
      "2019-04-10 01:19:49,224 root         INFO     Train Epoch: 48 [2048/8000 (26%)]\tTotal Loss: 0.751721\n",
      "Reconstruction: 0.433139, Regularization: 0.237584, Discriminator: 0.022052; Generator: 0.058946,\n",
      "D(x): 0.880, D(G(z)): 0.152\n",
      "2019-04-10 01:19:49,324 root         INFO     Train Epoch: 48 [2560/8000 (32%)]\tTotal Loss: 0.585793\n",
      "Reconstruction: 0.340362, Regularization: 0.169085, Discriminator: 0.018701; Generator: 0.057646,\n",
      "D(x): 0.799, D(G(z)): 0.161\n",
      "2019-04-10 01:19:49,425 root         INFO     Train Epoch: 48 [3072/8000 (38%)]\tTotal Loss: 1.484862\n",
      "Reconstruction: 0.945837, Regularization: 0.463603, Discriminator: 0.016975; Generator: 0.058447,\n",
      "D(x): 0.835, D(G(z)): 0.155\n",
      "2019-04-10 01:19:49,525 root         INFO     Train Epoch: 48 [3584/8000 (45%)]\tTotal Loss: 0.719334\n",
      "Reconstruction: 0.426941, Regularization: 0.218575, Discriminator: 0.016810; Generator: 0.057008,\n",
      "D(x): 0.831, D(G(z)): 0.165\n",
      "2019-04-10 01:19:49,625 root         INFO     Train Epoch: 48 [4096/8000 (51%)]\tTotal Loss: 0.692185\n",
      "Reconstruction: 0.405371, Regularization: 0.217456, Discriminator: 0.009950; Generator: 0.059408,\n",
      "D(x): 0.902, D(G(z)): 0.149\n",
      "2019-04-10 01:19:49,725 root         INFO     Train Epoch: 48 [4608/8000 (58%)]\tTotal Loss: 0.847749\n",
      "Reconstruction: 0.510072, Regularization: 0.267613, Discriminator: 0.013069; Generator: 0.056995,\n",
      "D(x): 0.863, D(G(z)): 0.164\n",
      "2019-04-10 01:19:49,825 root         INFO     Train Epoch: 48 [5120/8000 (64%)]\tTotal Loss: 0.667087\n",
      "Reconstruction: 0.387396, Regularization: 0.203210, Discriminator: 0.019987; Generator: 0.056494,\n",
      "D(x): 0.834, D(G(z)): 0.169\n",
      "2019-04-10 01:19:49,925 root         INFO     Train Epoch: 48 [5632/8000 (70%)]\tTotal Loss: 1.127671\n",
      "Reconstruction: 0.685964, Regularization: 0.367743, Discriminator: 0.019046; Generator: 0.054918,\n",
      "D(x): 0.905, D(G(z)): 0.184\n",
      "2019-04-10 01:19:50,025 root         INFO     Train Epoch: 48 [6144/8000 (77%)]\tTotal Loss: 0.806932\n",
      "Reconstruction: 0.481327, Regularization: 0.251516, Discriminator: 0.018970; Generator: 0.055119,\n",
      "D(x): 0.804, D(G(z)): 0.182\n",
      "2019-04-10 01:19:50,123 root         INFO     Train Epoch: 48 [6656/8000 (83%)]\tTotal Loss: 1.051558\n",
      "Reconstruction: 0.636381, Regularization: 0.337259, Discriminator: 0.019872; Generator: 0.058045,\n",
      "D(x): 0.863, D(G(z)): 0.157\n",
      "2019-04-10 01:19:50,222 root         INFO     Train Epoch: 48 [7168/8000 (90%)]\tTotal Loss: 1.092466\n",
      "Reconstruction: 0.694956, Regularization: 0.325402, Discriminator: 0.015626; Generator: 0.056481,\n",
      "D(x): 0.873, D(G(z)): 0.170\n",
      "2019-04-10 01:19:50,321 root         INFO     Train Epoch: 48 [7680/8000 (96%)]\tTotal Loss: 0.729669\n",
      "Reconstruction: 0.431047, Regularization: 0.229078, Discriminator: 0.015944; Generator: 0.053600,\n",
      "D(x): 0.864, D(G(z)): 0.193\n",
      "2019-04-10 01:19:50,395 root         INFO     ====> Epoch: 48 Average loss: 45.4581\n",
      "2019-04-10 01:19:50,421 root         INFO     Train Epoch: 49 [0/8000 (0%)]\tTotal Loss: 0.849733\n",
      "Reconstruction: 0.522936, Regularization: 0.246970, Discriminator: 0.023336; Generator: 0.056492,\n",
      "D(x): 0.805, D(G(z)): 0.169\n",
      "2019-04-10 01:19:50,522 root         INFO     Train Epoch: 49 [512/8000 (6%)]\tTotal Loss: 0.667199\n",
      "Reconstruction: 0.374978, Regularization: 0.205622, Discriminator: 0.031765; Generator: 0.054834,\n",
      "D(x): 0.799, D(G(z)): 0.182\n",
      "2019-04-10 01:19:50,623 root         INFO     Train Epoch: 49 [1024/8000 (13%)]\tTotal Loss: 0.884200\n",
      "Reconstruction: 0.532445, Regularization: 0.267487, Discriminator: 0.027876; Generator: 0.056393,\n",
      "D(x): 0.801, D(G(z)): 0.169\n",
      "2019-04-10 01:19:50,724 root         INFO     Train Epoch: 49 [1536/8000 (19%)]\tTotal Loss: 4.787373\n",
      "Reconstruction: 4.275897, Regularization: 0.443490, Discriminator: 0.011858; Generator: 0.056128,\n",
      "D(x): 0.887, D(G(z)): 0.170\n",
      "2019-04-10 01:19:50,824 root         INFO     Train Epoch: 49 [2048/8000 (26%)]\tTotal Loss: 0.623868\n",
      "Reconstruction: 0.358851, Regularization: 0.197542, Discriminator: 0.010958; Generator: 0.056516,\n",
      "D(x): 0.896, D(G(z)): 0.172\n",
      "2019-04-10 01:19:50,923 root         INFO     Train Epoch: 49 [2560/8000 (32%)]\tTotal Loss: 0.753680\n",
      "Reconstruction: 0.434363, Regularization: 0.242187, Discriminator: 0.022077; Generator: 0.055053,\n",
      "D(x): 0.814, D(G(z)): 0.182\n",
      "2019-04-10 01:19:51,021 root         INFO     Train Epoch: 49 [3072/8000 (38%)]\tTotal Loss: 1.246103\n",
      "Reconstruction: 0.784222, Regularization: 0.395077, Discriminator: 0.012919; Generator: 0.053884,\n",
      "D(x): 0.874, D(G(z)): 0.189\n",
      "2019-04-10 01:19:51,120 root         INFO     Train Epoch: 49 [3584/8000 (45%)]\tTotal Loss: 1.049648\n",
      "Reconstruction: 0.637868, Regularization: 0.331721, Discriminator: 0.025107; Generator: 0.054953,\n",
      "D(x): 0.806, D(G(z)): 0.182\n",
      "2019-04-10 01:19:51,217 root         INFO     Train Epoch: 49 [4096/8000 (51%)]\tTotal Loss: 0.628487\n",
      "Reconstruction: 0.370690, Regularization: 0.175030, Discriminator: 0.028231; Generator: 0.054534,\n",
      "D(x): 0.749, D(G(z)): 0.192\n",
      "2019-04-10 01:19:51,316 root         INFO     Train Epoch: 49 [4608/8000 (58%)]\tTotal Loss: 1.093473\n",
      "Reconstruction: 0.669949, Regularization: 0.342215, Discriminator: 0.024781; Generator: 0.056528,\n",
      "D(x): 0.809, D(G(z)): 0.172\n",
      "2019-04-10 01:19:51,415 root         INFO     Train Epoch: 49 [5120/8000 (64%)]\tTotal Loss: 0.704213\n",
      "Reconstruction: 0.412981, Regularization: 0.223798, Discriminator: 0.011209; Generator: 0.056226,\n",
      "D(x): 0.892, D(G(z)): 0.171\n",
      "2019-04-10 01:19:51,514 root         INFO     Train Epoch: 49 [5632/8000 (70%)]\tTotal Loss: 1.118308\n",
      "Reconstruction: 0.682091, Regularization: 0.355692, Discriminator: 0.025064; Generator: 0.055460,\n",
      "D(x): 0.814, D(G(z)): 0.177\n",
      "2019-04-10 01:19:51,613 root         INFO     Train Epoch: 49 [6144/8000 (77%)]\tTotal Loss: 1.122085\n",
      "Reconstruction: 0.686796, Regularization: 0.365951, Discriminator: 0.014649; Generator: 0.054689,\n",
      "D(x): 0.881, D(G(z)): 0.189\n",
      "2019-04-10 01:19:51,712 root         INFO     Train Epoch: 49 [6656/8000 (83%)]\tTotal Loss: 0.947584\n",
      "Reconstruction: 0.592113, Regularization: 0.271551, Discriminator: 0.025953; Generator: 0.057967,\n",
      "D(x): 0.733, D(G(z)): 0.160\n",
      "2019-04-10 01:19:51,812 root         INFO     Train Epoch: 49 [7168/8000 (90%)]\tTotal Loss: 0.704434\n",
      "Reconstruction: 0.417725, Regularization: 0.206185, Discriminator: 0.022619; Generator: 0.057904,\n",
      "D(x): 0.804, D(G(z)): 0.161\n",
      "2019-04-10 01:19:51,911 root         INFO     Train Epoch: 49 [7680/8000 (96%)]\tTotal Loss: 5.284859\n",
      "Reconstruction: 4.882623, Regularization: 0.331832, Discriminator: 0.017268; Generator: 0.053136,\n",
      "D(x): 0.863, D(G(z)): 0.205\n",
      "2019-04-10 01:19:51,984 root         INFO     ====> Epoch: 49 Average loss: 224265304.5202\n",
      "2019-04-10 01:19:52,012 root         INFO     Train Epoch: 50 [0/8000 (0%)]\tTotal Loss: 0.967955\n",
      "Reconstruction: 0.589846, Regularization: 0.311685, Discriminator: 0.011632; Generator: 0.054792,\n",
      "D(x): 0.921, D(G(z)): 0.185\n",
      "2019-04-10 01:19:52,113 root         INFO     Train Epoch: 50 [512/8000 (6%)]\tTotal Loss: 0.877354\n",
      "Reconstruction: 0.525951, Regularization: 0.277855, Discriminator: 0.016796; Generator: 0.056752,\n",
      "D(x): 0.829, D(G(z)): 0.167\n",
      "2019-04-10 01:19:52,213 root         INFO     Train Epoch: 50 [1024/8000 (13%)]\tTotal Loss: 0.706199\n",
      "Reconstruction: 0.407952, Regularization: 0.225366, Discriminator: 0.020369; Generator: 0.052511,\n",
      "D(x): 0.817, D(G(z)): 0.209\n",
      "2019-04-10 01:19:52,313 root         INFO     Train Epoch: 50 [1536/8000 (19%)]\tTotal Loss: 7.025075\n",
      "Reconstruction: 6.770017, Regularization: 0.171062, Discriminator: 0.031207; Generator: 0.052790,\n",
      "D(x): 0.699, D(G(z)): 0.203\n",
      "2019-04-10 01:19:52,414 root         INFO     Train Epoch: 50 [2048/8000 (26%)]\tTotal Loss: 0.724396\n",
      "Reconstruction: 0.413060, Regularization: 0.218657, Discriminator: 0.036834; Generator: 0.055845,\n",
      "D(x): 0.696, D(G(z)): 0.172\n",
      "2019-04-10 01:19:52,514 root         INFO     Train Epoch: 50 [2560/8000 (32%)]\tTotal Loss: 27.315550\n",
      "Reconstruction: 26.988169, Regularization: 0.250065, Discriminator: 0.021380; Generator: 0.055937,\n",
      "D(x): 0.788, D(G(z)): 0.174\n",
      "2019-04-10 01:19:52,614 root         INFO     Train Epoch: 50 [3072/8000 (38%)]\tTotal Loss: 0.510917\n",
      "Reconstruction: 0.289678, Regularization: 0.146724, Discriminator: 0.018643; Generator: 0.055871,\n",
      "D(x): 0.838, D(G(z)): 0.176\n",
      "2019-04-10 01:19:52,714 root         INFO     Train Epoch: 50 [3584/8000 (45%)]\tTotal Loss: 0.629518\n",
      "Reconstruction: 0.360623, Regularization: 0.194449, Discriminator: 0.017161; Generator: 0.057285,\n",
      "D(x): 0.880, D(G(z)): 0.163\n",
      "2019-04-10 01:19:52,815 root         INFO     Train Epoch: 50 [4096/8000 (51%)]\tTotal Loss: 1.109495\n",
      "Reconstruction: 0.675248, Regularization: 0.356120, Discriminator: 0.024590; Generator: 0.053536,\n",
      "D(x): 0.808, D(G(z)): 0.193\n",
      "2019-04-10 01:19:52,915 root         INFO     Train Epoch: 50 [4608/8000 (58%)]\tTotal Loss: 1.456553\n",
      "Reconstruction: 0.945254, Regularization: 0.439581, Discriminator: 0.016090; Generator: 0.055627,\n",
      "D(x): 0.862, D(G(z)): 0.182\n",
      "2019-04-10 01:19:53,015 root         INFO     Train Epoch: 50 [5120/8000 (64%)]\tTotal Loss: 1.023784\n",
      "Reconstruction: 0.617928, Regularization: 0.327902, Discriminator: 0.022092; Generator: 0.055862,\n",
      "D(x): 0.809, D(G(z)): 0.173\n",
      "2019-04-10 01:19:53,116 root         INFO     Train Epoch: 50 [5632/8000 (70%)]\tTotal Loss: 1.125456\n",
      "Reconstruction: 0.684638, Regularization: 0.371779, Discriminator: 0.011005; Generator: 0.058034,\n",
      "D(x): 0.901, D(G(z)): 0.158\n",
      "2019-04-10 01:19:53,216 root         INFO     Train Epoch: 50 [6144/8000 (77%)]\tTotal Loss: 0.643760\n",
      "Reconstruction: 0.365068, Regularization: 0.192587, Discriminator: 0.029524; Generator: 0.056581,\n",
      "D(x): 0.755, D(G(z)): 0.173\n",
      "2019-04-10 01:19:53,316 root         INFO     Train Epoch: 50 [6656/8000 (83%)]\tTotal Loss: 1.119947\n",
      "Reconstruction: 0.686641, Regularization: 0.343518, Discriminator: 0.031843; Generator: 0.057946,\n",
      "D(x): 0.712, D(G(z)): 0.158\n",
      "2019-04-10 01:19:53,416 root         INFO     Train Epoch: 50 [7168/8000 (90%)]\tTotal Loss: 0.774010\n",
      "Reconstruction: 0.481430, Regularization: 0.210211, Discriminator: 0.024624; Generator: 0.057745,\n",
      "D(x): 0.776, D(G(z)): 0.159\n",
      "2019-04-10 01:19:53,516 root         INFO     Train Epoch: 50 [7680/8000 (96%)]\tTotal Loss: 1.273131\n",
      "Reconstruction: 0.795268, Regularization: 0.401651, Discriminator: 0.021153; Generator: 0.055059,\n",
      "D(x): 0.796, D(G(z)): 0.182\n",
      "2019-04-10 01:19:53,591 root         INFO     ====> Epoch: 50 Average loss: 1965.4424\n",
      "2019-04-10 01:19:53,618 root         INFO     Train Epoch: 51 [0/8000 (0%)]\tTotal Loss: 1.093404\n",
      "Reconstruction: 0.664262, Regularization: 0.345463, Discriminator: 0.026475; Generator: 0.057205,\n",
      "D(x): 0.792, D(G(z)): 0.164\n",
      "2019-04-10 01:19:53,718 root         INFO     Train Epoch: 51 [512/8000 (6%)]\tTotal Loss: 0.823360\n",
      "Reconstruction: 0.485804, Regularization: 0.258155, Discriminator: 0.022637; Generator: 0.056764,\n",
      "D(x): 0.721, D(G(z)): 0.167\n",
      "2019-04-10 01:19:53,819 root         INFO     Train Epoch: 51 [1024/8000 (13%)]\tTotal Loss: 0.935230\n",
      "Reconstruction: 0.556725, Regularization: 0.309039, Discriminator: 0.015086; Generator: 0.054380,\n",
      "D(x): 0.868, D(G(z)): 0.188\n",
      "2019-04-10 01:19:53,919 root         INFO     Train Epoch: 51 [1536/8000 (19%)]\tTotal Loss: 0.970475\n",
      "Reconstruction: 0.588597, Regularization: 0.312969, Discriminator: 0.013517; Generator: 0.055391,\n",
      "D(x): 0.892, D(G(z)): 0.176\n",
      "2019-04-10 01:19:54,019 root         INFO     Train Epoch: 51 [2048/8000 (26%)]\tTotal Loss: 1.084141\n",
      "Reconstruction: 0.673391, Regularization: 0.339124, Discriminator: 0.019436; Generator: 0.052190,\n",
      "D(x): 0.829, D(G(z)): 0.209\n",
      "2019-04-10 01:19:54,120 root         INFO     Train Epoch: 51 [2560/8000 (32%)]\tTotal Loss: 0.649487\n",
      "Reconstruction: 0.372587, Regularization: 0.196383, Discriminator: 0.024603; Generator: 0.055914,\n",
      "D(x): 0.802, D(G(z)): 0.180\n",
      "2019-04-10 01:19:54,220 root         INFO     Train Epoch: 51 [3072/8000 (38%)]\tTotal Loss: 1.150649\n",
      "Reconstruction: 0.705109, Regularization: 0.373251, Discriminator: 0.019414; Generator: 0.052875,\n",
      "D(x): 0.859, D(G(z)): 0.201\n",
      "2019-04-10 01:19:54,320 root         INFO     Train Epoch: 51 [3584/8000 (45%)]\tTotal Loss: 0.910159\n",
      "Reconstruction: 0.564406, Regularization: 0.263891, Discriminator: 0.027111; Generator: 0.054752,\n",
      "D(x): 0.741, D(G(z)): 0.184\n",
      "2019-04-10 01:19:54,420 root         INFO     Train Epoch: 51 [4096/8000 (51%)]\tTotal Loss: 0.641149\n",
      "Reconstruction: 0.377293, Regularization: 0.186370, Discriminator: 0.024352; Generator: 0.053134,\n",
      "D(x): 0.791, D(G(z)): 0.197\n",
      "2019-04-10 01:19:54,520 root         INFO     Train Epoch: 51 [4608/8000 (58%)]\tTotal Loss: 0.726049\n",
      "Reconstruction: 0.412111, Regularization: 0.233161, Discriminator: 0.026718; Generator: 0.054059,\n",
      "D(x): 0.753, D(G(z)): 0.186\n",
      "2019-04-10 01:19:54,620 root         INFO     Train Epoch: 51 [5120/8000 (64%)]\tTotal Loss: 0.983744\n",
      "Reconstruction: 0.636909, Regularization: 0.281070, Discriminator: 0.011950; Generator: 0.053814,\n",
      "D(x): 0.902, D(G(z)): 0.195\n",
      "2019-04-10 01:19:54,721 root         INFO     Train Epoch: 51 [5632/8000 (70%)]\tTotal Loss: 1.103006\n",
      "Reconstruction: 0.700732, Regularization: 0.335165, Discriminator: 0.018011; Generator: 0.049097,\n",
      "D(x): 0.842, D(G(z)): 0.242\n",
      "2019-04-10 01:19:54,818 root         INFO     Train Epoch: 51 [6144/8000 (77%)]\tTotal Loss: 1.077964\n",
      "Reconstruction: 0.668406, Regularization: 0.342641, Discriminator: 0.016666; Generator: 0.050251,\n",
      "D(x): 0.871, D(G(z)): 0.227\n",
      "2019-04-10 01:19:54,915 root         INFO     Train Epoch: 51 [6656/8000 (83%)]\tTotal Loss: 0.794870\n",
      "Reconstruction: 0.493405, Regularization: 0.230471, Discriminator: 0.018276; Generator: 0.052718,\n",
      "D(x): 0.841, D(G(z)): 0.203\n",
      "2019-04-10 01:19:55,012 root         INFO     Train Epoch: 51 [7168/8000 (90%)]\tTotal Loss: 1.073528\n",
      "Reconstruction: 0.648127, Regularization: 0.359580, Discriminator: 0.013400; Generator: 0.052420,\n",
      "D(x): 0.912, D(G(z)): 0.202\n",
      "2019-04-10 01:19:55,109 root         INFO     Train Epoch: 51 [7680/8000 (96%)]\tTotal Loss: 0.974845\n",
      "Reconstruction: 0.600369, Regularization: 0.304946, Discriminator: 0.015853; Generator: 0.053677,\n",
      "D(x): 0.865, D(G(z)): 0.189\n",
      "2019-04-10 01:19:55,182 root         INFO     ====> Epoch: 51 Average loss: 700.3952\n",
      "2019-04-10 01:19:55,209 root         INFO     Train Epoch: 52 [0/8000 (0%)]\tTotal Loss: 1.132491\n",
      "Reconstruction: 0.685777, Regularization: 0.380287, Discriminator: 0.011682; Generator: 0.054745,\n",
      "D(x): 0.904, D(G(z)): 0.189\n",
      "2019-04-10 01:19:55,310 root         INFO     Train Epoch: 52 [512/8000 (6%)]\tTotal Loss: 129.657227\n",
      "Reconstruction: 129.320114, Regularization: 0.272207, Discriminator: 0.007754; Generator: 0.057164,\n",
      "D(x): 0.941, D(G(z)): 0.162\n",
      "2019-04-10 01:19:55,410 root         INFO     Train Epoch: 52 [1024/8000 (13%)]\tTotal Loss: 0.914226\n",
      "Reconstruction: 0.549307, Regularization: 0.293643, Discriminator: 0.017489; Generator: 0.053787,\n",
      "D(x): 0.804, D(G(z)): 0.188\n",
      "2019-04-10 01:19:55,510 root         INFO     Train Epoch: 52 [1536/8000 (19%)]\tTotal Loss: 80001.968750\n",
      "Reconstruction: 80001.570312, Regularization: 0.330349, Discriminator: 0.013698; Generator: 0.053721,\n",
      "D(x): 0.875, D(G(z)): 0.187\n",
      "2019-04-10 01:19:55,611 root         INFO     Train Epoch: 52 [2048/8000 (26%)]\tTotal Loss: 0.915801\n",
      "Reconstruction: 0.551933, Regularization: 0.293074, Discriminator: 0.020257; Generator: 0.050537,\n",
      "D(x): 0.840, D(G(z)): 0.225\n",
      "2019-04-10 01:19:55,711 root         INFO     Train Epoch: 52 [2560/8000 (32%)]\tTotal Loss: 6.345086\n",
      "Reconstruction: 6.068922, Regularization: 0.201333, Discriminator: 0.023436; Generator: 0.051395,\n",
      "D(x): 0.781, D(G(z)): 0.218\n",
      "2019-04-10 01:19:55,811 root         INFO     Train Epoch: 52 [3072/8000 (38%)]\tTotal Loss: 0.796412\n",
      "Reconstruction: 0.458634, Regularization: 0.248471, Discriminator: 0.036922; Generator: 0.052384,\n",
      "D(x): 0.723, D(G(z)): 0.207\n",
      "2019-04-10 01:19:55,911 root         INFO     Train Epoch: 52 [3584/8000 (45%)]\tTotal Loss: 1.045804\n",
      "Reconstruction: 0.640416, Regularization: 0.334574, Discriminator: 0.018587; Generator: 0.052227,\n",
      "D(x): 0.845, D(G(z)): 0.208\n",
      "2019-04-10 01:19:56,011 root         INFO     Train Epoch: 52 [4096/8000 (51%)]\tTotal Loss: 0.980432\n",
      "Reconstruction: 0.605132, Regularization: 0.303864, Discriminator: 0.017477; Generator: 0.053959,\n",
      "D(x): 0.799, D(G(z)): 0.188\n",
      "2019-04-10 01:19:56,111 root         INFO     Train Epoch: 52 [4608/8000 (58%)]\tTotal Loss: 14.090150\n",
      "Reconstruction: 13.476356, Regularization: 0.542097, Discriminator: 0.018991; Generator: 0.052706,\n",
      "D(x): 0.866, D(G(z)): 0.205\n",
      "2019-04-10 01:19:56,211 root         INFO     Train Epoch: 52 [5120/8000 (64%)]\tTotal Loss: 0.930220\n",
      "Reconstruction: 0.564829, Regularization: 0.292823, Discriminator: 0.022334; Generator: 0.050235,\n",
      "D(x): 0.779, D(G(z)): 0.228\n",
      "2019-04-10 01:19:56,311 root         INFO     Train Epoch: 52 [5632/8000 (70%)]\tTotal Loss: 1.109935\n",
      "Reconstruction: 0.665102, Regularization: 0.372759, Discriminator: 0.019890; Generator: 0.052183,\n",
      "D(x): 0.794, D(G(z)): 0.198\n",
      "2019-04-10 01:19:56,411 root         INFO     Train Epoch: 52 [6144/8000 (77%)]\tTotal Loss: 1.108542\n",
      "Reconstruction: 0.678806, Regularization: 0.363239, Discriminator: 0.015268; Generator: 0.051229,\n",
      "D(x): 0.886, D(G(z)): 0.218\n",
      "2019-04-10 01:19:56,512 root         INFO     Train Epoch: 52 [6656/8000 (83%)]\tTotal Loss: 0.818988\n",
      "Reconstruction: 0.489401, Regularization: 0.259466, Discriminator: 0.016856; Generator: 0.053264,\n",
      "D(x): 0.812, D(G(z)): 0.195\n",
      "2019-04-10 01:19:56,612 root         INFO     Train Epoch: 52 [7168/8000 (90%)]\tTotal Loss: 1.520744\n",
      "Reconstruction: 1.224548, Regularization: 0.207676, Discriminator: 0.036466; Generator: 0.052054,\n",
      "D(x): 0.671, D(G(z)): 0.206\n",
      "2019-04-10 01:19:56,712 root         INFO     Train Epoch: 52 [7680/8000 (96%)]\tTotal Loss: 1.219995\n",
      "Reconstruction: 0.760236, Regularization: 0.380487, Discriminator: 0.028771; Generator: 0.050501,\n",
      "D(x): 0.733, D(G(z)): 0.224\n",
      "2019-04-10 01:19:56,786 root         INFO     ====> Epoch: 52 Average loss: 27141.2358\n",
      "2019-04-10 01:19:56,813 root         INFO     Train Epoch: 53 [0/8000 (0%)]\tTotal Loss: 348.157410\n",
      "Reconstruction: 347.903046, Regularization: 0.171516, Discriminator: 0.030905; Generator: 0.051958,\n",
      "D(x): 0.735, D(G(z)): 0.210\n",
      "2019-04-10 01:19:56,913 root         INFO     Train Epoch: 53 [512/8000 (6%)]\tTotal Loss: 401.404541\n",
      "Reconstruction: 401.101135, Regularization: 0.223548, Discriminator: 0.025781; Generator: 0.054073,\n",
      "D(x): 0.756, D(G(z)): 0.185\n",
      "2019-04-10 01:19:57,013 root         INFO     Train Epoch: 53 [1024/8000 (13%)]\tTotal Loss: 0.966655\n",
      "Reconstruction: 0.592814, Regularization: 0.302785, Discriminator: 0.022245; Generator: 0.048810,\n",
      "D(x): 0.800, D(G(z)): 0.248\n",
      "2019-04-10 01:19:57,112 root         INFO     Train Epoch: 53 [1536/8000 (19%)]\tTotal Loss: 1.294877\n",
      "Reconstruction: 0.788519, Regularization: 0.438742, Discriminator: 0.017783; Generator: 0.049833,\n",
      "D(x): 0.881, D(G(z)): 0.227\n",
      "2019-04-10 01:19:57,211 root         INFO     Train Epoch: 53 [2048/8000 (26%)]\tTotal Loss: 1.536408\n",
      "Reconstruction: 1.070962, Regularization: 0.395795, Discriminator: 0.019437; Generator: 0.050215,\n",
      "D(x): 0.823, D(G(z)): 0.222\n",
      "2019-04-10 01:19:57,309 root         INFO     Train Epoch: 53 [2560/8000 (32%)]\tTotal Loss: 0.827303\n",
      "Reconstruction: 0.497365, Regularization: 0.261363, Discriminator: 0.020771; Generator: 0.047803,\n",
      "D(x): 0.791, D(G(z)): 0.246\n",
      "2019-04-10 01:19:57,409 root         INFO     Train Epoch: 53 [3072/8000 (38%)]\tTotal Loss: 1.643684\n",
      "Reconstruction: 1.308268, Regularization: 0.247799, Discriminator: 0.034850; Generator: 0.052767,\n",
      "D(x): 0.693, D(G(z)): 0.194\n",
      "2019-04-10 01:19:57,510 root         INFO     Train Epoch: 53 [3584/8000 (45%)]\tTotal Loss: 0.676970\n",
      "Reconstruction: 0.391385, Regularization: 0.213797, Discriminator: 0.021408; Generator: 0.050380,\n",
      "D(x): 0.819, D(G(z)): 0.219\n",
      "2019-04-10 01:19:57,610 root         INFO     Train Epoch: 53 [4096/8000 (51%)]\tTotal Loss: 9418.713867\n",
      "Reconstruction: 9418.329102, Regularization: 0.312898, Discriminator: 0.018941; Generator: 0.053767,\n",
      "D(x): 0.796, D(G(z)): 0.187\n",
      "2019-04-10 01:19:57,711 root         INFO     Train Epoch: 53 [4608/8000 (58%)]\tTotal Loss: 3.339046\n",
      "Reconstruction: 2.866301, Regularization: 0.390584, Discriminator: 0.029711; Generator: 0.052450,\n",
      "D(x): 0.741, D(G(z)): 0.195\n",
      "2019-04-10 01:19:57,811 root         INFO     Train Epoch: 53 [5120/8000 (64%)]\tTotal Loss: 1.411444\n",
      "Reconstruction: 0.871354, Regularization: 0.472948, Discriminator: 0.013590; Generator: 0.053552,\n",
      "D(x): 0.880, D(G(z)): 0.195\n",
      "2019-04-10 01:19:57,912 root         INFO     Train Epoch: 53 [5632/8000 (70%)]\tTotal Loss: 0.853696\n",
      "Reconstruction: 0.560376, Regularization: 0.221850, Discriminator: 0.022114; Generator: 0.049356,\n",
      "D(x): 0.815, D(G(z)): 0.229\n",
      "2019-04-10 01:19:58,013 root         INFO     Train Epoch: 53 [6144/8000 (77%)]\tTotal Loss: 0.961762\n",
      "Reconstruction: 0.574262, Regularization: 0.320156, Discriminator: 0.019832; Generator: 0.047512,\n",
      "D(x): 0.832, D(G(z)): 0.250\n",
      "2019-04-10 01:19:58,113 root         INFO     Train Epoch: 53 [6656/8000 (83%)]\tTotal Loss: 0.900970\n",
      "Reconstruction: 0.534975, Regularization: 0.295865, Discriminator: 0.016247; Generator: 0.053883,\n",
      "D(x): 0.850, D(G(z)): 0.191\n",
      "2019-04-10 01:19:58,214 root         INFO     Train Epoch: 53 [7168/8000 (90%)]\tTotal Loss: 0.924213\n",
      "Reconstruction: 0.556194, Regularization: 0.292457, Discriminator: 0.025536; Generator: 0.050026,\n",
      "D(x): 0.752, D(G(z)): 0.220\n",
      "2019-04-10 01:19:58,314 root         INFO     Train Epoch: 53 [7680/8000 (96%)]\tTotal Loss: 1.342052\n",
      "Reconstruction: 1.025684, Regularization: 0.238648, Discriminator: 0.028699; Generator: 0.049022,\n",
      "D(x): 0.727, D(G(z)): 0.235\n",
      "2019-04-10 01:19:58,389 root         INFO     ====> Epoch: 53 Average loss: 45936681.1135\n",
      "2019-04-10 01:19:58,416 root         INFO     Train Epoch: 54 [0/8000 (0%)]\tTotal Loss: 4.696368\n",
      "Reconstruction: 4.306667, Regularization: 0.319641, Discriminator: 0.016833; Generator: 0.053226,\n",
      "D(x): 0.847, D(G(z)): 0.189\n",
      "2019-04-10 01:19:58,516 root         INFO     Train Epoch: 54 [512/8000 (6%)]\tTotal Loss: 3.797089\n",
      "Reconstruction: 3.544580, Regularization: 0.177306, Discriminator: 0.025112; Generator: 0.050091,\n",
      "D(x): 0.797, D(G(z)): 0.226\n",
      "2019-04-10 01:19:58,616 root         INFO     Train Epoch: 54 [1024/8000 (13%)]\tTotal Loss: 1.453599\n",
      "Reconstruction: 1.056195, Regularization: 0.321807, Discriminator: 0.028453; Generator: 0.047144,\n",
      "D(x): 0.762, D(G(z)): 0.257\n",
      "2019-04-10 01:19:58,717 root         INFO     Train Epoch: 54 [1536/8000 (19%)]\tTotal Loss: 3.897783\n",
      "Reconstruction: 3.566208, Regularization: 0.256291, Discriminator: 0.021084; Generator: 0.054201,\n",
      "D(x): 0.795, D(G(z)): 0.186\n",
      "2019-04-10 01:19:58,818 root         INFO     Train Epoch: 54 [2048/8000 (26%)]\tTotal Loss: 0.699820\n",
      "Reconstruction: 0.404424, Regularization: 0.224148, Discriminator: 0.021031; Generator: 0.050217,\n",
      "D(x): 0.792, D(G(z)): 0.221\n",
      "2019-04-10 01:19:58,918 root         INFO     Train Epoch: 54 [2560/8000 (32%)]\tTotal Loss: 1.616627\n",
      "Reconstruction: 1.007909, Regularization: 0.535996, Discriminator: 0.023647; Generator: 0.049074,\n",
      "D(x): 0.802, D(G(z)): 0.226\n",
      "2019-04-10 01:19:59,018 root         INFO     Train Epoch: 54 [3072/8000 (38%)]\tTotal Loss: 0.859883\n",
      "Reconstruction: 0.503199, Regularization: 0.271636, Discriminator: 0.035443; Generator: 0.049605,\n",
      "D(x): 0.689, D(G(z)): 0.227\n",
      "2019-04-10 01:19:59,119 root         INFO     Train Epoch: 54 [3584/8000 (45%)]\tTotal Loss: 1.125378\n",
      "Reconstruction: 0.683653, Regularization: 0.372414, Discriminator: 0.019037; Generator: 0.050273,\n",
      "D(x): 0.832, D(G(z)): 0.213\n",
      "2019-04-10 01:19:59,219 root         INFO     Train Epoch: 54 [4096/8000 (51%)]\tTotal Loss: 0.740072\n",
      "Reconstruction: 0.437588, Regularization: 0.228561, Discriminator: 0.025867; Generator: 0.048056,\n",
      "D(x): 0.752, D(G(z)): 0.239\n",
      "2019-04-10 01:19:59,320 root         INFO     Train Epoch: 54 [4608/8000 (58%)]\tTotal Loss: 25.527691\n",
      "Reconstruction: 25.095076, Regularization: 0.357451, Discriminator: 0.025586; Generator: 0.049579,\n",
      "D(x): 0.803, D(G(z)): 0.221\n",
      "2019-04-10 01:19:59,420 root         INFO     Train Epoch: 54 [5120/8000 (64%)]\tTotal Loss: 0.984136\n",
      "Reconstruction: 0.645840, Regularization: 0.268615, Discriminator: 0.021133; Generator: 0.048548,\n",
      "D(x): 0.812, D(G(z)): 0.232\n",
      "2019-04-10 01:19:59,520 root         INFO     Train Epoch: 54 [5632/8000 (70%)]\tTotal Loss: 0.949335\n",
      "Reconstruction: 0.638819, Regularization: 0.234161, Discriminator: 0.026888; Generator: 0.049467,\n",
      "D(x): 0.766, D(G(z)): 0.226\n",
      "2019-04-10 01:19:59,618 root         INFO     Train Epoch: 54 [6144/8000 (77%)]\tTotal Loss: 208.647446\n",
      "Reconstruction: 208.288162, Regularization: 0.281927, Discriminator: 0.030075; Generator: 0.047290,\n",
      "D(x): 0.753, D(G(z)): 0.250\n",
      "2019-04-10 01:19:59,715 root         INFO     Train Epoch: 54 [6656/8000 (83%)]\tTotal Loss: 15.263288\n",
      "Reconstruction: 14.901978, Regularization: 0.282225, Discriminator: 0.024004; Generator: 0.055080,\n",
      "D(x): 0.757, D(G(z)): 0.181\n",
      "2019-04-10 01:19:59,813 root         INFO     Train Epoch: 54 [7168/8000 (90%)]\tTotal Loss: 0.625125\n",
      "Reconstruction: 0.348884, Regularization: 0.196936, Discriminator: 0.027808; Generator: 0.051497,\n",
      "D(x): 0.742, D(G(z)): 0.206\n",
      "2019-04-10 01:19:59,910 root         INFO     Train Epoch: 54 [7680/8000 (96%)]\tTotal Loss: 49.044056\n",
      "Reconstruction: 48.582310, Regularization: 0.395136, Discriminator: 0.018125; Generator: 0.048483,\n",
      "D(x): 0.843, D(G(z)): 0.229\n",
      "2019-04-10 01:19:59,982 root         INFO     ====> Epoch: 54 Average loss: 19213892627.3547\n",
      "2019-04-10 01:20:00,009 root         INFO     Train Epoch: 55 [0/8000 (0%)]\tTotal Loss: 0.911241\n",
      "Reconstruction: 0.544575, Regularization: 0.294136, Discriminator: 0.026303; Generator: 0.046227,\n",
      "D(x): 0.776, D(G(z)): 0.265\n",
      "2019-04-10 01:20:00,109 root         INFO     Train Epoch: 55 [512/8000 (6%)]\tTotal Loss: 0.837591\n",
      "Reconstruction: 0.564570, Regularization: 0.192440, Discriminator: 0.029252; Generator: 0.051328,\n",
      "D(x): 0.741, D(G(z)): 0.209\n",
      "2019-04-10 01:20:00,209 root         INFO     Train Epoch: 55 [1024/8000 (13%)]\tTotal Loss: 72.583588\n",
      "Reconstruction: 72.249992, Regularization: 0.258562, Discriminator: 0.024999; Generator: 0.050038,\n",
      "D(x): 0.717, D(G(z)): 0.216\n",
      "2019-04-10 01:20:00,310 root         INFO     Train Epoch: 55 [1536/8000 (19%)]\tTotal Loss: 1.175463\n",
      "Reconstruction: 0.736622, Regularization: 0.360479, Discriminator: 0.026208; Generator: 0.052154,\n",
      "D(x): 0.782, D(G(z)): 0.195\n",
      "2019-04-10 01:20:00,412 root         INFO     Train Epoch: 55 [2048/8000 (26%)]\tTotal Loss: 1.049946\n",
      "Reconstruction: 0.724874, Regularization: 0.250962, Discriminator: 0.023180; Generator: 0.050930,\n",
      "D(x): 0.775, D(G(z)): 0.216\n",
      "2019-04-10 01:20:00,513 root         INFO     Train Epoch: 55 [2560/8000 (32%)]\tTotal Loss: 1.296244\n",
      "Reconstruction: 0.796972, Regularization: 0.437182, Discriminator: 0.013536; Generator: 0.048553,\n",
      "D(x): 0.891, D(G(z)): 0.234\n",
      "2019-04-10 01:20:00,613 root         INFO     Train Epoch: 55 [3072/8000 (38%)]\tTotal Loss: 1.204851\n",
      "Reconstruction: 0.804530, Regularization: 0.319374, Discriminator: 0.031065; Generator: 0.049883,\n",
      "D(x): 0.699, D(G(z)): 0.220\n",
      "2019-04-10 01:20:00,712 root         INFO     Train Epoch: 55 [3584/8000 (45%)]\tTotal Loss: 1.006305\n",
      "Reconstruction: 0.684497, Regularization: 0.245644, Discriminator: 0.028369; Generator: 0.047795,\n",
      "D(x): 0.721, D(G(z)): 0.245\n",
      "2019-04-10 01:20:00,811 root         INFO     Train Epoch: 55 [4096/8000 (51%)]\tTotal Loss: 0.644217\n",
      "Reconstruction: 0.373780, Regularization: 0.198699, Discriminator: 0.026847; Generator: 0.044891,\n",
      "D(x): 0.752, D(G(z)): 0.266\n",
      "2019-04-10 01:20:00,909 root         INFO     Train Epoch: 55 [4608/8000 (58%)]\tTotal Loss: 0.832910\n",
      "Reconstruction: 0.526314, Regularization: 0.214300, Discriminator: 0.042853; Generator: 0.049443,\n",
      "D(x): 0.580, D(G(z)): 0.228\n",
      "2019-04-10 01:20:01,007 root         INFO     Train Epoch: 55 [5120/8000 (64%)]\tTotal Loss: 1.076986\n",
      "Reconstruction: 0.644480, Regularization: 0.357594, Discriminator: 0.022834; Generator: 0.052079,\n",
      "D(x): 0.794, D(G(z)): 0.208\n",
      "2019-04-10 01:20:01,106 root         INFO     Train Epoch: 55 [5632/8000 (70%)]\tTotal Loss: 4377352.500000\n",
      "Reconstruction: 4377352.000000, Regularization: 0.252009, Discriminator: 0.022387; Generator: 0.046894,\n",
      "D(x): 0.781, D(G(z)): 0.243\n",
      "2019-04-10 01:20:01,204 root         INFO     Train Epoch: 55 [6144/8000 (77%)]\tTotal Loss: 188.150024\n",
      "Reconstruction: 187.854492, Regularization: 0.220044, Discriminator: 0.028869; Generator: 0.046614,\n",
      "D(x): 0.737, D(G(z)): 0.254\n",
      "2019-04-10 01:20:01,302 root         INFO     Train Epoch: 55 [6656/8000 (83%)]\tTotal Loss: 0.826615\n",
      "Reconstruction: 0.490867, Regularization: 0.260829, Discriminator: 0.023488; Generator: 0.051431,\n",
      "D(x): 0.741, D(G(z)): 0.207\n",
      "2019-04-10 01:20:01,400 root         INFO     Train Epoch: 55 [7168/8000 (90%)]\tTotal Loss: 0.564850\n",
      "Reconstruction: 0.306862, Regularization: 0.176164, Discriminator: 0.030330; Generator: 0.051495,\n",
      "D(x): 0.686, D(G(z)): 0.200\n",
      "2019-04-10 01:20:01,498 root         INFO     Train Epoch: 55 [7680/8000 (96%)]\tTotal Loss: 1.118231\n",
      "Reconstruction: 0.691398, Regularization: 0.357237, Discriminator: 0.022202; Generator: 0.047395,\n",
      "D(x): 0.774, D(G(z)): 0.253\n",
      "2019-04-10 01:20:01,573 root         INFO     ====> Epoch: 55 Average loss: 24330.0600\n",
      "2019-04-10 01:20:01,599 root         INFO     Train Epoch: 56 [0/8000 (0%)]\tTotal Loss: 1.758736\n",
      "Reconstruction: 1.517722, Regularization: 0.161634, Discriminator: 0.030776; Generator: 0.048605,\n",
      "D(x): 0.651, D(G(z)): 0.223\n",
      "2019-04-10 01:20:01,700 root         INFO     Train Epoch: 56 [512/8000 (6%)]\tTotal Loss: 1.890008\n",
      "Reconstruction: 1.535465, Regularization: 0.289949, Discriminator: 0.014432; Generator: 0.050163,\n",
      "D(x): 0.897, D(G(z)): 0.212\n",
      "2019-04-10 01:20:01,800 root         INFO     Train Epoch: 56 [1024/8000 (13%)]\tTotal Loss: 5.153776\n",
      "Reconstruction: 4.685312, Regularization: 0.405077, Discriminator: 0.016542; Generator: 0.046845,\n",
      "D(x): 0.848, D(G(z)): 0.248\n",
      "2019-04-10 01:20:01,899 root         INFO     Train Epoch: 56 [1536/8000 (19%)]\tTotal Loss: 1.228353\n",
      "Reconstruction: 0.747493, Regularization: 0.400428, Discriminator: 0.034101; Generator: 0.046331,\n",
      "D(x): 0.704, D(G(z)): 0.250\n",
      "2019-04-10 01:20:02,001 root         INFO     Train Epoch: 56 [2048/8000 (26%)]\tTotal Loss: 1.212322\n",
      "Reconstruction: 0.869556, Regularization: 0.261845, Discriminator: 0.031392; Generator: 0.049528,\n",
      "D(x): 0.677, D(G(z)): 0.219\n",
      "2019-04-10 01:20:02,108 root         INFO     Train Epoch: 56 [2560/8000 (32%)]\tTotal Loss: 8.590233\n",
      "Reconstruction: 8.284656, Regularization: 0.228204, Discriminator: 0.027370; Generator: 0.050003,\n",
      "D(x): 0.728, D(G(z)): 0.212\n",
      "2019-04-10 01:20:02,212 root         INFO     Train Epoch: 56 [3072/8000 (38%)]\tTotal Loss: 268580.843750\n",
      "Reconstruction: 268580.562500, Regularization: 0.228386, Discriminator: 0.024139; Generator: 0.048916,\n",
      "D(x): 0.741, D(G(z)): 0.225\n",
      "2019-04-10 01:20:02,316 root         INFO     Train Epoch: 56 [3584/8000 (45%)]\tTotal Loss: 1.238421\n",
      "Reconstruction: 0.752065, Regularization: 0.410588, Discriminator: 0.029024; Generator: 0.046743,\n",
      "D(x): 0.778, D(G(z)): 0.247\n",
      "2019-04-10 01:20:02,419 root         INFO     Train Epoch: 56 [4096/8000 (51%)]\tTotal Loss: 1.203223\n",
      "Reconstruction: 0.736109, Regularization: 0.399239, Discriminator: 0.020378; Generator: 0.047498,\n",
      "D(x): 0.794, D(G(z)): 0.234\n",
      "2019-04-10 01:20:02,519 root         INFO     Train Epoch: 56 [4608/8000 (58%)]\tTotal Loss: 1734739072.000000\n",
      "Reconstruction: 1734739072.000000, Regularization: 0.263336, Discriminator: 0.029148; Generator: 0.043838,\n",
      "D(x): 0.721, D(G(z)): 0.278\n",
      "2019-04-10 01:20:02,619 root         INFO     Train Epoch: 56 [5120/8000 (64%)]\tTotal Loss: 1.312546\n",
      "Reconstruction: 0.803282, Regularization: 0.438686, Discriminator: 0.024014; Generator: 0.046564,\n",
      "D(x): 0.766, D(G(z)): 0.249\n",
      "2019-04-10 01:20:02,719 root         INFO     Train Epoch: 56 [5632/8000 (70%)]\tTotal Loss: 0.864481\n",
      "Reconstruction: 0.504318, Regularization: 0.287613, Discriminator: 0.025188; Generator: 0.047362,\n",
      "D(x): 0.706, D(G(z)): 0.238\n",
      "2019-04-10 01:20:02,818 root         INFO     Train Epoch: 56 [6144/8000 (77%)]\tTotal Loss: 2.074463\n",
      "Reconstruction: 1.534521, Regularization: 0.468771, Discriminator: 0.022129; Generator: 0.049043,\n",
      "D(x): 0.810, D(G(z)): 0.224\n",
      "2019-04-10 01:20:02,917 root         INFO     Train Epoch: 56 [6656/8000 (83%)]\tTotal Loss: 1480.036865\n",
      "Reconstruction: 1479.604858, Regularization: 0.360263, Discriminator: 0.025208; Generator: 0.046554,\n",
      "D(x): 0.802, D(G(z)): 0.258\n",
      "2019-04-10 01:20:03,016 root         INFO     Train Epoch: 56 [7168/8000 (90%)]\tTotal Loss: 1.292891\n",
      "Reconstruction: 0.930283, Regularization: 0.291262, Discriminator: 0.028024; Generator: 0.043322,\n",
      "D(x): 0.724, D(G(z)): 0.282\n",
      "2019-04-10 01:20:03,116 root         INFO     Train Epoch: 56 [7680/8000 (96%)]\tTotal Loss: 2.203005\n",
      "Reconstruction: 1.833915, Regularization: 0.289941, Discriminator: 0.030527; Generator: 0.048622,\n",
      "D(x): 0.651, D(G(z)): 0.226\n",
      "2019-04-10 01:20:03,190 root         INFO     ====> Epoch: 56 Average loss: 352617792.0087\n",
      "2019-04-10 01:20:03,217 root         INFO     Train Epoch: 57 [0/8000 (0%)]\tTotal Loss: 1.025629\n",
      "Reconstruction: 0.575656, Regularization: 0.377281, Discriminator: 0.028946; Generator: 0.043747,\n",
      "D(x): 0.745, D(G(z)): 0.293\n",
      "2019-04-10 01:20:03,318 root         INFO     Train Epoch: 57 [512/8000 (6%)]\tTotal Loss: 0.770571\n",
      "Reconstruction: 0.496003, Regularization: 0.202704, Discriminator: 0.026695; Generator: 0.045169,\n",
      "D(x): 0.750, D(G(z)): 0.270\n",
      "2019-04-10 01:20:03,419 root         INFO     Train Epoch: 57 [1024/8000 (13%)]\tTotal Loss: 0.823781\n",
      "Reconstruction: 0.495706, Regularization: 0.246967, Discriminator: 0.035166; Generator: 0.045943,\n",
      "D(x): 0.651, D(G(z)): 0.254\n",
      "2019-04-10 01:20:03,519 root         INFO     Train Epoch: 57 [1536/8000 (19%)]\tTotal Loss: 4.129996\n",
      "Reconstruction: 3.835270, Regularization: 0.223311, Discriminator: 0.023956; Generator: 0.047459,\n",
      "D(x): 0.775, D(G(z)): 0.229\n",
      "2019-04-10 01:20:03,623 root         INFO     Train Epoch: 57 [2048/8000 (26%)]\tTotal Loss: 0.714179\n",
      "Reconstruction: 0.418858, Regularization: 0.219796, Discriminator: 0.028987; Generator: 0.046538,\n",
      "D(x): 0.742, D(G(z)): 0.238\n",
      "2019-04-10 01:20:03,725 root         INFO     Train Epoch: 57 [2560/8000 (32%)]\tTotal Loss: 1.147329\n",
      "Reconstruction: 0.691278, Regularization: 0.393544, Discriminator: 0.019439; Generator: 0.043068,\n",
      "D(x): 0.854, D(G(z)): 0.283\n",
      "2019-04-10 01:20:03,824 root         INFO     Train Epoch: 57 [3072/8000 (38%)]\tTotal Loss: 1.315477\n",
      "Reconstruction: 0.810753, Regularization: 0.435153, Discriminator: 0.024426; Generator: 0.045145,\n",
      "D(x): 0.833, D(G(z)): 0.268\n",
      "2019-04-10 01:20:03,925 root         INFO     Train Epoch: 57 [3584/8000 (45%)]\tTotal Loss: 117.950714\n",
      "Reconstruction: 117.595238, Regularization: 0.284311, Discriminator: 0.024254; Generator: 0.046914,\n",
      "D(x): 0.774, D(G(z)): 0.246\n",
      "2019-04-10 01:20:04,024 root         INFO     Train Epoch: 57 [4096/8000 (51%)]\tTotal Loss: 157497.718750\n",
      "Reconstruction: 157497.250000, Regularization: 0.397654, Discriminator: 0.027010; Generator: 0.044404,\n",
      "D(x): 0.719, D(G(z)): 0.271\n",
      "2019-04-10 01:20:04,124 root         INFO     Train Epoch: 57 [4608/8000 (58%)]\tTotal Loss: 24884.066406\n",
      "Reconstruction: 24883.570312, Regularization: 0.416898, Discriminator: 0.035603; Generator: 0.044294,\n",
      "D(x): 0.701, D(G(z)): 0.278\n",
      "2019-04-10 01:20:04,222 root         INFO     Train Epoch: 57 [5120/8000 (64%)]\tTotal Loss: 1.140952\n",
      "Reconstruction: 0.695802, Regularization: 0.378135, Discriminator: 0.023122; Generator: 0.043893,\n",
      "D(x): 0.807, D(G(z)): 0.280\n",
      "2019-04-10 01:20:04,320 root         INFO     Train Epoch: 57 [5632/8000 (70%)]\tTotal Loss: 0.916509\n",
      "Reconstruction: 0.556034, Regularization: 0.284655, Discriminator: 0.028665; Generator: 0.047155,\n",
      "D(x): 0.701, D(G(z)): 0.236\n",
      "2019-04-10 01:20:04,419 root         INFO     Train Epoch: 57 [6144/8000 (77%)]\tTotal Loss: 0.853038\n",
      "Reconstruction: 0.492821, Regularization: 0.285497, Discriminator: 0.030450; Generator: 0.044268,\n",
      "D(x): 0.692, D(G(z)): 0.264\n",
      "2019-04-10 01:20:04,521 root         INFO     Train Epoch: 57 [6656/8000 (83%)]\tTotal Loss: 7.455615\n",
      "Reconstruction: 7.047483, Regularization: 0.339120, Discriminator: 0.019464; Generator: 0.049547,\n",
      "D(x): 0.821, D(G(z)): 0.227\n",
      "2019-04-10 01:20:04,623 root         INFO     Train Epoch: 57 [7168/8000 (90%)]\tTotal Loss: 1.488461\n",
      "Reconstruction: 0.969139, Regularization: 0.443781, Discriminator: 0.029201; Generator: 0.046341,\n",
      "D(x): 0.726, D(G(z)): 0.241\n",
      "2019-04-10 01:20:04,723 root         INFO     Train Epoch: 57 [7680/8000 (96%)]\tTotal Loss: 0.942241\n",
      "Reconstruction: 0.647311, Regularization: 0.227151, Discriminator: 0.023268; Generator: 0.044511,\n",
      "D(x): 0.774, D(G(z)): 0.272\n",
      "2019-04-10 01:20:04,797 root         INFO     ====> Epoch: 57 Average loss: 2017939.9979\n",
      "2019-04-10 01:20:04,825 root         INFO     Train Epoch: 58 [0/8000 (0%)]\tTotal Loss: 4.545347\n",
      "Reconstruction: 4.161211, Regularization: 0.313599, Discriminator: 0.024556; Generator: 0.045980,\n",
      "D(x): 0.754, D(G(z)): 0.254\n",
      "2019-04-10 01:20:04,930 root         INFO     Train Epoch: 58 [512/8000 (6%)]\tTotal Loss: 0.636604\n",
      "Reconstruction: 0.363471, Regularization: 0.202494, Discriminator: 0.026686; Generator: 0.043953,\n",
      "D(x): 0.684, D(G(z)): 0.261\n",
      "2019-04-10 01:20:05,032 root         INFO     Train Epoch: 58 [1024/8000 (13%)]\tTotal Loss: 0.995269\n",
      "Reconstruction: 0.591815, Regularization: 0.333136, Discriminator: 0.026488; Generator: 0.043829,\n",
      "D(x): 0.754, D(G(z)): 0.274\n",
      "2019-04-10 01:20:05,138 root         INFO     Train Epoch: 58 [1536/8000 (19%)]\tTotal Loss: 0.837396\n",
      "Reconstruction: 0.487533, Regularization: 0.278205, Discriminator: 0.027525; Generator: 0.044132,\n",
      "D(x): 0.737, D(G(z)): 0.276\n",
      "2019-04-10 01:20:05,244 root         INFO     Train Epoch: 58 [2048/8000 (26%)]\tTotal Loss: 0.935641\n",
      "Reconstruction: 0.552313, Regularization: 0.306114, Discriminator: 0.033273; Generator: 0.043941,\n",
      "D(x): 0.693, D(G(z)): 0.268\n",
      "2019-04-10 01:20:05,347 root         INFO     Train Epoch: 58 [2560/8000 (32%)]\tTotal Loss: 0.697158\n",
      "Reconstruction: 0.399579, Regularization: 0.229685, Discriminator: 0.028101; Generator: 0.039793,\n",
      "D(x): 0.746, D(G(z)): 0.314\n",
      "2019-04-10 01:20:05,447 root         INFO     Train Epoch: 58 [3072/8000 (38%)]\tTotal Loss: 0.897546\n",
      "Reconstruction: 0.527233, Regularization: 0.301757, Discriminator: 0.020920; Generator: 0.047636,\n",
      "D(x): 0.796, D(G(z)): 0.227\n",
      "2019-04-10 01:20:05,547 root         INFO     Train Epoch: 58 [3584/8000 (45%)]\tTotal Loss: 1.714428\n",
      "Reconstruction: 1.171401, Regularization: 0.480022, Discriminator: 0.016600; Generator: 0.046406,\n",
      "D(x): 0.866, D(G(z)): 0.245\n",
      "2019-04-10 01:20:05,648 root         INFO     Train Epoch: 58 [4096/8000 (51%)]\tTotal Loss: 1732.601807\n",
      "Reconstruction: 1732.293335, Regularization: 0.235201, Discriminator: 0.027469; Generator: 0.045777,\n",
      "D(x): 0.717, D(G(z)): 0.252\n",
      "2019-04-10 01:20:05,761 root         INFO     Train Epoch: 58 [4608/8000 (58%)]\tTotal Loss: 1.043793\n",
      "Reconstruction: 0.620296, Regularization: 0.347139, Discriminator: 0.028568; Generator: 0.047791,\n",
      "D(x): 0.687, D(G(z)): 0.232\n",
      "2019-04-10 01:20:05,869 root         INFO     Train Epoch: 58 [5120/8000 (64%)]\tTotal Loss: 0.793243\n",
      "Reconstruction: 0.496100, Regularization: 0.225400, Discriminator: 0.029332; Generator: 0.042411,\n",
      "D(x): 0.697, D(G(z)): 0.287\n",
      "2019-04-10 01:20:05,983 root         INFO     Train Epoch: 58 [5632/8000 (70%)]\tTotal Loss: 0.493634\n",
      "Reconstruction: 0.262394, Regularization: 0.150441, Discriminator: 0.034583; Generator: 0.046216,\n",
      "D(x): 0.625, D(G(z)): 0.239\n",
      "2019-04-10 01:20:06,090 root         INFO     Train Epoch: 58 [6144/8000 (77%)]\tTotal Loss: 0.655181\n",
      "Reconstruction: 0.424972, Regularization: 0.153407, Discriminator: 0.034671; Generator: 0.042132,\n",
      "D(x): 0.693, D(G(z)): 0.291\n",
      "2019-04-10 01:20:06,195 root         INFO     Train Epoch: 58 [6656/8000 (83%)]\tTotal Loss: 0.597649\n",
      "Reconstruction: 0.332208, Regularization: 0.193467, Discriminator: 0.029276; Generator: 0.042698,\n",
      "D(x): 0.727, D(G(z)): 0.286\n",
      "2019-04-10 01:20:06,300 root         INFO     Train Epoch: 58 [7168/8000 (90%)]\tTotal Loss: 0.768789\n",
      "Reconstruction: 0.455280, Regularization: 0.244946, Discriminator: 0.023591; Generator: 0.044971,\n",
      "D(x): 0.772, D(G(z)): 0.261\n",
      "2019-04-10 01:20:06,405 root         INFO     Train Epoch: 58 [7680/8000 (96%)]\tTotal Loss: 0.762834\n",
      "Reconstruction: 0.449187, Regularization: 0.242532, Discriminator: 0.028060; Generator: 0.043055,\n",
      "D(x): 0.719, D(G(z)): 0.280\n",
      "2019-04-10 01:20:06,483 root         INFO     ====> Epoch: 58 Average loss: 15594.2305\n",
      "2019-04-10 01:20:06,509 root         INFO     Train Epoch: 59 [0/8000 (0%)]\tTotal Loss: 2.448193\n",
      "Reconstruction: 1.995357, Regularization: 0.377995, Discriminator: 0.033741; Generator: 0.041100,\n",
      "D(x): 0.691, D(G(z)): 0.299\n",
      "2019-04-10 01:20:06,610 root         INFO     Train Epoch: 59 [512/8000 (6%)]\tTotal Loss: 5.197242\n",
      "Reconstruction: 4.814122, Regularization: 0.312920, Discriminator: 0.025434; Generator: 0.044766,\n",
      "D(x): 0.762, D(G(z)): 0.263\n",
      "2019-04-10 01:20:06,715 root         INFO     Train Epoch: 59 [1024/8000 (13%)]\tTotal Loss: 1.399754\n",
      "Reconstruction: 0.970106, Regularization: 0.361352, Discriminator: 0.022336; Generator: 0.045962,\n",
      "D(x): 0.814, D(G(z)): 0.248\n",
      "2019-04-10 01:20:06,814 root         INFO     Train Epoch: 59 [1536/8000 (19%)]\tTotal Loss: 0.681875\n",
      "Reconstruction: 0.385120, Regularization: 0.216062, Discriminator: 0.038316; Generator: 0.042376,\n",
      "D(x): 0.583, D(G(z)): 0.289\n",
      "2019-04-10 01:20:06,912 root         INFO     Train Epoch: 59 [2048/8000 (26%)]\tTotal Loss: 17.084024\n",
      "Reconstruction: 16.717417, Regularization: 0.294759, Discriminator: 0.031093; Generator: 0.040757,\n",
      "D(x): 0.702, D(G(z)): 0.307\n",
      "2019-04-10 01:20:07,009 root         INFO     Train Epoch: 59 [2560/8000 (32%)]\tTotal Loss: 2.287503\n",
      "Reconstruction: 1.710956, Regularization: 0.510530, Discriminator: 0.024458; Generator: 0.041559,\n",
      "D(x): 0.770, D(G(z)): 0.296\n",
      "2019-04-10 01:20:07,107 root         INFO     Train Epoch: 59 [3072/8000 (38%)]\tTotal Loss: 4.842957\n",
      "Reconstruction: 4.562044, Regularization: 0.207811, Discriminator: 0.028935; Generator: 0.044168,\n",
      "D(x): 0.696, D(G(z)): 0.264\n",
      "2019-04-10 01:20:07,205 root         INFO     Train Epoch: 59 [3584/8000 (45%)]\tTotal Loss: 0.910584\n",
      "Reconstruction: 0.554847, Regularization: 0.286916, Discriminator: 0.027712; Generator: 0.041108,\n",
      "D(x): 0.719, D(G(z)): 0.298\n",
      "2019-04-10 01:20:07,303 root         INFO     Train Epoch: 59 [4096/8000 (51%)]\tTotal Loss: 0.995463\n",
      "Reconstruction: 0.625316, Regularization: 0.295971, Discriminator: 0.029496; Generator: 0.044680,\n",
      "D(x): 0.699, D(G(z)): 0.258\n",
      "2019-04-10 01:20:07,401 root         INFO     Train Epoch: 59 [4608/8000 (58%)]\tTotal Loss: 0.982225\n",
      "Reconstruction: 0.601108, Regularization: 0.304249, Discriminator: 0.032259; Generator: 0.044609,\n",
      "D(x): 0.657, D(G(z)): 0.258\n",
      "2019-04-10 01:20:07,499 root         INFO     Train Epoch: 59 [5120/8000 (64%)]\tTotal Loss: 0.631327\n",
      "Reconstruction: 0.376665, Regularization: 0.182681, Discriminator: 0.028562; Generator: 0.043418,\n",
      "D(x): 0.657, D(G(z)): 0.268\n",
      "2019-04-10 01:20:07,597 root         INFO     Train Epoch: 59 [5632/8000 (70%)]\tTotal Loss: 0.964961\n",
      "Reconstruction: 0.610914, Regularization: 0.289044, Discriminator: 0.020661; Generator: 0.044342,\n",
      "D(x): 0.764, D(G(z)): 0.261\n",
      "2019-04-10 01:20:07,695 root         INFO     Train Epoch: 59 [6144/8000 (77%)]\tTotal Loss: 1.089420\n",
      "Reconstruction: 0.660985, Regularization: 0.352997, Discriminator: 0.033837; Generator: 0.041600,\n",
      "D(x): 0.671, D(G(z)): 0.278\n",
      "2019-04-10 01:20:07,792 root         INFO     Train Epoch: 59 [6656/8000 (83%)]\tTotal Loss: 0.582206\n",
      "Reconstruction: 0.339514, Regularization: 0.167036, Discriminator: 0.032616; Generator: 0.043039,\n",
      "D(x): 0.631, D(G(z)): 0.283\n",
      "2019-04-10 01:20:07,889 root         INFO     Train Epoch: 59 [7168/8000 (90%)]\tTotal Loss: 16.078655\n",
      "Reconstruction: 15.635608, Regularization: 0.364272, Discriminator: 0.036070; Generator: 0.042706,\n",
      "D(x): 0.634, D(G(z)): 0.282\n",
      "2019-04-10 01:20:07,987 root         INFO     Train Epoch: 59 [7680/8000 (96%)]\tTotal Loss: 2.340188\n",
      "Reconstruction: 1.903154, Regularization: 0.376363, Discriminator: 0.017696; Generator: 0.042975,\n",
      "D(x): 0.847, D(G(z)): 0.277\n",
      "2019-04-10 01:20:08,060 root         INFO     ====> Epoch: 59 Average loss: 928872.3211\n",
      "2019-04-10 01:20:08,087 root         INFO     Train Epoch: 60 [0/8000 (0%)]\tTotal Loss: 70.194275\n",
      "Reconstruction: 69.876755, Regularization: 0.237344, Discriminator: 0.037784; Generator: 0.042390,\n",
      "D(x): 0.619, D(G(z)): 0.285\n",
      "2019-04-10 01:20:08,189 root         INFO     Train Epoch: 60 [512/8000 (6%)]\tTotal Loss: 1.232551\n",
      "Reconstruction: 0.743886, Regularization: 0.422232, Discriminator: 0.023832; Generator: 0.042601,\n",
      "D(x): 0.743, D(G(z)): 0.276\n",
      "2019-04-10 01:20:08,291 root         INFO     Train Epoch: 60 [1024/8000 (13%)]\tTotal Loss: 4.957762\n",
      "Reconstruction: 4.685369, Regularization: 0.198032, Discriminator: 0.033609; Generator: 0.040752,\n",
      "D(x): 0.656, D(G(z)): 0.305\n",
      "2019-04-10 01:20:08,392 root         INFO     Train Epoch: 60 [1536/8000 (19%)]\tTotal Loss: 2.515444\n",
      "Reconstruction: 2.205196, Regularization: 0.236925, Discriminator: 0.029267; Generator: 0.044056,\n",
      "D(x): 0.695, D(G(z)): 0.259\n",
      "2019-04-10 01:20:08,494 root         INFO     Train Epoch: 60 [2048/8000 (26%)]\tTotal Loss: 465925.156250\n",
      "Reconstruction: 465924.656250, Regularization: 0.450321, Discriminator: 0.028798; Generator: 0.039655,\n",
      "D(x): 0.722, D(G(z)): 0.313\n",
      "2019-04-10 01:20:08,595 root         INFO     Train Epoch: 60 [2560/8000 (32%)]\tTotal Loss: 3.872585\n",
      "Reconstruction: 3.526972, Regularization: 0.275104, Discriminator: 0.026449; Generator: 0.044060,\n",
      "D(x): 0.708, D(G(z)): 0.261\n",
      "2019-04-10 01:20:08,696 root         INFO     Train Epoch: 60 [3072/8000 (38%)]\tTotal Loss: 1.277683\n",
      "Reconstruction: 0.777982, Regularization: 0.428583, Discriminator: 0.025408; Generator: 0.045711,\n",
      "D(x): 0.774, D(G(z)): 0.250\n",
      "2019-04-10 01:20:08,798 root         INFO     Train Epoch: 60 [3584/8000 (45%)]\tTotal Loss: 1.003334\n",
      "Reconstruction: 0.597547, Regularization: 0.339673, Discriminator: 0.025251; Generator: 0.040864,\n",
      "D(x): 0.771, D(G(z)): 0.289\n",
      "2019-04-10 01:20:08,899 root         INFO     Train Epoch: 60 [4096/8000 (51%)]\tTotal Loss: 2.124693\n",
      "Reconstruction: 1.826622, Regularization: 0.230032, Discriminator: 0.030457; Generator: 0.037581,\n",
      "D(x): 0.702, D(G(z)): 0.340\n",
      "2019-04-10 01:20:08,999 root         INFO     Train Epoch: 60 [4608/8000 (58%)]\tTotal Loss: 1.149207\n",
      "Reconstruction: 0.702573, Regularization: 0.384073, Discriminator: 0.027155; Generator: 0.035406,\n",
      "D(x): 0.761, D(G(z)): 0.355\n",
      "2019-04-10 01:20:09,097 root         INFO     Train Epoch: 60 [5120/8000 (64%)]\tTotal Loss: 1.229518\n",
      "Reconstruction: 0.807260, Regularization: 0.349767, Discriminator: 0.032779; Generator: 0.039712,\n",
      "D(x): 0.668, D(G(z)): 0.318\n",
      "2019-04-10 01:20:09,194 root         INFO     Train Epoch: 60 [5632/8000 (70%)]\tTotal Loss: 0.664646\n",
      "Reconstruction: 0.394157, Regularization: 0.201811, Discriminator: 0.027494; Generator: 0.041184,\n",
      "D(x): 0.718, D(G(z)): 0.296\n",
      "2019-04-10 01:20:09,293 root         INFO     Train Epoch: 60 [6144/8000 (77%)]\tTotal Loss: 251.537781\n",
      "Reconstruction: 251.263824, Regularization: 0.202867, Discriminator: 0.030075; Generator: 0.041010,\n",
      "D(x): 0.677, D(G(z)): 0.294\n",
      "2019-04-10 01:20:09,391 root         INFO     Train Epoch: 60 [6656/8000 (83%)]\tTotal Loss: 35825.640625\n",
      "Reconstruction: 35825.328125, Regularization: 0.237872, Discriminator: 0.029900; Generator: 0.042699,\n",
      "D(x): 0.649, D(G(z)): 0.274\n",
      "2019-04-10 01:20:09,489 root         INFO     Train Epoch: 60 [7168/8000 (90%)]\tTotal Loss: 1.877939\n",
      "Reconstruction: 1.338920, Regularization: 0.466566, Discriminator: 0.030711; Generator: 0.041741,\n",
      "D(x): 0.694, D(G(z)): 0.282\n",
      "2019-04-10 01:20:09,590 root         INFO     Train Epoch: 60 [7680/8000 (96%)]\tTotal Loss: 3.290040\n",
      "Reconstruction: 2.809227, Regularization: 0.409245, Discriminator: 0.028973; Generator: 0.042595,\n",
      "D(x): 0.676, D(G(z)): 0.274\n",
      "2019-04-10 01:20:09,665 root         INFO     ====> Epoch: 60 Average loss: 30948867.3753\n",
      "2019-04-10 01:20:09,692 root         INFO     Train Epoch: 61 [0/8000 (0%)]\tTotal Loss: 0.892752\n",
      "Reconstruction: 0.538200, Regularization: 0.283140, Discriminator: 0.032494; Generator: 0.038918,\n",
      "D(x): 0.703, D(G(z)): 0.316\n",
      "2019-04-10 01:20:09,791 root         INFO     Train Epoch: 61 [512/8000 (6%)]\tTotal Loss: 1.261386\n",
      "Reconstruction: 0.765114, Regularization: 0.422127, Discriminator: 0.032145; Generator: 0.042000,\n",
      "D(x): 0.736, D(G(z)): 0.284\n",
      "2019-04-10 01:20:09,891 root         INFO     Train Epoch: 61 [1024/8000 (13%)]\tTotal Loss: 55.581589\n",
      "Reconstruction: 55.152416, Regularization: 0.358961, Discriminator: 0.027593; Generator: 0.042618,\n",
      "D(x): 0.728, D(G(z)): 0.271\n",
      "2019-04-10 01:20:09,991 root         INFO     Train Epoch: 61 [1536/8000 (19%)]\tTotal Loss: 1.757978\n",
      "Reconstruction: 1.404316, Regularization: 0.283664, Discriminator: 0.029374; Generator: 0.040624,\n",
      "D(x): 0.710, D(G(z)): 0.305\n",
      "2019-04-10 01:20:10,091 root         INFO     Train Epoch: 61 [2048/8000 (26%)]\tTotal Loss: 1.060999\n",
      "Reconstruction: 0.736581, Regularization: 0.248094, Discriminator: 0.035390; Generator: 0.040934,\n",
      "D(x): 0.684, D(G(z)): 0.300\n",
      "2019-04-10 01:20:10,191 root         INFO     Train Epoch: 61 [2560/8000 (32%)]\tTotal Loss: 0.786517\n",
      "Reconstruction: 0.456192, Regularization: 0.263630, Discriminator: 0.027789; Generator: 0.038906,\n",
      "D(x): 0.724, D(G(z)): 0.320\n",
      "2019-04-10 01:20:10,290 root         INFO     Train Epoch: 61 [3072/8000 (38%)]\tTotal Loss: 112.654358\n",
      "Reconstruction: 112.267708, Regularization: 0.315224, Discriminator: 0.030080; Generator: 0.041348,\n",
      "D(x): 0.683, D(G(z)): 0.289\n",
      "2019-04-10 01:20:10,389 root         INFO     Train Epoch: 61 [3584/8000 (45%)]\tTotal Loss: 0.946721\n",
      "Reconstruction: 0.563828, Regularization: 0.311052, Discriminator: 0.032408; Generator: 0.039433,\n",
      "D(x): 0.706, D(G(z)): 0.313\n",
      "2019-04-10 01:20:10,488 root         INFO     Train Epoch: 61 [4096/8000 (51%)]\tTotal Loss: 119027.156250\n",
      "Reconstruction: 119026.742188, Regularization: 0.353851, Discriminator: 0.023511; Generator: 0.042350,\n",
      "D(x): 0.755, D(G(z)): 0.278\n",
      "2019-04-10 01:20:10,588 root         INFO     Train Epoch: 61 [4608/8000 (58%)]\tTotal Loss: 289.904449\n",
      "Reconstruction: 289.593445, Regularization: 0.237505, Discriminator: 0.033724; Generator: 0.039758,\n",
      "D(x): 0.608, D(G(z)): 0.305\n",
      "2019-04-10 01:20:10,687 root         INFO     Train Epoch: 61 [5120/8000 (64%)]\tTotal Loss: 0.665366\n",
      "Reconstruction: 0.413211, Regularization: 0.177768, Discriminator: 0.029836; Generator: 0.044550,\n",
      "D(x): 0.675, D(G(z)): 0.261\n",
      "2019-04-10 01:20:10,787 root         INFO     Train Epoch: 61 [5632/8000 (70%)]\tTotal Loss: 0.901839\n",
      "Reconstruction: 0.528634, Regularization: 0.304854, Discriminator: 0.026340; Generator: 0.042011,\n",
      "D(x): 0.708, D(G(z)): 0.276\n",
      "2019-04-10 01:20:10,887 root         INFO     Train Epoch: 61 [6144/8000 (77%)]\tTotal Loss: 1.044327\n",
      "Reconstruction: 0.682726, Regularization: 0.297803, Discriminator: 0.022374; Generator: 0.041424,\n",
      "D(x): 0.755, D(G(z)): 0.282\n",
      "2019-04-10 01:20:10,986 root         INFO     Train Epoch: 61 [6656/8000 (83%)]\tTotal Loss: 0.916503\n",
      "Reconstruction: 0.588938, Regularization: 0.254699, Discriminator: 0.029377; Generator: 0.043490,\n",
      "D(x): 0.690, D(G(z)): 0.264\n",
      "2019-04-10 01:20:11,085 root         INFO     Train Epoch: 61 [7168/8000 (90%)]\tTotal Loss: 0.686462\n",
      "Reconstruction: 0.395053, Regularization: 0.220065, Discriminator: 0.030883; Generator: 0.040461,\n",
      "D(x): 0.678, D(G(z)): 0.290\n",
      "2019-04-10 01:20:11,185 root         INFO     Train Epoch: 61 [7680/8000 (96%)]\tTotal Loss: 1.425835\n",
      "Reconstruction: 0.914808, Regularization: 0.442886, Discriminator: 0.027215; Generator: 0.040926,\n",
      "D(x): 0.738, D(G(z)): 0.297\n",
      "2019-04-10 01:20:11,259 root         INFO     ====> Epoch: 61 Average loss: 260478.6116\n",
      "2019-04-10 01:20:11,285 root         INFO     Train Epoch: 62 [0/8000 (0%)]\tTotal Loss: 221188.359375\n",
      "Reconstruction: 221188.046875, Regularization: 0.227094, Discriminator: 0.027137; Generator: 0.043555,\n",
      "D(x): 0.684, D(G(z)): 0.268\n",
      "2019-04-10 01:20:11,386 root         INFO     Train Epoch: 62 [512/8000 (6%)]\tTotal Loss: 7138.930176\n",
      "Reconstruction: 7138.581543, Regularization: 0.275082, Discriminator: 0.033637; Generator: 0.040282,\n",
      "D(x): 0.618, D(G(z)): 0.295\n",
      "2019-04-10 01:20:11,484 root         INFO     Train Epoch: 62 [1024/8000 (13%)]\tTotal Loss: 1.036644\n",
      "Reconstruction: 0.616817, Regularization: 0.352754, Discriminator: 0.025506; Generator: 0.041567,\n",
      "D(x): 0.720, D(G(z)): 0.282\n",
      "2019-04-10 01:20:11,584 root         INFO     Train Epoch: 62 [1536/8000 (19%)]\tTotal Loss: 1.067954\n",
      "Reconstruction: 0.711671, Regularization: 0.282515, Discriminator: 0.037363; Generator: 0.036405,\n",
      "D(x): 0.637, D(G(z)): 0.345\n",
      "2019-04-10 01:20:11,682 root         INFO     Train Epoch: 62 [2048/8000 (26%)]\tTotal Loss: 1.445050\n",
      "Reconstruction: 0.897553, Regularization: 0.484626, Discriminator: 0.024682; Generator: 0.038189,\n",
      "D(x): 0.749, D(G(z)): 0.320\n",
      "2019-04-10 01:20:11,779 root         INFO     Train Epoch: 62 [2560/8000 (32%)]\tTotal Loss: 1.905726\n",
      "Reconstruction: 1.606822, Regularization: 0.231525, Discriminator: 0.031969; Generator: 0.035410,\n",
      "D(x): 0.693, D(G(z)): 0.368\n",
      "2019-04-10 01:20:11,877 root         INFO     Train Epoch: 62 [3072/8000 (38%)]\tTotal Loss: 1.043208\n",
      "Reconstruction: 0.624699, Regularization: 0.344861, Discriminator: 0.037200; Generator: 0.036447,\n",
      "D(x): 0.666, D(G(z)): 0.343\n",
      "2019-04-10 01:20:11,974 root         INFO     Train Epoch: 62 [3584/8000 (45%)]\tTotal Loss: 1.953274\n",
      "Reconstruction: 1.436542, Regularization: 0.449968, Discriminator: 0.028481; Generator: 0.038284,\n",
      "D(x): 0.798, D(G(z)): 0.336\n",
      "2019-04-10 01:20:12,072 root         INFO     Train Epoch: 62 [4096/8000 (51%)]\tTotal Loss: 1.128011\n",
      "Reconstruction: 0.673688, Regularization: 0.389947, Discriminator: 0.023920; Generator: 0.040457,\n",
      "D(x): 0.787, D(G(z)): 0.297\n",
      "2019-04-10 01:20:12,170 root         INFO     Train Epoch: 62 [4608/8000 (58%)]\tTotal Loss: 76.970581\n",
      "Reconstruction: 76.705826, Regularization: 0.196988, Discriminator: 0.030044; Generator: 0.037721,\n",
      "D(x): 0.684, D(G(z)): 0.327\n",
      "2019-04-10 01:20:12,270 root         INFO     Train Epoch: 62 [5120/8000 (64%)]\tTotal Loss: 0.808366\n",
      "Reconstruction: 0.473164, Regularization: 0.262192, Discriminator: 0.032028; Generator: 0.040982,\n",
      "D(x): 0.691, D(G(z)): 0.287\n",
      "2019-04-10 01:20:12,370 root         INFO     Train Epoch: 62 [5632/8000 (70%)]\tTotal Loss: 574.981750\n",
      "Reconstruction: 574.674805, Regularization: 0.237010, Discriminator: 0.031650; Generator: 0.038317,\n",
      "D(x): 0.692, D(G(z)): 0.315\n",
      "2019-04-10 01:20:12,470 root         INFO     Train Epoch: 62 [6144/8000 (77%)]\tTotal Loss: 15.523291\n",
      "Reconstruction: 15.084505, Regularization: 0.377643, Discriminator: 0.022181; Generator: 0.038961,\n",
      "D(x): 0.800, D(G(z)): 0.310\n",
      "2019-04-10 01:20:12,570 root         INFO     Train Epoch: 62 [6656/8000 (83%)]\tTotal Loss: 14816732.000000\n",
      "Reconstruction: 14816732.000000, Regularization: 0.380150, Discriminator: 0.030315; Generator: 0.035460,\n",
      "D(x): 0.731, D(G(z)): 0.358\n",
      "2019-04-10 01:20:12,670 root         INFO     Train Epoch: 62 [7168/8000 (90%)]\tTotal Loss: 67.028580\n",
      "Reconstruction: 66.655746, Regularization: 0.303408, Discriminator: 0.030939; Generator: 0.038485,\n",
      "D(x): 0.687, D(G(z)): 0.312\n",
      "2019-04-10 01:20:12,770 root         INFO     Train Epoch: 62 [7680/8000 (96%)]\tTotal Loss: 0.913488\n",
      "Reconstruction: 0.581000, Regularization: 0.263788, Discriminator: 0.029797; Generator: 0.038903,\n",
      "D(x): 0.704, D(G(z)): 0.306\n",
      "2019-04-10 01:20:12,844 root         INFO     ====> Epoch: 62 Average loss: 315637.4052\n",
      "2019-04-10 01:20:12,870 root         INFO     Train Epoch: 63 [0/8000 (0%)]\tTotal Loss: 3184.665771\n",
      "Reconstruction: 3184.289551, Regularization: 0.309367, Discriminator: 0.026384; Generator: 0.040428,\n",
      "D(x): 0.701, D(G(z)): 0.295\n",
      "2019-04-10 01:20:12,971 root         INFO     Train Epoch: 63 [512/8000 (6%)]\tTotal Loss: 2.302368\n",
      "Reconstruction: 1.866263, Regularization: 0.369788, Discriminator: 0.026745; Generator: 0.039572,\n",
      "D(x): 0.737, D(G(z)): 0.306\n",
      "2019-04-10 01:20:13,071 root         INFO     Train Epoch: 63 [1024/8000 (13%)]\tTotal Loss: 1.608148\n",
      "Reconstruction: 1.008164, Regularization: 0.537715, Discriminator: 0.027051; Generator: 0.035218,\n",
      "D(x): 0.749, D(G(z)): 0.352\n",
      "2019-04-10 01:20:13,171 root         INFO     Train Epoch: 63 [1536/8000 (19%)]\tTotal Loss: 7457.182617\n",
      "Reconstruction: 7456.859375, Regularization: 0.254024, Discriminator: 0.029843; Generator: 0.039270,\n",
      "D(x): 0.655, D(G(z)): 0.300\n",
      "2019-04-10 01:20:13,272 root         INFO     Train Epoch: 63 [2048/8000 (26%)]\tTotal Loss: 9402.938477\n",
      "Reconstruction: 9402.528320, Regularization: 0.347359, Discriminator: 0.024943; Generator: 0.037911,\n",
      "D(x): 0.769, D(G(z)): 0.323\n",
      "2019-04-10 01:20:13,372 root         INFO     Train Epoch: 63 [2560/8000 (32%)]\tTotal Loss: 1509.784424\n",
      "Reconstruction: 1509.359619, Regularization: 0.357680, Discriminator: 0.029457; Generator: 0.037705,\n",
      "D(x): 0.730, D(G(z)): 0.324\n",
      "2019-04-10 01:20:13,472 root         INFO     Train Epoch: 63 [3072/8000 (38%)]\tTotal Loss: 1.378745\n",
      "Reconstruction: 0.879197, Regularization: 0.436926, Discriminator: 0.025682; Generator: 0.036940,\n",
      "D(x): 0.801, D(G(z)): 0.332\n",
      "2019-04-10 01:20:13,572 root         INFO     Train Epoch: 63 [3584/8000 (45%)]\tTotal Loss: 0.795305\n",
      "Reconstruction: 0.462188, Regularization: 0.257784, Discriminator: 0.037355; Generator: 0.037978,\n",
      "D(x): 0.633, D(G(z)): 0.331\n",
      "2019-04-10 01:20:13,672 root         INFO     Train Epoch: 63 [4096/8000 (51%)]\tTotal Loss: 1.072513\n",
      "Reconstruction: 0.657382, Regularization: 0.339002, Discriminator: 0.038359; Generator: 0.037770,\n",
      "D(x): 0.593, D(G(z)): 0.331\n",
      "2019-04-10 01:20:13,770 root         INFO     Train Epoch: 63 [4608/8000 (58%)]\tTotal Loss: 1.070666\n",
      "Reconstruction: 0.700177, Regularization: 0.312769, Discriminator: 0.020538; Generator: 0.037181,\n",
      "D(x): 0.825, D(G(z)): 0.323\n",
      "2019-04-10 01:20:13,867 root         INFO     Train Epoch: 63 [5120/8000 (64%)]\tTotal Loss: 40754.214844\n",
      "Reconstruction: 40753.843750, Regularization: 0.303822, Discriminator: 0.025544; Generator: 0.041294,\n",
      "D(x): 0.737, D(G(z)): 0.280\n",
      "2019-04-10 01:20:13,964 root         INFO     Train Epoch: 63 [5632/8000 (70%)]\tTotal Loss: 172757.718750\n",
      "Reconstruction: 172757.328125, Regularization: 0.311114, Discriminator: 0.029363; Generator: 0.042335,\n",
      "D(x): 0.641, D(G(z)): 0.268\n",
      "2019-04-10 01:20:14,062 root         INFO     Train Epoch: 63 [6144/8000 (77%)]\tTotal Loss: 2.296397\n",
      "Reconstruction: 1.910053, Regularization: 0.319829, Discriminator: 0.026282; Generator: 0.040233,\n",
      "D(x): 0.719, D(G(z)): 0.297\n",
      "2019-04-10 01:20:14,158 root         INFO     Train Epoch: 63 [6656/8000 (83%)]\tTotal Loss: 53.100815\n",
      "Reconstruction: 52.784561, Regularization: 0.251982, Discriminator: 0.025408; Generator: 0.038863,\n",
      "D(x): 0.727, D(G(z)): 0.312\n",
      "2019-04-10 01:20:14,256 root         INFO     Train Epoch: 63 [7168/8000 (90%)]\tTotal Loss: 146.916580\n",
      "Reconstruction: 146.675171, Regularization: 0.163371, Discriminator: 0.036450; Generator: 0.041581,\n",
      "D(x): 0.533, D(G(z)): 0.274\n",
      "2019-04-10 01:20:14,352 root         INFO     Train Epoch: 63 [7680/8000 (96%)]\tTotal Loss: 99501.859375\n",
      "Reconstruction: 99501.507812, Regularization: 0.274147, Discriminator: 0.037177; Generator: 0.039863,\n",
      "D(x): 0.578, D(G(z)): 0.292\n",
      "2019-04-10 01:20:14,425 root         INFO     ====> Epoch: 63 Average loss: 50628.7600\n",
      "2019-04-10 01:20:14,452 root         INFO     Train Epoch: 64 [0/8000 (0%)]\tTotal Loss: 2.211905\n",
      "Reconstruction: 1.706996, Regularization: 0.434540, Discriminator: 0.032699; Generator: 0.037669,\n",
      "D(x): 0.696, D(G(z)): 0.325\n",
      "2019-04-10 01:20:14,556 root         INFO     Train Epoch: 64 [512/8000 (6%)]\tTotal Loss: 0.958340\n",
      "Reconstruction: 0.656095, Regularization: 0.232308, Discriminator: 0.034687; Generator: 0.035250,\n",
      "D(x): 0.621, D(G(z)): 0.353\n",
      "2019-04-10 01:20:14,658 root         INFO     Train Epoch: 64 [1024/8000 (13%)]\tTotal Loss: 354.807800\n",
      "Reconstruction: 354.513489, Regularization: 0.224671, Discriminator: 0.028548; Generator: 0.041102,\n",
      "D(x): 0.667, D(G(z)): 0.284\n",
      "2019-04-10 01:20:14,760 root         INFO     Train Epoch: 64 [1536/8000 (19%)]\tTotal Loss: 16.372686\n",
      "Reconstruction: 15.950207, Regularization: 0.345869, Discriminator: 0.039650; Generator: 0.036961,\n",
      "D(x): 0.571, D(G(z)): 0.334\n",
      "2019-04-10 01:20:14,862 root         INFO     Train Epoch: 64 [2048/8000 (26%)]\tTotal Loss: 1.021516\n",
      "Reconstruction: 0.620907, Regularization: 0.326395, Discriminator: 0.039899; Generator: 0.034315,\n",
      "D(x): 0.616, D(G(z)): 0.371\n",
      "2019-04-10 01:20:14,962 root         INFO     Train Epoch: 64 [2560/8000 (32%)]\tTotal Loss: 15977.821289\n",
      "Reconstruction: 15977.410156, Regularization: 0.344772, Discriminator: 0.028524; Generator: 0.037834,\n",
      "D(x): 0.719, D(G(z)): 0.320\n",
      "2019-04-10 01:20:15,063 root         INFO     Train Epoch: 64 [3072/8000 (38%)]\tTotal Loss: 866.338562\n",
      "Reconstruction: 865.962769, Regularization: 0.308405, Discriminator: 0.025482; Generator: 0.041926,\n",
      "D(x): 0.713, D(G(z)): 0.273\n",
      "2019-04-10 01:20:15,163 root         INFO     Train Epoch: 64 [3584/8000 (45%)]\tTotal Loss: 218.051651\n",
      "Reconstruction: 217.588882, Regularization: 0.396456, Discriminator: 0.029146; Generator: 0.037171,\n",
      "D(x): 0.713, D(G(z)): 0.325\n",
      "2019-04-10 01:20:15,263 root         INFO     Train Epoch: 64 [4096/8000 (51%)]\tTotal Loss: 393.364166\n",
      "Reconstruction: 392.984131, Regularization: 0.311927, Discriminator: 0.028167; Generator: 0.039956,\n",
      "D(x): 0.681, D(G(z)): 0.292\n",
      "2019-04-10 01:20:15,363 root         INFO     Train Epoch: 64 [4608/8000 (58%)]\tTotal Loss: 0.962451\n",
      "Reconstruction: 0.568900, Regularization: 0.327252, Discriminator: 0.028615; Generator: 0.037685,\n",
      "D(x): 0.697, D(G(z)): 0.323\n",
      "2019-04-10 01:20:15,463 root         INFO     Train Epoch: 64 [5120/8000 (64%)]\tTotal Loss: 7415.036621\n",
      "Reconstruction: 7414.618164, Regularization: 0.353857, Discriminator: 0.025637; Generator: 0.038838,\n",
      "D(x): 0.710, D(G(z)): 0.311\n",
      "2019-04-10 01:20:15,563 root         INFO     Train Epoch: 64 [5632/8000 (70%)]\tTotal Loss: 0.884990\n",
      "Reconstruction: 0.574997, Regularization: 0.241827, Discriminator: 0.032909; Generator: 0.035257,\n",
      "D(x): 0.668, D(G(z)): 0.355\n",
      "2019-04-10 01:20:15,664 root         INFO     Train Epoch: 64 [6144/8000 (77%)]\tTotal Loss: 0.995935\n",
      "Reconstruction: 0.595366, Regularization: 0.331097, Discriminator: 0.033369; Generator: 0.036102,\n",
      "D(x): 0.708, D(G(z)): 0.339\n",
      "2019-04-10 01:20:15,764 root         INFO     Train Epoch: 64 [6656/8000 (83%)]\tTotal Loss: 0.857690\n",
      "Reconstruction: 0.515662, Regularization: 0.270455, Discriminator: 0.034430; Generator: 0.037143,\n",
      "D(x): 0.589, D(G(z)): 0.326\n",
      "2019-04-10 01:20:15,864 root         INFO     Train Epoch: 64 [7168/8000 (90%)]\tTotal Loss: 2.358392\n",
      "Reconstruction: 1.960312, Regularization: 0.324662, Discriminator: 0.032771; Generator: 0.040648,\n",
      "D(x): 0.657, D(G(z)): 0.283\n",
      "2019-04-10 01:20:15,965 root         INFO     Train Epoch: 64 [7680/8000 (96%)]\tTotal Loss: 1.228894\n",
      "Reconstruction: 0.745056, Regularization: 0.412962, Discriminator: 0.033989; Generator: 0.036888,\n",
      "D(x): 0.676, D(G(z)): 0.331\n",
      "2019-04-10 01:20:16,040 root         INFO     ====> Epoch: 64 Average loss: 5875941353.7315\n",
      "2019-04-10 01:20:16,067 root         INFO     Train Epoch: 65 [0/8000 (0%)]\tTotal Loss: 0.619239\n",
      "Reconstruction: 0.405308, Regularization: 0.135684, Discriminator: 0.043347; Generator: 0.034900,\n",
      "D(x): 0.535, D(G(z)): 0.356\n",
      "2019-04-10 01:20:16,168 root         INFO     Train Epoch: 65 [512/8000 (6%)]\tTotal Loss: 4.474581\n",
      "Reconstruction: 4.185588, Regularization: 0.219605, Discriminator: 0.031684; Generator: 0.037704,\n",
      "D(x): 0.642, D(G(z)): 0.322\n",
      "2019-04-10 01:20:16,268 root         INFO     Train Epoch: 65 [1024/8000 (13%)]\tTotal Loss: 6.138193\n",
      "Reconstruction: 5.636559, Regularization: 0.435997, Discriminator: 0.024820; Generator: 0.040818,\n",
      "D(x): 0.721, D(G(z)): 0.288\n",
      "2019-04-10 01:20:16,368 root         INFO     Train Epoch: 65 [1536/8000 (19%)]\tTotal Loss: 1.715533\n",
      "Reconstruction: 1.402066, Regularization: 0.237551, Discriminator: 0.039836; Generator: 0.036081,\n",
      "D(x): 0.591, D(G(z)): 0.339\n",
      "2019-04-10 01:20:16,468 root         INFO     Train Epoch: 65 [2048/8000 (26%)]\tTotal Loss: 1.002858\n",
      "Reconstruction: 0.607994, Regularization: 0.320830, Discriminator: 0.034710; Generator: 0.039324,\n",
      "D(x): 0.585, D(G(z)): 0.303\n",
      "2019-04-10 01:20:16,569 root         INFO     Train Epoch: 65 [2560/8000 (32%)]\tTotal Loss: 0.755691\n",
      "Reconstruction: 0.446295, Regularization: 0.237188, Discriminator: 0.038462; Generator: 0.033746,\n",
      "D(x): 0.612, D(G(z)): 0.377\n",
      "2019-04-10 01:20:16,669 root         INFO     Train Epoch: 65 [3072/8000 (38%)]\tTotal Loss: 0.838778\n",
      "Reconstruction: 0.512868, Regularization: 0.251280, Discriminator: 0.037514; Generator: 0.037116,\n",
      "D(x): 0.598, D(G(z)): 0.330\n",
      "2019-04-10 01:20:16,769 root         INFO     Train Epoch: 65 [3584/8000 (45%)]\tTotal Loss: 2.922695\n",
      "Reconstruction: 2.470523, Regularization: 0.387916, Discriminator: 0.030147; Generator: 0.034109,\n",
      "D(x): 0.714, D(G(z)): 0.363\n",
      "2019-04-10 01:20:16,870 root         INFO     Train Epoch: 65 [4096/8000 (51%)]\tTotal Loss: 282.418274\n",
      "Reconstruction: 281.939606, Regularization: 0.407075, Discriminator: 0.036272; Generator: 0.035324,\n",
      "D(x): 0.656, D(G(z)): 0.355\n",
      "2019-04-10 01:20:16,970 root         INFO     Train Epoch: 65 [4608/8000 (58%)]\tTotal Loss: 0.812816\n",
      "Reconstruction: 0.557097, Regularization: 0.182295, Discriminator: 0.034548; Generator: 0.038876,\n",
      "D(x): 0.573, D(G(z)): 0.305\n",
      "2019-04-10 01:20:17,070 root         INFO     Train Epoch: 65 [5120/8000 (64%)]\tTotal Loss: 0.606416\n",
      "Reconstruction: 0.337929, Regularization: 0.193242, Discriminator: 0.038927; Generator: 0.036317,\n",
      "D(x): 0.556, D(G(z)): 0.334\n",
      "2019-04-10 01:20:17,171 root         INFO     Train Epoch: 65 [5632/8000 (70%)]\tTotal Loss: 2.071114\n",
      "Reconstruction: 1.752099, Regularization: 0.251561, Discriminator: 0.031894; Generator: 0.035560,\n",
      "D(x): 0.654, D(G(z)): 0.350\n",
      "2019-04-10 01:20:17,271 root         INFO     Train Epoch: 65 [6144/8000 (77%)]\tTotal Loss: 1.162177\n",
      "Reconstruction: 0.778840, Regularization: 0.311025, Discriminator: 0.032994; Generator: 0.039319,\n",
      "D(x): 0.615, D(G(z)): 0.296\n",
      "2019-04-10 01:20:17,371 root         INFO     Train Epoch: 65 [6656/8000 (83%)]\tTotal Loss: 46.587105\n",
      "Reconstruction: 46.357056, Regularization: 0.160445, Discriminator: 0.032948; Generator: 0.036655,\n",
      "D(x): 0.620, D(G(z)): 0.337\n",
      "2019-04-10 01:20:17,471 root         INFO     Train Epoch: 65 [7168/8000 (90%)]\tTotal Loss: 1.561488\n",
      "Reconstruction: 1.125340, Regularization: 0.371210, Discriminator: 0.028550; Generator: 0.036387,\n",
      "D(x): 0.728, D(G(z)): 0.330\n",
      "2019-04-10 01:20:17,571 root         INFO     Train Epoch: 65 [7680/8000 (96%)]\tTotal Loss: 0.844006\n",
      "Reconstruction: 0.499215, Regularization: 0.280508, Discriminator: 0.028566; Generator: 0.035716,\n",
      "D(x): 0.734, D(G(z)): 0.343\n",
      "2019-04-10 01:20:17,646 root         INFO     ====> Epoch: 65 Average loss: 290746.2728\n",
      "2019-04-10 01:20:17,672 root         INFO     Train Epoch: 66 [0/8000 (0%)]\tTotal Loss: 0.993528\n",
      "Reconstruction: 0.600497, Regularization: 0.325913, Discriminator: 0.032181; Generator: 0.034937,\n",
      "D(x): 0.692, D(G(z)): 0.353\n",
      "2019-04-10 01:20:17,773 root         INFO     Train Epoch: 66 [512/8000 (6%)]\tTotal Loss: 1.222770\n",
      "Reconstruction: 0.843350, Regularization: 0.310807, Discriminator: 0.034797; Generator: 0.033816,\n",
      "D(x): 0.656, D(G(z)): 0.366\n",
      "2019-04-10 01:20:17,873 root         INFO     Train Epoch: 66 [1024/8000 (13%)]\tTotal Loss: 3.414910\n",
      "Reconstruction: 3.119053, Regularization: 0.222059, Discriminator: 0.035103; Generator: 0.038694,\n",
      "D(x): 0.583, D(G(z)): 0.304\n",
      "2019-04-10 01:20:17,972 root         INFO     Train Epoch: 66 [1536/8000 (19%)]\tTotal Loss: 2585912.000000\n",
      "Reconstruction: 2585911.750000, Regularization: 0.180649, Discriminator: 0.041387; Generator: 0.034876,\n",
      "D(x): 0.525, D(G(z)): 0.353\n",
      "2019-04-10 01:20:18,072 root         INFO     Train Epoch: 66 [2048/8000 (26%)]\tTotal Loss: 1.610461\n",
      "Reconstruction: 1.293532, Regularization: 0.250549, Discriminator: 0.034256; Generator: 0.032123,\n",
      "D(x): 0.663, D(G(z)): 0.389\n",
      "2019-04-10 01:20:18,172 root         INFO     Train Epoch: 66 [2560/8000 (32%)]\tTotal Loss: 0.840964\n",
      "Reconstruction: 0.567057, Regularization: 0.202600, Discriminator: 0.035759; Generator: 0.035547,\n",
      "D(x): 0.588, D(G(z)): 0.349\n",
      "2019-04-10 01:20:18,272 root         INFO     Train Epoch: 66 [3072/8000 (38%)]\tTotal Loss: 193950.437500\n",
      "Reconstruction: 193950.156250, Regularization: 0.206765, Discriminator: 0.037301; Generator: 0.036527,\n",
      "D(x): 0.575, D(G(z)): 0.330\n",
      "2019-04-10 01:20:18,371 root         INFO     Train Epoch: 66 [3584/8000 (45%)]\tTotal Loss: 1.360349\n",
      "Reconstruction: 0.825094, Regularization: 0.473908, Discriminator: 0.026140; Generator: 0.035207,\n",
      "D(x): 0.762, D(G(z)): 0.349\n",
      "2019-04-10 01:20:18,471 root         INFO     Train Epoch: 66 [4096/8000 (51%)]\tTotal Loss: 0.683750\n",
      "Reconstruction: 0.390612, Regularization: 0.219841, Discriminator: 0.033607; Generator: 0.039691,\n",
      "D(x): 0.614, D(G(z)): 0.286\n",
      "2019-04-10 01:20:18,571 root         INFO     Train Epoch: 66 [4608/8000 (58%)]\tTotal Loss: 0.999573\n",
      "Reconstruction: 0.612350, Regularization: 0.319913, Discriminator: 0.030874; Generator: 0.036436,\n",
      "D(x): 0.655, D(G(z)): 0.329\n",
      "2019-04-10 01:20:18,670 root         INFO     Train Epoch: 66 [5120/8000 (64%)]\tTotal Loss: 1.845290\n",
      "Reconstruction: 1.417197, Regularization: 0.358323, Discriminator: 0.034444; Generator: 0.035326,\n",
      "D(x): 0.641, D(G(z)): 0.344\n",
      "2019-04-10 01:20:18,769 root         INFO     Train Epoch: 66 [5632/8000 (70%)]\tTotal Loss: 3.535990\n",
      "Reconstruction: 3.187206, Regularization: 0.273992, Discriminator: 0.040103; Generator: 0.034689,\n",
      "D(x): 0.560, D(G(z)): 0.358\n",
      "2019-04-10 01:20:18,869 root         INFO     Train Epoch: 66 [6144/8000 (77%)]\tTotal Loss: 6.445699\n",
      "Reconstruction: 5.789175, Regularization: 0.592135, Discriminator: 0.028693; Generator: 0.035696,\n",
      "D(x): 0.721, D(G(z)): 0.345\n",
      "2019-04-10 01:20:18,969 root         INFO     Train Epoch: 66 [6656/8000 (83%)]\tTotal Loss: 2.494078\n",
      "Reconstruction: 2.053282, Regularization: 0.376115, Discriminator: 0.028497; Generator: 0.036184,\n",
      "D(x): 0.723, D(G(z)): 0.330\n",
      "2019-04-10 01:20:19,068 root         INFO     Train Epoch: 66 [7168/8000 (90%)]\tTotal Loss: 2.862463\n",
      "Reconstruction: 2.406994, Regularization: 0.392773, Discriminator: 0.027213; Generator: 0.035483,\n",
      "D(x): 0.730, D(G(z)): 0.342\n",
      "2019-04-10 01:20:19,167 root         INFO     Train Epoch: 66 [7680/8000 (96%)]\tTotal Loss: 1.210220\n",
      "Reconstruction: 0.737007, Regularization: 0.412854, Discriminator: 0.024825; Generator: 0.035534,\n",
      "D(x): 0.758, D(G(z)): 0.340\n",
      "2019-04-10 01:20:19,241 root         INFO     ====> Epoch: 66 Average loss: 1566785.5221\n",
      "2019-04-10 01:20:19,267 root         INFO     Train Epoch: 67 [0/8000 (0%)]\tTotal Loss: 76.650009\n",
      "Reconstruction: 76.321373, Regularization: 0.263655, Discriminator: 0.028675; Generator: 0.036301,\n",
      "D(x): 0.700, D(G(z)): 0.332\n",
      "2019-04-10 01:20:19,368 root         INFO     Train Epoch: 67 [512/8000 (6%)]\tTotal Loss: 0.922283\n",
      "Reconstruction: 0.583775, Regularization: 0.270497, Discriminator: 0.031603; Generator: 0.036409,\n",
      "D(x): 0.669, D(G(z)): 0.340\n",
      "2019-04-10 01:20:19,468 root         INFO     Train Epoch: 67 [1024/8000 (13%)]\tTotal Loss: 1.045391\n",
      "Reconstruction: 0.625897, Regularization: 0.352503, Discriminator: 0.029220; Generator: 0.037771,\n",
      "D(x): 0.675, D(G(z)): 0.311\n",
      "2019-04-10 01:20:19,568 root         INFO     Train Epoch: 67 [1536/8000 (19%)]\tTotal Loss: 953794.812500\n",
      "Reconstruction: 953794.500000, Regularization: 0.254513, Discriminator: 0.034594; Generator: 0.034672,\n",
      "D(x): 0.620, D(G(z)): 0.345\n",
      "2019-04-10 01:20:19,668 root         INFO     Train Epoch: 67 [2048/8000 (26%)]\tTotal Loss: 0.758202\n",
      "Reconstruction: 0.439338, Regularization: 0.241922, Discriminator: 0.039456; Generator: 0.037485,\n",
      "D(x): 0.567, D(G(z)): 0.320\n",
      "2019-04-10 01:20:19,768 root         INFO     Train Epoch: 67 [2560/8000 (32%)]\tTotal Loss: 7.937710\n",
      "Reconstruction: 7.612268, Regularization: 0.255699, Discriminator: 0.037856; Generator: 0.031886,\n",
      "D(x): 0.599, D(G(z)): 0.388\n",
      "2019-04-10 01:20:19,868 root         INFO     Train Epoch: 67 [3072/8000 (38%)]\tTotal Loss: 0.922517\n",
      "Reconstruction: 0.619735, Regularization: 0.233840, Discriminator: 0.034189; Generator: 0.034754,\n",
      "D(x): 0.646, D(G(z)): 0.353\n",
      "2019-04-10 01:20:19,967 root         INFO     Train Epoch: 67 [3584/8000 (45%)]\tTotal Loss: 55337.550781\n",
      "Reconstruction: 55337.109375, Regularization: 0.370164, Discriminator: 0.032007; Generator: 0.036583,\n",
      "D(x): 0.650, D(G(z)): 0.325\n",
      "2019-04-10 01:20:20,067 root         INFO     Train Epoch: 67 [4096/8000 (51%)]\tTotal Loss: 1.959593\n",
      "Reconstruction: 1.337566, Regularization: 0.555198, Discriminator: 0.032278; Generator: 0.034551,\n",
      "D(x): 0.675, D(G(z)): 0.352\n",
      "2019-04-10 01:20:20,167 root         INFO     Train Epoch: 67 [4608/8000 (58%)]\tTotal Loss: 0.778602\n",
      "Reconstruction: 0.467241, Regularization: 0.239311, Discriminator: 0.037215; Generator: 0.034834,\n",
      "D(x): 0.579, D(G(z)): 0.349\n",
      "2019-04-10 01:20:20,267 root         INFO     Train Epoch: 67 [5120/8000 (64%)]\tTotal Loss: 942.942566\n",
      "Reconstruction: 942.527710, Regularization: 0.349555, Discriminator: 0.032772; Generator: 0.032539,\n",
      "D(x): 0.673, D(G(z)): 0.380\n",
      "2019-04-10 01:20:20,366 root         INFO     Train Epoch: 67 [5632/8000 (70%)]\tTotal Loss: 0.864887\n",
      "Reconstruction: 0.565666, Regularization: 0.224870, Discriminator: 0.038406; Generator: 0.035946,\n",
      "D(x): 0.557, D(G(z)): 0.335\n",
      "2019-04-10 01:20:20,466 root         INFO     Train Epoch: 67 [6144/8000 (77%)]\tTotal Loss: 0.670989\n",
      "Reconstruction: 0.384631, Regularization: 0.207605, Discriminator: 0.044312; Generator: 0.034440,\n",
      "D(x): 0.508, D(G(z)): 0.354\n",
      "2019-04-10 01:20:20,566 root         INFO     Train Epoch: 67 [6656/8000 (83%)]\tTotal Loss: 0.930585\n",
      "Reconstruction: 0.594513, Regularization: 0.264169, Discriminator: 0.039404; Generator: 0.032499,\n",
      "D(x): 0.579, D(G(z)): 0.377\n",
      "2019-04-10 01:20:20,667 root         INFO     Train Epoch: 67 [7168/8000 (90%)]\tTotal Loss: 1.098144\n",
      "Reconstruction: 0.650656, Regularization: 0.383522, Discriminator: 0.028254; Generator: 0.035712,\n",
      "D(x): 0.738, D(G(z)): 0.346\n",
      "2019-04-10 01:20:20,768 root         INFO     Train Epoch: 67 [7680/8000 (96%)]\tTotal Loss: 0.759417\n",
      "Reconstruction: 0.500343, Regularization: 0.181937, Discriminator: 0.042042; Generator: 0.035095,\n",
      "D(x): 0.507, D(G(z)): 0.340\n",
      "2019-04-10 01:20:20,842 root         INFO     ====> Epoch: 67 Average loss: 7407875117070.9902\n",
      "2019-04-10 01:20:20,869 root         INFO     Train Epoch: 68 [0/8000 (0%)]\tTotal Loss: 1.097639\n",
      "Reconstruction: 0.774736, Regularization: 0.248165, Discriminator: 0.039734; Generator: 0.035004,\n",
      "D(x): 0.557, D(G(z)): 0.349\n",
      "2019-04-10 01:20:20,966 root         INFO     Train Epoch: 68 [512/8000 (6%)]\tTotal Loss: 0.983347\n",
      "Reconstruction: 0.629175, Regularization: 0.289079, Discriminator: 0.032727; Generator: 0.032366,\n",
      "D(x): 0.661, D(G(z)): 0.384\n",
      "2019-04-10 01:20:21,062 root         INFO     Train Epoch: 68 [1024/8000 (13%)]\tTotal Loss: 1.236787\n",
      "Reconstruction: 0.798570, Regularization: 0.369778, Discriminator: 0.034026; Generator: 0.034413,\n",
      "D(x): 0.648, D(G(z)): 0.356\n",
      "2019-04-10 01:20:21,159 root         INFO     Train Epoch: 68 [1536/8000 (19%)]\tTotal Loss: 1.675636\n",
      "Reconstruction: 1.409791, Regularization: 0.198432, Discriminator: 0.030285; Generator: 0.037127,\n",
      "D(x): 0.633, D(G(z)): 0.321\n",
      "2019-04-10 01:20:21,256 root         INFO     Train Epoch: 68 [2048/8000 (26%)]\tTotal Loss: 34.337685\n",
      "Reconstruction: 33.975670, Regularization: 0.293012, Discriminator: 0.036962; Generator: 0.032043,\n",
      "D(x): 0.643, D(G(z)): 0.388\n",
      "2019-04-10 01:20:21,352 root         INFO     Train Epoch: 68 [2560/8000 (32%)]\tTotal Loss: 1.074269\n",
      "Reconstruction: 0.656143, Regularization: 0.353934, Discriminator: 0.028224; Generator: 0.035966,\n",
      "D(x): 0.696, D(G(z)): 0.333\n",
      "2019-04-10 01:20:21,449 root         INFO     Train Epoch: 68 [3072/8000 (38%)]\tTotal Loss: 76.086815\n",
      "Reconstruction: 75.689522, Regularization: 0.332562, Discriminator: 0.029362; Generator: 0.035366,\n",
      "D(x): 0.705, D(G(z)): 0.337\n",
      "2019-04-10 01:20:21,545 root         INFO     Train Epoch: 68 [3584/8000 (45%)]\tTotal Loss: 4.363779\n",
      "Reconstruction: 4.020858, Regularization: 0.272865, Discriminator: 0.034434; Generator: 0.035623,\n",
      "D(x): 0.616, D(G(z)): 0.341\n",
      "2019-04-10 01:20:21,642 root         INFO     Train Epoch: 68 [4096/8000 (51%)]\tTotal Loss: 1.242510\n",
      "Reconstruction: 0.747014, Regularization: 0.429525, Discriminator: 0.030997; Generator: 0.034975,\n",
      "D(x): 0.676, D(G(z)): 0.343\n",
      "2019-04-10 01:20:21,739 root         INFO     Train Epoch: 68 [4608/8000 (58%)]\tTotal Loss: 520.397766\n",
      "Reconstruction: 520.078430, Regularization: 0.251319, Discriminator: 0.032265; Generator: 0.035747,\n",
      "D(x): 0.645, D(G(z)): 0.333\n",
      "2019-04-10 01:20:21,836 root         INFO     Train Epoch: 68 [5120/8000 (64%)]\tTotal Loss: 2.057451\n",
      "Reconstruction: 1.566840, Regularization: 0.421299, Discriminator: 0.033711; Generator: 0.035601,\n",
      "D(x): 0.639, D(G(z)): 0.335\n",
      "2019-04-10 01:20:21,934 root         INFO     Train Epoch: 68 [5632/8000 (70%)]\tTotal Loss: 937024.750000\n",
      "Reconstruction: 937024.375000, Regularization: 0.294401, Discriminator: 0.035491; Generator: 0.035722,\n",
      "D(x): 0.577, D(G(z)): 0.333\n",
      "2019-04-10 01:20:22,030 root         INFO     Train Epoch: 68 [6144/8000 (77%)]\tTotal Loss: 3142244.750000\n",
      "Reconstruction: 3142244.500000, Regularization: 0.322649, Discriminator: 0.038646; Generator: 0.032407,\n",
      "D(x): 0.599, D(G(z)): 0.383\n",
      "2019-04-10 01:20:22,127 root         INFO     Train Epoch: 68 [6656/8000 (83%)]\tTotal Loss: 0.646548\n",
      "Reconstruction: 0.385832, Regularization: 0.187598, Discriminator: 0.040743; Generator: 0.032375,\n",
      "D(x): 0.534, D(G(z)): 0.372\n",
      "2019-04-10 01:20:22,223 root         INFO     Train Epoch: 68 [7168/8000 (90%)]\tTotal Loss: 105585.257812\n",
      "Reconstruction: 105584.781250, Regularization: 0.408913, Discriminator: 0.032137; Generator: 0.034721,\n",
      "D(x): 0.676, D(G(z)): 0.345\n",
      "2019-04-10 01:20:22,320 root         INFO     Train Epoch: 68 [7680/8000 (96%)]\tTotal Loss: 1.069552\n",
      "Reconstruction: 0.677242, Regularization: 0.323035, Discriminator: 0.035069; Generator: 0.034206,\n",
      "D(x): 0.603, D(G(z)): 0.358\n",
      "2019-04-10 01:20:22,392 root         INFO     ====> Epoch: 68 Average loss: 788850.3252\n",
      "2019-04-10 01:20:22,418 root         INFO     Train Epoch: 69 [0/8000 (0%)]\tTotal Loss: 0.768227\n",
      "Reconstruction: 0.447529, Regularization: 0.252635, Discriminator: 0.035503; Generator: 0.032560,\n",
      "D(x): 0.612, D(G(z)): 0.376\n",
      "2019-04-10 01:20:22,518 root         INFO     Train Epoch: 69 [512/8000 (6%)]\tTotal Loss: 0.737399\n",
      "Reconstruction: 0.423835, Regularization: 0.245151, Discriminator: 0.034384; Generator: 0.034029,\n",
      "D(x): 0.605, D(G(z)): 0.359\n",
      "2019-04-10 01:20:22,619 root         INFO     Train Epoch: 69 [1024/8000 (13%)]\tTotal Loss: 1.027941\n",
      "Reconstruction: 0.618098, Regularization: 0.342107, Discriminator: 0.031942; Generator: 0.035794,\n",
      "D(x): 0.644, D(G(z)): 0.327\n",
      "2019-04-10 01:20:22,718 root         INFO     Train Epoch: 69 [1536/8000 (19%)]\tTotal Loss: 1.090325\n",
      "Reconstruction: 0.653657, Regularization: 0.370574, Discriminator: 0.033653; Generator: 0.032441,\n",
      "D(x): 0.646, D(G(z)): 0.374\n",
      "2019-04-10 01:20:22,817 root         INFO     Train Epoch: 69 [2048/8000 (26%)]\tTotal Loss: 79737806848.000000\n",
      "Reconstruction: 79737806848.000000, Regularization: 0.394613, Discriminator: 0.035717; Generator: 0.032475,\n",
      "D(x): 0.627, D(G(z)): 0.382\n",
      "2019-04-10 01:20:22,916 root         INFO     Train Epoch: 69 [2560/8000 (32%)]\tTotal Loss: 0.879551\n",
      "Reconstruction: 0.557046, Regularization: 0.254117, Discriminator: 0.034393; Generator: 0.033994,\n",
      "D(x): 0.581, D(G(z)): 0.349\n",
      "2019-04-10 01:20:23,015 root         INFO     Train Epoch: 69 [3072/8000 (38%)]\tTotal Loss: 26.897894\n",
      "Reconstruction: 26.570593, Regularization: 0.253986, Discriminator: 0.041055; Generator: 0.032259,\n",
      "D(x): 0.522, D(G(z)): 0.375\n",
      "2019-04-10 01:20:23,114 root         INFO     Train Epoch: 69 [3584/8000 (45%)]\tTotal Loss: 571.506042\n",
      "Reconstruction: 571.259766, Regularization: 0.173242, Discriminator: 0.039709; Generator: 0.033377,\n",
      "D(x): 0.520, D(G(z)): 0.362\n",
      "2019-04-10 01:20:23,213 root         INFO     Train Epoch: 69 [4096/8000 (51%)]\tTotal Loss: 0.701758\n",
      "Reconstruction: 0.396916, Regularization: 0.236492, Discriminator: 0.038571; Generator: 0.029779,\n",
      "D(x): 0.588, D(G(z)): 0.418\n",
      "2019-04-10 01:20:23,312 root         INFO     Train Epoch: 69 [4608/8000 (58%)]\tTotal Loss: 1.001159\n",
      "Reconstruction: 0.593014, Regularization: 0.345837, Discriminator: 0.031001; Generator: 0.031306,\n",
      "D(x): 0.689, D(G(z)): 0.390\n",
      "2019-04-10 01:20:23,413 root         INFO     Train Epoch: 69 [5120/8000 (64%)]\tTotal Loss: 12.613826\n",
      "Reconstruction: 12.249245, Regularization: 0.293637, Discriminator: 0.037663; Generator: 0.033280,\n",
      "D(x): 0.601, D(G(z)): 0.361\n",
      "2019-04-10 01:20:23,514 root         INFO     Train Epoch: 69 [5632/8000 (70%)]\tTotal Loss: 1.040368\n",
      "Reconstruction: 0.652575, Regularization: 0.320664, Discriminator: 0.031565; Generator: 0.035564,\n",
      "D(x): 0.638, D(G(z)): 0.337\n",
      "2019-04-10 01:20:23,614 root         INFO     Train Epoch: 69 [6144/8000 (77%)]\tTotal Loss: 617230.000000\n",
      "Reconstruction: 617229.687500, Regularization: 0.225411, Discriminator: 0.043500; Generator: 0.029801,\n",
      "D(x): 0.577, D(G(z)): 0.422\n",
      "2019-04-10 01:20:23,715 root         INFO     Train Epoch: 69 [6656/8000 (83%)]\tTotal Loss: 4.342773\n",
      "Reconstruction: 4.110188, Regularization: 0.168180, Discriminator: 0.035028; Generator: 0.029378,\n",
      "D(x): 0.613, D(G(z)): 0.410\n",
      "2019-04-10 01:20:23,815 root         INFO     Train Epoch: 69 [7168/8000 (90%)]\tTotal Loss: 14.520066\n",
      "Reconstruction: 14.175262, Regularization: 0.276488, Discriminator: 0.033053; Generator: 0.035262,\n",
      "D(x): 0.610, D(G(z)): 0.336\n",
      "2019-04-10 01:20:23,916 root         INFO     Train Epoch: 69 [7680/8000 (96%)]\tTotal Loss: 2580811520.000000\n",
      "Reconstruction: 2580811520.000000, Regularization: 0.373655, Discriminator: 0.033411; Generator: 0.032444,\n",
      "D(x): 0.656, D(G(z)): 0.376\n",
      "2019-04-10 01:20:23,991 root         INFO     ====> Epoch: 69 Average loss: 49678941246.4773\n",
      "2019-04-10 01:20:24,017 root         INFO     Train Epoch: 70 [0/8000 (0%)]\tTotal Loss: 0.835622\n",
      "Reconstruction: 0.488563, Regularization: 0.277838, Discriminator: 0.037135; Generator: 0.032085,\n",
      "D(x): 0.606, D(G(z)): 0.381\n",
      "2019-04-10 01:20:24,118 root         INFO     Train Epoch: 70 [512/8000 (6%)]\tTotal Loss: 1.213557\n",
      "Reconstruction: 0.737125, Regularization: 0.408991, Discriminator: 0.035768; Generator: 0.031672,\n",
      "D(x): 0.629, D(G(z)): 0.381\n",
      "2019-04-10 01:20:24,219 root         INFO     Train Epoch: 70 [1024/8000 (13%)]\tTotal Loss: 0.845732\n",
      "Reconstruction: 0.527836, Regularization: 0.252129, Discriminator: 0.032987; Generator: 0.032780,\n",
      "D(x): 0.625, D(G(z)): 0.367\n",
      "2019-04-10 01:20:24,320 root         INFO     Train Epoch: 70 [1536/8000 (19%)]\tTotal Loss: 1.627557\n",
      "Reconstruction: 1.148133, Regularization: 0.410718, Discriminator: 0.037209; Generator: 0.031496,\n",
      "D(x): 0.622, D(G(z)): 0.384\n",
      "2019-04-10 01:20:24,420 root         INFO     Train Epoch: 70 [2048/8000 (26%)]\tTotal Loss: 1.362300\n",
      "Reconstruction: 1.016345, Regularization: 0.278807, Discriminator: 0.033103; Generator: 0.034047,\n",
      "D(x): 0.636, D(G(z)): 0.354\n",
      "2019-04-10 01:20:24,521 root         INFO     Train Epoch: 70 [2560/8000 (32%)]\tTotal Loss: 25.781330\n",
      "Reconstruction: 25.457207, Regularization: 0.252837, Discriminator: 0.036269; Generator: 0.035018,\n",
      "D(x): 0.565, D(G(z)): 0.341\n",
      "2019-04-10 01:20:24,621 root         INFO     Train Epoch: 70 [3072/8000 (38%)]\tTotal Loss: 2733.473633\n",
      "Reconstruction: 2733.097412, Regularization: 0.309179, Discriminator: 0.033886; Generator: 0.033252,\n",
      "D(x): 0.628, D(G(z)): 0.364\n",
      "2019-04-10 01:20:24,722 root         INFO     Train Epoch: 70 [3584/8000 (45%)]\tTotal Loss: 85.337349\n",
      "Reconstruction: 84.941628, Regularization: 0.333585, Discriminator: 0.030413; Generator: 0.031720,\n",
      "D(x): 0.686, D(G(z)): 0.384\n",
      "2019-04-10 01:20:24,822 root         INFO     Train Epoch: 70 [4096/8000 (51%)]\tTotal Loss: 178.590073\n",
      "Reconstruction: 178.120407, Regularization: 0.398947, Discriminator: 0.035439; Generator: 0.035292,\n",
      "D(x): 0.569, D(G(z)): 0.333\n",
      "2019-04-10 01:20:24,923 root         INFO     Train Epoch: 70 [4608/8000 (58%)]\tTotal Loss: 0.767750\n",
      "Reconstruction: 0.461120, Regularization: 0.240855, Discriminator: 0.034541; Generator: 0.031234,\n",
      "D(x): 0.679, D(G(z)): 0.397\n",
      "2019-04-10 01:20:25,024 root         INFO     Train Epoch: 70 [5120/8000 (64%)]\tTotal Loss: 0.971475\n",
      "Reconstruction: 0.572974, Regularization: 0.333767, Discriminator: 0.033891; Generator: 0.030844,\n",
      "D(x): 0.641, D(G(z)): 0.397\n",
      "2019-04-10 01:20:25,124 root         INFO     Train Epoch: 70 [5632/8000 (70%)]\tTotal Loss: 3.705785\n",
      "Reconstruction: 3.376622, Regularization: 0.263865, Discriminator: 0.033591; Generator: 0.031706,\n",
      "D(x): 0.637, D(G(z)): 0.382\n",
      "2019-04-10 01:20:25,225 root         INFO     Train Epoch: 70 [6144/8000 (77%)]\tTotal Loss: 4.494463\n",
      "Reconstruction: 4.158885, Regularization: 0.268499, Discriminator: 0.036350; Generator: 0.030729,\n",
      "D(x): 0.589, D(G(z)): 0.391\n",
      "2019-04-10 01:20:25,325 root         INFO     Train Epoch: 70 [6656/8000 (83%)]\tTotal Loss: 27.718554\n",
      "Reconstruction: 27.468174, Regularization: 0.179992, Discriminator: 0.037932; Generator: 0.032455,\n",
      "D(x): 0.550, D(G(z)): 0.373\n",
      "2019-04-10 01:20:25,425 root         INFO     Train Epoch: 70 [7168/8000 (90%)]\tTotal Loss: 27.846970\n",
      "Reconstruction: 27.508448, Regularization: 0.268066, Discriminator: 0.036367; Generator: 0.034088,\n",
      "D(x): 0.569, D(G(z)): 0.351\n",
      "2019-04-10 01:20:25,525 root         INFO     Train Epoch: 70 [7680/8000 (96%)]\tTotal Loss: 117275.320312\n",
      "Reconstruction: 117275.015625, Regularization: 0.234114, Discriminator: 0.032729; Generator: 0.034147,\n",
      "D(x): 0.611, D(G(z)): 0.349\n",
      "2019-04-10 01:20:25,600 root         INFO     ====> Epoch: 70 Average loss: 145731683.2924\n",
      "2019-04-10 01:20:25,627 root         INFO     Train Epoch: 71 [0/8000 (0%)]\tTotal Loss: 0.630592\n",
      "Reconstruction: 0.351674, Regularization: 0.209188, Discriminator: 0.036790; Generator: 0.032940,\n",
      "D(x): 0.590, D(G(z)): 0.361\n",
      "2019-04-10 01:20:25,729 root         INFO     Train Epoch: 71 [512/8000 (6%)]\tTotal Loss: 98.787056\n",
      "Reconstruction: 98.300461, Regularization: 0.419291, Discriminator: 0.035203; Generator: 0.032102,\n",
      "D(x): 0.623, D(G(z)): 0.377\n",
      "2019-04-10 01:20:25,831 root         INFO     Train Epoch: 71 [1024/8000 (13%)]\tTotal Loss: 24025.535156\n",
      "Reconstruction: 24025.076172, Regularization: 0.391911, Discriminator: 0.035588; Generator: 0.030306,\n",
      "D(x): 0.657, D(G(z)): 0.404\n",
      "2019-04-10 01:20:25,933 root         INFO     Train Epoch: 71 [1536/8000 (19%)]\tTotal Loss: 24.058176\n",
      "Reconstruction: 23.694521, Regularization: 0.301369, Discriminator: 0.029470; Generator: 0.032817,\n",
      "D(x): 0.682, D(G(z)): 0.364\n",
      "2019-04-10 01:20:26,034 root         INFO     Train Epoch: 71 [2048/8000 (26%)]\tTotal Loss: 133.259109\n",
      "Reconstruction: 132.907166, Regularization: 0.285715, Discriminator: 0.035139; Generator: 0.031078,\n",
      "D(x): 0.611, D(G(z)): 0.387\n",
      "2019-04-10 01:20:26,135 root         INFO     Train Epoch: 71 [2560/8000 (32%)]\tTotal Loss: 1.311601\n",
      "Reconstruction: 0.998157, Regularization: 0.241272, Discriminator: 0.042245; Generator: 0.029926,\n",
      "D(x): 0.530, D(G(z)): 0.409\n",
      "2019-04-10 01:20:26,235 root         INFO     Train Epoch: 71 [3072/8000 (38%)]\tTotal Loss: 1.206454\n",
      "Reconstruction: 0.726123, Regularization: 0.414660, Discriminator: 0.031780; Generator: 0.033890,\n",
      "D(x): 0.647, D(G(z)): 0.351\n",
      "2019-04-10 01:20:26,335 root         INFO     Train Epoch: 71 [3584/8000 (45%)]\tTotal Loss: 2.378418\n",
      "Reconstruction: 1.905676, Regularization: 0.405958, Discriminator: 0.034334; Generator: 0.032450,\n",
      "D(x): 0.605, D(G(z)): 0.371\n",
      "2019-04-10 01:20:26,435 root         INFO     Train Epoch: 71 [4096/8000 (51%)]\tTotal Loss: 8242.946289\n",
      "Reconstruction: 8242.558594, Regularization: 0.318481, Discriminator: 0.038147; Generator: 0.030896,\n",
      "D(x): 0.595, D(G(z)): 0.399\n",
      "2019-04-10 01:20:26,535 root         INFO     Train Epoch: 71 [4608/8000 (58%)]\tTotal Loss: 0.726433\n",
      "Reconstruction: 0.455353, Regularization: 0.200184, Discriminator: 0.039601; Generator: 0.031295,\n",
      "D(x): 0.543, D(G(z)): 0.386\n",
      "2019-04-10 01:20:26,635 root         INFO     Train Epoch: 71 [5120/8000 (64%)]\tTotal Loss: 1.321755\n",
      "Reconstruction: 1.103807, Regularization: 0.150387, Discriminator: 0.035295; Generator: 0.032266,\n",
      "D(x): 0.568, D(G(z)): 0.374\n",
      "2019-04-10 01:20:26,735 root         INFO     Train Epoch: 71 [5632/8000 (70%)]\tTotal Loss: 632.630249\n",
      "Reconstruction: 632.327881, Regularization: 0.232765, Discriminator: 0.037746; Generator: 0.031825,\n",
      "D(x): 0.577, D(G(z)): 0.383\n",
      "2019-04-10 01:20:26,836 root         INFO     Train Epoch: 71 [6144/8000 (77%)]\tTotal Loss: 338.428955\n",
      "Reconstruction: 338.130219, Regularization: 0.229590, Discriminator: 0.036366; Generator: 0.032778,\n",
      "D(x): 0.578, D(G(z)): 0.368\n",
      "2019-04-10 01:20:26,935 root         INFO     Train Epoch: 71 [6656/8000 (83%)]\tTotal Loss: 0.835772\n",
      "Reconstruction: 0.483671, Regularization: 0.284913, Discriminator: 0.033227; Generator: 0.033961,\n",
      "D(x): 0.602, D(G(z)): 0.351\n",
      "2019-04-10 01:20:27,035 root         INFO     Train Epoch: 71 [7168/8000 (90%)]\tTotal Loss: 4.605636\n",
      "Reconstruction: 4.178987, Regularization: 0.356992, Discriminator: 0.036730; Generator: 0.032927,\n",
      "D(x): 0.566, D(G(z)): 0.362\n",
      "2019-04-10 01:20:27,135 root         INFO     Train Epoch: 71 [7680/8000 (96%)]\tTotal Loss: 10831035.000000\n",
      "Reconstruction: 10831035.000000, Regularization: 0.294599, Discriminator: 0.039660; Generator: 0.029305,\n",
      "D(x): 0.583, D(G(z)): 0.410\n",
      "2019-04-10 01:20:27,210 root         INFO     ====> Epoch: 71 Average loss: 25701193556.4214\n",
      "2019-04-10 01:20:27,237 root         INFO     Train Epoch: 72 [0/8000 (0%)]\tTotal Loss: 9.857510\n",
      "Reconstruction: 9.477932, Regularization: 0.312202, Discriminator: 0.034410; Generator: 0.032966,\n",
      "D(x): 0.613, D(G(z)): 0.364\n",
      "2019-04-10 01:20:27,338 root         INFO     Train Epoch: 72 [512/8000 (6%)]\tTotal Loss: 2.633503\n",
      "Reconstruction: 2.363033, Regularization: 0.199172, Discriminator: 0.038583; Generator: 0.032714,\n",
      "D(x): 0.542, D(G(z)): 0.370\n",
      "2019-04-10 01:20:27,438 root         INFO     Train Epoch: 72 [1024/8000 (13%)]\tTotal Loss: 0.921891\n",
      "Reconstruction: 0.568596, Regularization: 0.286066, Discriminator: 0.037316; Generator: 0.029912,\n",
      "D(x): 0.589, D(G(z)): 0.401\n",
      "2019-04-10 01:20:27,539 root         INFO     Train Epoch: 72 [1536/8000 (19%)]\tTotal Loss: 3.143806\n",
      "Reconstruction: 2.715420, Regularization: 0.363403, Discriminator: 0.030678; Generator: 0.034305,\n",
      "D(x): 0.638, D(G(z)): 0.344\n",
      "2019-04-10 01:20:27,640 root         INFO     Train Epoch: 72 [2048/8000 (26%)]\tTotal Loss: 1.552654\n",
      "Reconstruction: 1.240363, Regularization: 0.238603, Discriminator: 0.041109; Generator: 0.032579,\n",
      "D(x): 0.520, D(G(z)): 0.371\n",
      "2019-04-10 01:20:27,740 root         INFO     Train Epoch: 72 [2560/8000 (32%)]\tTotal Loss: 2.044160\n",
      "Reconstruction: 1.492208, Regularization: 0.490803, Discriminator: 0.028075; Generator: 0.033073,\n",
      "D(x): 0.712, D(G(z)): 0.359\n",
      "2019-04-10 01:20:27,842 root         INFO     Train Epoch: 72 [3072/8000 (38%)]\tTotal Loss: 1.444469\n",
      "Reconstruction: 0.959777, Regularization: 0.414922, Discriminator: 0.037370; Generator: 0.032400,\n",
      "D(x): 0.596, D(G(z)): 0.368\n",
      "2019-04-10 01:20:27,944 root         INFO     Train Epoch: 72 [3584/8000 (45%)]\tTotal Loss: 20371.736328\n",
      "Reconstruction: 20371.302734, Regularization: 0.364380, Discriminator: 0.037785; Generator: 0.031370,\n",
      "D(x): 0.561, D(G(z)): 0.380\n",
      "2019-04-10 01:20:28,044 root         INFO     Train Epoch: 72 [4096/8000 (51%)]\tTotal Loss: 1.316864\n",
      "Reconstruction: 1.028413, Regularization: 0.217330, Discriminator: 0.038796; Generator: 0.032325,\n",
      "D(x): 0.533, D(G(z)): 0.374\n",
      "2019-04-10 01:20:28,144 root         INFO     Train Epoch: 72 [4608/8000 (58%)]\tTotal Loss: 6.052708\n",
      "Reconstruction: 5.645268, Regularization: 0.342270, Discriminator: 0.033475; Generator: 0.031694,\n",
      "D(x): 0.622, D(G(z)): 0.377\n",
      "2019-04-10 01:20:28,244 root         INFO     Train Epoch: 72 [5120/8000 (64%)]\tTotal Loss: 51.424660\n",
      "Reconstruction: 51.119408, Regularization: 0.232988, Discriminator: 0.040031; Generator: 0.032235,\n",
      "D(x): 0.538, D(G(z)): 0.368\n",
      "2019-04-10 01:20:28,344 root         INFO     Train Epoch: 72 [5632/8000 (70%)]\tTotal Loss: 14327328.000000\n",
      "Reconstruction: 14327328.000000, Regularization: 0.148150, Discriminator: 0.044039; Generator: 0.031380,\n",
      "D(x): 0.471, D(G(z)): 0.380\n",
      "2019-04-10 01:20:28,444 root         INFO     Train Epoch: 72 [6144/8000 (77%)]\tTotal Loss: 0.883486\n",
      "Reconstruction: 0.524315, Regularization: 0.288365, Discriminator: 0.036509; Generator: 0.034297,\n",
      "D(x): 0.553, D(G(z)): 0.345\n",
      "2019-04-10 01:20:28,544 root         INFO     Train Epoch: 72 [6656/8000 (83%)]\tTotal Loss: 0.841125\n",
      "Reconstruction: 0.485120, Regularization: 0.289368, Discriminator: 0.036720; Generator: 0.029916,\n",
      "D(x): 0.595, D(G(z)): 0.403\n",
      "2019-04-10 01:20:28,643 root         INFO     Train Epoch: 72 [7168/8000 (90%)]\tTotal Loss: 237.004257\n",
      "Reconstruction: 236.667160, Regularization: 0.268440, Discriminator: 0.039479; Generator: 0.029179,\n",
      "D(x): 0.578, D(G(z)): 0.409\n",
      "2019-04-10 01:20:28,743 root         INFO     Train Epoch: 72 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030652; Generator: 0.032214,\n",
      "D(x): 0.654, D(G(z)): 0.368\n",
      "2019-04-10 01:20:28,817 root         INFO     ====> Epoch: 72 Average loss: nan\n",
      "2019-04-10 01:20:28,844 root         INFO     Train Epoch: 73 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038819; Generator: 0.033350,\n",
      "D(x): 0.515, D(G(z)): 0.355\n",
      "2019-04-10 01:20:28,946 root         INFO     Train Epoch: 73 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035972; Generator: 0.030752,\n",
      "D(x): 0.585, D(G(z)): 0.389\n",
      "2019-04-10 01:20:29,047 root         INFO     Train Epoch: 73 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041858; Generator: 0.030552,\n",
      "D(x): 0.502, D(G(z)): 0.394\n",
      "2019-04-10 01:20:29,147 root         INFO     Train Epoch: 73 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037605; Generator: 0.030971,\n",
      "D(x): 0.566, D(G(z)): 0.388\n",
      "2019-04-10 01:20:29,248 root         INFO     Train Epoch: 73 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037427; Generator: 0.032785,\n",
      "D(x): 0.535, D(G(z)): 0.363\n",
      "2019-04-10 01:20:29,348 root         INFO     Train Epoch: 73 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041992; Generator: 0.025885,\n",
      "D(x): 0.602, D(G(z)): 0.464\n",
      "2019-04-10 01:20:29,449 root         INFO     Train Epoch: 73 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033823; Generator: 0.031232,\n",
      "D(x): 0.606, D(G(z)): 0.381\n",
      "2019-04-10 01:20:29,551 root         INFO     Train Epoch: 73 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032759; Generator: 0.033342,\n",
      "D(x): 0.639, D(G(z)): 0.355\n",
      "2019-04-10 01:20:29,651 root         INFO     Train Epoch: 73 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033963; Generator: 0.032572,\n",
      "D(x): 0.603, D(G(z)): 0.363\n",
      "2019-04-10 01:20:29,752 root         INFO     Train Epoch: 73 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040782; Generator: 0.029504,\n",
      "D(x): 0.544, D(G(z)): 0.404\n",
      "2019-04-10 01:20:29,853 root         INFO     Train Epoch: 73 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038245; Generator: 0.029153,\n",
      "D(x): 0.594, D(G(z)): 0.412\n",
      "2019-04-10 01:20:29,953 root         INFO     Train Epoch: 73 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035517; Generator: 0.030637,\n",
      "D(x): 0.586, D(G(z)): 0.392\n",
      "2019-04-10 01:20:30,053 root         INFO     Train Epoch: 73 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037134; Generator: 0.030797,\n",
      "D(x): 0.603, D(G(z)): 0.389\n",
      "2019-04-10 01:20:30,154 root         INFO     Train Epoch: 73 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035669; Generator: 0.027648,\n",
      "D(x): 0.649, D(G(z)): 0.431\n",
      "2019-04-10 01:20:30,254 root         INFO     Train Epoch: 73 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039073; Generator: 0.029999,\n",
      "D(x): 0.570, D(G(z)): 0.401\n",
      "2019-04-10 01:20:30,354 root         INFO     Train Epoch: 73 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042215; Generator: 0.027376,\n",
      "D(x): 0.538, D(G(z)): 0.440\n",
      "2019-04-10 01:20:30,429 root         INFO     ====> Epoch: 73 Average loss: nan\n",
      "2019-04-10 01:20:30,456 root         INFO     Train Epoch: 74 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035722; Generator: 0.026727,\n",
      "D(x): 0.652, D(G(z)): 0.444\n",
      "2019-04-10 01:20:30,557 root         INFO     Train Epoch: 74 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041984; Generator: 0.029650,\n",
      "D(x): 0.540, D(G(z)): 0.405\n",
      "2019-04-10 01:20:30,657 root         INFO     Train Epoch: 74 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035494; Generator: 0.030592,\n",
      "D(x): 0.599, D(G(z)): 0.390\n",
      "2019-04-10 01:20:30,758 root         INFO     Train Epoch: 74 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035686; Generator: 0.031491,\n",
      "D(x): 0.593, D(G(z)): 0.375\n",
      "2019-04-10 01:20:30,858 root         INFO     Train Epoch: 74 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040196; Generator: 0.028317,\n",
      "D(x): 0.580, D(G(z)): 0.431\n",
      "2019-04-10 01:20:30,959 root         INFO     Train Epoch: 74 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037792; Generator: 0.028852,\n",
      "D(x): 0.593, D(G(z)): 0.422\n",
      "2019-04-10 01:20:31,060 root         INFO     Train Epoch: 74 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038342; Generator: 0.025682,\n",
      "D(x): 0.621, D(G(z)): 0.452\n",
      "2019-04-10 01:20:31,161 root         INFO     Train Epoch: 74 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037131; Generator: 0.029150,\n",
      "D(x): 0.593, D(G(z)): 0.411\n",
      "2019-04-10 01:20:31,261 root         INFO     Train Epoch: 74 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037602; Generator: 0.030731,\n",
      "D(x): 0.549, D(G(z)): 0.389\n",
      "2019-04-10 01:20:31,362 root         INFO     Train Epoch: 74 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037617; Generator: 0.030608,\n",
      "D(x): 0.558, D(G(z)): 0.390\n",
      "2019-04-10 01:20:31,463 root         INFO     Train Epoch: 74 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035992; Generator: 0.030153,\n",
      "D(x): 0.620, D(G(z)): 0.399\n",
      "2019-04-10 01:20:31,563 root         INFO     Train Epoch: 74 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035217; Generator: 0.029021,\n",
      "D(x): 0.639, D(G(z)): 0.413\n",
      "2019-04-10 01:20:31,664 root         INFO     Train Epoch: 74 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042249; Generator: 0.029432,\n",
      "D(x): 0.502, D(G(z)): 0.404\n",
      "2019-04-10 01:20:31,764 root         INFO     Train Epoch: 74 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039465; Generator: 0.029890,\n",
      "D(x): 0.558, D(G(z)): 0.400\n",
      "2019-04-10 01:20:31,864 root         INFO     Train Epoch: 74 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032801; Generator: 0.032345,\n",
      "D(x): 0.618, D(G(z)): 0.366\n",
      "2019-04-10 01:20:31,966 root         INFO     Train Epoch: 74 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040734; Generator: 0.027195,\n",
      "D(x): 0.561, D(G(z)): 0.437\n",
      "2019-04-10 01:20:32,041 root         INFO     ====> Epoch: 74 Average loss: nan\n",
      "2019-04-10 01:20:32,068 root         INFO     Train Epoch: 75 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036938; Generator: 0.029977,\n",
      "D(x): 0.584, D(G(z)): 0.399\n",
      "2019-04-10 01:20:32,166 root         INFO     Train Epoch: 75 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039548; Generator: 0.026493,\n",
      "D(x): 0.599, D(G(z)): 0.452\n",
      "2019-04-10 01:20:32,264 root         INFO     Train Epoch: 75 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039338; Generator: 0.029853,\n",
      "D(x): 0.554, D(G(z)): 0.400\n",
      "2019-04-10 01:20:32,362 root         INFO     Train Epoch: 75 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040275; Generator: 0.029084,\n",
      "D(x): 0.541, D(G(z)): 0.409\n",
      "2019-04-10 01:20:32,460 root         INFO     Train Epoch: 75 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039312; Generator: 0.030006,\n",
      "D(x): 0.558, D(G(z)): 0.399\n",
      "2019-04-10 01:20:32,558 root         INFO     Train Epoch: 75 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039719; Generator: 0.026153,\n",
      "D(x): 0.599, D(G(z)): 0.457\n",
      "2019-04-10 01:20:32,656 root         INFO     Train Epoch: 75 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037599; Generator: 0.028540,\n",
      "D(x): 0.596, D(G(z)): 0.419\n",
      "2019-04-10 01:20:32,754 root         INFO     Train Epoch: 75 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030883; Generator: 0.031214,\n",
      "D(x): 0.661, D(G(z)): 0.378\n",
      "2019-04-10 01:20:32,854 root         INFO     Train Epoch: 75 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040612; Generator: 0.027849,\n",
      "D(x): 0.557, D(G(z)): 0.430\n",
      "2019-04-10 01:20:32,954 root         INFO     Train Epoch: 75 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039041; Generator: 0.029057,\n",
      "D(x): 0.574, D(G(z)): 0.413\n",
      "2019-04-10 01:20:33,055 root         INFO     Train Epoch: 75 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037760; Generator: 0.029592,\n",
      "D(x): 0.560, D(G(z)): 0.402\n",
      "2019-04-10 01:20:33,155 root         INFO     Train Epoch: 75 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042738; Generator: 0.029108,\n",
      "D(x): 0.486, D(G(z)): 0.406\n",
      "2019-04-10 01:20:33,255 root         INFO     Train Epoch: 75 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036166; Generator: 0.030712,\n",
      "D(x): 0.574, D(G(z)): 0.386\n",
      "2019-04-10 01:20:33,355 root         INFO     Train Epoch: 75 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042943; Generator: 0.028209,\n",
      "D(x): 0.480, D(G(z)): 0.418\n",
      "2019-04-10 01:20:33,455 root         INFO     Train Epoch: 75 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038160; Generator: 0.028632,\n",
      "D(x): 0.580, D(G(z)): 0.415\n",
      "2019-04-10 01:20:33,555 root         INFO     Train Epoch: 75 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036380; Generator: 0.028876,\n",
      "D(x): 0.596, D(G(z)): 0.413\n",
      "2019-04-10 01:20:33,629 root         INFO     ====> Epoch: 75 Average loss: nan\n",
      "2019-04-10 01:20:33,656 root         INFO     Train Epoch: 76 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041543; Generator: 0.029446,\n",
      "D(x): 0.510, D(G(z)): 0.406\n",
      "2019-04-10 01:20:33,756 root         INFO     Train Epoch: 76 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037326; Generator: 0.029180,\n",
      "D(x): 0.573, D(G(z)): 0.403\n",
      "2019-04-10 01:20:33,856 root         INFO     Train Epoch: 76 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037658; Generator: 0.026766,\n",
      "D(x): 0.608, D(G(z)): 0.439\n",
      "2019-04-10 01:20:33,955 root         INFO     Train Epoch: 76 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041705; Generator: 0.027269,\n",
      "D(x): 0.529, D(G(z)): 0.436\n",
      "2019-04-10 01:20:34,055 root         INFO     Train Epoch: 76 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044132; Generator: 0.027960,\n",
      "D(x): 0.497, D(G(z)): 0.427\n",
      "2019-04-10 01:20:34,155 root         INFO     Train Epoch: 76 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036161; Generator: 0.028434,\n",
      "D(x): 0.609, D(G(z)): 0.417\n",
      "2019-04-10 01:20:34,255 root         INFO     Train Epoch: 76 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038798; Generator: 0.028483,\n",
      "D(x): 0.536, D(G(z)): 0.414\n",
      "2019-04-10 01:20:34,355 root         INFO     Train Epoch: 76 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032481; Generator: 0.030376,\n",
      "D(x): 0.650, D(G(z)): 0.393\n",
      "2019-04-10 01:20:34,454 root         INFO     Train Epoch: 76 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040630; Generator: 0.029307,\n",
      "D(x): 0.526, D(G(z)): 0.406\n",
      "2019-04-10 01:20:34,554 root         INFO     Train Epoch: 76 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032995; Generator: 0.030318,\n",
      "D(x): 0.631, D(G(z)): 0.388\n",
      "2019-04-10 01:20:34,654 root         INFO     Train Epoch: 76 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033733; Generator: 0.031434,\n",
      "D(x): 0.597, D(G(z)): 0.372\n",
      "2019-04-10 01:20:34,754 root         INFO     Train Epoch: 76 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036250; Generator: 0.029350,\n",
      "D(x): 0.582, D(G(z)): 0.404\n",
      "2019-04-10 01:20:34,854 root         INFO     Train Epoch: 76 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037625; Generator: 0.029284,\n",
      "D(x): 0.554, D(G(z)): 0.402\n",
      "2019-04-10 01:20:34,954 root         INFO     Train Epoch: 76 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034109; Generator: 0.028485,\n",
      "D(x): 0.635, D(G(z)): 0.415\n",
      "2019-04-10 01:20:35,054 root         INFO     Train Epoch: 76 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039655; Generator: 0.028228,\n",
      "D(x): 0.547, D(G(z)): 0.420\n",
      "2019-04-10 01:20:35,154 root         INFO     Train Epoch: 76 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041125; Generator: 0.027925,\n",
      "D(x): 0.544, D(G(z)): 0.429\n",
      "2019-04-10 01:20:35,228 root         INFO     ====> Epoch: 76 Average loss: nan\n",
      "2019-04-10 01:20:35,255 root         INFO     Train Epoch: 77 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036042; Generator: 0.028350,\n",
      "D(x): 0.605, D(G(z)): 0.419\n",
      "2019-04-10 01:20:35,355 root         INFO     Train Epoch: 77 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036709; Generator: 0.028634,\n",
      "D(x): 0.590, D(G(z)): 0.420\n",
      "2019-04-10 01:20:35,456 root         INFO     Train Epoch: 77 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039619; Generator: 0.026397,\n",
      "D(x): 0.567, D(G(z)): 0.448\n",
      "2019-04-10 01:20:35,556 root         INFO     Train Epoch: 77 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037553; Generator: 0.027342,\n",
      "D(x): 0.595, D(G(z)): 0.433\n",
      "2019-04-10 01:20:35,655 root         INFO     Train Epoch: 77 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041880; Generator: 0.030600,\n",
      "D(x): 0.483, D(G(z)): 0.385\n",
      "2019-04-10 01:20:35,755 root         INFO     Train Epoch: 77 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035100; Generator: 0.029473,\n",
      "D(x): 0.604, D(G(z)): 0.401\n",
      "2019-04-10 01:20:35,855 root         INFO     Train Epoch: 77 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035064; Generator: 0.031246,\n",
      "D(x): 0.569, D(G(z)): 0.374\n",
      "2019-04-10 01:20:35,955 root         INFO     Train Epoch: 77 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033417; Generator: 0.030579,\n",
      "D(x): 0.616, D(G(z)): 0.386\n",
      "2019-04-10 01:20:36,054 root         INFO     Train Epoch: 77 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034577; Generator: 0.029539,\n",
      "D(x): 0.602, D(G(z)): 0.399\n",
      "2019-04-10 01:20:36,154 root         INFO     Train Epoch: 77 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036935; Generator: 0.025800,\n",
      "D(x): 0.638, D(G(z)): 0.457\n",
      "2019-04-10 01:20:36,254 root         INFO     Train Epoch: 77 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037406; Generator: 0.029585,\n",
      "D(x): 0.544, D(G(z)): 0.395\n",
      "2019-04-10 01:20:36,354 root         INFO     Train Epoch: 77 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040506; Generator: 0.028609,\n",
      "D(x): 0.534, D(G(z)): 0.411\n",
      "2019-04-10 01:20:36,454 root         INFO     Train Epoch: 77 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033418; Generator: 0.029267,\n",
      "D(x): 0.633, D(G(z)): 0.406\n",
      "2019-04-10 01:20:36,554 root         INFO     Train Epoch: 77 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.043484; Generator: 0.029252,\n",
      "D(x): 0.481, D(G(z)): 0.406\n",
      "2019-04-10 01:20:36,655 root         INFO     Train Epoch: 77 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038709; Generator: 0.029317,\n",
      "D(x): 0.556, D(G(z)): 0.403\n",
      "2019-04-10 01:20:36,757 root         INFO     Train Epoch: 77 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038868; Generator: 0.028137,\n",
      "D(x): 0.554, D(G(z)): 0.420\n",
      "2019-04-10 01:20:36,833 root         INFO     ====> Epoch: 77 Average loss: nan\n",
      "2019-04-10 01:20:36,860 root         INFO     Train Epoch: 78 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039114; Generator: 0.027740,\n",
      "D(x): 0.564, D(G(z)): 0.427\n",
      "2019-04-10 01:20:36,961 root         INFO     Train Epoch: 78 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.043351; Generator: 0.025119,\n",
      "D(x): 0.532, D(G(z)): 0.467\n",
      "2019-04-10 01:20:37,066 root         INFO     Train Epoch: 78 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034653; Generator: 0.028321,\n",
      "D(x): 0.609, D(G(z)): 0.417\n",
      "2019-04-10 01:20:37,165 root         INFO     Train Epoch: 78 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037348; Generator: 0.030415,\n",
      "D(x): 0.560, D(G(z)): 0.385\n",
      "2019-04-10 01:20:37,264 root         INFO     Train Epoch: 78 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036848; Generator: 0.030491,\n",
      "D(x): 0.566, D(G(z)): 0.383\n",
      "2019-04-10 01:20:37,363 root         INFO     Train Epoch: 78 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042422; Generator: 0.028576,\n",
      "D(x): 0.485, D(G(z)): 0.409\n",
      "2019-04-10 01:20:37,464 root         INFO     Train Epoch: 78 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041526; Generator: 0.029640,\n",
      "D(x): 0.494, D(G(z)): 0.397\n",
      "2019-04-10 01:20:37,565 root         INFO     Train Epoch: 78 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037847; Generator: 0.030024,\n",
      "D(x): 0.547, D(G(z)): 0.389\n",
      "2019-04-10 01:20:37,665 root         INFO     Train Epoch: 78 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037371; Generator: 0.027865,\n",
      "D(x): 0.589, D(G(z)): 0.427\n",
      "2019-04-10 01:20:37,766 root         INFO     Train Epoch: 78 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033428; Generator: 0.027760,\n",
      "D(x): 0.658, D(G(z)): 0.423\n",
      "2019-04-10 01:20:37,866 root         INFO     Train Epoch: 78 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041583; Generator: 0.025718,\n",
      "D(x): 0.546, D(G(z)): 0.456\n",
      "2019-04-10 01:20:37,966 root         INFO     Train Epoch: 78 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039256; Generator: 0.028189,\n",
      "D(x): 0.546, D(G(z)): 0.418\n",
      "2019-04-10 01:20:38,066 root         INFO     Train Epoch: 78 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034633; Generator: 0.027465,\n",
      "D(x): 0.622, D(G(z)): 0.426\n",
      "2019-04-10 01:20:38,166 root         INFO     Train Epoch: 78 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038083; Generator: 0.028792,\n",
      "D(x): 0.542, D(G(z)): 0.410\n",
      "2019-04-10 01:20:38,266 root         INFO     Train Epoch: 78 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039414; Generator: 0.029011,\n",
      "D(x): 0.537, D(G(z)): 0.404\n",
      "2019-04-10 01:20:38,367 root         INFO     Train Epoch: 78 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039039; Generator: 0.029624,\n",
      "D(x): 0.529, D(G(z)): 0.395\n",
      "2019-04-10 01:20:38,442 root         INFO     ====> Epoch: 78 Average loss: nan\n",
      "2019-04-10 01:20:38,469 root         INFO     Train Epoch: 79 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040743; Generator: 0.027058,\n",
      "D(x): 0.521, D(G(z)): 0.435\n",
      "2019-04-10 01:20:38,571 root         INFO     Train Epoch: 79 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041331; Generator: 0.030058,\n",
      "D(x): 0.490, D(G(z)): 0.395\n",
      "2019-04-10 01:20:38,673 root         INFO     Train Epoch: 79 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034629; Generator: 0.028895,\n",
      "D(x): 0.605, D(G(z)): 0.406\n",
      "2019-04-10 01:20:38,775 root         INFO     Train Epoch: 79 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040121; Generator: 0.028196,\n",
      "D(x): 0.533, D(G(z)): 0.418\n",
      "2019-04-10 01:20:38,874 root         INFO     Train Epoch: 79 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037244; Generator: 0.028797,\n",
      "D(x): 0.572, D(G(z)): 0.408\n",
      "2019-04-10 01:20:38,971 root         INFO     Train Epoch: 79 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037959; Generator: 0.027850,\n",
      "D(x): 0.577, D(G(z)): 0.422\n",
      "2019-04-10 01:20:39,069 root         INFO     Train Epoch: 79 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037602; Generator: 0.028057,\n",
      "D(x): 0.562, D(G(z)): 0.418\n",
      "2019-04-10 01:20:39,166 root         INFO     Train Epoch: 79 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037381; Generator: 0.029702,\n",
      "D(x): 0.549, D(G(z)): 0.395\n",
      "2019-04-10 01:20:39,264 root         INFO     Train Epoch: 79 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039464; Generator: 0.025846,\n",
      "D(x): 0.584, D(G(z)): 0.454\n",
      "2019-04-10 01:20:39,361 root         INFO     Train Epoch: 79 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036038; Generator: 0.026143,\n",
      "D(x): 0.628, D(G(z)): 0.446\n",
      "2019-04-10 01:20:39,459 root         INFO     Train Epoch: 79 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039193; Generator: 0.028065,\n",
      "D(x): 0.557, D(G(z)): 0.422\n",
      "2019-04-10 01:20:39,557 root         INFO     Train Epoch: 79 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040235; Generator: 0.028214,\n",
      "D(x): 0.537, D(G(z)): 0.419\n",
      "2019-04-10 01:20:39,656 root         INFO     Train Epoch: 79 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038516; Generator: 0.027862,\n",
      "D(x): 0.556, D(G(z)): 0.421\n",
      "2019-04-10 01:20:39,754 root         INFO     Train Epoch: 79 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041690; Generator: 0.026976,\n",
      "D(x): 0.525, D(G(z)): 0.437\n",
      "2019-04-10 01:20:39,853 root         INFO     Train Epoch: 79 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038717; Generator: 0.029453,\n",
      "D(x): 0.528, D(G(z)): 0.398\n",
      "2019-04-10 01:20:39,953 root         INFO     Train Epoch: 79 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040436; Generator: 0.026277,\n",
      "D(x): 0.551, D(G(z)): 0.447\n",
      "2019-04-10 01:20:40,027 root         INFO     ====> Epoch: 79 Average loss: nan\n",
      "2019-04-10 01:20:40,055 root         INFO     Train Epoch: 80 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038688; Generator: 0.027523,\n",
      "D(x): 0.572, D(G(z)): 0.429\n",
      "2019-04-10 01:20:40,156 root         INFO     Train Epoch: 80 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039459; Generator: 0.027418,\n",
      "D(x): 0.554, D(G(z)): 0.430\n",
      "2019-04-10 01:20:40,257 root         INFO     Train Epoch: 80 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038987; Generator: 0.027862,\n",
      "D(x): 0.556, D(G(z)): 0.421\n",
      "2019-04-10 01:20:40,359 root         INFO     Train Epoch: 80 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039158; Generator: 0.029438,\n",
      "D(x): 0.517, D(G(z)): 0.396\n",
      "2019-04-10 01:20:40,460 root         INFO     Train Epoch: 80 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038421; Generator: 0.028325,\n",
      "D(x): 0.552, D(G(z)): 0.416\n",
      "2019-04-10 01:20:40,562 root         INFO     Train Epoch: 80 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036763; Generator: 0.028352,\n",
      "D(x): 0.577, D(G(z)): 0.415\n",
      "2019-04-10 01:20:40,663 root         INFO     Train Epoch: 80 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038669; Generator: 0.028622,\n",
      "D(x): 0.542, D(G(z)): 0.409\n",
      "2019-04-10 01:20:40,764 root         INFO     Train Epoch: 80 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037756; Generator: 0.027685,\n",
      "D(x): 0.564, D(G(z)): 0.420\n",
      "2019-04-10 01:20:40,864 root         INFO     Train Epoch: 80 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033563; Generator: 0.028367,\n",
      "D(x): 0.628, D(G(z)): 0.413\n",
      "2019-04-10 01:20:40,965 root         INFO     Train Epoch: 80 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039012; Generator: 0.026717,\n",
      "D(x): 0.558, D(G(z)): 0.436\n",
      "2019-04-10 01:20:41,065 root         INFO     Train Epoch: 80 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040664; Generator: 0.026954,\n",
      "D(x): 0.528, D(G(z)): 0.433\n",
      "2019-04-10 01:20:41,165 root         INFO     Train Epoch: 80 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036287; Generator: 0.027493,\n",
      "D(x): 0.592, D(G(z)): 0.424\n",
      "2019-04-10 01:20:41,266 root         INFO     Train Epoch: 80 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037622; Generator: 0.026790,\n",
      "D(x): 0.591, D(G(z)): 0.435\n",
      "2019-04-10 01:20:41,367 root         INFO     Train Epoch: 80 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040377; Generator: 0.029546,\n",
      "D(x): 0.504, D(G(z)): 0.396\n",
      "2019-04-10 01:20:41,468 root         INFO     Train Epoch: 80 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038293; Generator: 0.026665,\n",
      "D(x): 0.604, D(G(z)): 0.443\n",
      "2019-04-10 01:20:41,568 root         INFO     Train Epoch: 80 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.045959; Generator: 0.026799,\n",
      "D(x): 0.446, D(G(z)): 0.434\n",
      "2019-04-10 01:20:41,644 root         INFO     ====> Epoch: 80 Average loss: nan\n",
      "2019-04-10 01:20:41,670 root         INFO     Train Epoch: 81 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039635; Generator: 0.027807,\n",
      "D(x): 0.532, D(G(z)): 0.418\n",
      "2019-04-10 01:20:41,774 root         INFO     Train Epoch: 81 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037891; Generator: 0.027636,\n",
      "D(x): 0.558, D(G(z)): 0.422\n",
      "2019-04-10 01:20:41,874 root         INFO     Train Epoch: 81 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042224; Generator: 0.027356,\n",
      "D(x): 0.504, D(G(z)): 0.427\n",
      "2019-04-10 01:20:41,973 root         INFO     Train Epoch: 81 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035780; Generator: 0.028776,\n",
      "D(x): 0.596, D(G(z)): 0.406\n",
      "2019-04-10 01:20:42,071 root         INFO     Train Epoch: 81 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044154; Generator: 0.026461,\n",
      "D(x): 0.486, D(G(z)): 0.441\n",
      "2019-04-10 01:20:42,170 root         INFO     Train Epoch: 81 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038981; Generator: 0.027239,\n",
      "D(x): 0.543, D(G(z)): 0.428\n",
      "2019-04-10 01:20:42,268 root         INFO     Train Epoch: 81 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040637; Generator: 0.026672,\n",
      "D(x): 0.542, D(G(z)): 0.439\n",
      "2019-04-10 01:20:42,366 root         INFO     Train Epoch: 81 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038417; Generator: 0.027251,\n",
      "D(x): 0.564, D(G(z)): 0.428\n",
      "2019-04-10 01:20:42,464 root         INFO     Train Epoch: 81 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041526; Generator: 0.026323,\n",
      "D(x): 0.540, D(G(z)): 0.444\n",
      "2019-04-10 01:20:42,566 luigi-interface INFO     Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=7676) was stopped. Shutting down Keep-Alive thread\n",
      "\n",
      "\n",
      "-- Log file: logs2019-04-10 01:12:32.924088.txt\n",
      "\n",
      "2019-04-10 01:12:32,924 root         INFO     start\n",
      "2019-04-10 01:12:32,939 luigi        INFO     logging configured by default settings\n",
      "2019-04-10 01:12:32,959 luigi-interface DEBUG    Checking if RunAll() is complete\n",
      "2019-04-10 01:12:32,960 luigi-interface DEBUG    Checking if TrainVAE() is complete\n",
      "2019-04-10 01:12:32,961 luigi-interface DEBUG    Checking if TrainVEM() is complete\n",
      "2019-04-10 01:12:32,961 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   PENDING\n",
      "2019-04-10 01:12:32,962 luigi-interface DEBUG    Checking if MakeDataSet() is complete\n",
      "2019-04-10 01:12:32,962 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   PENDING\n",
      "2019-04-10 01:12:32,962 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   DONE\n",
      "2019-04-10 01:12:32,963 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   PENDING\n",
      "2019-04-10 01:12:32,963 luigi-interface INFO     Done scheduling tasks\n",
      "2019-04-10 01:12:32,963 luigi-interface INFO     Running Worker with 1 processes\n",
      "2019-04-10 01:12:32,963 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:12:32,964 luigi-interface DEBUG    Pending tasks: 3\n",
      "2019-04-10 01:12:32,964 luigi-interface INFO     [pid 6006] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=6006) running   TrainVAE()\n",
      "2019-04-10 01:12:32,979 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-10 01:12:32,980 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-10 01:12:36,786 root         INFO     Values of VAE's decoder parameters before training:\n",
      "2019-04-10 01:12:36,786 root         INFO     layers.0.weight\n",
      "2019-04-10 01:12:36,786 root         INFO     tensor([[-0.2651],\n",
      "        [ 0.1353]], device='cuda:0')\n",
      "2019-04-10 01:12:36,806 root         INFO     layers.0.bias\n",
      "2019-04-10 01:12:36,806 root         INFO     tensor([-0.3248, -0.5761], device='cuda:0')\n",
      "2019-04-10 01:12:36,807 root         INFO     layers.1.weight\n",
      "2019-04-10 01:12:36,807 root         INFO     tensor([[-0.0574,  0.4460],\n",
      "        [ 0.5879, -0.3491]], device='cuda:0')\n",
      "2019-04-10 01:12:36,808 root         INFO     layers.1.bias\n",
      "2019-04-10 01:12:36,808 root         INFO     tensor([-0.4054, -0.0326], device='cuda:0')\n",
      "2019-04-10 01:12:36,809 root         INFO     layers.2.weight\n",
      "2019-04-10 01:12:36,809 root         INFO     tensor([[ 0.3113,  0.3165],\n",
      "        [-0.2632,  0.2449]], device='cuda:0')\n",
      "2019-04-10 01:12:36,810 root         INFO     layers.2.bias\n",
      "2019-04-10 01:12:36,810 root         INFO     tensor([-0.1202, -0.0849], device='cuda:0')\n",
      "2019-04-10 01:12:36,873 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 1.767577\n",
      "Reconstruction: 1.697181, Regularization: 0.070396\n",
      "2019-04-10 01:12:36,928 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: 0.746119\n",
      "Reconstruction: 0.696957, Regularization: 0.049163\n",
      "2019-04-10 01:12:36,983 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: 0.583874\n",
      "Reconstruction: 0.542920, Regularization: 0.040954\n",
      "2019-04-10 01:12:37,038 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: 0.647860\n",
      "Reconstruction: 0.587962, Regularization: 0.059897\n",
      "2019-04-10 01:12:37,093 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: 0.940730\n",
      "Reconstruction: 0.892908, Regularization: 0.047821\n",
      "2019-04-10 01:12:37,148 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: 1.527953\n",
      "Reconstruction: 1.462637, Regularization: 0.065316\n",
      "2019-04-10 01:12:37,203 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: 1.132540\n",
      "Reconstruction: 1.082303, Regularization: 0.050237\n",
      "2019-04-10 01:12:37,257 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: 0.898782\n",
      "Reconstruction: 0.824768, Regularization: 0.074014\n",
      "2019-04-10 01:12:37,312 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: 0.759314\n",
      "Reconstruction: 0.702005, Regularization: 0.057309\n",
      "2019-04-10 01:12:37,367 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: 1.181170\n",
      "Reconstruction: 1.124548, Regularization: 0.056622\n",
      "2019-04-10 01:12:37,423 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: 0.807170\n",
      "Reconstruction: 0.739403, Regularization: 0.067766\n",
      "2019-04-10 01:12:37,478 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: 0.605904\n",
      "Reconstruction: 0.559386, Regularization: 0.046518\n",
      "2019-04-10 01:12:37,534 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: 0.917551\n",
      "Reconstruction: 0.866392, Regularization: 0.051159\n",
      "2019-04-10 01:12:37,590 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: 1.380796\n",
      "Reconstruction: 1.324733, Regularization: 0.056063\n",
      "2019-04-10 01:12:37,645 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: 1.143103\n",
      "Reconstruction: 1.078468, Regularization: 0.064635\n",
      "2019-04-10 01:12:37,700 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: 0.579306\n",
      "Reconstruction: 0.520910, Regularization: 0.058396\n",
      "2019-04-10 01:12:37,750 root         INFO     ====> Epoch: 0 Average loss: 1.0024\n",
      "2019-04-10 01:12:37,773 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: 0.417196\n",
      "Reconstruction: 0.362255, Regularization: 0.054941\n",
      "2019-04-10 01:12:37,828 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: 0.631288\n",
      "Reconstruction: 0.580896, Regularization: 0.050393\n",
      "2019-04-10 01:12:37,883 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: 0.708997\n",
      "Reconstruction: 0.658638, Regularization: 0.050359\n",
      "2019-04-10 01:12:37,938 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: 0.955330\n",
      "Reconstruction: 0.913773, Regularization: 0.041557\n",
      "2019-04-10 01:12:37,994 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: 0.556665\n",
      "Reconstruction: 0.503004, Regularization: 0.053661\n",
      "2019-04-10 01:12:38,049 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: 1.470325\n",
      "Reconstruction: 1.410686, Regularization: 0.059639\n",
      "2019-04-10 01:12:38,103 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: 1.341791\n",
      "Reconstruction: 1.292398, Regularization: 0.049393\n",
      "2019-04-10 01:12:38,158 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: 0.745721\n",
      "Reconstruction: 0.687097, Regularization: 0.058623\n",
      "2019-04-10 01:12:38,213 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: 1.166090\n",
      "Reconstruction: 1.119855, Regularization: 0.046235\n",
      "2019-04-10 01:12:38,268 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: 0.552731\n",
      "Reconstruction: 0.505672, Regularization: 0.047059\n",
      "2019-04-10 01:12:38,324 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: 0.610796\n",
      "Reconstruction: 0.563686, Regularization: 0.047110\n",
      "2019-04-10 01:12:38,379 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: 0.945664\n",
      "Reconstruction: 0.888585, Regularization: 0.057079\n",
      "2019-04-10 01:12:38,434 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: 1.190821\n",
      "Reconstruction: 1.141826, Regularization: 0.048994\n",
      "2019-04-10 01:12:38,489 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: 1.341901\n",
      "Reconstruction: 1.287421, Regularization: 0.054480\n",
      "2019-04-10 01:12:38,545 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: 1.030848\n",
      "Reconstruction: 0.985569, Regularization: 0.045279\n",
      "2019-04-10 01:12:38,599 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: 0.736345\n",
      "Reconstruction: 0.669158, Regularization: 0.067187\n",
      "2019-04-10 01:12:38,649 root         INFO     ====> Epoch: 1 Average loss: 0.8802\n",
      "2019-04-10 01:12:38,672 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: 0.890086\n",
      "Reconstruction: 0.849075, Regularization: 0.041011\n",
      "2019-04-10 01:12:38,729 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: 0.841894\n",
      "Reconstruction: 0.807272, Regularization: 0.034621\n",
      "2019-04-10 01:12:38,786 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: 0.460874\n",
      "Reconstruction: 0.398110, Regularization: 0.062763\n",
      "2019-04-10 01:12:38,842 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: 0.830249\n",
      "Reconstruction: 0.763036, Regularization: 0.067213\n",
      "2019-04-10 01:12:38,899 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: 0.673454\n",
      "Reconstruction: 0.627217, Regularization: 0.046237\n",
      "2019-04-10 01:12:38,955 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: 0.672915\n",
      "Reconstruction: 0.631888, Regularization: 0.041027\n",
      "2019-04-10 01:12:39,012 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: 0.547402\n",
      "Reconstruction: 0.503894, Regularization: 0.043508\n",
      "2019-04-10 01:12:39,069 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: 0.844747\n",
      "Reconstruction: 0.788008, Regularization: 0.056739\n",
      "2019-04-10 01:12:39,126 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: 0.579606\n",
      "Reconstruction: 0.531582, Regularization: 0.048024\n",
      "2019-04-10 01:12:39,182 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: 0.736618\n",
      "Reconstruction: 0.687554, Regularization: 0.049064\n",
      "2019-04-10 01:12:39,239 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: 0.904004\n",
      "Reconstruction: 0.852608, Regularization: 0.051396\n",
      "2019-04-10 01:12:39,295 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: 0.929308\n",
      "Reconstruction: 0.873278, Regularization: 0.056031\n",
      "2019-04-10 01:12:39,352 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: 0.863699\n",
      "Reconstruction: 0.818416, Regularization: 0.045283\n",
      "2019-04-10 01:12:39,409 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: 0.623508\n",
      "Reconstruction: 0.589405, Regularization: 0.034103\n",
      "2019-04-10 01:12:39,466 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: 0.357982\n",
      "Reconstruction: 0.323061, Regularization: 0.034920\n",
      "2019-04-10 01:12:39,523 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: 0.741584\n",
      "Reconstruction: 0.694596, Regularization: 0.046988\n",
      "2019-04-10 01:12:39,573 root         INFO     ====> Epoch: 2 Average loss: 0.7997\n",
      "2019-04-10 01:12:39,596 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: 0.790475\n",
      "Reconstruction: 0.749480, Regularization: 0.040995\n",
      "2019-04-10 01:12:39,653 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: 0.761480\n",
      "Reconstruction: 0.705918, Regularization: 0.055561\n",
      "2019-04-10 01:12:39,710 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: 0.564238\n",
      "Reconstruction: 0.515508, Regularization: 0.048730\n",
      "2019-04-10 01:12:39,767 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: 0.856186\n",
      "Reconstruction: 0.803247, Regularization: 0.052939\n",
      "2019-04-10 01:12:39,825 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: 0.804011\n",
      "Reconstruction: 0.749234, Regularization: 0.054776\n",
      "2019-04-10 01:12:39,882 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: 0.636356\n",
      "Reconstruction: 0.595623, Regularization: 0.040733\n",
      "2019-04-10 01:12:39,939 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: 0.743120\n",
      "Reconstruction: 0.703947, Regularization: 0.039173\n",
      "2019-04-10 01:12:39,996 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: 1.078482\n",
      "Reconstruction: 1.033571, Regularization: 0.044910\n",
      "2019-04-10 01:12:40,053 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: 0.793451\n",
      "Reconstruction: 0.735339, Regularization: 0.058112\n",
      "2019-04-10 01:12:40,111 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: 0.827096\n",
      "Reconstruction: 0.775941, Regularization: 0.051154\n",
      "2019-04-10 01:12:40,167 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: 0.795755\n",
      "Reconstruction: 0.741231, Regularization: 0.054524\n",
      "2019-04-10 01:12:40,224 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: 0.670027\n",
      "Reconstruction: 0.622846, Regularization: 0.047180\n",
      "2019-04-10 01:12:40,280 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: 0.611093\n",
      "Reconstruction: 0.571644, Regularization: 0.039449\n",
      "2019-04-10 01:12:40,337 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: 0.603915\n",
      "Reconstruction: 0.567931, Regularization: 0.035985\n",
      "2019-04-10 01:12:40,392 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: 0.777772\n",
      "Reconstruction: 0.737247, Regularization: 0.040525\n",
      "2019-04-10 01:12:40,447 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: 0.516402\n",
      "Reconstruction: 0.474680, Regularization: 0.041722\n",
      "2019-04-10 01:12:40,497 root         INFO     ====> Epoch: 3 Average loss: 0.7438\n",
      "2019-04-10 01:12:40,520 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: 0.875608\n",
      "Reconstruction: 0.830315, Regularization: 0.045293\n",
      "2019-04-10 01:12:40,578 root         INFO     Train Epoch: 4 [512/8000 (6%)]\tTotal Loss: 0.846040\n",
      "Reconstruction: 0.799684, Regularization: 0.046356\n",
      "2019-04-10 01:12:40,634 root         INFO     Train Epoch: 4 [1024/8000 (13%)]\tTotal Loss: 0.553518\n",
      "Reconstruction: 0.501731, Regularization: 0.051786\n",
      "2019-04-10 01:12:40,691 root         INFO     Train Epoch: 4 [1536/8000 (19%)]\tTotal Loss: 0.844647\n",
      "Reconstruction: 0.792830, Regularization: 0.051817\n",
      "2019-04-10 01:12:40,748 root         INFO     Train Epoch: 4 [2048/8000 (26%)]\tTotal Loss: 0.809108\n",
      "Reconstruction: 0.730708, Regularization: 0.078400\n",
      "2019-04-10 01:12:40,805 root         INFO     Train Epoch: 4 [2560/8000 (32%)]\tTotal Loss: 0.553260\n",
      "Reconstruction: 0.507340, Regularization: 0.045920\n",
      "2019-04-10 01:12:40,862 root         INFO     Train Epoch: 4 [3072/8000 (38%)]\tTotal Loss: 0.626010\n",
      "Reconstruction: 0.583272, Regularization: 0.042738\n",
      "2019-04-10 01:12:40,919 root         INFO     Train Epoch: 4 [3584/8000 (45%)]\tTotal Loss: 1.015239\n",
      "Reconstruction: 0.964197, Regularization: 0.051042\n",
      "2019-04-10 01:12:40,975 root         INFO     Train Epoch: 4 [4096/8000 (51%)]\tTotal Loss: 0.659582\n",
      "Reconstruction: 0.609782, Regularization: 0.049800\n",
      "2019-04-10 01:12:41,032 root         INFO     Train Epoch: 4 [4608/8000 (58%)]\tTotal Loss: 0.641445\n",
      "Reconstruction: 0.600715, Regularization: 0.040730\n",
      "2019-04-10 01:12:41,089 root         INFO     Train Epoch: 4 [5120/8000 (64%)]\tTotal Loss: 0.779360\n",
      "Reconstruction: 0.731686, Regularization: 0.047675\n",
      "2019-04-10 01:12:41,146 root         INFO     Train Epoch: 4 [5632/8000 (70%)]\tTotal Loss: 0.397753\n",
      "Reconstruction: 0.357174, Regularization: 0.040579\n",
      "2019-04-10 01:12:41,203 root         INFO     Train Epoch: 4 [6144/8000 (77%)]\tTotal Loss: 0.669381\n",
      "Reconstruction: 0.625645, Regularization: 0.043736\n",
      "2019-04-10 01:12:41,260 root         INFO     Train Epoch: 4 [6656/8000 (83%)]\tTotal Loss: 0.646816\n",
      "Reconstruction: 0.613260, Regularization: 0.033556\n",
      "2019-04-10 01:12:41,316 root         INFO     Train Epoch: 4 [7168/8000 (90%)]\tTotal Loss: 0.518745\n",
      "Reconstruction: 0.476965, Regularization: 0.041780\n",
      "2019-04-10 01:12:41,373 root         INFO     Train Epoch: 4 [7680/8000 (96%)]\tTotal Loss: 1.023537\n",
      "Reconstruction: 0.982266, Regularization: 0.041271\n",
      "2019-04-10 01:12:41,424 root         INFO     ====> Epoch: 4 Average loss: 0.6990\n",
      "2019-04-10 01:12:41,448 root         INFO     Train Epoch: 5 [0/8000 (0%)]\tTotal Loss: 0.475052\n",
      "Reconstruction: 0.437961, Regularization: 0.037091\n",
      "2019-04-10 01:12:41,504 root         INFO     Train Epoch: 5 [512/8000 (6%)]\tTotal Loss: 0.899374\n",
      "Reconstruction: 0.849784, Regularization: 0.049591\n",
      "2019-04-10 01:12:41,560 root         INFO     Train Epoch: 5 [1024/8000 (13%)]\tTotal Loss: 0.531789\n",
      "Reconstruction: 0.494438, Regularization: 0.037351\n",
      "2019-04-10 01:12:41,616 root         INFO     Train Epoch: 5 [1536/8000 (19%)]\tTotal Loss: 0.427233\n",
      "Reconstruction: 0.379961, Regularization: 0.047272\n",
      "2019-04-10 01:12:41,671 root         INFO     Train Epoch: 5 [2048/8000 (26%)]\tTotal Loss: 0.848055\n",
      "Reconstruction: 0.803038, Regularization: 0.045017\n",
      "2019-04-10 01:12:41,727 root         INFO     Train Epoch: 5 [2560/8000 (32%)]\tTotal Loss: 0.881006\n",
      "Reconstruction: 0.845162, Regularization: 0.035844\n",
      "2019-04-10 01:12:41,783 root         INFO     Train Epoch: 5 [3072/8000 (38%)]\tTotal Loss: 0.433704\n",
      "Reconstruction: 0.397582, Regularization: 0.036121\n",
      "2019-04-10 01:12:41,838 root         INFO     Train Epoch: 5 [3584/8000 (45%)]\tTotal Loss: 1.148864\n",
      "Reconstruction: 1.090102, Regularization: 0.058762\n",
      "2019-04-10 01:12:41,894 root         INFO     Train Epoch: 5 [4096/8000 (51%)]\tTotal Loss: 0.598123\n",
      "Reconstruction: 0.561810, Regularization: 0.036313\n",
      "2019-04-10 01:12:41,950 root         INFO     Train Epoch: 5 [4608/8000 (58%)]\tTotal Loss: 0.744742\n",
      "Reconstruction: 0.700244, Regularization: 0.044499\n",
      "2019-04-10 01:12:42,007 root         INFO     Train Epoch: 5 [5120/8000 (64%)]\tTotal Loss: 0.732405\n",
      "Reconstruction: 0.687338, Regularization: 0.045067\n",
      "2019-04-10 01:12:42,062 root         INFO     Train Epoch: 5 [5632/8000 (70%)]\tTotal Loss: 0.651228\n",
      "Reconstruction: 0.607766, Regularization: 0.043462\n",
      "2019-04-10 01:12:42,118 root         INFO     Train Epoch: 5 [6144/8000 (77%)]\tTotal Loss: 0.636665\n",
      "Reconstruction: 0.587850, Regularization: 0.048815\n",
      "2019-04-10 01:12:42,174 root         INFO     Train Epoch: 5 [6656/8000 (83%)]\tTotal Loss: 0.585982\n",
      "Reconstruction: 0.549679, Regularization: 0.036303\n",
      "2019-04-10 01:12:42,230 root         INFO     Train Epoch: 5 [7168/8000 (90%)]\tTotal Loss: 0.615971\n",
      "Reconstruction: 0.575297, Regularization: 0.040674\n",
      "2019-04-10 01:12:42,286 root         INFO     Train Epoch: 5 [7680/8000 (96%)]\tTotal Loss: 0.740694\n",
      "Reconstruction: 0.700573, Regularization: 0.040120\n",
      "2019-04-10 01:12:42,336 root         INFO     ====> Epoch: 5 Average loss: 0.6627\n",
      "2019-04-10 01:12:42,359 root         INFO     Train Epoch: 6 [0/8000 (0%)]\tTotal Loss: 0.954638\n",
      "Reconstruction: 0.919950, Regularization: 0.034688\n",
      "2019-04-10 01:12:42,416 root         INFO     Train Epoch: 6 [512/8000 (6%)]\tTotal Loss: 0.785322\n",
      "Reconstruction: 0.755629, Regularization: 0.029693\n",
      "2019-04-10 01:12:42,472 root         INFO     Train Epoch: 6 [1024/8000 (13%)]\tTotal Loss: 0.863733\n",
      "Reconstruction: 0.816197, Regularization: 0.047536\n",
      "2019-04-10 01:12:42,528 root         INFO     Train Epoch: 6 [1536/8000 (19%)]\tTotal Loss: 0.806426\n",
      "Reconstruction: 0.760291, Regularization: 0.046135\n",
      "2019-04-10 01:12:42,584 root         INFO     Train Epoch: 6 [2048/8000 (26%)]\tTotal Loss: 0.536933\n",
      "Reconstruction: 0.508520, Regularization: 0.028413\n",
      "2019-04-10 01:12:42,640 root         INFO     Train Epoch: 6 [2560/8000 (32%)]\tTotal Loss: 0.385713\n",
      "Reconstruction: 0.348147, Regularization: 0.037566\n",
      "2019-04-10 01:12:42,696 root         INFO     Train Epoch: 6 [3072/8000 (38%)]\tTotal Loss: 0.398980\n",
      "Reconstruction: 0.359544, Regularization: 0.039436\n",
      "2019-04-10 01:12:42,752 root         INFO     Train Epoch: 6 [3584/8000 (45%)]\tTotal Loss: 0.367102\n",
      "Reconstruction: 0.324109, Regularization: 0.042993\n",
      "2019-04-10 01:12:42,808 root         INFO     Train Epoch: 6 [4096/8000 (51%)]\tTotal Loss: 0.491405\n",
      "Reconstruction: 0.457650, Regularization: 0.033755\n",
      "2019-04-10 01:12:42,864 root         INFO     Train Epoch: 6 [4608/8000 (58%)]\tTotal Loss: 0.598701\n",
      "Reconstruction: 0.556526, Regularization: 0.042174\n",
      "2019-04-10 01:12:42,919 root         INFO     Train Epoch: 6 [5120/8000 (64%)]\tTotal Loss: 0.560352\n",
      "Reconstruction: 0.527373, Regularization: 0.032979\n",
      "2019-04-10 01:12:42,975 root         INFO     Train Epoch: 6 [5632/8000 (70%)]\tTotal Loss: 0.699390\n",
      "Reconstruction: 0.665602, Regularization: 0.033788\n",
      "2019-04-10 01:12:43,031 root         INFO     Train Epoch: 6 [6144/8000 (77%)]\tTotal Loss: 1.274551\n",
      "Reconstruction: 1.237391, Regularization: 0.037160\n",
      "2019-04-10 01:12:43,087 root         INFO     Train Epoch: 6 [6656/8000 (83%)]\tTotal Loss: 0.642942\n",
      "Reconstruction: 0.602486, Regularization: 0.040455\n",
      "2019-04-10 01:12:43,143 root         INFO     Train Epoch: 6 [7168/8000 (90%)]\tTotal Loss: 0.528456\n",
      "Reconstruction: 0.487554, Regularization: 0.040901\n",
      "2019-04-10 01:12:43,198 root         INFO     Train Epoch: 6 [7680/8000 (96%)]\tTotal Loss: 0.440605\n",
      "Reconstruction: 0.410230, Regularization: 0.030375\n",
      "2019-04-10 01:12:43,249 root         INFO     ====> Epoch: 6 Average loss: 0.6290\n",
      "2019-04-10 01:12:43,272 root         INFO     Train Epoch: 7 [0/8000 (0%)]\tTotal Loss: 0.566535\n",
      "Reconstruction: 0.531386, Regularization: 0.035149\n",
      "2019-04-10 01:12:43,329 root         INFO     Train Epoch: 7 [512/8000 (6%)]\tTotal Loss: 0.619906\n",
      "Reconstruction: 0.582780, Regularization: 0.037126\n",
      "2019-04-10 01:12:43,385 root         INFO     Train Epoch: 7 [1024/8000 (13%)]\tTotal Loss: 0.446296\n",
      "Reconstruction: 0.409213, Regularization: 0.037083\n",
      "2019-04-10 01:12:43,441 root         INFO     Train Epoch: 7 [1536/8000 (19%)]\tTotal Loss: 0.564808\n",
      "Reconstruction: 0.529109, Regularization: 0.035698\n",
      "2019-04-10 01:12:43,497 root         INFO     Train Epoch: 7 [2048/8000 (26%)]\tTotal Loss: 0.431268\n",
      "Reconstruction: 0.398687, Regularization: 0.032581\n",
      "2019-04-10 01:12:43,553 root         INFO     Train Epoch: 7 [2560/8000 (32%)]\tTotal Loss: 0.353713\n",
      "Reconstruction: 0.313334, Regularization: 0.040379\n",
      "2019-04-10 01:12:43,609 root         INFO     Train Epoch: 7 [3072/8000 (38%)]\tTotal Loss: 0.584729\n",
      "Reconstruction: 0.556494, Regularization: 0.028235\n",
      "2019-04-10 01:12:43,666 root         INFO     Train Epoch: 7 [3584/8000 (45%)]\tTotal Loss: 0.525445\n",
      "Reconstruction: 0.494841, Regularization: 0.030605\n",
      "2019-04-10 01:12:43,722 root         INFO     Train Epoch: 7 [4096/8000 (51%)]\tTotal Loss: 0.799467\n",
      "Reconstruction: 0.768616, Regularization: 0.030851\n",
      "2019-04-10 01:12:43,778 root         INFO     Train Epoch: 7 [4608/8000 (58%)]\tTotal Loss: 0.591041\n",
      "Reconstruction: 0.562817, Regularization: 0.028224\n",
      "2019-04-10 01:12:43,834 root         INFO     Train Epoch: 7 [5120/8000 (64%)]\tTotal Loss: 0.478443\n",
      "Reconstruction: 0.435403, Regularization: 0.043040\n",
      "2019-04-10 01:12:43,890 root         INFO     Train Epoch: 7 [5632/8000 (70%)]\tTotal Loss: 0.516251\n",
      "Reconstruction: 0.489585, Regularization: 0.026666\n",
      "2019-04-10 01:12:43,946 root         INFO     Train Epoch: 7 [6144/8000 (77%)]\tTotal Loss: 0.743362\n",
      "Reconstruction: 0.705250, Regularization: 0.038112\n",
      "2019-04-10 01:12:44,002 root         INFO     Train Epoch: 7 [6656/8000 (83%)]\tTotal Loss: 0.448528\n",
      "Reconstruction: 0.411265, Regularization: 0.037263\n",
      "2019-04-10 01:12:44,057 root         INFO     Train Epoch: 7 [7168/8000 (90%)]\tTotal Loss: 0.412754\n",
      "Reconstruction: 0.379249, Regularization: 0.033506\n",
      "2019-04-10 01:12:44,113 root         INFO     Train Epoch: 7 [7680/8000 (96%)]\tTotal Loss: 0.638337\n",
      "Reconstruction: 0.607353, Regularization: 0.030984\n",
      "2019-04-10 01:12:44,162 root         INFO     ====> Epoch: 7 Average loss: 0.5960\n",
      "2019-04-10 01:12:44,185 root         INFO     Train Epoch: 8 [0/8000 (0%)]\tTotal Loss: 0.604512\n",
      "Reconstruction: 0.570561, Regularization: 0.033951\n",
      "2019-04-10 01:12:44,242 root         INFO     Train Epoch: 8 [512/8000 (6%)]\tTotal Loss: 0.555258\n",
      "Reconstruction: 0.526594, Regularization: 0.028663\n",
      "2019-04-10 01:12:44,298 root         INFO     Train Epoch: 8 [1024/8000 (13%)]\tTotal Loss: 0.618760\n",
      "Reconstruction: 0.574755, Regularization: 0.044005\n",
      "2019-04-10 01:12:44,354 root         INFO     Train Epoch: 8 [1536/8000 (19%)]\tTotal Loss: 0.472131\n",
      "Reconstruction: 0.434896, Regularization: 0.037235\n",
      "2019-04-10 01:12:44,412 root         INFO     Train Epoch: 8 [2048/8000 (26%)]\tTotal Loss: 0.573184\n",
      "Reconstruction: 0.532125, Regularization: 0.041059\n",
      "2019-04-10 01:12:44,468 root         INFO     Train Epoch: 8 [2560/8000 (32%)]\tTotal Loss: 0.523301\n",
      "Reconstruction: 0.476108, Regularization: 0.047192\n",
      "2019-04-10 01:12:44,524 root         INFO     Train Epoch: 8 [3072/8000 (38%)]\tTotal Loss: 0.452974\n",
      "Reconstruction: 0.423094, Regularization: 0.029880\n",
      "2019-04-10 01:12:44,580 root         INFO     Train Epoch: 8 [3584/8000 (45%)]\tTotal Loss: 0.608918\n",
      "Reconstruction: 0.572989, Regularization: 0.035929\n",
      "2019-04-10 01:12:44,635 root         INFO     Train Epoch: 8 [4096/8000 (51%)]\tTotal Loss: 0.456799\n",
      "Reconstruction: 0.421699, Regularization: 0.035100\n",
      "2019-04-10 01:12:44,690 root         INFO     Train Epoch: 8 [4608/8000 (58%)]\tTotal Loss: 0.480631\n",
      "Reconstruction: 0.441676, Regularization: 0.038955\n",
      "2019-04-10 01:12:44,745 root         INFO     Train Epoch: 8 [5120/8000 (64%)]\tTotal Loss: 0.479415\n",
      "Reconstruction: 0.450404, Regularization: 0.029011\n",
      "2019-04-10 01:12:44,800 root         INFO     Train Epoch: 8 [5632/8000 (70%)]\tTotal Loss: 0.555084\n",
      "Reconstruction: 0.524678, Regularization: 0.030407\n",
      "2019-04-10 01:12:44,855 root         INFO     Train Epoch: 8 [6144/8000 (77%)]\tTotal Loss: 0.477156\n",
      "Reconstruction: 0.411045, Regularization: 0.066110\n",
      "2019-04-10 01:12:44,910 root         INFO     Train Epoch: 8 [6656/8000 (83%)]\tTotal Loss: 0.403327\n",
      "Reconstruction: 0.351711, Regularization: 0.051616\n",
      "2019-04-10 01:12:44,965 root         INFO     Train Epoch: 8 [7168/8000 (90%)]\tTotal Loss: 0.665997\n",
      "Reconstruction: 0.618918, Regularization: 0.047079\n",
      "2019-04-10 01:12:45,020 root         INFO     Train Epoch: 8 [7680/8000 (96%)]\tTotal Loss: 0.462691\n",
      "Reconstruction: 0.431237, Regularization: 0.031454\n",
      "2019-04-10 01:12:45,070 root         INFO     ====> Epoch: 8 Average loss: 0.5582\n",
      "2019-04-10 01:12:45,093 root         INFO     Train Epoch: 9 [0/8000 (0%)]\tTotal Loss: 0.725629\n",
      "Reconstruction: 0.688137, Regularization: 0.037492\n",
      "2019-04-10 01:12:45,150 root         INFO     Train Epoch: 9 [512/8000 (6%)]\tTotal Loss: 0.409325\n",
      "Reconstruction: 0.373775, Regularization: 0.035550\n",
      "2019-04-10 01:12:45,207 root         INFO     Train Epoch: 9 [1024/8000 (13%)]\tTotal Loss: 0.349819\n",
      "Reconstruction: 0.307610, Regularization: 0.042209\n",
      "2019-04-10 01:12:45,263 root         INFO     Train Epoch: 9 [1536/8000 (19%)]\tTotal Loss: 0.562334\n",
      "Reconstruction: 0.529368, Regularization: 0.032966\n",
      "2019-04-10 01:12:45,320 root         INFO     Train Epoch: 9 [2048/8000 (26%)]\tTotal Loss: 0.453029\n",
      "Reconstruction: 0.407633, Regularization: 0.045395\n",
      "2019-04-10 01:12:45,376 root         INFO     Train Epoch: 9 [2560/8000 (32%)]\tTotal Loss: 0.781494\n",
      "Reconstruction: 0.739135, Regularization: 0.042359\n",
      "2019-04-10 01:12:45,432 root         INFO     Train Epoch: 9 [3072/8000 (38%)]\tTotal Loss: 0.422134\n",
      "Reconstruction: 0.387795, Regularization: 0.034339\n",
      "2019-04-10 01:12:45,486 root         INFO     Train Epoch: 9 [3584/8000 (45%)]\tTotal Loss: 0.561893\n",
      "Reconstruction: 0.526828, Regularization: 0.035065\n",
      "2019-04-10 01:12:45,540 root         INFO     Train Epoch: 9 [4096/8000 (51%)]\tTotal Loss: 0.365588\n",
      "Reconstruction: 0.333270, Regularization: 0.032319\n",
      "2019-04-10 01:12:45,596 root         INFO     Train Epoch: 9 [4608/8000 (58%)]\tTotal Loss: 0.456302\n",
      "Reconstruction: 0.418969, Regularization: 0.037332\n",
      "2019-04-10 01:12:45,650 root         INFO     Train Epoch: 9 [5120/8000 (64%)]\tTotal Loss: 0.436987\n",
      "Reconstruction: 0.402462, Regularization: 0.034525\n",
      "2019-04-10 01:12:45,705 root         INFO     Train Epoch: 9 [5632/8000 (70%)]\tTotal Loss: 0.362639\n",
      "Reconstruction: 0.332349, Regularization: 0.030290\n",
      "2019-04-10 01:12:45,760 root         INFO     Train Epoch: 9 [6144/8000 (77%)]\tTotal Loss: 0.610225\n",
      "Reconstruction: 0.564322, Regularization: 0.045903\n",
      "2019-04-10 01:12:45,815 root         INFO     Train Epoch: 9 [6656/8000 (83%)]\tTotal Loss: 0.534829\n",
      "Reconstruction: 0.494423, Regularization: 0.040406\n",
      "2019-04-10 01:12:45,869 root         INFO     Train Epoch: 9 [7168/8000 (90%)]\tTotal Loss: 0.417864\n",
      "Reconstruction: 0.381928, Regularization: 0.035936\n",
      "2019-04-10 01:12:45,924 root         INFO     Train Epoch: 9 [7680/8000 (96%)]\tTotal Loss: 0.382170\n",
      "Reconstruction: 0.332249, Regularization: 0.049921\n",
      "2019-04-10 01:12:45,973 root         INFO     ====> Epoch: 9 Average loss: 0.5080\n",
      "2019-04-10 01:12:45,997 root         INFO     Train Epoch: 10 [0/8000 (0%)]\tTotal Loss: 0.431101\n",
      "Reconstruction: 0.399719, Regularization: 0.031382\n",
      "2019-04-10 01:12:46,054 root         INFO     Train Epoch: 10 [512/8000 (6%)]\tTotal Loss: 0.347526\n",
      "Reconstruction: 0.316111, Regularization: 0.031415\n",
      "2019-04-10 01:12:46,111 root         INFO     Train Epoch: 10 [1024/8000 (13%)]\tTotal Loss: 0.673003\n",
      "Reconstruction: 0.635629, Regularization: 0.037374\n",
      "2019-04-10 01:12:46,168 root         INFO     Train Epoch: 10 [1536/8000 (19%)]\tTotal Loss: 0.559585\n",
      "Reconstruction: 0.531070, Regularization: 0.028514\n",
      "2019-04-10 01:12:46,225 root         INFO     Train Epoch: 10 [2048/8000 (26%)]\tTotal Loss: 0.591579\n",
      "Reconstruction: 0.552350, Regularization: 0.039229\n",
      "2019-04-10 01:12:46,282 root         INFO     Train Epoch: 10 [2560/8000 (32%)]\tTotal Loss: 0.340178\n",
      "Reconstruction: 0.306668, Regularization: 0.033509\n",
      "2019-04-10 01:12:46,339 root         INFO     Train Epoch: 10 [3072/8000 (38%)]\tTotal Loss: 0.352424\n",
      "Reconstruction: 0.322491, Regularization: 0.029934\n",
      "2019-04-10 01:12:46,396 root         INFO     Train Epoch: 10 [3584/8000 (45%)]\tTotal Loss: 0.374973\n",
      "Reconstruction: 0.330997, Regularization: 0.043975\n",
      "2019-04-10 01:12:46,451 root         INFO     Train Epoch: 10 [4096/8000 (51%)]\tTotal Loss: 0.268202\n",
      "Reconstruction: 0.222589, Regularization: 0.045612\n",
      "2019-04-10 01:12:46,507 root         INFO     Train Epoch: 10 [4608/8000 (58%)]\tTotal Loss: 0.410976\n",
      "Reconstruction: 0.355582, Regularization: 0.055394\n",
      "2019-04-10 01:12:46,564 root         INFO     Train Epoch: 10 [5120/8000 (64%)]\tTotal Loss: 0.543171\n",
      "Reconstruction: 0.504533, Regularization: 0.038638\n",
      "2019-04-10 01:12:46,621 root         INFO     Train Epoch: 10 [5632/8000 (70%)]\tTotal Loss: 0.326911\n",
      "Reconstruction: 0.288450, Regularization: 0.038461\n",
      "2019-04-10 01:12:46,676 root         INFO     Train Epoch: 10 [6144/8000 (77%)]\tTotal Loss: 0.356436\n",
      "Reconstruction: 0.317054, Regularization: 0.039382\n",
      "2019-04-10 01:12:46,731 root         INFO     Train Epoch: 10 [6656/8000 (83%)]\tTotal Loss: 0.301198\n",
      "Reconstruction: 0.254235, Regularization: 0.046964\n",
      "2019-04-10 01:12:46,787 root         INFO     Train Epoch: 10 [7168/8000 (90%)]\tTotal Loss: 0.289870\n",
      "Reconstruction: 0.252381, Regularization: 0.037489\n",
      "2019-04-10 01:12:46,844 root         INFO     Train Epoch: 10 [7680/8000 (96%)]\tTotal Loss: 0.337003\n",
      "Reconstruction: 0.292205, Regularization: 0.044798\n",
      "2019-04-10 01:12:46,895 root         INFO     ====> Epoch: 10 Average loss: 0.4499\n",
      "2019-04-10 01:12:46,918 root         INFO     Train Epoch: 11 [0/8000 (0%)]\tTotal Loss: 0.355571\n",
      "Reconstruction: 0.327910, Regularization: 0.027661\n",
      "2019-04-10 01:12:46,973 root         INFO     Train Epoch: 11 [512/8000 (6%)]\tTotal Loss: 0.289604\n",
      "Reconstruction: 0.242728, Regularization: 0.046876\n",
      "2019-04-10 01:12:47,027 root         INFO     Train Epoch: 11 [1024/8000 (13%)]\tTotal Loss: 0.507702\n",
      "Reconstruction: 0.462179, Regularization: 0.045523\n",
      "2019-04-10 01:12:47,081 root         INFO     Train Epoch: 11 [1536/8000 (19%)]\tTotal Loss: 0.527865\n",
      "Reconstruction: 0.489259, Regularization: 0.038606\n",
      "2019-04-10 01:12:47,135 root         INFO     Train Epoch: 11 [2048/8000 (26%)]\tTotal Loss: 0.466413\n",
      "Reconstruction: 0.424375, Regularization: 0.042038\n",
      "2019-04-10 01:12:47,189 root         INFO     Train Epoch: 11 [2560/8000 (32%)]\tTotal Loss: 0.438101\n",
      "Reconstruction: 0.401158, Regularization: 0.036943\n",
      "2019-04-10 01:12:47,243 root         INFO     Train Epoch: 11 [3072/8000 (38%)]\tTotal Loss: 0.434894\n",
      "Reconstruction: 0.383737, Regularization: 0.051157\n",
      "2019-04-10 01:12:47,296 root         INFO     Train Epoch: 11 [3584/8000 (45%)]\tTotal Loss: 0.347673\n",
      "Reconstruction: 0.305943, Regularization: 0.041730\n",
      "2019-04-10 01:12:47,350 root         INFO     Train Epoch: 11 [4096/8000 (51%)]\tTotal Loss: 0.380443\n",
      "Reconstruction: 0.340799, Regularization: 0.039643\n",
      "2019-04-10 01:12:47,404 root         INFO     Train Epoch: 11 [4608/8000 (58%)]\tTotal Loss: 0.446535\n",
      "Reconstruction: 0.410848, Regularization: 0.035687\n",
      "2019-04-10 01:12:47,457 root         INFO     Train Epoch: 11 [5120/8000 (64%)]\tTotal Loss: 0.344315\n",
      "Reconstruction: 0.306831, Regularization: 0.037484\n",
      "2019-04-10 01:12:47,511 root         INFO     Train Epoch: 11 [5632/8000 (70%)]\tTotal Loss: 0.278448\n",
      "Reconstruction: 0.242743, Regularization: 0.035705\n",
      "2019-04-10 01:12:47,564 root         INFO     Train Epoch: 11 [6144/8000 (77%)]\tTotal Loss: 0.371603\n",
      "Reconstruction: 0.326315, Regularization: 0.045289\n",
      "2019-04-10 01:12:47,618 root         INFO     Train Epoch: 11 [6656/8000 (83%)]\tTotal Loss: 0.510171\n",
      "Reconstruction: 0.470300, Regularization: 0.039871\n",
      "2019-04-10 01:12:47,672 root         INFO     Train Epoch: 11 [7168/8000 (90%)]\tTotal Loss: 0.266355\n",
      "Reconstruction: 0.222830, Regularization: 0.043526\n",
      "2019-04-10 01:12:47,725 root         INFO     Train Epoch: 11 [7680/8000 (96%)]\tTotal Loss: 0.349023\n",
      "Reconstruction: 0.302313, Regularization: 0.046710\n",
      "2019-04-10 01:12:47,774 root         INFO     ====> Epoch: 11 Average loss: 0.3888\n",
      "2019-04-10 01:12:47,797 root         INFO     Train Epoch: 12 [0/8000 (0%)]\tTotal Loss: 0.264684\n",
      "Reconstruction: 0.223619, Regularization: 0.041066\n",
      "2019-04-10 01:12:47,853 root         INFO     Train Epoch: 12 [512/8000 (6%)]\tTotal Loss: 0.312309\n",
      "Reconstruction: 0.263614, Regularization: 0.048695\n",
      "2019-04-10 01:12:47,909 root         INFO     Train Epoch: 12 [1024/8000 (13%)]\tTotal Loss: 0.318504\n",
      "Reconstruction: 0.282086, Regularization: 0.036419\n",
      "2019-04-10 01:12:47,963 root         INFO     Train Epoch: 12 [1536/8000 (19%)]\tTotal Loss: 0.366968\n",
      "Reconstruction: 0.313999, Regularization: 0.052969\n",
      "2019-04-10 01:12:48,018 root         INFO     Train Epoch: 12 [2048/8000 (26%)]\tTotal Loss: 0.470509\n",
      "Reconstruction: 0.423065, Regularization: 0.047444\n",
      "2019-04-10 01:12:48,073 root         INFO     Train Epoch: 12 [2560/8000 (32%)]\tTotal Loss: 0.311338\n",
      "Reconstruction: 0.269596, Regularization: 0.041742\n",
      "2019-04-10 01:12:48,128 root         INFO     Train Epoch: 12 [3072/8000 (38%)]\tTotal Loss: 0.287704\n",
      "Reconstruction: 0.240590, Regularization: 0.047114\n",
      "2019-04-10 01:12:48,182 root         INFO     Train Epoch: 12 [3584/8000 (45%)]\tTotal Loss: 0.394844\n",
      "Reconstruction: 0.353502, Regularization: 0.041342\n",
      "2019-04-10 01:12:48,238 root         INFO     Train Epoch: 12 [4096/8000 (51%)]\tTotal Loss: 0.295625\n",
      "Reconstruction: 0.244855, Regularization: 0.050769\n",
      "2019-04-10 01:12:48,293 root         INFO     Train Epoch: 12 [4608/8000 (58%)]\tTotal Loss: 0.412065\n",
      "Reconstruction: 0.370820, Regularization: 0.041245\n",
      "2019-04-10 01:12:48,347 root         INFO     Train Epoch: 12 [5120/8000 (64%)]\tTotal Loss: 0.258741\n",
      "Reconstruction: 0.218650, Regularization: 0.040091\n",
      "2019-04-10 01:12:48,402 root         INFO     Train Epoch: 12 [5632/8000 (70%)]\tTotal Loss: 0.323426\n",
      "Reconstruction: 0.285516, Regularization: 0.037911\n",
      "2019-04-10 01:12:48,457 root         INFO     Train Epoch: 12 [6144/8000 (77%)]\tTotal Loss: 0.433334\n",
      "Reconstruction: 0.386653, Regularization: 0.046680\n",
      "2019-04-10 01:12:48,512 root         INFO     Train Epoch: 12 [6656/8000 (83%)]\tTotal Loss: 0.310073\n",
      "Reconstruction: 0.264645, Regularization: 0.045428\n",
      "2019-04-10 01:12:48,567 root         INFO     Train Epoch: 12 [7168/8000 (90%)]\tTotal Loss: 0.324934\n",
      "Reconstruction: 0.288574, Regularization: 0.036360\n",
      "2019-04-10 01:12:48,622 root         INFO     Train Epoch: 12 [7680/8000 (96%)]\tTotal Loss: 0.428311\n",
      "Reconstruction: 0.380544, Regularization: 0.047767\n",
      "2019-04-10 01:12:48,671 root         INFO     ====> Epoch: 12 Average loss: 0.3397\n",
      "2019-04-10 01:12:48,694 root         INFO     Train Epoch: 13 [0/8000 (0%)]\tTotal Loss: 0.273632\n",
      "Reconstruction: 0.235696, Regularization: 0.037936\n",
      "2019-04-10 01:12:48,750 root         INFO     Train Epoch: 13 [512/8000 (6%)]\tTotal Loss: 0.291585\n",
      "Reconstruction: 0.246126, Regularization: 0.045459\n",
      "2019-04-10 01:12:48,806 root         INFO     Train Epoch: 13 [1024/8000 (13%)]\tTotal Loss: 0.366329\n",
      "Reconstruction: 0.319505, Regularization: 0.046824\n",
      "2019-04-10 01:12:48,862 root         INFO     Train Epoch: 13 [1536/8000 (19%)]\tTotal Loss: 0.339512\n",
      "Reconstruction: 0.290764, Regularization: 0.048747\n",
      "2019-04-10 01:12:48,918 root         INFO     Train Epoch: 13 [2048/8000 (26%)]\tTotal Loss: 0.308357\n",
      "Reconstruction: 0.260357, Regularization: 0.048000\n",
      "2019-04-10 01:12:48,974 root         INFO     Train Epoch: 13 [2560/8000 (32%)]\tTotal Loss: 0.272633\n",
      "Reconstruction: 0.231730, Regularization: 0.040903\n",
      "2019-04-10 01:12:49,030 root         INFO     Train Epoch: 13 [3072/8000 (38%)]\tTotal Loss: 0.210135\n",
      "Reconstruction: 0.173827, Regularization: 0.036308\n",
      "2019-04-10 01:12:49,087 root         INFO     Train Epoch: 13 [3584/8000 (45%)]\tTotal Loss: 0.275387\n",
      "Reconstruction: 0.229372, Regularization: 0.046015\n",
      "2019-04-10 01:12:49,144 root         INFO     Train Epoch: 13 [4096/8000 (51%)]\tTotal Loss: 0.283341\n",
      "Reconstruction: 0.233661, Regularization: 0.049679\n",
      "2019-04-10 01:12:49,201 root         INFO     Train Epoch: 13 [4608/8000 (58%)]\tTotal Loss: 0.355189\n",
      "Reconstruction: 0.314416, Regularization: 0.040774\n",
      "2019-04-10 01:12:49,258 root         INFO     Train Epoch: 13 [5120/8000 (64%)]\tTotal Loss: 0.249420\n",
      "Reconstruction: 0.207805, Regularization: 0.041615\n",
      "2019-04-10 01:12:49,315 root         INFO     Train Epoch: 13 [5632/8000 (70%)]\tTotal Loss: 0.280537\n",
      "Reconstruction: 0.235771, Regularization: 0.044766\n",
      "2019-04-10 01:12:49,371 root         INFO     Train Epoch: 13 [6144/8000 (77%)]\tTotal Loss: 0.238708\n",
      "Reconstruction: 0.195925, Regularization: 0.042783\n",
      "2019-04-10 01:12:49,427 root         INFO     Train Epoch: 13 [6656/8000 (83%)]\tTotal Loss: 0.281922\n",
      "Reconstruction: 0.229480, Regularization: 0.052442\n",
      "2019-04-10 01:12:49,483 root         INFO     Train Epoch: 13 [7168/8000 (90%)]\tTotal Loss: 0.316808\n",
      "Reconstruction: 0.262332, Regularization: 0.054477\n",
      "2019-04-10 01:12:49,539 root         INFO     Train Epoch: 13 [7680/8000 (96%)]\tTotal Loss: 0.292745\n",
      "Reconstruction: 0.248953, Regularization: 0.043792\n",
      "2019-04-10 01:12:49,589 root         INFO     ====> Epoch: 13 Average loss: 0.3036\n",
      "2019-04-10 01:12:49,612 root         INFO     Train Epoch: 14 [0/8000 (0%)]\tTotal Loss: 0.258592\n",
      "Reconstruction: 0.208170, Regularization: 0.050422\n",
      "2019-04-10 01:12:49,670 root         INFO     Train Epoch: 14 [512/8000 (6%)]\tTotal Loss: 0.256888\n",
      "Reconstruction: 0.213519, Regularization: 0.043369\n",
      "2019-04-10 01:12:49,727 root         INFO     Train Epoch: 14 [1024/8000 (13%)]\tTotal Loss: 0.208176\n",
      "Reconstruction: 0.169121, Regularization: 0.039056\n",
      "2019-04-10 01:12:49,783 root         INFO     Train Epoch: 14 [1536/8000 (19%)]\tTotal Loss: 0.257024\n",
      "Reconstruction: 0.210874, Regularization: 0.046151\n",
      "2019-04-10 01:12:49,840 root         INFO     Train Epoch: 14 [2048/8000 (26%)]\tTotal Loss: 0.223999\n",
      "Reconstruction: 0.180798, Regularization: 0.043201\n",
      "2019-04-10 01:12:49,897 root         INFO     Train Epoch: 14 [2560/8000 (32%)]\tTotal Loss: 0.225029\n",
      "Reconstruction: 0.177454, Regularization: 0.047575\n",
      "2019-04-10 01:12:49,954 root         INFO     Train Epoch: 14 [3072/8000 (38%)]\tTotal Loss: 0.311317\n",
      "Reconstruction: 0.264565, Regularization: 0.046752\n",
      "2019-04-10 01:12:50,011 root         INFO     Train Epoch: 14 [3584/8000 (45%)]\tTotal Loss: 0.297297\n",
      "Reconstruction: 0.243190, Regularization: 0.054107\n",
      "2019-04-10 01:12:50,067 root         INFO     Train Epoch: 14 [4096/8000 (51%)]\tTotal Loss: 0.303398\n",
      "Reconstruction: 0.256393, Regularization: 0.047005\n",
      "2019-04-10 01:12:50,124 root         INFO     Train Epoch: 14 [4608/8000 (58%)]\tTotal Loss: 0.230026\n",
      "Reconstruction: 0.184531, Regularization: 0.045495\n",
      "2019-04-10 01:12:50,179 root         INFO     Train Epoch: 14 [5120/8000 (64%)]\tTotal Loss: 0.196638\n",
      "Reconstruction: 0.154267, Regularization: 0.042372\n",
      "2019-04-10 01:12:50,234 root         INFO     Train Epoch: 14 [5632/8000 (70%)]\tTotal Loss: 0.203763\n",
      "Reconstruction: 0.160195, Regularization: 0.043568\n",
      "2019-04-10 01:12:50,290 root         INFO     Train Epoch: 14 [6144/8000 (77%)]\tTotal Loss: 0.246819\n",
      "Reconstruction: 0.206897, Regularization: 0.039922\n",
      "2019-04-10 01:12:50,346 root         INFO     Train Epoch: 14 [6656/8000 (83%)]\tTotal Loss: 0.305792\n",
      "Reconstruction: 0.256407, Regularization: 0.049385\n",
      "2019-04-10 01:12:50,402 root         INFO     Train Epoch: 14 [7168/8000 (90%)]\tTotal Loss: 0.243222\n",
      "Reconstruction: 0.200681, Regularization: 0.042541\n",
      "2019-04-10 01:12:50,457 root         INFO     Train Epoch: 14 [7680/8000 (96%)]\tTotal Loss: 0.264221\n",
      "Reconstruction: 0.218774, Regularization: 0.045447\n",
      "2019-04-10 01:12:50,507 root         INFO     ====> Epoch: 14 Average loss: 0.2743\n",
      "2019-04-10 01:12:50,530 root         INFO     Train Epoch: 15 [0/8000 (0%)]\tTotal Loss: 0.236735\n",
      "Reconstruction: 0.197773, Regularization: 0.038962\n",
      "2019-04-10 01:12:50,587 root         INFO     Train Epoch: 15 [512/8000 (6%)]\tTotal Loss: 0.251313\n",
      "Reconstruction: 0.198820, Regularization: 0.052493\n",
      "2019-04-10 01:12:50,643 root         INFO     Train Epoch: 15 [1024/8000 (13%)]\tTotal Loss: 0.290788\n",
      "Reconstruction: 0.245127, Regularization: 0.045660\n",
      "2019-04-10 01:12:50,699 root         INFO     Train Epoch: 15 [1536/8000 (19%)]\tTotal Loss: 0.241186\n",
      "Reconstruction: 0.195120, Regularization: 0.046067\n",
      "2019-04-10 01:12:50,756 root         INFO     Train Epoch: 15 [2048/8000 (26%)]\tTotal Loss: 0.222007\n",
      "Reconstruction: 0.175892, Regularization: 0.046115\n",
      "2019-04-10 01:12:50,812 root         INFO     Train Epoch: 15 [2560/8000 (32%)]\tTotal Loss: 0.282599\n",
      "Reconstruction: 0.232601, Regularization: 0.049998\n",
      "2019-04-10 01:12:50,868 root         INFO     Train Epoch: 15 [3072/8000 (38%)]\tTotal Loss: 0.265303\n",
      "Reconstruction: 0.219541, Regularization: 0.045762\n",
      "2019-04-10 01:12:50,925 root         INFO     Train Epoch: 15 [3584/8000 (45%)]\tTotal Loss: 0.254243\n",
      "Reconstruction: 0.209038, Regularization: 0.045205\n",
      "2019-04-10 01:12:50,980 root         INFO     Train Epoch: 15 [4096/8000 (51%)]\tTotal Loss: 0.204441\n",
      "Reconstruction: 0.157785, Regularization: 0.046655\n",
      "2019-04-10 01:12:51,037 root         INFO     Train Epoch: 15 [4608/8000 (58%)]\tTotal Loss: 0.216815\n",
      "Reconstruction: 0.177344, Regularization: 0.039470\n",
      "2019-04-10 01:12:51,093 root         INFO     Train Epoch: 15 [5120/8000 (64%)]\tTotal Loss: 0.263797\n",
      "Reconstruction: 0.214125, Regularization: 0.049672\n",
      "2019-04-10 01:12:51,150 root         INFO     Train Epoch: 15 [5632/8000 (70%)]\tTotal Loss: 0.291840\n",
      "Reconstruction: 0.249520, Regularization: 0.042321\n",
      "2019-04-10 01:12:51,206 root         INFO     Train Epoch: 15 [6144/8000 (77%)]\tTotal Loss: 0.194808\n",
      "Reconstruction: 0.153758, Regularization: 0.041050\n",
      "2019-04-10 01:12:51,262 root         INFO     Train Epoch: 15 [6656/8000 (83%)]\tTotal Loss: 0.239715\n",
      "Reconstruction: 0.192826, Regularization: 0.046889\n",
      "2019-04-10 01:12:51,318 root         INFO     Train Epoch: 15 [7168/8000 (90%)]\tTotal Loss: 0.222975\n",
      "Reconstruction: 0.178810, Regularization: 0.044165\n",
      "2019-04-10 01:12:51,374 root         INFO     Train Epoch: 15 [7680/8000 (96%)]\tTotal Loss: 0.285109\n",
      "Reconstruction: 0.232900, Regularization: 0.052208\n",
      "2019-04-10 01:12:51,424 root         INFO     ====> Epoch: 15 Average loss: 0.2545\n",
      "2019-04-10 01:12:51,447 root         INFO     Train Epoch: 16 [0/8000 (0%)]\tTotal Loss: 0.266765\n",
      "Reconstruction: 0.215357, Regularization: 0.051408\n",
      "2019-04-10 01:12:51,505 root         INFO     Train Epoch: 16 [512/8000 (6%)]\tTotal Loss: 0.256797\n",
      "Reconstruction: 0.212919, Regularization: 0.043878\n",
      "2019-04-10 01:12:51,561 root         INFO     Train Epoch: 16 [1024/8000 (13%)]\tTotal Loss: 0.264499\n",
      "Reconstruction: 0.214973, Regularization: 0.049526\n",
      "2019-04-10 01:12:51,618 root         INFO     Train Epoch: 16 [1536/8000 (19%)]\tTotal Loss: 0.250670\n",
      "Reconstruction: 0.195174, Regularization: 0.055496\n",
      "2019-04-10 01:12:51,675 root         INFO     Train Epoch: 16 [2048/8000 (26%)]\tTotal Loss: 0.205113\n",
      "Reconstruction: 0.157278, Regularization: 0.047835\n",
      "2019-04-10 01:12:51,732 root         INFO     Train Epoch: 16 [2560/8000 (32%)]\tTotal Loss: 0.189488\n",
      "Reconstruction: 0.149721, Regularization: 0.039767\n",
      "2019-04-10 01:12:51,789 root         INFO     Train Epoch: 16 [3072/8000 (38%)]\tTotal Loss: 0.270668\n",
      "Reconstruction: 0.217892, Regularization: 0.052777\n",
      "2019-04-10 01:12:51,845 root         INFO     Train Epoch: 16 [3584/8000 (45%)]\tTotal Loss: 0.245792\n",
      "Reconstruction: 0.190982, Regularization: 0.054810\n",
      "2019-04-10 01:12:51,901 root         INFO     Train Epoch: 16 [4096/8000 (51%)]\tTotal Loss: 0.278131\n",
      "Reconstruction: 0.220669, Regularization: 0.057462\n",
      "2019-04-10 01:12:51,957 root         INFO     Train Epoch: 16 [4608/8000 (58%)]\tTotal Loss: 0.189951\n",
      "Reconstruction: 0.149966, Regularization: 0.039984\n",
      "2019-04-10 01:12:52,012 root         INFO     Train Epoch: 16 [5120/8000 (64%)]\tTotal Loss: 0.224585\n",
      "Reconstruction: 0.174975, Regularization: 0.049609\n",
      "2019-04-10 01:12:52,068 root         INFO     Train Epoch: 16 [5632/8000 (70%)]\tTotal Loss: 0.221596\n",
      "Reconstruction: 0.174230, Regularization: 0.047365\n",
      "2019-04-10 01:12:52,124 root         INFO     Train Epoch: 16 [6144/8000 (77%)]\tTotal Loss: 0.276409\n",
      "Reconstruction: 0.220089, Regularization: 0.056321\n",
      "2019-04-10 01:12:52,179 root         INFO     Train Epoch: 16 [6656/8000 (83%)]\tTotal Loss: 0.260901\n",
      "Reconstruction: 0.206836, Regularization: 0.054065\n",
      "2019-04-10 01:12:52,235 root         INFO     Train Epoch: 16 [7168/8000 (90%)]\tTotal Loss: 0.240671\n",
      "Reconstruction: 0.188471, Regularization: 0.052200\n",
      "2019-04-10 01:12:52,291 root         INFO     Train Epoch: 16 [7680/8000 (96%)]\tTotal Loss: 0.226209\n",
      "Reconstruction: 0.180940, Regularization: 0.045269\n",
      "2019-04-10 01:12:52,341 root         INFO     ====> Epoch: 16 Average loss: 0.2400\n",
      "2019-04-10 01:12:52,365 root         INFO     Train Epoch: 17 [0/8000 (0%)]\tTotal Loss: 0.248266\n",
      "Reconstruction: 0.197553, Regularization: 0.050713\n",
      "2019-04-10 01:12:52,420 root         INFO     Train Epoch: 17 [512/8000 (6%)]\tTotal Loss: 0.258835\n",
      "Reconstruction: 0.206793, Regularization: 0.052043\n",
      "2019-04-10 01:12:52,475 root         INFO     Train Epoch: 17 [1024/8000 (13%)]\tTotal Loss: 0.283405\n",
      "Reconstruction: 0.224146, Regularization: 0.059259\n",
      "2019-04-10 01:12:52,529 root         INFO     Train Epoch: 17 [1536/8000 (19%)]\tTotal Loss: 0.214462\n",
      "Reconstruction: 0.160441, Regularization: 0.054021\n",
      "2019-04-10 01:12:52,583 root         INFO     Train Epoch: 17 [2048/8000 (26%)]\tTotal Loss: 0.212398\n",
      "Reconstruction: 0.162709, Regularization: 0.049689\n",
      "2019-04-10 01:12:52,637 root         INFO     Train Epoch: 17 [2560/8000 (32%)]\tTotal Loss: 0.241017\n",
      "Reconstruction: 0.190916, Regularization: 0.050101\n",
      "2019-04-10 01:12:52,693 root         INFO     Train Epoch: 17 [3072/8000 (38%)]\tTotal Loss: 0.210223\n",
      "Reconstruction: 0.160433, Regularization: 0.049790\n",
      "2019-04-10 01:12:52,748 root         INFO     Train Epoch: 17 [3584/8000 (45%)]\tTotal Loss: 0.209251\n",
      "Reconstruction: 0.159295, Regularization: 0.049956\n",
      "2019-04-10 01:12:52,802 root         INFO     Train Epoch: 17 [4096/8000 (51%)]\tTotal Loss: 0.223397\n",
      "Reconstruction: 0.173967, Regularization: 0.049429\n",
      "2019-04-10 01:12:52,856 root         INFO     Train Epoch: 17 [4608/8000 (58%)]\tTotal Loss: 0.217147\n",
      "Reconstruction: 0.171704, Regularization: 0.045443\n",
      "2019-04-10 01:12:52,911 root         INFO     Train Epoch: 17 [5120/8000 (64%)]\tTotal Loss: 0.221817\n",
      "Reconstruction: 0.168554, Regularization: 0.053262\n",
      "2019-04-10 01:12:52,966 root         INFO     Train Epoch: 17 [5632/8000 (70%)]\tTotal Loss: 0.221178\n",
      "Reconstruction: 0.165413, Regularization: 0.055766\n",
      "2019-04-10 01:12:53,020 root         INFO     Train Epoch: 17 [6144/8000 (77%)]\tTotal Loss: 0.217760\n",
      "Reconstruction: 0.164933, Regularization: 0.052827\n",
      "2019-04-10 01:12:53,075 root         INFO     Train Epoch: 17 [6656/8000 (83%)]\tTotal Loss: 0.205437\n",
      "Reconstruction: 0.153856, Regularization: 0.051581\n",
      "2019-04-10 01:12:53,129 root         INFO     Train Epoch: 17 [7168/8000 (90%)]\tTotal Loss: 0.231904\n",
      "Reconstruction: 0.181875, Regularization: 0.050029\n",
      "2019-04-10 01:12:53,184 root         INFO     Train Epoch: 17 [7680/8000 (96%)]\tTotal Loss: 0.203227\n",
      "Reconstruction: 0.152503, Regularization: 0.050724\n",
      "2019-04-10 01:12:53,234 root         INFO     ====> Epoch: 17 Average loss: 0.2298\n",
      "2019-04-10 01:12:53,257 root         INFO     Train Epoch: 18 [0/8000 (0%)]\tTotal Loss: 0.190637\n",
      "Reconstruction: 0.143865, Regularization: 0.046773\n",
      "2019-04-10 01:12:53,313 root         INFO     Train Epoch: 18 [512/8000 (6%)]\tTotal Loss: 0.222021\n",
      "Reconstruction: 0.174516, Regularization: 0.047504\n",
      "2019-04-10 01:12:53,368 root         INFO     Train Epoch: 18 [1024/8000 (13%)]\tTotal Loss: 0.235281\n",
      "Reconstruction: 0.185075, Regularization: 0.050207\n",
      "2019-04-10 01:12:53,424 root         INFO     Train Epoch: 18 [1536/8000 (19%)]\tTotal Loss: 0.193658\n",
      "Reconstruction: 0.147830, Regularization: 0.045829\n",
      "2019-04-10 01:12:53,479 root         INFO     Train Epoch: 18 [2048/8000 (26%)]\tTotal Loss: 0.219438\n",
      "Reconstruction: 0.164905, Regularization: 0.054533\n",
      "2019-04-10 01:12:53,535 root         INFO     Train Epoch: 18 [2560/8000 (32%)]\tTotal Loss: 0.234426\n",
      "Reconstruction: 0.178591, Regularization: 0.055835\n",
      "2019-04-10 01:12:53,591 root         INFO     Train Epoch: 18 [3072/8000 (38%)]\tTotal Loss: 0.215675\n",
      "Reconstruction: 0.169695, Regularization: 0.045980\n",
      "2019-04-10 01:12:53,645 root         INFO     Train Epoch: 18 [3584/8000 (45%)]\tTotal Loss: 0.197677\n",
      "Reconstruction: 0.153189, Regularization: 0.044488\n",
      "2019-04-10 01:12:53,701 root         INFO     Train Epoch: 18 [4096/8000 (51%)]\tTotal Loss: 0.231307\n",
      "Reconstruction: 0.178610, Regularization: 0.052697\n",
      "2019-04-10 01:12:53,755 root         INFO     Train Epoch: 18 [4608/8000 (58%)]\tTotal Loss: 0.188689\n",
      "Reconstruction: 0.147640, Regularization: 0.041049\n",
      "2019-04-10 01:12:53,810 root         INFO     Train Epoch: 18 [5120/8000 (64%)]\tTotal Loss: 0.189339\n",
      "Reconstruction: 0.143907, Regularization: 0.045431\n",
      "2019-04-10 01:12:53,865 root         INFO     Train Epoch: 18 [5632/8000 (70%)]\tTotal Loss: 0.218943\n",
      "Reconstruction: 0.168730, Regularization: 0.050212\n",
      "2019-04-10 01:12:53,920 root         INFO     Train Epoch: 18 [6144/8000 (77%)]\tTotal Loss: 0.205499\n",
      "Reconstruction: 0.155932, Regularization: 0.049567\n",
      "2019-04-10 01:12:53,975 root         INFO     Train Epoch: 18 [6656/8000 (83%)]\tTotal Loss: 0.213398\n",
      "Reconstruction: 0.168457, Regularization: 0.044941\n",
      "2019-04-10 01:12:54,030 root         INFO     Train Epoch: 18 [7168/8000 (90%)]\tTotal Loss: 0.185411\n",
      "Reconstruction: 0.141068, Regularization: 0.044343\n",
      "2019-04-10 01:12:54,084 root         INFO     Train Epoch: 18 [7680/8000 (96%)]\tTotal Loss: 0.199285\n",
      "Reconstruction: 0.150379, Regularization: 0.048906\n",
      "2019-04-10 01:12:54,134 root         INFO     ====> Epoch: 18 Average loss: 0.2218\n",
      "2019-04-10 01:12:54,157 root         INFO     Train Epoch: 19 [0/8000 (0%)]\tTotal Loss: 0.194580\n",
      "Reconstruction: 0.148256, Regularization: 0.046324\n",
      "2019-04-10 01:12:54,214 root         INFO     Train Epoch: 19 [512/8000 (6%)]\tTotal Loss: 0.220733\n",
      "Reconstruction: 0.168451, Regularization: 0.052282\n",
      "2019-04-10 01:12:54,271 root         INFO     Train Epoch: 19 [1024/8000 (13%)]\tTotal Loss: 0.217809\n",
      "Reconstruction: 0.168375, Regularization: 0.049435\n",
      "2019-04-10 01:12:54,327 root         INFO     Train Epoch: 19 [1536/8000 (19%)]\tTotal Loss: 0.214273\n",
      "Reconstruction: 0.166245, Regularization: 0.048028\n",
      "2019-04-10 01:12:54,384 root         INFO     Train Epoch: 19 [2048/8000 (26%)]\tTotal Loss: 0.258228\n",
      "Reconstruction: 0.195022, Regularization: 0.063206\n",
      "2019-04-10 01:12:54,440 root         INFO     Train Epoch: 19 [2560/8000 (32%)]\tTotal Loss: 0.239085\n",
      "Reconstruction: 0.178825, Regularization: 0.060260\n",
      "2019-04-10 01:12:54,496 root         INFO     Train Epoch: 19 [3072/8000 (38%)]\tTotal Loss: 0.222294\n",
      "Reconstruction: 0.166465, Regularization: 0.055828\n",
      "2019-04-10 01:12:54,552 root         INFO     Train Epoch: 19 [3584/8000 (45%)]\tTotal Loss: 0.253317\n",
      "Reconstruction: 0.194144, Regularization: 0.059173\n",
      "2019-04-10 01:12:54,606 root         INFO     Train Epoch: 19 [4096/8000 (51%)]\tTotal Loss: 0.243670\n",
      "Reconstruction: 0.183429, Regularization: 0.060241\n",
      "2019-04-10 01:12:54,660 root         INFO     Train Epoch: 19 [4608/8000 (58%)]\tTotal Loss: 0.210553\n",
      "Reconstruction: 0.158047, Regularization: 0.052506\n",
      "2019-04-10 01:12:54,714 root         INFO     Train Epoch: 19 [5120/8000 (64%)]\tTotal Loss: 0.215681\n",
      "Reconstruction: 0.165272, Regularization: 0.050409\n",
      "2019-04-10 01:12:54,767 root         INFO     Train Epoch: 19 [5632/8000 (70%)]\tTotal Loss: 0.206004\n",
      "Reconstruction: 0.157803, Regularization: 0.048201\n",
      "2019-04-10 01:12:54,821 root         INFO     Train Epoch: 19 [6144/8000 (77%)]\tTotal Loss: 0.189111\n",
      "Reconstruction: 0.142357, Regularization: 0.046754\n",
      "2019-04-10 01:12:54,876 root         INFO     Train Epoch: 19 [6656/8000 (83%)]\tTotal Loss: 0.209737\n",
      "Reconstruction: 0.160137, Regularization: 0.049600\n",
      "2019-04-10 01:12:54,932 root         INFO     Train Epoch: 19 [7168/8000 (90%)]\tTotal Loss: 0.199795\n",
      "Reconstruction: 0.152185, Regularization: 0.047610\n",
      "2019-04-10 01:12:54,988 root         INFO     Train Epoch: 19 [7680/8000 (96%)]\tTotal Loss: 0.227947\n",
      "Reconstruction: 0.173901, Regularization: 0.054046\n",
      "2019-04-10 01:12:55,037 root         INFO     ====> Epoch: 19 Average loss: 0.2156\n",
      "2019-04-10 01:12:55,060 root         INFO     Train Epoch: 20 [0/8000 (0%)]\tTotal Loss: 0.209962\n",
      "Reconstruction: 0.158234, Regularization: 0.051728\n",
      "2019-04-10 01:12:55,117 root         INFO     Train Epoch: 20 [512/8000 (6%)]\tTotal Loss: 0.210312\n",
      "Reconstruction: 0.160219, Regularization: 0.050093\n",
      "2019-04-10 01:12:55,174 root         INFO     Train Epoch: 20 [1024/8000 (13%)]\tTotal Loss: 0.207678\n",
      "Reconstruction: 0.161817, Regularization: 0.045861\n",
      "2019-04-10 01:12:55,230 root         INFO     Train Epoch: 20 [1536/8000 (19%)]\tTotal Loss: 0.200142\n",
      "Reconstruction: 0.151448, Regularization: 0.048693\n",
      "2019-04-10 01:12:55,286 root         INFO     Train Epoch: 20 [2048/8000 (26%)]\tTotal Loss: 0.222821\n",
      "Reconstruction: 0.166961, Regularization: 0.055859\n",
      "2019-04-10 01:12:55,342 root         INFO     Train Epoch: 20 [2560/8000 (32%)]\tTotal Loss: 0.236582\n",
      "Reconstruction: 0.175283, Regularization: 0.061299\n",
      "2019-04-10 01:12:55,398 root         INFO     Train Epoch: 20 [3072/8000 (38%)]\tTotal Loss: 0.220294\n",
      "Reconstruction: 0.163508, Regularization: 0.056787\n",
      "2019-04-10 01:12:55,454 root         INFO     Train Epoch: 20 [3584/8000 (45%)]\tTotal Loss: 0.222128\n",
      "Reconstruction: 0.168759, Regularization: 0.053369\n",
      "2019-04-10 01:12:55,511 root         INFO     Train Epoch: 20 [4096/8000 (51%)]\tTotal Loss: 0.212986\n",
      "Reconstruction: 0.157138, Regularization: 0.055847\n",
      "2019-04-10 01:12:55,567 root         INFO     Train Epoch: 20 [4608/8000 (58%)]\tTotal Loss: 0.200494\n",
      "Reconstruction: 0.155165, Regularization: 0.045329\n",
      "2019-04-10 01:12:55,623 root         INFO     Train Epoch: 20 [5120/8000 (64%)]\tTotal Loss: 0.222761\n",
      "Reconstruction: 0.167829, Regularization: 0.054932\n",
      "2019-04-10 01:12:55,680 root         INFO     Train Epoch: 20 [5632/8000 (70%)]\tTotal Loss: 0.205706\n",
      "Reconstruction: 0.159253, Regularization: 0.046454\n",
      "2019-04-10 01:12:55,736 root         INFO     Train Epoch: 20 [6144/8000 (77%)]\tTotal Loss: 0.239946\n",
      "Reconstruction: 0.182235, Regularization: 0.057711\n",
      "2019-04-10 01:12:55,792 root         INFO     Train Epoch: 20 [6656/8000 (83%)]\tTotal Loss: 0.223731\n",
      "Reconstruction: 0.163593, Regularization: 0.060139\n",
      "2019-04-10 01:12:55,849 root         INFO     Train Epoch: 20 [7168/8000 (90%)]\tTotal Loss: 0.195358\n",
      "Reconstruction: 0.149688, Regularization: 0.045670\n",
      "2019-04-10 01:12:55,905 root         INFO     Train Epoch: 20 [7680/8000 (96%)]\tTotal Loss: 0.193635\n",
      "Reconstruction: 0.147718, Regularization: 0.045917\n",
      "2019-04-10 01:12:55,954 root         INFO     ====> Epoch: 20 Average loss: 0.2117\n",
      "2019-04-10 01:12:55,978 root         INFO     Train Epoch: 21 [0/8000 (0%)]\tTotal Loss: 0.203217\n",
      "Reconstruction: 0.153679, Regularization: 0.049538\n",
      "2019-04-10 01:12:56,035 root         INFO     Train Epoch: 21 [512/8000 (6%)]\tTotal Loss: 0.218965\n",
      "Reconstruction: 0.163574, Regularization: 0.055391\n",
      "2019-04-10 01:12:56,092 root         INFO     Train Epoch: 21 [1024/8000 (13%)]\tTotal Loss: 0.247568\n",
      "Reconstruction: 0.192749, Regularization: 0.054819\n",
      "2019-04-10 01:12:56,150 root         INFO     Train Epoch: 21 [1536/8000 (19%)]\tTotal Loss: 0.223044\n",
      "Reconstruction: 0.166010, Regularization: 0.057034\n",
      "2019-04-10 01:12:56,207 root         INFO     Train Epoch: 21 [2048/8000 (26%)]\tTotal Loss: 0.210423\n",
      "Reconstruction: 0.160874, Regularization: 0.049550\n",
      "2019-04-10 01:12:56,264 root         INFO     Train Epoch: 21 [2560/8000 (32%)]\tTotal Loss: 0.188754\n",
      "Reconstruction: 0.142961, Regularization: 0.045792\n",
      "2019-04-10 01:12:56,321 root         INFO     Train Epoch: 21 [3072/8000 (38%)]\tTotal Loss: 0.202177\n",
      "Reconstruction: 0.149998, Regularization: 0.052179\n",
      "2019-04-10 01:12:56,377 root         INFO     Train Epoch: 21 [3584/8000 (45%)]\tTotal Loss: 0.217767\n",
      "Reconstruction: 0.165983, Regularization: 0.051784\n",
      "2019-04-10 01:12:56,434 root         INFO     Train Epoch: 21 [4096/8000 (51%)]\tTotal Loss: 0.229462\n",
      "Reconstruction: 0.171628, Regularization: 0.057834\n",
      "2019-04-10 01:12:56,491 root         INFO     Train Epoch: 21 [4608/8000 (58%)]\tTotal Loss: 0.178949\n",
      "Reconstruction: 0.136307, Regularization: 0.042642\n",
      "2019-04-10 01:12:56,548 root         INFO     Train Epoch: 21 [5120/8000 (64%)]\tTotal Loss: 0.196279\n",
      "Reconstruction: 0.147416, Regularization: 0.048864\n",
      "2019-04-10 01:12:56,605 root         INFO     Train Epoch: 21 [5632/8000 (70%)]\tTotal Loss: 0.214468\n",
      "Reconstruction: 0.162240, Regularization: 0.052228\n",
      "2019-04-10 01:12:56,663 root         INFO     Train Epoch: 21 [6144/8000 (77%)]\tTotal Loss: 0.187110\n",
      "Reconstruction: 0.139683, Regularization: 0.047428\n",
      "2019-04-10 01:12:56,721 root         INFO     Train Epoch: 21 [6656/8000 (83%)]\tTotal Loss: 0.210537\n",
      "Reconstruction: 0.159522, Regularization: 0.051015\n",
      "2019-04-10 01:12:56,778 root         INFO     Train Epoch: 21 [7168/8000 (90%)]\tTotal Loss: 0.224342\n",
      "Reconstruction: 0.170188, Regularization: 0.054154\n",
      "2019-04-10 01:12:56,836 root         INFO     Train Epoch: 21 [7680/8000 (96%)]\tTotal Loss: 0.216734\n",
      "Reconstruction: 0.161007, Regularization: 0.055727\n",
      "2019-04-10 01:12:56,886 root         INFO     ====> Epoch: 21 Average loss: 0.2082\n",
      "2019-04-10 01:12:56,909 root         INFO     Train Epoch: 22 [0/8000 (0%)]\tTotal Loss: 0.176276\n",
      "Reconstruction: 0.133745, Regularization: 0.042530\n",
      "2019-04-10 01:12:56,967 root         INFO     Train Epoch: 22 [512/8000 (6%)]\tTotal Loss: 0.195332\n",
      "Reconstruction: 0.151853, Regularization: 0.043479\n",
      "2019-04-10 01:12:57,023 root         INFO     Train Epoch: 22 [1024/8000 (13%)]\tTotal Loss: 0.197721\n",
      "Reconstruction: 0.151143, Regularization: 0.046578\n",
      "2019-04-10 01:12:57,080 root         INFO     Train Epoch: 22 [1536/8000 (19%)]\tTotal Loss: 0.183844\n",
      "Reconstruction: 0.138348, Regularization: 0.045496\n",
      "2019-04-10 01:12:57,137 root         INFO     Train Epoch: 22 [2048/8000 (26%)]\tTotal Loss: 0.207493\n",
      "Reconstruction: 0.155577, Regularization: 0.051917\n",
      "2019-04-10 01:12:57,193 root         INFO     Train Epoch: 22 [2560/8000 (32%)]\tTotal Loss: 0.214170\n",
      "Reconstruction: 0.159870, Regularization: 0.054299\n",
      "2019-04-10 01:12:57,250 root         INFO     Train Epoch: 22 [3072/8000 (38%)]\tTotal Loss: 0.211706\n",
      "Reconstruction: 0.156812, Regularization: 0.054894\n",
      "2019-04-10 01:12:57,307 root         INFO     Train Epoch: 22 [3584/8000 (45%)]\tTotal Loss: 0.239192\n",
      "Reconstruction: 0.176575, Regularization: 0.062617\n",
      "2019-04-10 01:12:57,363 root         INFO     Train Epoch: 22 [4096/8000 (51%)]\tTotal Loss: 0.221466\n",
      "Reconstruction: 0.162194, Regularization: 0.059272\n",
      "2019-04-10 01:12:57,420 root         INFO     Train Epoch: 22 [4608/8000 (58%)]\tTotal Loss: 0.208311\n",
      "Reconstruction: 0.155874, Regularization: 0.052437\n",
      "2019-04-10 01:12:57,477 root         INFO     Train Epoch: 22 [5120/8000 (64%)]\tTotal Loss: 0.182440\n",
      "Reconstruction: 0.141657, Regularization: 0.040783\n",
      "2019-04-10 01:12:57,534 root         INFO     Train Epoch: 22 [5632/8000 (70%)]\tTotal Loss: 0.196188\n",
      "Reconstruction: 0.145609, Regularization: 0.050579\n",
      "2019-04-10 01:12:57,591 root         INFO     Train Epoch: 22 [6144/8000 (77%)]\tTotal Loss: 0.166713\n",
      "Reconstruction: 0.129764, Regularization: 0.036949\n",
      "2019-04-10 01:12:57,648 root         INFO     Train Epoch: 22 [6656/8000 (83%)]\tTotal Loss: 0.225679\n",
      "Reconstruction: 0.159865, Regularization: 0.065815\n",
      "2019-04-10 01:12:57,705 root         INFO     Train Epoch: 22 [7168/8000 (90%)]\tTotal Loss: 0.183692\n",
      "Reconstruction: 0.138359, Regularization: 0.045333\n",
      "2019-04-10 01:12:57,762 root         INFO     Train Epoch: 22 [7680/8000 (96%)]\tTotal Loss: 0.212234\n",
      "Reconstruction: 0.157695, Regularization: 0.054539\n",
      "2019-04-10 01:12:57,812 root         INFO     ====> Epoch: 22 Average loss: 0.2047\n",
      "2019-04-10 01:12:57,835 root         INFO     Train Epoch: 23 [0/8000 (0%)]\tTotal Loss: 0.215415\n",
      "Reconstruction: 0.157588, Regularization: 0.057827\n",
      "2019-04-10 01:12:57,893 root         INFO     Train Epoch: 23 [512/8000 (6%)]\tTotal Loss: 0.204933\n",
      "Reconstruction: 0.154899, Regularization: 0.050034\n",
      "2019-04-10 01:12:57,950 root         INFO     Train Epoch: 23 [1024/8000 (13%)]\tTotal Loss: 0.206841\n",
      "Reconstruction: 0.155537, Regularization: 0.051304\n",
      "2019-04-10 01:12:58,008 root         INFO     Train Epoch: 23 [1536/8000 (19%)]\tTotal Loss: 0.192199\n",
      "Reconstruction: 0.145254, Regularization: 0.046946\n",
      "2019-04-10 01:12:58,065 root         INFO     Train Epoch: 23 [2048/8000 (26%)]\tTotal Loss: 0.212489\n",
      "Reconstruction: 0.159633, Regularization: 0.052856\n",
      "2019-04-10 01:12:58,121 root         INFO     Train Epoch: 23 [2560/8000 (32%)]\tTotal Loss: 0.221380\n",
      "Reconstruction: 0.166379, Regularization: 0.055000\n",
      "2019-04-10 01:12:58,178 root         INFO     Train Epoch: 23 [3072/8000 (38%)]\tTotal Loss: 0.210618\n",
      "Reconstruction: 0.154880, Regularization: 0.055738\n",
      "2019-04-10 01:12:58,234 root         INFO     Train Epoch: 23 [3584/8000 (45%)]\tTotal Loss: 0.207554\n",
      "Reconstruction: 0.157264, Regularization: 0.050290\n",
      "2019-04-10 01:12:58,291 root         INFO     Train Epoch: 23 [4096/8000 (51%)]\tTotal Loss: 0.209361\n",
      "Reconstruction: 0.160900, Regularization: 0.048461\n",
      "2019-04-10 01:12:58,348 root         INFO     Train Epoch: 23 [4608/8000 (58%)]\tTotal Loss: 0.207505\n",
      "Reconstruction: 0.157992, Regularization: 0.049513\n",
      "2019-04-10 01:12:58,406 root         INFO     Train Epoch: 23 [5120/8000 (64%)]\tTotal Loss: 0.178365\n",
      "Reconstruction: 0.137974, Regularization: 0.040392\n",
      "2019-04-10 01:12:58,463 root         INFO     Train Epoch: 23 [5632/8000 (70%)]\tTotal Loss: 0.204068\n",
      "Reconstruction: 0.150613, Regularization: 0.053455\n",
      "2019-04-10 01:12:58,520 root         INFO     Train Epoch: 23 [6144/8000 (77%)]\tTotal Loss: 0.236769\n",
      "Reconstruction: 0.178866, Regularization: 0.057903\n",
      "2019-04-10 01:12:58,577 root         INFO     Train Epoch: 23 [6656/8000 (83%)]\tTotal Loss: 0.182960\n",
      "Reconstruction: 0.135320, Regularization: 0.047640\n",
      "2019-04-10 01:12:58,634 root         INFO     Train Epoch: 23 [7168/8000 (90%)]\tTotal Loss: 0.205705\n",
      "Reconstruction: 0.153589, Regularization: 0.052116\n",
      "2019-04-10 01:12:58,691 root         INFO     Train Epoch: 23 [7680/8000 (96%)]\tTotal Loss: 0.202022\n",
      "Reconstruction: 0.151281, Regularization: 0.050741\n",
      "2019-04-10 01:12:58,741 root         INFO     ====> Epoch: 23 Average loss: 0.2025\n",
      "2019-04-10 01:12:58,764 root         INFO     Train Epoch: 24 [0/8000 (0%)]\tTotal Loss: 0.200863\n",
      "Reconstruction: 0.146985, Regularization: 0.053877\n",
      "2019-04-10 01:12:58,822 root         INFO     Train Epoch: 24 [512/8000 (6%)]\tTotal Loss: 0.221123\n",
      "Reconstruction: 0.166790, Regularization: 0.054334\n",
      "2019-04-10 01:12:58,879 root         INFO     Train Epoch: 24 [1024/8000 (13%)]\tTotal Loss: 0.205050\n",
      "Reconstruction: 0.151290, Regularization: 0.053760\n",
      "2019-04-10 01:12:58,936 root         INFO     Train Epoch: 24 [1536/8000 (19%)]\tTotal Loss: 0.235575\n",
      "Reconstruction: 0.167176, Regularization: 0.068399\n",
      "2019-04-10 01:12:58,991 root         INFO     Train Epoch: 24 [2048/8000 (26%)]\tTotal Loss: 0.202969\n",
      "Reconstruction: 0.151224, Regularization: 0.051745\n",
      "2019-04-10 01:12:59,047 root         INFO     Train Epoch: 24 [2560/8000 (32%)]\tTotal Loss: 0.204352\n",
      "Reconstruction: 0.151476, Regularization: 0.052876\n",
      "2019-04-10 01:12:59,102 root         INFO     Train Epoch: 24 [3072/8000 (38%)]\tTotal Loss: 0.203773\n",
      "Reconstruction: 0.152980, Regularization: 0.050793\n",
      "2019-04-10 01:12:59,156 root         INFO     Train Epoch: 24 [3584/8000 (45%)]\tTotal Loss: 0.191320\n",
      "Reconstruction: 0.146009, Regularization: 0.045312\n",
      "2019-04-10 01:12:59,211 root         INFO     Train Epoch: 24 [4096/8000 (51%)]\tTotal Loss: 0.186963\n",
      "Reconstruction: 0.141800, Regularization: 0.045163\n",
      "2019-04-10 01:12:59,265 root         INFO     Train Epoch: 24 [4608/8000 (58%)]\tTotal Loss: 0.201267\n",
      "Reconstruction: 0.153115, Regularization: 0.048152\n",
      "2019-04-10 01:12:59,321 root         INFO     Train Epoch: 24 [5120/8000 (64%)]\tTotal Loss: 0.209996\n",
      "Reconstruction: 0.158217, Regularization: 0.051779\n",
      "2019-04-10 01:12:59,378 root         INFO     Train Epoch: 24 [5632/8000 (70%)]\tTotal Loss: 0.213338\n",
      "Reconstruction: 0.165171, Regularization: 0.048168\n",
      "2019-04-10 01:12:59,433 root         INFO     Train Epoch: 24 [6144/8000 (77%)]\tTotal Loss: 0.200713\n",
      "Reconstruction: 0.150822, Regularization: 0.049891\n",
      "2019-04-10 01:12:59,488 root         INFO     Train Epoch: 24 [6656/8000 (83%)]\tTotal Loss: 0.194812\n",
      "Reconstruction: 0.151419, Regularization: 0.043393\n",
      "2019-04-10 01:12:59,543 root         INFO     Train Epoch: 24 [7168/8000 (90%)]\tTotal Loss: 0.212946\n",
      "Reconstruction: 0.161187, Regularization: 0.051758\n",
      "2019-04-10 01:12:59,598 root         INFO     Train Epoch: 24 [7680/8000 (96%)]\tTotal Loss: 0.194136\n",
      "Reconstruction: 0.149999, Regularization: 0.044137\n",
      "2019-04-10 01:12:59,649 root         INFO     ====> Epoch: 24 Average loss: 0.2007\n",
      "2019-04-10 01:12:59,672 root         INFO     Train Epoch: 25 [0/8000 (0%)]\tTotal Loss: 0.189888\n",
      "Reconstruction: 0.146506, Regularization: 0.043382\n",
      "2019-04-10 01:12:59,729 root         INFO     Train Epoch: 25 [512/8000 (6%)]\tTotal Loss: 0.201298\n",
      "Reconstruction: 0.152202, Regularization: 0.049096\n",
      "2019-04-10 01:12:59,786 root         INFO     Train Epoch: 25 [1024/8000 (13%)]\tTotal Loss: 0.194990\n",
      "Reconstruction: 0.149237, Regularization: 0.045753\n",
      "2019-04-10 01:12:59,843 root         INFO     Train Epoch: 25 [1536/8000 (19%)]\tTotal Loss: 0.200079\n",
      "Reconstruction: 0.151265, Regularization: 0.048814\n",
      "2019-04-10 01:12:59,900 root         INFO     Train Epoch: 25 [2048/8000 (26%)]\tTotal Loss: 0.193655\n",
      "Reconstruction: 0.149703, Regularization: 0.043952\n",
      "2019-04-10 01:12:59,955 root         INFO     Train Epoch: 25 [2560/8000 (32%)]\tTotal Loss: 0.208413\n",
      "Reconstruction: 0.158940, Regularization: 0.049473\n",
      "2019-04-10 01:13:00,010 root         INFO     Train Epoch: 25 [3072/8000 (38%)]\tTotal Loss: 0.198219\n",
      "Reconstruction: 0.145468, Regularization: 0.052751\n",
      "2019-04-10 01:13:00,066 root         INFO     Train Epoch: 25 [3584/8000 (45%)]\tTotal Loss: 0.185650\n",
      "Reconstruction: 0.138894, Regularization: 0.046756\n",
      "2019-04-10 01:13:00,121 root         INFO     Train Epoch: 25 [4096/8000 (51%)]\tTotal Loss: 0.198458\n",
      "Reconstruction: 0.153970, Regularization: 0.044488\n",
      "2019-04-10 01:13:00,175 root         INFO     Train Epoch: 25 [4608/8000 (58%)]\tTotal Loss: 0.188539\n",
      "Reconstruction: 0.143237, Regularization: 0.045302\n",
      "2019-04-10 01:13:00,230 root         INFO     Train Epoch: 25 [5120/8000 (64%)]\tTotal Loss: 0.215898\n",
      "Reconstruction: 0.165633, Regularization: 0.050265\n",
      "2019-04-10 01:13:00,287 root         INFO     Train Epoch: 25 [5632/8000 (70%)]\tTotal Loss: 0.211640\n",
      "Reconstruction: 0.157608, Regularization: 0.054033\n",
      "2019-04-10 01:13:00,343 root         INFO     Train Epoch: 25 [6144/8000 (77%)]\tTotal Loss: 0.197596\n",
      "Reconstruction: 0.148711, Regularization: 0.048885\n",
      "2019-04-10 01:13:00,398 root         INFO     Train Epoch: 25 [6656/8000 (83%)]\tTotal Loss: 0.200820\n",
      "Reconstruction: 0.147664, Regularization: 0.053156\n",
      "2019-04-10 01:13:00,455 root         INFO     Train Epoch: 25 [7168/8000 (90%)]\tTotal Loss: 0.204097\n",
      "Reconstruction: 0.161165, Regularization: 0.042932\n",
      "2019-04-10 01:13:00,510 root         INFO     Train Epoch: 25 [7680/8000 (96%)]\tTotal Loss: 0.198787\n",
      "Reconstruction: 0.151041, Regularization: 0.047746\n",
      "2019-04-10 01:13:00,561 root         INFO     ====> Epoch: 25 Average loss: 0.1994\n",
      "2019-04-10 01:13:00,584 root         INFO     Train Epoch: 26 [0/8000 (0%)]\tTotal Loss: 0.212367\n",
      "Reconstruction: 0.162566, Regularization: 0.049801\n",
      "2019-04-10 01:13:00,641 root         INFO     Train Epoch: 26 [512/8000 (6%)]\tTotal Loss: 0.195908\n",
      "Reconstruction: 0.150952, Regularization: 0.044957\n",
      "2019-04-10 01:13:00,698 root         INFO     Train Epoch: 26 [1024/8000 (13%)]\tTotal Loss: 0.207120\n",
      "Reconstruction: 0.154999, Regularization: 0.052121\n",
      "2019-04-10 01:13:00,755 root         INFO     Train Epoch: 26 [1536/8000 (19%)]\tTotal Loss: 0.172434\n",
      "Reconstruction: 0.136568, Regularization: 0.035866\n",
      "2019-04-10 01:13:00,812 root         INFO     Train Epoch: 26 [2048/8000 (26%)]\tTotal Loss: 0.189702\n",
      "Reconstruction: 0.146811, Regularization: 0.042892\n",
      "2019-04-10 01:13:00,869 root         INFO     Train Epoch: 26 [2560/8000 (32%)]\tTotal Loss: 0.187051\n",
      "Reconstruction: 0.147254, Regularization: 0.039797\n",
      "2019-04-10 01:13:00,926 root         INFO     Train Epoch: 26 [3072/8000 (38%)]\tTotal Loss: 0.195312\n",
      "Reconstruction: 0.146250, Regularization: 0.049062\n",
      "2019-04-10 01:13:00,982 root         INFO     Train Epoch: 26 [3584/8000 (45%)]\tTotal Loss: 0.198574\n",
      "Reconstruction: 0.155640, Regularization: 0.042934\n",
      "2019-04-10 01:13:01,038 root         INFO     Train Epoch: 26 [4096/8000 (51%)]\tTotal Loss: 0.177399\n",
      "Reconstruction: 0.133811, Regularization: 0.043588\n",
      "2019-04-10 01:13:01,094 root         INFO     Train Epoch: 26 [4608/8000 (58%)]\tTotal Loss: 0.191032\n",
      "Reconstruction: 0.142775, Regularization: 0.048257\n",
      "2019-04-10 01:13:01,150 root         INFO     Train Epoch: 26 [5120/8000 (64%)]\tTotal Loss: 0.172277\n",
      "Reconstruction: 0.132585, Regularization: 0.039693\n",
      "2019-04-10 01:13:01,206 root         INFO     Train Epoch: 26 [5632/8000 (70%)]\tTotal Loss: 0.194394\n",
      "Reconstruction: 0.150514, Regularization: 0.043880\n",
      "2019-04-10 01:13:01,262 root         INFO     Train Epoch: 26 [6144/8000 (77%)]\tTotal Loss: 0.198408\n",
      "Reconstruction: 0.150705, Regularization: 0.047703\n",
      "2019-04-10 01:13:01,318 root         INFO     Train Epoch: 26 [6656/8000 (83%)]\tTotal Loss: 0.177403\n",
      "Reconstruction: 0.141684, Regularization: 0.035719\n",
      "2019-04-10 01:13:01,374 root         INFO     Train Epoch: 26 [7168/8000 (90%)]\tTotal Loss: 0.192559\n",
      "Reconstruction: 0.148757, Regularization: 0.043802\n",
      "2019-04-10 01:13:01,430 root         INFO     Train Epoch: 26 [7680/8000 (96%)]\tTotal Loss: 0.219512\n",
      "Reconstruction: 0.160739, Regularization: 0.058773\n",
      "2019-04-10 01:13:01,480 root         INFO     ====> Epoch: 26 Average loss: 0.1971\n",
      "2019-04-10 01:13:01,503 root         INFO     Train Epoch: 27 [0/8000 (0%)]\tTotal Loss: 0.195542\n",
      "Reconstruction: 0.149225, Regularization: 0.046317\n",
      "2019-04-10 01:13:01,560 root         INFO     Train Epoch: 27 [512/8000 (6%)]\tTotal Loss: 0.206657\n",
      "Reconstruction: 0.152709, Regularization: 0.053948\n",
      "2019-04-10 01:13:01,616 root         INFO     Train Epoch: 27 [1024/8000 (13%)]\tTotal Loss: 0.206645\n",
      "Reconstruction: 0.161543, Regularization: 0.045102\n",
      "2019-04-10 01:13:01,673 root         INFO     Train Epoch: 27 [1536/8000 (19%)]\tTotal Loss: 0.198894\n",
      "Reconstruction: 0.149002, Regularization: 0.049892\n",
      "2019-04-10 01:13:01,729 root         INFO     Train Epoch: 27 [2048/8000 (26%)]\tTotal Loss: 0.203495\n",
      "Reconstruction: 0.154790, Regularization: 0.048705\n",
      "2019-04-10 01:13:01,786 root         INFO     Train Epoch: 27 [2560/8000 (32%)]\tTotal Loss: 0.185777\n",
      "Reconstruction: 0.146285, Regularization: 0.039493\n",
      "2019-04-10 01:13:01,843 root         INFO     Train Epoch: 27 [3072/8000 (38%)]\tTotal Loss: 0.192460\n",
      "Reconstruction: 0.146031, Regularization: 0.046429\n",
      "2019-04-10 01:13:01,899 root         INFO     Train Epoch: 27 [3584/8000 (45%)]\tTotal Loss: 0.202806\n",
      "Reconstruction: 0.153095, Regularization: 0.049711\n",
      "2019-04-10 01:13:01,955 root         INFO     Train Epoch: 27 [4096/8000 (51%)]\tTotal Loss: 0.204657\n",
      "Reconstruction: 0.157755, Regularization: 0.046903\n",
      "2019-04-10 01:13:02,011 root         INFO     Train Epoch: 27 [4608/8000 (58%)]\tTotal Loss: 0.195606\n",
      "Reconstruction: 0.148910, Regularization: 0.046696\n",
      "2019-04-10 01:13:02,067 root         INFO     Train Epoch: 27 [5120/8000 (64%)]\tTotal Loss: 0.181253\n",
      "Reconstruction: 0.141644, Regularization: 0.039609\n",
      "2019-04-10 01:13:02,123 root         INFO     Train Epoch: 27 [5632/8000 (70%)]\tTotal Loss: 0.213532\n",
      "Reconstruction: 0.158653, Regularization: 0.054879\n",
      "2019-04-10 01:13:02,178 root         INFO     Train Epoch: 27 [6144/8000 (77%)]\tTotal Loss: 0.209028\n",
      "Reconstruction: 0.155830, Regularization: 0.053198\n",
      "2019-04-10 01:13:02,234 root         INFO     Train Epoch: 27 [6656/8000 (83%)]\tTotal Loss: 0.174713\n",
      "Reconstruction: 0.135640, Regularization: 0.039073\n",
      "2019-04-10 01:13:02,291 root         INFO     Train Epoch: 27 [7168/8000 (90%)]\tTotal Loss: 0.184281\n",
      "Reconstruction: 0.141741, Regularization: 0.042540\n",
      "2019-04-10 01:13:02,348 root         INFO     Train Epoch: 27 [7680/8000 (96%)]\tTotal Loss: 0.191949\n",
      "Reconstruction: 0.149156, Regularization: 0.042794\n",
      "2019-04-10 01:13:02,398 root         INFO     ====> Epoch: 27 Average loss: 0.1953\n",
      "2019-04-10 01:13:02,421 root         INFO     Train Epoch: 28 [0/8000 (0%)]\tTotal Loss: 0.187061\n",
      "Reconstruction: 0.140827, Regularization: 0.046235\n",
      "2019-04-10 01:13:02,479 root         INFO     Train Epoch: 28 [512/8000 (6%)]\tTotal Loss: 0.213799\n",
      "Reconstruction: 0.159097, Regularization: 0.054703\n",
      "2019-04-10 01:13:02,536 root         INFO     Train Epoch: 28 [1024/8000 (13%)]\tTotal Loss: 0.191010\n",
      "Reconstruction: 0.150107, Regularization: 0.040903\n",
      "2019-04-10 01:13:02,593 root         INFO     Train Epoch: 28 [1536/8000 (19%)]\tTotal Loss: 0.192396\n",
      "Reconstruction: 0.145309, Regularization: 0.047086\n",
      "2019-04-10 01:13:02,650 root         INFO     Train Epoch: 28 [2048/8000 (26%)]\tTotal Loss: 0.197962\n",
      "Reconstruction: 0.151779, Regularization: 0.046183\n",
      "2019-04-10 01:13:02,707 root         INFO     Train Epoch: 28 [2560/8000 (32%)]\tTotal Loss: 0.200611\n",
      "Reconstruction: 0.155717, Regularization: 0.044894\n",
      "2019-04-10 01:13:02,763 root         INFO     Train Epoch: 28 [3072/8000 (38%)]\tTotal Loss: 0.188319\n",
      "Reconstruction: 0.146829, Regularization: 0.041490\n",
      "2019-04-10 01:13:02,820 root         INFO     Train Epoch: 28 [3584/8000 (45%)]\tTotal Loss: 0.184400\n",
      "Reconstruction: 0.145264, Regularization: 0.039135\n",
      "2019-04-10 01:13:02,877 root         INFO     Train Epoch: 28 [4096/8000 (51%)]\tTotal Loss: 0.201970\n",
      "Reconstruction: 0.153677, Regularization: 0.048293\n",
      "2019-04-10 01:13:02,934 root         INFO     Train Epoch: 28 [4608/8000 (58%)]\tTotal Loss: 0.164304\n",
      "Reconstruction: 0.129646, Regularization: 0.034658\n",
      "2019-04-10 01:13:02,991 root         INFO     Train Epoch: 28 [5120/8000 (64%)]\tTotal Loss: 0.205938\n",
      "Reconstruction: 0.155061, Regularization: 0.050877\n",
      "2019-04-10 01:13:03,047 root         INFO     Train Epoch: 28 [5632/8000 (70%)]\tTotal Loss: 0.190906\n",
      "Reconstruction: 0.147530, Regularization: 0.043376\n",
      "2019-04-10 01:13:03,104 root         INFO     Train Epoch: 28 [6144/8000 (77%)]\tTotal Loss: 0.199355\n",
      "Reconstruction: 0.154124, Regularization: 0.045231\n",
      "2019-04-10 01:13:03,160 root         INFO     Train Epoch: 28 [6656/8000 (83%)]\tTotal Loss: 0.204479\n",
      "Reconstruction: 0.153261, Regularization: 0.051219\n",
      "2019-04-10 01:13:03,217 root         INFO     Train Epoch: 28 [7168/8000 (90%)]\tTotal Loss: 0.194004\n",
      "Reconstruction: 0.143297, Regularization: 0.050707\n",
      "2019-04-10 01:13:03,274 root         INFO     Train Epoch: 28 [7680/8000 (96%)]\tTotal Loss: 0.174482\n",
      "Reconstruction: 0.133782, Regularization: 0.040700\n",
      "2019-04-10 01:13:03,324 root         INFO     ====> Epoch: 28 Average loss: 0.1938\n",
      "2019-04-10 01:13:03,347 root         INFO     Train Epoch: 29 [0/8000 (0%)]\tTotal Loss: 0.180310\n",
      "Reconstruction: 0.140725, Regularization: 0.039584\n",
      "2019-04-10 01:13:03,404 root         INFO     Train Epoch: 29 [512/8000 (6%)]\tTotal Loss: 0.196596\n",
      "Reconstruction: 0.155727, Regularization: 0.040869\n",
      "2019-04-10 01:13:03,461 root         INFO     Train Epoch: 29 [1024/8000 (13%)]\tTotal Loss: 0.186253\n",
      "Reconstruction: 0.144161, Regularization: 0.042092\n",
      "2019-04-10 01:13:03,518 root         INFO     Train Epoch: 29 [1536/8000 (19%)]\tTotal Loss: 0.188957\n",
      "Reconstruction: 0.149862, Regularization: 0.039095\n",
      "2019-04-10 01:13:03,574 root         INFO     Train Epoch: 29 [2048/8000 (26%)]\tTotal Loss: 0.179535\n",
      "Reconstruction: 0.140140, Regularization: 0.039395\n",
      "2019-04-10 01:13:03,631 root         INFO     Train Epoch: 29 [2560/8000 (32%)]\tTotal Loss: 0.177328\n",
      "Reconstruction: 0.144199, Regularization: 0.033129\n",
      "2019-04-10 01:13:03,687 root         INFO     Train Epoch: 29 [3072/8000 (38%)]\tTotal Loss: 0.185803\n",
      "Reconstruction: 0.141802, Regularization: 0.044001\n",
      "2019-04-10 01:13:03,744 root         INFO     Train Epoch: 29 [3584/8000 (45%)]\tTotal Loss: 0.192651\n",
      "Reconstruction: 0.150753, Regularization: 0.041898\n",
      "2019-04-10 01:13:03,801 root         INFO     Train Epoch: 29 [4096/8000 (51%)]\tTotal Loss: 0.187555\n",
      "Reconstruction: 0.145757, Regularization: 0.041797\n",
      "2019-04-10 01:13:03,857 root         INFO     Train Epoch: 29 [4608/8000 (58%)]\tTotal Loss: 0.201950\n",
      "Reconstruction: 0.154831, Regularization: 0.047119\n",
      "2019-04-10 01:13:03,914 root         INFO     Train Epoch: 29 [5120/8000 (64%)]\tTotal Loss: 0.206318\n",
      "Reconstruction: 0.162280, Regularization: 0.044038\n",
      "2019-04-10 01:13:03,971 root         INFO     Train Epoch: 29 [5632/8000 (70%)]\tTotal Loss: 0.172842\n",
      "Reconstruction: 0.135282, Regularization: 0.037559\n",
      "2019-04-10 01:13:04,028 root         INFO     Train Epoch: 29 [6144/8000 (77%)]\tTotal Loss: 0.163020\n",
      "Reconstruction: 0.132280, Regularization: 0.030740\n",
      "2019-04-10 01:13:04,085 root         INFO     Train Epoch: 29 [6656/8000 (83%)]\tTotal Loss: 0.212980\n",
      "Reconstruction: 0.161545, Regularization: 0.051434\n",
      "2019-04-10 01:13:04,142 root         INFO     Train Epoch: 29 [7168/8000 (90%)]\tTotal Loss: 0.195270\n",
      "Reconstruction: 0.144883, Regularization: 0.050387\n",
      "2019-04-10 01:13:04,198 root         INFO     Train Epoch: 29 [7680/8000 (96%)]\tTotal Loss: 0.174914\n",
      "Reconstruction: 0.137410, Regularization: 0.037504\n",
      "2019-04-10 01:13:04,248 root         INFO     ====> Epoch: 29 Average loss: 0.1926\n",
      "2019-04-10 01:13:04,272 root         INFO     Train Epoch: 30 [0/8000 (0%)]\tTotal Loss: 0.200493\n",
      "Reconstruction: 0.153381, Regularization: 0.047112\n",
      "2019-04-10 01:13:04,329 root         INFO     Train Epoch: 30 [512/8000 (6%)]\tTotal Loss: 0.170514\n",
      "Reconstruction: 0.134144, Regularization: 0.036370\n",
      "2019-04-10 01:13:04,386 root         INFO     Train Epoch: 30 [1024/8000 (13%)]\tTotal Loss: 0.185156\n",
      "Reconstruction: 0.144893, Regularization: 0.040263\n",
      "2019-04-10 01:13:04,442 root         INFO     Train Epoch: 30 [1536/8000 (19%)]\tTotal Loss: 0.215792\n",
      "Reconstruction: 0.165507, Regularization: 0.050285\n",
      "2019-04-10 01:13:04,498 root         INFO     Train Epoch: 30 [2048/8000 (26%)]\tTotal Loss: 0.180060\n",
      "Reconstruction: 0.140370, Regularization: 0.039690\n",
      "2019-04-10 01:13:04,554 root         INFO     Train Epoch: 30 [2560/8000 (32%)]\tTotal Loss: 0.172969\n",
      "Reconstruction: 0.133716, Regularization: 0.039253\n",
      "2019-04-10 01:13:04,610 root         INFO     Train Epoch: 30 [3072/8000 (38%)]\tTotal Loss: 0.162126\n",
      "Reconstruction: 0.126796, Regularization: 0.035330\n",
      "2019-04-10 01:13:04,667 root         INFO     Train Epoch: 30 [3584/8000 (45%)]\tTotal Loss: 0.170893\n",
      "Reconstruction: 0.137744, Regularization: 0.033149\n",
      "2019-04-10 01:13:04,723 root         INFO     Train Epoch: 30 [4096/8000 (51%)]\tTotal Loss: 0.177584\n",
      "Reconstruction: 0.142131, Regularization: 0.035453\n",
      "2019-04-10 01:13:04,779 root         INFO     Train Epoch: 30 [4608/8000 (58%)]\tTotal Loss: 0.187542\n",
      "Reconstruction: 0.147155, Regularization: 0.040387\n",
      "2019-04-10 01:13:04,834 root         INFO     Train Epoch: 30 [5120/8000 (64%)]\tTotal Loss: 0.193945\n",
      "Reconstruction: 0.147484, Regularization: 0.046461\n",
      "2019-04-10 01:13:04,889 root         INFO     Train Epoch: 30 [5632/8000 (70%)]\tTotal Loss: 0.171370\n",
      "Reconstruction: 0.134311, Regularization: 0.037059\n",
      "2019-04-10 01:13:04,944 root         INFO     Train Epoch: 30 [6144/8000 (77%)]\tTotal Loss: 0.171310\n",
      "Reconstruction: 0.131959, Regularization: 0.039351\n",
      "2019-04-10 01:13:05,000 root         INFO     Train Epoch: 30 [6656/8000 (83%)]\tTotal Loss: 0.186793\n",
      "Reconstruction: 0.141220, Regularization: 0.045573\n",
      "2019-04-10 01:13:05,055 root         INFO     Train Epoch: 30 [7168/8000 (90%)]\tTotal Loss: 0.183880\n",
      "Reconstruction: 0.146113, Regularization: 0.037767\n",
      "2019-04-10 01:13:05,110 root         INFO     Train Epoch: 30 [7680/8000 (96%)]\tTotal Loss: 0.177723\n",
      "Reconstruction: 0.137744, Regularization: 0.039980\n",
      "2019-04-10 01:13:05,161 root         INFO     ====> Epoch: 30 Average loss: 0.1918\n",
      "2019-04-10 01:13:05,185 root         INFO     Train Epoch: 31 [0/8000 (0%)]\tTotal Loss: 0.163653\n",
      "Reconstruction: 0.127712, Regularization: 0.035942\n",
      "2019-04-10 01:13:05,242 root         INFO     Train Epoch: 31 [512/8000 (6%)]\tTotal Loss: 0.181427\n",
      "Reconstruction: 0.143637, Regularization: 0.037791\n",
      "2019-04-10 01:13:05,298 root         INFO     Train Epoch: 31 [1024/8000 (13%)]\tTotal Loss: 0.195013\n",
      "Reconstruction: 0.147264, Regularization: 0.047750\n",
      "2019-04-10 01:13:05,355 root         INFO     Train Epoch: 31 [1536/8000 (19%)]\tTotal Loss: 0.197448\n",
      "Reconstruction: 0.154805, Regularization: 0.042642\n",
      "2019-04-10 01:13:05,412 root         INFO     Train Epoch: 31 [2048/8000 (26%)]\tTotal Loss: 0.171974\n",
      "Reconstruction: 0.135681, Regularization: 0.036294\n",
      "2019-04-10 01:13:05,469 root         INFO     Train Epoch: 31 [2560/8000 (32%)]\tTotal Loss: 0.162952\n",
      "Reconstruction: 0.129305, Regularization: 0.033648\n",
      "2019-04-10 01:13:05,526 root         INFO     Train Epoch: 31 [3072/8000 (38%)]\tTotal Loss: 0.199185\n",
      "Reconstruction: 0.154907, Regularization: 0.044278\n",
      "2019-04-10 01:13:05,583 root         INFO     Train Epoch: 31 [3584/8000 (45%)]\tTotal Loss: 0.200043\n",
      "Reconstruction: 0.157261, Regularization: 0.042783\n",
      "2019-04-10 01:13:05,640 root         INFO     Train Epoch: 31 [4096/8000 (51%)]\tTotal Loss: 0.178864\n",
      "Reconstruction: 0.140802, Regularization: 0.038063\n",
      "2019-04-10 01:13:05,696 root         INFO     Train Epoch: 31 [4608/8000 (58%)]\tTotal Loss: 0.171719\n",
      "Reconstruction: 0.139271, Regularization: 0.032448\n",
      "2019-04-10 01:13:05,753 root         INFO     Train Epoch: 31 [5120/8000 (64%)]\tTotal Loss: 0.188317\n",
      "Reconstruction: 0.151933, Regularization: 0.036384\n",
      "2019-04-10 01:13:05,810 root         INFO     Train Epoch: 31 [5632/8000 (70%)]\tTotal Loss: 0.184934\n",
      "Reconstruction: 0.145936, Regularization: 0.038998\n",
      "2019-04-10 01:13:05,867 root         INFO     Train Epoch: 31 [6144/8000 (77%)]\tTotal Loss: 0.189495\n",
      "Reconstruction: 0.144878, Regularization: 0.044617\n",
      "2019-04-10 01:13:05,924 root         INFO     Train Epoch: 31 [6656/8000 (83%)]\tTotal Loss: 0.177361\n",
      "Reconstruction: 0.142883, Regularization: 0.034478\n",
      "2019-04-10 01:13:05,981 root         INFO     Train Epoch: 31 [7168/8000 (90%)]\tTotal Loss: 0.196873\n",
      "Reconstruction: 0.152960, Regularization: 0.043913\n",
      "2019-04-10 01:13:06,037 root         INFO     Train Epoch: 31 [7680/8000 (96%)]\tTotal Loss: 0.204616\n",
      "Reconstruction: 0.156917, Regularization: 0.047699\n",
      "2019-04-10 01:13:06,088 root         INFO     ====> Epoch: 31 Average loss: 0.1894\n",
      "2019-04-10 01:13:06,112 root         INFO     Train Epoch: 32 [0/8000 (0%)]\tTotal Loss: 0.222451\n",
      "Reconstruction: 0.169573, Regularization: 0.052878\n",
      "2019-04-10 01:13:06,169 root         INFO     Train Epoch: 32 [512/8000 (6%)]\tTotal Loss: 0.176875\n",
      "Reconstruction: 0.139945, Regularization: 0.036930\n",
      "2019-04-10 01:13:06,227 root         INFO     Train Epoch: 32 [1024/8000 (13%)]\tTotal Loss: 0.190153\n",
      "Reconstruction: 0.151632, Regularization: 0.038522\n",
      "2019-04-10 01:13:06,285 root         INFO     Train Epoch: 32 [1536/8000 (19%)]\tTotal Loss: 0.190609\n",
      "Reconstruction: 0.148284, Regularization: 0.042325\n",
      "2019-04-10 01:13:06,342 root         INFO     Train Epoch: 32 [2048/8000 (26%)]\tTotal Loss: 0.192107\n",
      "Reconstruction: 0.154873, Regularization: 0.037234\n",
      "2019-04-10 01:13:06,399 root         INFO     Train Epoch: 32 [2560/8000 (32%)]\tTotal Loss: 0.183426\n",
      "Reconstruction: 0.151245, Regularization: 0.032181\n",
      "2019-04-10 01:13:06,456 root         INFO     Train Epoch: 32 [3072/8000 (38%)]\tTotal Loss: 0.182970\n",
      "Reconstruction: 0.144488, Regularization: 0.038481\n",
      "2019-04-10 01:13:06,513 root         INFO     Train Epoch: 32 [3584/8000 (45%)]\tTotal Loss: 0.186494\n",
      "Reconstruction: 0.144742, Regularization: 0.041751\n",
      "2019-04-10 01:13:06,569 root         INFO     Train Epoch: 32 [4096/8000 (51%)]\tTotal Loss: 0.192110\n",
      "Reconstruction: 0.150153, Regularization: 0.041957\n",
      "2019-04-10 01:13:06,626 root         INFO     Train Epoch: 32 [4608/8000 (58%)]\tTotal Loss: 0.169251\n",
      "Reconstruction: 0.134916, Regularization: 0.034335\n",
      "2019-04-10 01:13:06,683 root         INFO     Train Epoch: 32 [5120/8000 (64%)]\tTotal Loss: 0.180015\n",
      "Reconstruction: 0.140937, Regularization: 0.039078\n",
      "2019-04-10 01:13:06,741 root         INFO     Train Epoch: 32 [5632/8000 (70%)]\tTotal Loss: 0.193627\n",
      "Reconstruction: 0.154962, Regularization: 0.038666\n",
      "2019-04-10 01:13:06,798 root         INFO     Train Epoch: 32 [6144/8000 (77%)]\tTotal Loss: 0.186664\n",
      "Reconstruction: 0.149582, Regularization: 0.037081\n",
      "2019-04-10 01:13:06,856 root         INFO     Train Epoch: 32 [6656/8000 (83%)]\tTotal Loss: 0.204667\n",
      "Reconstruction: 0.169799, Regularization: 0.034868\n",
      "2019-04-10 01:13:06,915 root         INFO     Train Epoch: 32 [7168/8000 (90%)]\tTotal Loss: 0.175729\n",
      "Reconstruction: 0.144104, Regularization: 0.031625\n",
      "2019-04-10 01:13:06,972 root         INFO     Train Epoch: 32 [7680/8000 (96%)]\tTotal Loss: 0.195467\n",
      "Reconstruction: 0.152307, Regularization: 0.043160\n",
      "2019-04-10 01:13:07,023 root         INFO     ====> Epoch: 32 Average loss: 0.1886\n",
      "2019-04-10 01:13:07,047 root         INFO     Train Epoch: 33 [0/8000 (0%)]\tTotal Loss: 0.183616\n",
      "Reconstruction: 0.145546, Regularization: 0.038070\n",
      "2019-04-10 01:13:07,104 root         INFO     Train Epoch: 33 [512/8000 (6%)]\tTotal Loss: 0.197118\n",
      "Reconstruction: 0.153608, Regularization: 0.043510\n",
      "2019-04-10 01:13:07,162 root         INFO     Train Epoch: 33 [1024/8000 (13%)]\tTotal Loss: 0.193447\n",
      "Reconstruction: 0.149798, Regularization: 0.043649\n",
      "2019-04-10 01:13:07,220 root         INFO     Train Epoch: 33 [1536/8000 (19%)]\tTotal Loss: 0.188819\n",
      "Reconstruction: 0.153269, Regularization: 0.035549\n",
      "2019-04-10 01:13:07,278 root         INFO     Train Epoch: 33 [2048/8000 (26%)]\tTotal Loss: 0.184344\n",
      "Reconstruction: 0.149581, Regularization: 0.034763\n",
      "2019-04-10 01:13:07,336 root         INFO     Train Epoch: 33 [2560/8000 (32%)]\tTotal Loss: 0.182029\n",
      "Reconstruction: 0.144761, Regularization: 0.037268\n",
      "2019-04-10 01:13:07,393 root         INFO     Train Epoch: 33 [3072/8000 (38%)]\tTotal Loss: 0.171247\n",
      "Reconstruction: 0.139431, Regularization: 0.031817\n",
      "2019-04-10 01:13:07,450 root         INFO     Train Epoch: 33 [3584/8000 (45%)]\tTotal Loss: 0.182466\n",
      "Reconstruction: 0.144891, Regularization: 0.037575\n",
      "2019-04-10 01:13:07,508 root         INFO     Train Epoch: 33 [4096/8000 (51%)]\tTotal Loss: 0.185829\n",
      "Reconstruction: 0.154489, Regularization: 0.031340\n",
      "2019-04-10 01:13:07,565 root         INFO     Train Epoch: 33 [4608/8000 (58%)]\tTotal Loss: 0.200249\n",
      "Reconstruction: 0.150586, Regularization: 0.049663\n",
      "2019-04-10 01:13:07,622 root         INFO     Train Epoch: 33 [5120/8000 (64%)]\tTotal Loss: 0.195234\n",
      "Reconstruction: 0.154487, Regularization: 0.040747\n",
      "2019-04-10 01:13:07,679 root         INFO     Train Epoch: 33 [5632/8000 (70%)]\tTotal Loss: 0.190750\n",
      "Reconstruction: 0.154174, Regularization: 0.036576\n",
      "2019-04-10 01:13:07,737 root         INFO     Train Epoch: 33 [6144/8000 (77%)]\tTotal Loss: 0.181095\n",
      "Reconstruction: 0.141868, Regularization: 0.039227\n",
      "2019-04-10 01:13:07,795 root         INFO     Train Epoch: 33 [6656/8000 (83%)]\tTotal Loss: 0.197794\n",
      "Reconstruction: 0.160927, Regularization: 0.036868\n",
      "2019-04-10 01:13:07,852 root         INFO     Train Epoch: 33 [7168/8000 (90%)]\tTotal Loss: 0.189672\n",
      "Reconstruction: 0.150146, Regularization: 0.039526\n",
      "2019-04-10 01:13:07,909 root         INFO     Train Epoch: 33 [7680/8000 (96%)]\tTotal Loss: 0.189658\n",
      "Reconstruction: 0.155759, Regularization: 0.033898\n",
      "2019-04-10 01:13:07,960 root         INFO     ====> Epoch: 33 Average loss: 0.1870\n",
      "2019-04-10 01:13:07,983 root         INFO     Train Epoch: 34 [0/8000 (0%)]\tTotal Loss: 0.182103\n",
      "Reconstruction: 0.147120, Regularization: 0.034984\n",
      "2019-04-10 01:13:08,042 root         INFO     Train Epoch: 34 [512/8000 (6%)]\tTotal Loss: 0.193093\n",
      "Reconstruction: 0.146810, Regularization: 0.046283\n",
      "2019-04-10 01:13:08,100 root         INFO     Train Epoch: 34 [1024/8000 (13%)]\tTotal Loss: 0.197513\n",
      "Reconstruction: 0.155950, Regularization: 0.041563\n",
      "2019-04-10 01:13:08,158 root         INFO     Train Epoch: 34 [1536/8000 (19%)]\tTotal Loss: 0.161043\n",
      "Reconstruction: 0.133357, Regularization: 0.027686\n",
      "2019-04-10 01:13:08,216 root         INFO     Train Epoch: 34 [2048/8000 (26%)]\tTotal Loss: 0.170503\n",
      "Reconstruction: 0.136897, Regularization: 0.033606\n",
      "2019-04-10 01:13:08,273 root         INFO     Train Epoch: 34 [2560/8000 (32%)]\tTotal Loss: 0.168859\n",
      "Reconstruction: 0.134173, Regularization: 0.034685\n",
      "2019-04-10 01:13:08,329 root         INFO     Train Epoch: 34 [3072/8000 (38%)]\tTotal Loss: 0.200689\n",
      "Reconstruction: 0.164095, Regularization: 0.036594\n",
      "2019-04-10 01:13:08,387 root         INFO     Train Epoch: 34 [3584/8000 (45%)]\tTotal Loss: 0.202067\n",
      "Reconstruction: 0.158638, Regularization: 0.043428\n",
      "2019-04-10 01:13:08,444 root         INFO     Train Epoch: 34 [4096/8000 (51%)]\tTotal Loss: 0.166169\n",
      "Reconstruction: 0.136410, Regularization: 0.029759\n",
      "2019-04-10 01:13:08,500 root         INFO     Train Epoch: 34 [4608/8000 (58%)]\tTotal Loss: 0.171058\n",
      "Reconstruction: 0.140823, Regularization: 0.030235\n",
      "2019-04-10 01:13:08,557 root         INFO     Train Epoch: 34 [5120/8000 (64%)]\tTotal Loss: 0.164738\n",
      "Reconstruction: 0.133664, Regularization: 0.031074\n",
      "2019-04-10 01:13:08,614 root         INFO     Train Epoch: 34 [5632/8000 (70%)]\tTotal Loss: 0.174592\n",
      "Reconstruction: 0.139884, Regularization: 0.034707\n",
      "2019-04-10 01:13:08,670 root         INFO     Train Epoch: 34 [6144/8000 (77%)]\tTotal Loss: 0.188247\n",
      "Reconstruction: 0.149397, Regularization: 0.038849\n",
      "2019-04-10 01:13:08,726 root         INFO     Train Epoch: 34 [6656/8000 (83%)]\tTotal Loss: 0.196111\n",
      "Reconstruction: 0.156776, Regularization: 0.039334\n",
      "2019-04-10 01:13:08,782 root         INFO     Train Epoch: 34 [7168/8000 (90%)]\tTotal Loss: 0.191570\n",
      "Reconstruction: 0.154236, Regularization: 0.037334\n",
      "2019-04-10 01:13:08,838 root         INFO     Train Epoch: 34 [7680/8000 (96%)]\tTotal Loss: 0.184911\n",
      "Reconstruction: 0.153181, Regularization: 0.031730\n",
      "2019-04-10 01:13:08,889 root         INFO     ====> Epoch: 34 Average loss: 0.1864\n",
      "2019-04-10 01:13:08,912 root         INFO     Train Epoch: 35 [0/8000 (0%)]\tTotal Loss: 0.194829\n",
      "Reconstruction: 0.157270, Regularization: 0.037559\n",
      "2019-04-10 01:13:08,968 root         INFO     Train Epoch: 35 [512/8000 (6%)]\tTotal Loss: 0.184343\n",
      "Reconstruction: 0.148875, Regularization: 0.035468\n",
      "2019-04-10 01:13:09,025 root         INFO     Train Epoch: 35 [1024/8000 (13%)]\tTotal Loss: 0.201454\n",
      "Reconstruction: 0.155782, Regularization: 0.045672\n",
      "2019-04-10 01:13:09,082 root         INFO     Train Epoch: 35 [1536/8000 (19%)]\tTotal Loss: 0.203807\n",
      "Reconstruction: 0.164857, Regularization: 0.038950\n",
      "2019-04-10 01:13:09,139 root         INFO     Train Epoch: 35 [2048/8000 (26%)]\tTotal Loss: 0.184913\n",
      "Reconstruction: 0.146554, Regularization: 0.038359\n",
      "2019-04-10 01:13:09,196 root         INFO     Train Epoch: 35 [2560/8000 (32%)]\tTotal Loss: 0.177311\n",
      "Reconstruction: 0.143138, Regularization: 0.034173\n",
      "2019-04-10 01:13:09,253 root         INFO     Train Epoch: 35 [3072/8000 (38%)]\tTotal Loss: 0.179583\n",
      "Reconstruction: 0.145838, Regularization: 0.033745\n",
      "2019-04-10 01:13:09,308 root         INFO     Train Epoch: 35 [3584/8000 (45%)]\tTotal Loss: 0.187096\n",
      "Reconstruction: 0.144551, Regularization: 0.042545\n",
      "2019-04-10 01:13:09,363 root         INFO     Train Epoch: 35 [4096/8000 (51%)]\tTotal Loss: 0.186520\n",
      "Reconstruction: 0.152173, Regularization: 0.034347\n",
      "2019-04-10 01:13:09,417 root         INFO     Train Epoch: 35 [4608/8000 (58%)]\tTotal Loss: 0.199764\n",
      "Reconstruction: 0.160054, Regularization: 0.039710\n",
      "2019-04-10 01:13:09,472 root         INFO     Train Epoch: 35 [5120/8000 (64%)]\tTotal Loss: 0.175825\n",
      "Reconstruction: 0.142726, Regularization: 0.033100\n",
      "2019-04-10 01:13:09,527 root         INFO     Train Epoch: 35 [5632/8000 (70%)]\tTotal Loss: 0.190033\n",
      "Reconstruction: 0.156568, Regularization: 0.033465\n",
      "2019-04-10 01:13:09,582 root         INFO     Train Epoch: 35 [6144/8000 (77%)]\tTotal Loss: 0.192514\n",
      "Reconstruction: 0.154238, Regularization: 0.038276\n",
      "2019-04-10 01:13:09,636 root         INFO     Train Epoch: 35 [6656/8000 (83%)]\tTotal Loss: 0.186140\n",
      "Reconstruction: 0.149004, Regularization: 0.037137\n",
      "2019-04-10 01:13:09,693 root         INFO     Train Epoch: 35 [7168/8000 (90%)]\tTotal Loss: 0.206082\n",
      "Reconstruction: 0.165017, Regularization: 0.041065\n",
      "2019-04-10 01:13:09,749 root         INFO     Train Epoch: 35 [7680/8000 (96%)]\tTotal Loss: 0.202513\n",
      "Reconstruction: 0.165127, Regularization: 0.037386\n",
      "2019-04-10 01:13:09,798 root         INFO     ====> Epoch: 35 Average loss: 0.1855\n",
      "2019-04-10 01:13:09,822 root         INFO     Train Epoch: 36 [0/8000 (0%)]\tTotal Loss: 0.184269\n",
      "Reconstruction: 0.146207, Regularization: 0.038061\n",
      "2019-04-10 01:13:09,878 root         INFO     Train Epoch: 36 [512/8000 (6%)]\tTotal Loss: 0.222959\n",
      "Reconstruction: 0.186432, Regularization: 0.036528\n",
      "2019-04-10 01:13:09,935 root         INFO     Train Epoch: 36 [1024/8000 (13%)]\tTotal Loss: 0.205153\n",
      "Reconstruction: 0.162631, Regularization: 0.042522\n",
      "2019-04-10 01:13:09,992 root         INFO     Train Epoch: 36 [1536/8000 (19%)]\tTotal Loss: 0.188796\n",
      "Reconstruction: 0.152906, Regularization: 0.035890\n",
      "2019-04-10 01:13:10,049 root         INFO     Train Epoch: 36 [2048/8000 (26%)]\tTotal Loss: 0.195526\n",
      "Reconstruction: 0.152715, Regularization: 0.042811\n",
      "2019-04-10 01:13:10,105 root         INFO     Train Epoch: 36 [2560/8000 (32%)]\tTotal Loss: 0.169997\n",
      "Reconstruction: 0.136435, Regularization: 0.033562\n",
      "2019-04-10 01:13:10,161 root         INFO     Train Epoch: 36 [3072/8000 (38%)]\tTotal Loss: 0.153902\n",
      "Reconstruction: 0.129071, Regularization: 0.024831\n",
      "2019-04-10 01:13:10,218 root         INFO     Train Epoch: 36 [3584/8000 (45%)]\tTotal Loss: 0.195559\n",
      "Reconstruction: 0.161152, Regularization: 0.034408\n",
      "2019-04-10 01:13:10,274 root         INFO     Train Epoch: 36 [4096/8000 (51%)]\tTotal Loss: 0.191622\n",
      "Reconstruction: 0.151471, Regularization: 0.040151\n",
      "2019-04-10 01:13:10,329 root         INFO     Train Epoch: 36 [4608/8000 (58%)]\tTotal Loss: 0.163207\n",
      "Reconstruction: 0.132957, Regularization: 0.030250\n",
      "2019-04-10 01:13:10,384 root         INFO     Train Epoch: 36 [5120/8000 (64%)]\tTotal Loss: 0.190486\n",
      "Reconstruction: 0.150080, Regularization: 0.040405\n",
      "2019-04-10 01:13:10,440 root         INFO     Train Epoch: 36 [5632/8000 (70%)]\tTotal Loss: 0.163277\n",
      "Reconstruction: 0.136091, Regularization: 0.027185\n",
      "2019-04-10 01:13:10,495 root         INFO     Train Epoch: 36 [6144/8000 (77%)]\tTotal Loss: 0.207132\n",
      "Reconstruction: 0.166162, Regularization: 0.040970\n",
      "2019-04-10 01:13:10,550 root         INFO     Train Epoch: 36 [6656/8000 (83%)]\tTotal Loss: 0.207931\n",
      "Reconstruction: 0.160898, Regularization: 0.047033\n",
      "2019-04-10 01:13:10,605 root         INFO     Train Epoch: 36 [7168/8000 (90%)]\tTotal Loss: 0.177507\n",
      "Reconstruction: 0.142911, Regularization: 0.034596\n",
      "2019-04-10 01:13:10,661 root         INFO     Train Epoch: 36 [7680/8000 (96%)]\tTotal Loss: 0.182771\n",
      "Reconstruction: 0.147159, Regularization: 0.035611\n",
      "2019-04-10 01:13:10,711 root         INFO     ====> Epoch: 36 Average loss: 0.1832\n",
      "2019-04-10 01:13:10,734 root         INFO     Train Epoch: 37 [0/8000 (0%)]\tTotal Loss: 0.196964\n",
      "Reconstruction: 0.158064, Regularization: 0.038900\n",
      "2019-04-10 01:13:10,791 root         INFO     Train Epoch: 37 [512/8000 (6%)]\tTotal Loss: 0.185454\n",
      "Reconstruction: 0.146767, Regularization: 0.038687\n",
      "2019-04-10 01:13:10,847 root         INFO     Train Epoch: 37 [1024/8000 (13%)]\tTotal Loss: 0.182479\n",
      "Reconstruction: 0.147157, Regularization: 0.035322\n",
      "2019-04-10 01:13:10,904 root         INFO     Train Epoch: 37 [1536/8000 (19%)]\tTotal Loss: 0.199305\n",
      "Reconstruction: 0.162617, Regularization: 0.036688\n",
      "2019-04-10 01:13:10,960 root         INFO     Train Epoch: 37 [2048/8000 (26%)]\tTotal Loss: 0.187437\n",
      "Reconstruction: 0.149070, Regularization: 0.038367\n",
      "2019-04-10 01:13:11,016 root         INFO     Train Epoch: 37 [2560/8000 (32%)]\tTotal Loss: 0.191331\n",
      "Reconstruction: 0.154567, Regularization: 0.036763\n",
      "2019-04-10 01:13:11,072 root         INFO     Train Epoch: 37 [3072/8000 (38%)]\tTotal Loss: 0.193267\n",
      "Reconstruction: 0.155506, Regularization: 0.037761\n",
      "2019-04-10 01:13:11,129 root         INFO     Train Epoch: 37 [3584/8000 (45%)]\tTotal Loss: 0.162799\n",
      "Reconstruction: 0.135627, Regularization: 0.027171\n",
      "2019-04-10 01:13:11,185 root         INFO     Train Epoch: 37 [4096/8000 (51%)]\tTotal Loss: 0.188873\n",
      "Reconstruction: 0.151098, Regularization: 0.037775\n",
      "2019-04-10 01:13:11,240 root         INFO     Train Epoch: 37 [4608/8000 (58%)]\tTotal Loss: 0.184254\n",
      "Reconstruction: 0.149803, Regularization: 0.034451\n",
      "2019-04-10 01:13:11,295 root         INFO     Train Epoch: 37 [5120/8000 (64%)]\tTotal Loss: 0.176737\n",
      "Reconstruction: 0.143649, Regularization: 0.033088\n",
      "2019-04-10 01:13:11,350 root         INFO     Train Epoch: 37 [5632/8000 (70%)]\tTotal Loss: 0.168110\n",
      "Reconstruction: 0.136132, Regularization: 0.031979\n",
      "2019-04-10 01:13:11,406 root         INFO     Train Epoch: 37 [6144/8000 (77%)]\tTotal Loss: 0.197315\n",
      "Reconstruction: 0.158090, Regularization: 0.039224\n",
      "2019-04-10 01:13:11,461 root         INFO     Train Epoch: 37 [6656/8000 (83%)]\tTotal Loss: 0.193505\n",
      "Reconstruction: 0.160996, Regularization: 0.032509\n",
      "2019-04-10 01:13:11,516 root         INFO     Train Epoch: 37 [7168/8000 (90%)]\tTotal Loss: 0.195913\n",
      "Reconstruction: 0.158948, Regularization: 0.036965\n",
      "2019-04-10 01:13:11,571 root         INFO     Train Epoch: 37 [7680/8000 (96%)]\tTotal Loss: 0.192366\n",
      "Reconstruction: 0.156716, Regularization: 0.035651\n",
      "2019-04-10 01:13:11,620 root         INFO     ====> Epoch: 37 Average loss: 0.1825\n",
      "2019-04-10 01:13:11,644 root         INFO     Train Epoch: 38 [0/8000 (0%)]\tTotal Loss: 0.165972\n",
      "Reconstruction: 0.134954, Regularization: 0.031018\n",
      "2019-04-10 01:13:11,699 root         INFO     Train Epoch: 38 [512/8000 (6%)]\tTotal Loss: 0.174733\n",
      "Reconstruction: 0.140685, Regularization: 0.034049\n",
      "2019-04-10 01:13:11,754 root         INFO     Train Epoch: 38 [1024/8000 (13%)]\tTotal Loss: 0.177703\n",
      "Reconstruction: 0.141508, Regularization: 0.036195\n",
      "2019-04-10 01:13:11,808 root         INFO     Train Epoch: 38 [1536/8000 (19%)]\tTotal Loss: 0.172854\n",
      "Reconstruction: 0.143455, Regularization: 0.029398\n",
      "2019-04-10 01:13:11,863 root         INFO     Train Epoch: 38 [2048/8000 (26%)]\tTotal Loss: 0.170850\n",
      "Reconstruction: 0.141414, Regularization: 0.029435\n",
      "2019-04-10 01:13:11,918 root         INFO     Train Epoch: 38 [2560/8000 (32%)]\tTotal Loss: 0.177644\n",
      "Reconstruction: 0.141936, Regularization: 0.035708\n",
      "2019-04-10 01:13:11,972 root         INFO     Train Epoch: 38 [3072/8000 (38%)]\tTotal Loss: 0.178181\n",
      "Reconstruction: 0.145892, Regularization: 0.032290\n",
      "2019-04-10 01:13:12,027 root         INFO     Train Epoch: 38 [3584/8000 (45%)]\tTotal Loss: 0.171757\n",
      "Reconstruction: 0.138984, Regularization: 0.032773\n",
      "2019-04-10 01:13:12,081 root         INFO     Train Epoch: 38 [4096/8000 (51%)]\tTotal Loss: 0.211619\n",
      "Reconstruction: 0.168918, Regularization: 0.042701\n",
      "2019-04-10 01:13:12,135 root         INFO     Train Epoch: 38 [4608/8000 (58%)]\tTotal Loss: 0.176290\n",
      "Reconstruction: 0.143735, Regularization: 0.032555\n",
      "2019-04-10 01:13:12,189 root         INFO     Train Epoch: 38 [5120/8000 (64%)]\tTotal Loss: 0.181140\n",
      "Reconstruction: 0.151516, Regularization: 0.029624\n",
      "2019-04-10 01:13:12,244 root         INFO     Train Epoch: 38 [5632/8000 (70%)]\tTotal Loss: 0.172261\n",
      "Reconstruction: 0.141192, Regularization: 0.031069\n",
      "2019-04-10 01:13:12,298 root         INFO     Train Epoch: 38 [6144/8000 (77%)]\tTotal Loss: 0.189132\n",
      "Reconstruction: 0.156653, Regularization: 0.032479\n",
      "2019-04-10 01:13:12,352 root         INFO     Train Epoch: 38 [6656/8000 (83%)]\tTotal Loss: 0.172714\n",
      "Reconstruction: 0.142126, Regularization: 0.030588\n",
      "2019-04-10 01:13:12,406 root         INFO     Train Epoch: 38 [7168/8000 (90%)]\tTotal Loss: 0.170399\n",
      "Reconstruction: 0.139052, Regularization: 0.031347\n",
      "2019-04-10 01:13:12,460 root         INFO     Train Epoch: 38 [7680/8000 (96%)]\tTotal Loss: 0.174597\n",
      "Reconstruction: 0.142412, Regularization: 0.032186\n",
      "2019-04-10 01:13:12,509 root         INFO     ====> Epoch: 38 Average loss: 0.1813\n",
      "2019-04-10 01:13:12,532 root         INFO     Train Epoch: 39 [0/8000 (0%)]\tTotal Loss: 0.165343\n",
      "Reconstruction: 0.134876, Regularization: 0.030467\n",
      "2019-04-10 01:13:12,587 root         INFO     Train Epoch: 39 [512/8000 (6%)]\tTotal Loss: 0.195257\n",
      "Reconstruction: 0.154876, Regularization: 0.040381\n",
      "2019-04-10 01:13:12,642 root         INFO     Train Epoch: 39 [1024/8000 (13%)]\tTotal Loss: 0.167516\n",
      "Reconstruction: 0.141849, Regularization: 0.025668\n",
      "2019-04-10 01:13:12,698 root         INFO     Train Epoch: 39 [1536/8000 (19%)]\tTotal Loss: 0.185763\n",
      "Reconstruction: 0.153508, Regularization: 0.032255\n",
      "2019-04-10 01:13:12,753 root         INFO     Train Epoch: 39 [2048/8000 (26%)]\tTotal Loss: 0.173297\n",
      "Reconstruction: 0.143104, Regularization: 0.030192\n",
      "2019-04-10 01:13:12,808 root         INFO     Train Epoch: 39 [2560/8000 (32%)]\tTotal Loss: 0.189738\n",
      "Reconstruction: 0.158042, Regularization: 0.031696\n",
      "2019-04-10 01:13:12,865 root         INFO     Train Epoch: 39 [3072/8000 (38%)]\tTotal Loss: 0.184107\n",
      "Reconstruction: 0.151776, Regularization: 0.032331\n",
      "2019-04-10 01:13:12,919 root         INFO     Train Epoch: 39 [3584/8000 (45%)]\tTotal Loss: 0.174360\n",
      "Reconstruction: 0.142311, Regularization: 0.032048\n",
      "2019-04-10 01:13:12,975 root         INFO     Train Epoch: 39 [4096/8000 (51%)]\tTotal Loss: 0.177073\n",
      "Reconstruction: 0.145248, Regularization: 0.031824\n",
      "2019-04-10 01:13:13,029 root         INFO     Train Epoch: 39 [4608/8000 (58%)]\tTotal Loss: 0.169260\n",
      "Reconstruction: 0.143065, Regularization: 0.026195\n",
      "2019-04-10 01:13:13,084 root         INFO     Train Epoch: 39 [5120/8000 (64%)]\tTotal Loss: 0.189785\n",
      "Reconstruction: 0.150252, Regularization: 0.039533\n",
      "2019-04-10 01:13:13,138 root         INFO     Train Epoch: 39 [5632/8000 (70%)]\tTotal Loss: 0.167955\n",
      "Reconstruction: 0.142085, Regularization: 0.025870\n",
      "2019-04-10 01:13:13,193 root         INFO     Train Epoch: 39 [6144/8000 (77%)]\tTotal Loss: 0.173861\n",
      "Reconstruction: 0.140516, Regularization: 0.033345\n",
      "2019-04-10 01:13:13,247 root         INFO     Train Epoch: 39 [6656/8000 (83%)]\tTotal Loss: 0.189008\n",
      "Reconstruction: 0.150492, Regularization: 0.038516\n",
      "2019-04-10 01:13:13,302 root         INFO     Train Epoch: 39 [7168/8000 (90%)]\tTotal Loss: 0.165643\n",
      "Reconstruction: 0.136886, Regularization: 0.028757\n",
      "2019-04-10 01:13:13,358 root         INFO     Train Epoch: 39 [7680/8000 (96%)]\tTotal Loss: 0.177027\n",
      "Reconstruction: 0.146989, Regularization: 0.030038\n",
      "2019-04-10 01:13:13,408 root         INFO     ====> Epoch: 39 Average loss: 0.1799\n",
      "2019-04-10 01:13:13,432 root         INFO     Train Epoch: 40 [0/8000 (0%)]\tTotal Loss: 0.175409\n",
      "Reconstruction: 0.140089, Regularization: 0.035320\n",
      "2019-04-10 01:13:13,489 root         INFO     Train Epoch: 40 [512/8000 (6%)]\tTotal Loss: 0.167864\n",
      "Reconstruction: 0.137904, Regularization: 0.029959\n",
      "2019-04-10 01:13:13,546 root         INFO     Train Epoch: 40 [1024/8000 (13%)]\tTotal Loss: 0.187786\n",
      "Reconstruction: 0.153587, Regularization: 0.034199\n",
      "2019-04-10 01:13:13,603 root         INFO     Train Epoch: 40 [1536/8000 (19%)]\tTotal Loss: 0.159193\n",
      "Reconstruction: 0.132085, Regularization: 0.027108\n",
      "2019-04-10 01:13:13,659 root         INFO     Train Epoch: 40 [2048/8000 (26%)]\tTotal Loss: 0.183250\n",
      "Reconstruction: 0.153127, Regularization: 0.030123\n",
      "2019-04-10 01:13:13,716 root         INFO     Train Epoch: 40 [2560/8000 (32%)]\tTotal Loss: 0.188617\n",
      "Reconstruction: 0.160488, Regularization: 0.028129\n",
      "2019-04-10 01:13:13,773 root         INFO     Train Epoch: 40 [3072/8000 (38%)]\tTotal Loss: 0.185383\n",
      "Reconstruction: 0.149139, Regularization: 0.036244\n",
      "2019-04-10 01:13:13,830 root         INFO     Train Epoch: 40 [3584/8000 (45%)]\tTotal Loss: 0.171241\n",
      "Reconstruction: 0.139669, Regularization: 0.031572\n",
      "2019-04-10 01:13:13,887 root         INFO     Train Epoch: 40 [4096/8000 (51%)]\tTotal Loss: 0.184869\n",
      "Reconstruction: 0.152138, Regularization: 0.032731\n",
      "2019-04-10 01:13:13,943 root         INFO     Train Epoch: 40 [4608/8000 (58%)]\tTotal Loss: 0.179213\n",
      "Reconstruction: 0.149979, Regularization: 0.029235\n",
      "2019-04-10 01:13:14,000 root         INFO     Train Epoch: 40 [5120/8000 (64%)]\tTotal Loss: 0.179224\n",
      "Reconstruction: 0.151167, Regularization: 0.028056\n",
      "2019-04-10 01:13:14,057 root         INFO     Train Epoch: 40 [5632/8000 (70%)]\tTotal Loss: 0.166288\n",
      "Reconstruction: 0.138492, Regularization: 0.027795\n",
      "2019-04-10 01:13:14,113 root         INFO     Train Epoch: 40 [6144/8000 (77%)]\tTotal Loss: 0.184520\n",
      "Reconstruction: 0.151952, Regularization: 0.032568\n",
      "2019-04-10 01:13:14,170 root         INFO     Train Epoch: 40 [6656/8000 (83%)]\tTotal Loss: 0.175700\n",
      "Reconstruction: 0.146996, Regularization: 0.028705\n",
      "2019-04-10 01:13:14,227 root         INFO     Train Epoch: 40 [7168/8000 (90%)]\tTotal Loss: 0.175488\n",
      "Reconstruction: 0.147177, Regularization: 0.028311\n",
      "2019-04-10 01:13:14,283 root         INFO     Train Epoch: 40 [7680/8000 (96%)]\tTotal Loss: 0.185961\n",
      "Reconstruction: 0.157292, Regularization: 0.028669\n",
      "2019-04-10 01:13:14,333 root         INFO     ====> Epoch: 40 Average loss: 0.1793\n",
      "2019-04-10 01:13:14,357 root         INFO     Train Epoch: 41 [0/8000 (0%)]\tTotal Loss: 0.201305\n",
      "Reconstruction: 0.166030, Regularization: 0.035274\n",
      "2019-04-10 01:13:14,415 root         INFO     Train Epoch: 41 [512/8000 (6%)]\tTotal Loss: 0.188499\n",
      "Reconstruction: 0.152885, Regularization: 0.035614\n",
      "2019-04-10 01:13:14,471 root         INFO     Train Epoch: 41 [1024/8000 (13%)]\tTotal Loss: 0.188810\n",
      "Reconstruction: 0.156340, Regularization: 0.032470\n",
      "2019-04-10 01:13:14,528 root         INFO     Train Epoch: 41 [1536/8000 (19%)]\tTotal Loss: 0.167850\n",
      "Reconstruction: 0.142594, Regularization: 0.025256\n",
      "2019-04-10 01:13:14,584 root         INFO     Train Epoch: 41 [2048/8000 (26%)]\tTotal Loss: 0.189801\n",
      "Reconstruction: 0.154308, Regularization: 0.035493\n",
      "2019-04-10 01:13:14,641 root         INFO     Train Epoch: 41 [2560/8000 (32%)]\tTotal Loss: 0.174075\n",
      "Reconstruction: 0.143332, Regularization: 0.030743\n",
      "2019-04-10 01:13:14,698 root         INFO     Train Epoch: 41 [3072/8000 (38%)]\tTotal Loss: 0.170405\n",
      "Reconstruction: 0.144426, Regularization: 0.025979\n",
      "2019-04-10 01:13:14,754 root         INFO     Train Epoch: 41 [3584/8000 (45%)]\tTotal Loss: 0.168713\n",
      "Reconstruction: 0.136557, Regularization: 0.032156\n",
      "2019-04-10 01:13:14,811 root         INFO     Train Epoch: 41 [4096/8000 (51%)]\tTotal Loss: 0.185886\n",
      "Reconstruction: 0.154555, Regularization: 0.031331\n",
      "2019-04-10 01:13:14,867 root         INFO     Train Epoch: 41 [4608/8000 (58%)]\tTotal Loss: 0.175818\n",
      "Reconstruction: 0.145873, Regularization: 0.029946\n",
      "2019-04-10 01:13:14,924 root         INFO     Train Epoch: 41 [5120/8000 (64%)]\tTotal Loss: 0.184026\n",
      "Reconstruction: 0.151507, Regularization: 0.032519\n",
      "2019-04-10 01:13:14,981 root         INFO     Train Epoch: 41 [5632/8000 (70%)]\tTotal Loss: 0.167187\n",
      "Reconstruction: 0.143470, Regularization: 0.023717\n",
      "2019-04-10 01:13:15,037 root         INFO     Train Epoch: 41 [6144/8000 (77%)]\tTotal Loss: 0.172053\n",
      "Reconstruction: 0.144135, Regularization: 0.027918\n",
      "2019-04-10 01:13:15,094 root         INFO     Train Epoch: 41 [6656/8000 (83%)]\tTotal Loss: 0.191580\n",
      "Reconstruction: 0.157928, Regularization: 0.033653\n",
      "2019-04-10 01:13:15,150 root         INFO     Train Epoch: 41 [7168/8000 (90%)]\tTotal Loss: 0.186030\n",
      "Reconstruction: 0.146950, Regularization: 0.039080\n",
      "2019-04-10 01:13:15,207 root         INFO     Train Epoch: 41 [7680/8000 (96%)]\tTotal Loss: 0.188328\n",
      "Reconstruction: 0.149879, Regularization: 0.038449\n",
      "2019-04-10 01:13:15,256 root         INFO     ====> Epoch: 41 Average loss: 0.1788\n",
      "2019-04-10 01:13:15,280 root         INFO     Train Epoch: 42 [0/8000 (0%)]\tTotal Loss: 0.195725\n",
      "Reconstruction: 0.165178, Regularization: 0.030547\n",
      "2019-04-10 01:13:15,335 root         INFO     Train Epoch: 42 [512/8000 (6%)]\tTotal Loss: 0.184561\n",
      "Reconstruction: 0.150591, Regularization: 0.033969\n",
      "2019-04-10 01:13:15,390 root         INFO     Train Epoch: 42 [1024/8000 (13%)]\tTotal Loss: 0.175844\n",
      "Reconstruction: 0.149530, Regularization: 0.026314\n",
      "2019-04-10 01:13:15,443 root         INFO     Train Epoch: 42 [1536/8000 (19%)]\tTotal Loss: 0.177782\n",
      "Reconstruction: 0.148286, Regularization: 0.029496\n",
      "2019-04-10 01:13:15,497 root         INFO     Train Epoch: 42 [2048/8000 (26%)]\tTotal Loss: 0.179359\n",
      "Reconstruction: 0.150346, Regularization: 0.029013\n",
      "2019-04-10 01:13:15,552 root         INFO     Train Epoch: 42 [2560/8000 (32%)]\tTotal Loss: 0.170934\n",
      "Reconstruction: 0.141961, Regularization: 0.028972\n",
      "2019-04-10 01:13:15,607 root         INFO     Train Epoch: 42 [3072/8000 (38%)]\tTotal Loss: 0.180411\n",
      "Reconstruction: 0.145132, Regularization: 0.035278\n",
      "2019-04-10 01:13:15,662 root         INFO     Train Epoch: 42 [3584/8000 (45%)]\tTotal Loss: 0.173494\n",
      "Reconstruction: 0.147408, Regularization: 0.026086\n",
      "2019-04-10 01:13:15,717 root         INFO     Train Epoch: 42 [4096/8000 (51%)]\tTotal Loss: 0.160351\n",
      "Reconstruction: 0.135447, Regularization: 0.024905\n",
      "2019-04-10 01:13:15,771 root         INFO     Train Epoch: 42 [4608/8000 (58%)]\tTotal Loss: 0.175333\n",
      "Reconstruction: 0.145524, Regularization: 0.029809\n",
      "2019-04-10 01:13:15,825 root         INFO     Train Epoch: 42 [5120/8000 (64%)]\tTotal Loss: 0.175554\n",
      "Reconstruction: 0.148202, Regularization: 0.027352\n",
      "2019-04-10 01:13:15,878 root         INFO     Train Epoch: 42 [5632/8000 (70%)]\tTotal Loss: 0.164122\n",
      "Reconstruction: 0.135477, Regularization: 0.028645\n",
      "2019-04-10 01:13:15,932 root         INFO     Train Epoch: 42 [6144/8000 (77%)]\tTotal Loss: 0.176787\n",
      "Reconstruction: 0.149748, Regularization: 0.027039\n",
      "2019-04-10 01:13:15,986 root         INFO     Train Epoch: 42 [6656/8000 (83%)]\tTotal Loss: 0.184282\n",
      "Reconstruction: 0.150348, Regularization: 0.033934\n",
      "2019-04-10 01:13:16,040 root         INFO     Train Epoch: 42 [7168/8000 (90%)]\tTotal Loss: 0.177033\n",
      "Reconstruction: 0.144321, Regularization: 0.032712\n",
      "2019-04-10 01:13:16,094 root         INFO     Train Epoch: 42 [7680/8000 (96%)]\tTotal Loss: 0.162619\n",
      "Reconstruction: 0.138154, Regularization: 0.024465\n",
      "2019-04-10 01:13:16,142 root         INFO     ====> Epoch: 42 Average loss: 0.1772\n",
      "2019-04-10 01:13:16,165 root         INFO     Train Epoch: 43 [0/8000 (0%)]\tTotal Loss: 0.188226\n",
      "Reconstruction: 0.156573, Regularization: 0.031653\n",
      "2019-04-10 01:13:16,221 root         INFO     Train Epoch: 43 [512/8000 (6%)]\tTotal Loss: 0.165239\n",
      "Reconstruction: 0.139484, Regularization: 0.025755\n",
      "2019-04-10 01:13:16,277 root         INFO     Train Epoch: 43 [1024/8000 (13%)]\tTotal Loss: 0.179023\n",
      "Reconstruction: 0.149161, Regularization: 0.029862\n",
      "2019-04-10 01:13:16,332 root         INFO     Train Epoch: 43 [1536/8000 (19%)]\tTotal Loss: 0.179915\n",
      "Reconstruction: 0.154956, Regularization: 0.024959\n",
      "2019-04-10 01:13:16,387 root         INFO     Train Epoch: 43 [2048/8000 (26%)]\tTotal Loss: 0.169818\n",
      "Reconstruction: 0.138193, Regularization: 0.031625\n",
      "2019-04-10 01:13:16,443 root         INFO     Train Epoch: 43 [2560/8000 (32%)]\tTotal Loss: 0.170954\n",
      "Reconstruction: 0.143907, Regularization: 0.027047\n",
      "2019-04-10 01:13:16,498 root         INFO     Train Epoch: 43 [3072/8000 (38%)]\tTotal Loss: 0.170380\n",
      "Reconstruction: 0.143129, Regularization: 0.027251\n",
      "2019-04-10 01:13:16,554 root         INFO     Train Epoch: 43 [3584/8000 (45%)]\tTotal Loss: 0.157789\n",
      "Reconstruction: 0.132235, Regularization: 0.025553\n",
      "2019-04-10 01:13:16,609 root         INFO     Train Epoch: 43 [4096/8000 (51%)]\tTotal Loss: 0.180508\n",
      "Reconstruction: 0.150742, Regularization: 0.029766\n",
      "2019-04-10 01:13:16,665 root         INFO     Train Epoch: 43 [4608/8000 (58%)]\tTotal Loss: 0.186123\n",
      "Reconstruction: 0.157814, Regularization: 0.028309\n",
      "2019-04-10 01:13:16,720 root         INFO     Train Epoch: 43 [5120/8000 (64%)]\tTotal Loss: 0.179138\n",
      "Reconstruction: 0.150253, Regularization: 0.028885\n",
      "2019-04-10 01:13:16,775 root         INFO     Train Epoch: 43 [5632/8000 (70%)]\tTotal Loss: 0.189234\n",
      "Reconstruction: 0.159509, Regularization: 0.029724\n",
      "2019-04-10 01:13:16,830 root         INFO     Train Epoch: 43 [6144/8000 (77%)]\tTotal Loss: 0.176465\n",
      "Reconstruction: 0.150755, Regularization: 0.025710\n",
      "2019-04-10 01:13:16,886 root         INFO     Train Epoch: 43 [6656/8000 (83%)]\tTotal Loss: 0.168327\n",
      "Reconstruction: 0.141377, Regularization: 0.026950\n",
      "2019-04-10 01:13:16,941 root         INFO     Train Epoch: 43 [7168/8000 (90%)]\tTotal Loss: 0.185368\n",
      "Reconstruction: 0.156167, Regularization: 0.029201\n",
      "2019-04-10 01:13:16,996 root         INFO     Train Epoch: 43 [7680/8000 (96%)]\tTotal Loss: 0.174846\n",
      "Reconstruction: 0.145014, Regularization: 0.029832\n",
      "2019-04-10 01:13:17,046 root         INFO     ====> Epoch: 43 Average loss: 0.1761\n",
      "2019-04-10 01:13:17,069 root         INFO     Train Epoch: 44 [0/8000 (0%)]\tTotal Loss: 0.174560\n",
      "Reconstruction: 0.146781, Regularization: 0.027779\n",
      "2019-04-10 01:13:17,125 root         INFO     Train Epoch: 44 [512/8000 (6%)]\tTotal Loss: 0.169900\n",
      "Reconstruction: 0.142213, Regularization: 0.027687\n",
      "2019-04-10 01:13:17,181 root         INFO     Train Epoch: 44 [1024/8000 (13%)]\tTotal Loss: 0.166357\n",
      "Reconstruction: 0.139136, Regularization: 0.027220\n",
      "2019-04-10 01:13:17,236 root         INFO     Train Epoch: 44 [1536/8000 (19%)]\tTotal Loss: 0.186352\n",
      "Reconstruction: 0.157385, Regularization: 0.028967\n",
      "2019-04-10 01:13:17,291 root         INFO     Train Epoch: 44 [2048/8000 (26%)]\tTotal Loss: 0.162568\n",
      "Reconstruction: 0.136526, Regularization: 0.026042\n",
      "2019-04-10 01:13:17,346 root         INFO     Train Epoch: 44 [2560/8000 (32%)]\tTotal Loss: 0.168130\n",
      "Reconstruction: 0.141360, Regularization: 0.026770\n",
      "2019-04-10 01:13:17,402 root         INFO     Train Epoch: 44 [3072/8000 (38%)]\tTotal Loss: 0.185857\n",
      "Reconstruction: 0.153430, Regularization: 0.032427\n",
      "2019-04-10 01:13:17,457 root         INFO     Train Epoch: 44 [3584/8000 (45%)]\tTotal Loss: 0.177470\n",
      "Reconstruction: 0.152600, Regularization: 0.024870\n",
      "2019-04-10 01:13:17,512 root         INFO     Train Epoch: 44 [4096/8000 (51%)]\tTotal Loss: 0.175122\n",
      "Reconstruction: 0.144066, Regularization: 0.031056\n",
      "2019-04-10 01:13:17,567 root         INFO     Train Epoch: 44 [4608/8000 (58%)]\tTotal Loss: 0.169734\n",
      "Reconstruction: 0.144695, Regularization: 0.025039\n",
      "2019-04-10 01:13:17,623 root         INFO     Train Epoch: 44 [5120/8000 (64%)]\tTotal Loss: 0.168732\n",
      "Reconstruction: 0.141440, Regularization: 0.027292\n",
      "2019-04-10 01:13:17,678 root         INFO     Train Epoch: 44 [5632/8000 (70%)]\tTotal Loss: 0.176417\n",
      "Reconstruction: 0.148451, Regularization: 0.027966\n",
      "2019-04-10 01:13:17,733 root         INFO     Train Epoch: 44 [6144/8000 (77%)]\tTotal Loss: 0.167386\n",
      "Reconstruction: 0.141702, Regularization: 0.025684\n",
      "2019-04-10 01:13:17,789 root         INFO     Train Epoch: 44 [6656/8000 (83%)]\tTotal Loss: 0.180596\n",
      "Reconstruction: 0.153394, Regularization: 0.027202\n",
      "2019-04-10 01:13:17,844 root         INFO     Train Epoch: 44 [7168/8000 (90%)]\tTotal Loss: 0.191003\n",
      "Reconstruction: 0.162666, Regularization: 0.028337\n",
      "2019-04-10 01:13:17,899 root         INFO     Train Epoch: 44 [7680/8000 (96%)]\tTotal Loss: 0.181687\n",
      "Reconstruction: 0.152030, Regularization: 0.029658\n",
      "2019-04-10 01:13:17,949 root         INFO     ====> Epoch: 44 Average loss: 0.1757\n",
      "2019-04-10 01:13:17,972 root         INFO     Train Epoch: 45 [0/8000 (0%)]\tTotal Loss: 0.182953\n",
      "Reconstruction: 0.155282, Regularization: 0.027672\n",
      "2019-04-10 01:13:18,028 root         INFO     Train Epoch: 45 [512/8000 (6%)]\tTotal Loss: 0.174304\n",
      "Reconstruction: 0.146979, Regularization: 0.027325\n",
      "2019-04-10 01:13:18,083 root         INFO     Train Epoch: 45 [1024/8000 (13%)]\tTotal Loss: 0.162698\n",
      "Reconstruction: 0.138728, Regularization: 0.023970\n",
      "2019-04-10 01:13:18,139 root         INFO     Train Epoch: 45 [1536/8000 (19%)]\tTotal Loss: 0.179259\n",
      "Reconstruction: 0.149002, Regularization: 0.030257\n",
      "2019-04-10 01:13:18,195 root         INFO     Train Epoch: 45 [2048/8000 (26%)]\tTotal Loss: 0.170993\n",
      "Reconstruction: 0.143229, Regularization: 0.027763\n",
      "2019-04-10 01:13:18,251 root         INFO     Train Epoch: 45 [2560/8000 (32%)]\tTotal Loss: 0.178719\n",
      "Reconstruction: 0.148300, Regularization: 0.030419\n",
      "2019-04-10 01:13:18,307 root         INFO     Train Epoch: 45 [3072/8000 (38%)]\tTotal Loss: 0.182470\n",
      "Reconstruction: 0.154769, Regularization: 0.027701\n",
      "2019-04-10 01:13:18,363 root         INFO     Train Epoch: 45 [3584/8000 (45%)]\tTotal Loss: 0.167585\n",
      "Reconstruction: 0.138949, Regularization: 0.028637\n",
      "2019-04-10 01:13:18,418 root         INFO     Train Epoch: 45 [4096/8000 (51%)]\tTotal Loss: 0.183800\n",
      "Reconstruction: 0.155231, Regularization: 0.028570\n",
      "2019-04-10 01:13:18,473 root         INFO     Train Epoch: 45 [4608/8000 (58%)]\tTotal Loss: 0.173762\n",
      "Reconstruction: 0.142109, Regularization: 0.031653\n",
      "2019-04-10 01:13:18,527 root         INFO     Train Epoch: 45 [5120/8000 (64%)]\tTotal Loss: 0.179101\n",
      "Reconstruction: 0.152364, Regularization: 0.026737\n",
      "2019-04-10 01:13:18,582 root         INFO     Train Epoch: 45 [5632/8000 (70%)]\tTotal Loss: 0.179381\n",
      "Reconstruction: 0.150516, Regularization: 0.028864\n",
      "2019-04-10 01:13:18,637 root         INFO     Train Epoch: 45 [6144/8000 (77%)]\tTotal Loss: 0.179528\n",
      "Reconstruction: 0.155513, Regularization: 0.024015\n",
      "2019-04-10 01:13:18,692 root         INFO     Train Epoch: 45 [6656/8000 (83%)]\tTotal Loss: 0.177156\n",
      "Reconstruction: 0.146039, Regularization: 0.031117\n",
      "2019-04-10 01:13:18,747 root         INFO     Train Epoch: 45 [7168/8000 (90%)]\tTotal Loss: 0.167943\n",
      "Reconstruction: 0.141554, Regularization: 0.026389\n",
      "2019-04-10 01:13:18,802 root         INFO     Train Epoch: 45 [7680/8000 (96%)]\tTotal Loss: 0.165818\n",
      "Reconstruction: 0.142076, Regularization: 0.023742\n",
      "2019-04-10 01:13:18,851 root         INFO     ====> Epoch: 45 Average loss: 0.1748\n",
      "2019-04-10 01:13:18,874 root         INFO     Train Epoch: 46 [0/8000 (0%)]\tTotal Loss: 0.194848\n",
      "Reconstruction: 0.162395, Regularization: 0.032453\n",
      "2019-04-10 01:13:18,931 root         INFO     Train Epoch: 46 [512/8000 (6%)]\tTotal Loss: 0.176125\n",
      "Reconstruction: 0.147933, Regularization: 0.028193\n",
      "2019-04-10 01:13:18,987 root         INFO     Train Epoch: 46 [1024/8000 (13%)]\tTotal Loss: 0.174779\n",
      "Reconstruction: 0.149780, Regularization: 0.025000\n",
      "2019-04-10 01:13:19,043 root         INFO     Train Epoch: 46 [1536/8000 (19%)]\tTotal Loss: 0.159985\n",
      "Reconstruction: 0.137882, Regularization: 0.022103\n",
      "2019-04-10 01:13:19,098 root         INFO     Train Epoch: 46 [2048/8000 (26%)]\tTotal Loss: 0.180234\n",
      "Reconstruction: 0.152065, Regularization: 0.028169\n",
      "2019-04-10 01:13:19,154 root         INFO     Train Epoch: 46 [2560/8000 (32%)]\tTotal Loss: 0.163513\n",
      "Reconstruction: 0.139217, Regularization: 0.024295\n",
      "2019-04-10 01:13:19,209 root         INFO     Train Epoch: 46 [3072/8000 (38%)]\tTotal Loss: 0.168888\n",
      "Reconstruction: 0.141347, Regularization: 0.027541\n",
      "2019-04-10 01:13:19,264 root         INFO     Train Epoch: 46 [3584/8000 (45%)]\tTotal Loss: 0.196814\n",
      "Reconstruction: 0.163444, Regularization: 0.033370\n",
      "2019-04-10 01:13:19,320 root         INFO     Train Epoch: 46 [4096/8000 (51%)]\tTotal Loss: 0.180397\n",
      "Reconstruction: 0.150250, Regularization: 0.030147\n",
      "2019-04-10 01:13:19,375 root         INFO     Train Epoch: 46 [4608/8000 (58%)]\tTotal Loss: 0.171558\n",
      "Reconstruction: 0.146086, Regularization: 0.025472\n",
      "2019-04-10 01:13:19,430 root         INFO     Train Epoch: 46 [5120/8000 (64%)]\tTotal Loss: 0.173735\n",
      "Reconstruction: 0.144163, Regularization: 0.029572\n",
      "2019-04-10 01:13:19,486 root         INFO     Train Epoch: 46 [5632/8000 (70%)]\tTotal Loss: 0.167302\n",
      "Reconstruction: 0.142111, Regularization: 0.025191\n",
      "2019-04-10 01:13:19,542 root         INFO     Train Epoch: 46 [6144/8000 (77%)]\tTotal Loss: 0.184183\n",
      "Reconstruction: 0.155347, Regularization: 0.028836\n",
      "2019-04-10 01:13:19,598 root         INFO     Train Epoch: 46 [6656/8000 (83%)]\tTotal Loss: 0.160739\n",
      "Reconstruction: 0.137548, Regularization: 0.023191\n",
      "2019-04-10 01:13:19,653 root         INFO     Train Epoch: 46 [7168/8000 (90%)]\tTotal Loss: 0.171101\n",
      "Reconstruction: 0.146523, Regularization: 0.024577\n",
      "2019-04-10 01:13:19,709 root         INFO     Train Epoch: 46 [7680/8000 (96%)]\tTotal Loss: 0.182244\n",
      "Reconstruction: 0.153945, Regularization: 0.028300\n",
      "2019-04-10 01:13:19,758 root         INFO     ====> Epoch: 46 Average loss: 0.1744\n",
      "2019-04-10 01:13:19,781 root         INFO     Train Epoch: 47 [0/8000 (0%)]\tTotal Loss: 0.171717\n",
      "Reconstruction: 0.149773, Regularization: 0.021944\n",
      "2019-04-10 01:13:19,836 root         INFO     Train Epoch: 47 [512/8000 (6%)]\tTotal Loss: 0.180468\n",
      "Reconstruction: 0.152371, Regularization: 0.028097\n",
      "2019-04-10 01:13:19,890 root         INFO     Train Epoch: 47 [1024/8000 (13%)]\tTotal Loss: 0.183108\n",
      "Reconstruction: 0.152616, Regularization: 0.030492\n",
      "2019-04-10 01:13:19,945 root         INFO     Train Epoch: 47 [1536/8000 (19%)]\tTotal Loss: 0.152518\n",
      "Reconstruction: 0.127386, Regularization: 0.025133\n",
      "2019-04-10 01:13:20,000 root         INFO     Train Epoch: 47 [2048/8000 (26%)]\tTotal Loss: 0.176945\n",
      "Reconstruction: 0.153695, Regularization: 0.023250\n",
      "2019-04-10 01:13:20,056 root         INFO     Train Epoch: 47 [2560/8000 (32%)]\tTotal Loss: 0.166778\n",
      "Reconstruction: 0.141010, Regularization: 0.025768\n",
      "2019-04-10 01:13:20,111 root         INFO     Train Epoch: 47 [3072/8000 (38%)]\tTotal Loss: 0.165943\n",
      "Reconstruction: 0.144413, Regularization: 0.021530\n",
      "2019-04-10 01:13:20,167 root         INFO     Train Epoch: 47 [3584/8000 (45%)]\tTotal Loss: 0.161776\n",
      "Reconstruction: 0.139989, Regularization: 0.021787\n",
      "2019-04-10 01:13:20,222 root         INFO     Train Epoch: 47 [4096/8000 (51%)]\tTotal Loss: 0.159258\n",
      "Reconstruction: 0.136298, Regularization: 0.022960\n",
      "2019-04-10 01:13:20,278 root         INFO     Train Epoch: 47 [4608/8000 (58%)]\tTotal Loss: 0.165057\n",
      "Reconstruction: 0.139847, Regularization: 0.025210\n",
      "2019-04-10 01:13:20,333 root         INFO     Train Epoch: 47 [5120/8000 (64%)]\tTotal Loss: 0.170634\n",
      "Reconstruction: 0.140439, Regularization: 0.030195\n",
      "2019-04-10 01:13:20,388 root         INFO     Train Epoch: 47 [5632/8000 (70%)]\tTotal Loss: 0.157078\n",
      "Reconstruction: 0.133128, Regularization: 0.023950\n",
      "2019-04-10 01:13:20,444 root         INFO     Train Epoch: 47 [6144/8000 (77%)]\tTotal Loss: 0.177172\n",
      "Reconstruction: 0.145830, Regularization: 0.031342\n",
      "2019-04-10 01:13:20,498 root         INFO     Train Epoch: 47 [6656/8000 (83%)]\tTotal Loss: 0.182054\n",
      "Reconstruction: 0.151661, Regularization: 0.030393\n",
      "2019-04-10 01:13:20,554 root         INFO     Train Epoch: 47 [7168/8000 (90%)]\tTotal Loss: 0.174179\n",
      "Reconstruction: 0.147813, Regularization: 0.026366\n",
      "2019-04-10 01:13:20,609 root         INFO     Train Epoch: 47 [7680/8000 (96%)]\tTotal Loss: 0.175657\n",
      "Reconstruction: 0.149892, Regularization: 0.025765\n",
      "2019-04-10 01:13:20,659 root         INFO     ====> Epoch: 47 Average loss: 0.1735\n",
      "2019-04-10 01:13:20,683 root         INFO     Train Epoch: 48 [0/8000 (0%)]\tTotal Loss: 0.176882\n",
      "Reconstruction: 0.148460, Regularization: 0.028422\n",
      "2019-04-10 01:13:20,738 root         INFO     Train Epoch: 48 [512/8000 (6%)]\tTotal Loss: 0.193432\n",
      "Reconstruction: 0.163559, Regularization: 0.029873\n",
      "2019-04-10 01:13:20,793 root         INFO     Train Epoch: 48 [1024/8000 (13%)]\tTotal Loss: 0.188535\n",
      "Reconstruction: 0.159920, Regularization: 0.028615\n",
      "2019-04-10 01:13:20,848 root         INFO     Train Epoch: 48 [1536/8000 (19%)]\tTotal Loss: 0.181645\n",
      "Reconstruction: 0.149921, Regularization: 0.031725\n",
      "2019-04-10 01:13:20,902 root         INFO     Train Epoch: 48 [2048/8000 (26%)]\tTotal Loss: 0.179546\n",
      "Reconstruction: 0.153901, Regularization: 0.025645\n",
      "2019-04-10 01:13:20,957 root         INFO     Train Epoch: 48 [2560/8000 (32%)]\tTotal Loss: 0.181924\n",
      "Reconstruction: 0.154467, Regularization: 0.027457\n",
      "2019-04-10 01:13:21,014 root         INFO     Train Epoch: 48 [3072/8000 (38%)]\tTotal Loss: 0.158277\n",
      "Reconstruction: 0.135489, Regularization: 0.022789\n",
      "2019-04-10 01:13:21,072 root         INFO     Train Epoch: 48 [3584/8000 (45%)]\tTotal Loss: 0.160165\n",
      "Reconstruction: 0.135284, Regularization: 0.024881\n",
      "2019-04-10 01:13:21,129 root         INFO     Train Epoch: 48 [4096/8000 (51%)]\tTotal Loss: 0.178726\n",
      "Reconstruction: 0.151342, Regularization: 0.027384\n",
      "2019-04-10 01:13:21,186 root         INFO     Train Epoch: 48 [4608/8000 (58%)]\tTotal Loss: 0.173251\n",
      "Reconstruction: 0.146957, Regularization: 0.026294\n",
      "2019-04-10 01:13:21,243 root         INFO     Train Epoch: 48 [5120/8000 (64%)]\tTotal Loss: 0.201335\n",
      "Reconstruction: 0.178127, Regularization: 0.023209\n",
      "2019-04-10 01:13:21,300 root         INFO     Train Epoch: 48 [5632/8000 (70%)]\tTotal Loss: 0.175894\n",
      "Reconstruction: 0.153246, Regularization: 0.022648\n",
      "2019-04-10 01:13:21,357 root         INFO     Train Epoch: 48 [6144/8000 (77%)]\tTotal Loss: 0.147641\n",
      "Reconstruction: 0.127595, Regularization: 0.020046\n",
      "2019-04-10 01:13:21,413 root         INFO     Train Epoch: 48 [6656/8000 (83%)]\tTotal Loss: 0.189517\n",
      "Reconstruction: 0.161826, Regularization: 0.027690\n",
      "2019-04-10 01:13:21,470 root         INFO     Train Epoch: 48 [7168/8000 (90%)]\tTotal Loss: 0.177065\n",
      "Reconstruction: 0.151244, Regularization: 0.025820\n",
      "2019-04-10 01:13:21,528 root         INFO     Train Epoch: 48 [7680/8000 (96%)]\tTotal Loss: 0.177278\n",
      "Reconstruction: 0.152175, Regularization: 0.025103\n",
      "2019-04-10 01:13:21,578 root         INFO     ====> Epoch: 48 Average loss: 0.1734\n",
      "2019-04-10 01:13:21,601 root         INFO     Train Epoch: 49 [0/8000 (0%)]\tTotal Loss: 0.167757\n",
      "Reconstruction: 0.141317, Regularization: 0.026441\n",
      "2019-04-10 01:13:21,658 root         INFO     Train Epoch: 49 [512/8000 (6%)]\tTotal Loss: 0.185000\n",
      "Reconstruction: 0.155389, Regularization: 0.029611\n",
      "2019-04-10 01:13:21,715 root         INFO     Train Epoch: 49 [1024/8000 (13%)]\tTotal Loss: 0.155393\n",
      "Reconstruction: 0.132589, Regularization: 0.022804\n",
      "2019-04-10 01:13:21,770 root         INFO     Train Epoch: 49 [1536/8000 (19%)]\tTotal Loss: 0.184842\n",
      "Reconstruction: 0.158015, Regularization: 0.026826\n",
      "2019-04-10 01:13:21,825 root         INFO     Train Epoch: 49 [2048/8000 (26%)]\tTotal Loss: 0.167265\n",
      "Reconstruction: 0.142461, Regularization: 0.024804\n",
      "2019-04-10 01:13:21,881 root         INFO     Train Epoch: 49 [2560/8000 (32%)]\tTotal Loss: 0.166784\n",
      "Reconstruction: 0.143284, Regularization: 0.023500\n",
      "2019-04-10 01:13:21,936 root         INFO     Train Epoch: 49 [3072/8000 (38%)]\tTotal Loss: 0.174163\n",
      "Reconstruction: 0.146483, Regularization: 0.027680\n",
      "2019-04-10 01:13:21,993 root         INFO     Train Epoch: 49 [3584/8000 (45%)]\tTotal Loss: 0.191742\n",
      "Reconstruction: 0.164478, Regularization: 0.027264\n",
      "2019-04-10 01:13:22,049 root         INFO     Train Epoch: 49 [4096/8000 (51%)]\tTotal Loss: 0.155303\n",
      "Reconstruction: 0.135221, Regularization: 0.020083\n",
      "2019-04-10 01:13:22,105 root         INFO     Train Epoch: 49 [4608/8000 (58%)]\tTotal Loss: 0.149819\n",
      "Reconstruction: 0.129286, Regularization: 0.020534\n",
      "2019-04-10 01:13:22,162 root         INFO     Train Epoch: 49 [5120/8000 (64%)]\tTotal Loss: 0.170998\n",
      "Reconstruction: 0.144464, Regularization: 0.026534\n",
      "2019-04-10 01:13:22,219 root         INFO     Train Epoch: 49 [5632/8000 (70%)]\tTotal Loss: 0.185066\n",
      "Reconstruction: 0.154600, Regularization: 0.030466\n",
      "2019-04-10 01:13:22,276 root         INFO     Train Epoch: 49 [6144/8000 (77%)]\tTotal Loss: 0.171180\n",
      "Reconstruction: 0.146476, Regularization: 0.024704\n",
      "2019-04-10 01:13:22,332 root         INFO     Train Epoch: 49 [6656/8000 (83%)]\tTotal Loss: 0.180292\n",
      "Reconstruction: 0.149617, Regularization: 0.030676\n",
      "2019-04-10 01:13:22,388 root         INFO     Train Epoch: 49 [7168/8000 (90%)]\tTotal Loss: 0.157848\n",
      "Reconstruction: 0.132760, Regularization: 0.025088\n",
      "2019-04-10 01:13:22,444 root         INFO     Train Epoch: 49 [7680/8000 (96%)]\tTotal Loss: 0.173115\n",
      "Reconstruction: 0.151432, Regularization: 0.021683\n",
      "2019-04-10 01:13:22,493 root         INFO     ====> Epoch: 49 Average loss: 0.1723\n",
      "2019-04-10 01:13:22,518 root         INFO     Train Epoch: 50 [0/8000 (0%)]\tTotal Loss: 0.165473\n",
      "Reconstruction: 0.140826, Regularization: 0.024648\n",
      "2019-04-10 01:13:22,573 root         INFO     Train Epoch: 50 [512/8000 (6%)]\tTotal Loss: 0.173610\n",
      "Reconstruction: 0.151312, Regularization: 0.022298\n",
      "2019-04-10 01:13:22,630 root         INFO     Train Epoch: 50 [1024/8000 (13%)]\tTotal Loss: 0.173856\n",
      "Reconstruction: 0.148165, Regularization: 0.025691\n",
      "2019-04-10 01:13:22,686 root         INFO     Train Epoch: 50 [1536/8000 (19%)]\tTotal Loss: 0.177900\n",
      "Reconstruction: 0.151099, Regularization: 0.026801\n",
      "2019-04-10 01:13:22,741 root         INFO     Train Epoch: 50 [2048/8000 (26%)]\tTotal Loss: 0.152758\n",
      "Reconstruction: 0.132218, Regularization: 0.020540\n",
      "2019-04-10 01:13:22,796 root         INFO     Train Epoch: 50 [2560/8000 (32%)]\tTotal Loss: 0.170827\n",
      "Reconstruction: 0.151037, Regularization: 0.019790\n",
      "2019-04-10 01:13:22,851 root         INFO     Train Epoch: 50 [3072/8000 (38%)]\tTotal Loss: 0.163706\n",
      "Reconstruction: 0.142069, Regularization: 0.021637\n",
      "2019-04-10 01:13:22,906 root         INFO     Train Epoch: 50 [3584/8000 (45%)]\tTotal Loss: 0.162680\n",
      "Reconstruction: 0.140672, Regularization: 0.022008\n",
      "2019-04-10 01:13:22,960 root         INFO     Train Epoch: 50 [4096/8000 (51%)]\tTotal Loss: 0.167082\n",
      "Reconstruction: 0.142775, Regularization: 0.024307\n",
      "2019-04-10 01:13:23,016 root         INFO     Train Epoch: 50 [4608/8000 (58%)]\tTotal Loss: 0.180972\n",
      "Reconstruction: 0.154936, Regularization: 0.026036\n",
      "2019-04-10 01:13:23,071 root         INFO     Train Epoch: 50 [5120/8000 (64%)]\tTotal Loss: 0.158852\n",
      "Reconstruction: 0.136092, Regularization: 0.022759\n",
      "2019-04-10 01:13:23,125 root         INFO     Train Epoch: 50 [5632/8000 (70%)]\tTotal Loss: 0.175313\n",
      "Reconstruction: 0.153107, Regularization: 0.022207\n",
      "2019-04-10 01:13:23,180 root         INFO     Train Epoch: 50 [6144/8000 (77%)]\tTotal Loss: 0.184882\n",
      "Reconstruction: 0.159782, Regularization: 0.025100\n",
      "2019-04-10 01:13:23,235 root         INFO     Train Epoch: 50 [6656/8000 (83%)]\tTotal Loss: 0.160831\n",
      "Reconstruction: 0.135969, Regularization: 0.024862\n",
      "2019-04-10 01:13:23,290 root         INFO     Train Epoch: 50 [7168/8000 (90%)]\tTotal Loss: 0.173694\n",
      "Reconstruction: 0.149963, Regularization: 0.023731\n",
      "2019-04-10 01:13:23,345 root         INFO     Train Epoch: 50 [7680/8000 (96%)]\tTotal Loss: 0.154899\n",
      "Reconstruction: 0.133267, Regularization: 0.021633\n",
      "2019-04-10 01:13:23,394 root         INFO     ====> Epoch: 50 Average loss: 0.1712\n",
      "2019-04-10 01:13:23,417 root         INFO     Train Epoch: 51 [0/8000 (0%)]\tTotal Loss: 0.172921\n",
      "Reconstruction: 0.147213, Regularization: 0.025708\n",
      "2019-04-10 01:13:23,474 root         INFO     Train Epoch: 51 [512/8000 (6%)]\tTotal Loss: 0.171625\n",
      "Reconstruction: 0.145572, Regularization: 0.026053\n",
      "2019-04-10 01:13:23,531 root         INFO     Train Epoch: 51 [1024/8000 (13%)]\tTotal Loss: 0.195218\n",
      "Reconstruction: 0.164647, Regularization: 0.030571\n",
      "2019-04-10 01:13:23,587 root         INFO     Train Epoch: 51 [1536/8000 (19%)]\tTotal Loss: 0.175229\n",
      "Reconstruction: 0.148307, Regularization: 0.026921\n",
      "2019-04-10 01:13:23,644 root         INFO     Train Epoch: 51 [2048/8000 (26%)]\tTotal Loss: 0.186466\n",
      "Reconstruction: 0.162560, Regularization: 0.023905\n",
      "2019-04-10 01:13:23,700 root         INFO     Train Epoch: 51 [2560/8000 (32%)]\tTotal Loss: 0.160705\n",
      "Reconstruction: 0.139313, Regularization: 0.021392\n",
      "2019-04-10 01:13:23,756 root         INFO     Train Epoch: 51 [3072/8000 (38%)]\tTotal Loss: 0.164874\n",
      "Reconstruction: 0.144378, Regularization: 0.020496\n",
      "2019-04-10 01:13:23,812 root         INFO     Train Epoch: 51 [3584/8000 (45%)]\tTotal Loss: 0.180569\n",
      "Reconstruction: 0.158763, Regularization: 0.021806\n",
      "2019-04-10 01:13:23,867 root         INFO     Train Epoch: 51 [4096/8000 (51%)]\tTotal Loss: 0.161473\n",
      "Reconstruction: 0.141454, Regularization: 0.020019\n",
      "2019-04-10 01:13:23,922 root         INFO     Train Epoch: 51 [4608/8000 (58%)]\tTotal Loss: 0.174057\n",
      "Reconstruction: 0.150668, Regularization: 0.023388\n",
      "2019-04-10 01:13:23,977 root         INFO     Train Epoch: 51 [5120/8000 (64%)]\tTotal Loss: 0.172795\n",
      "Reconstruction: 0.146740, Regularization: 0.026055\n",
      "2019-04-10 01:13:24,032 root         INFO     Train Epoch: 51 [5632/8000 (70%)]\tTotal Loss: 0.158762\n",
      "Reconstruction: 0.137301, Regularization: 0.021461\n",
      "2019-04-10 01:13:24,087 root         INFO     Train Epoch: 51 [6144/8000 (77%)]\tTotal Loss: 0.159856\n",
      "Reconstruction: 0.139755, Regularization: 0.020101\n",
      "2019-04-10 01:13:24,143 root         INFO     Train Epoch: 51 [6656/8000 (83%)]\tTotal Loss: 0.195823\n",
      "Reconstruction: 0.172634, Regularization: 0.023189\n",
      "2019-04-10 01:13:24,198 root         INFO     Train Epoch: 51 [7168/8000 (90%)]\tTotal Loss: 0.166309\n",
      "Reconstruction: 0.143686, Regularization: 0.022623\n",
      "2019-04-10 01:13:24,253 root         INFO     Train Epoch: 51 [7680/8000 (96%)]\tTotal Loss: 0.156923\n",
      "Reconstruction: 0.134989, Regularization: 0.021933\n",
      "2019-04-10 01:13:24,303 root         INFO     ====> Epoch: 51 Average loss: 0.1704\n",
      "2019-04-10 01:13:24,326 root         INFO     Train Epoch: 52 [0/8000 (0%)]\tTotal Loss: 0.183358\n",
      "Reconstruction: 0.154242, Regularization: 0.029116\n",
      "2019-04-10 01:13:24,383 root         INFO     Train Epoch: 52 [512/8000 (6%)]\tTotal Loss: 0.172838\n",
      "Reconstruction: 0.148109, Regularization: 0.024729\n",
      "2019-04-10 01:13:24,440 root         INFO     Train Epoch: 52 [1024/8000 (13%)]\tTotal Loss: 0.171921\n",
      "Reconstruction: 0.151868, Regularization: 0.020053\n",
      "2019-04-10 01:13:24,496 root         INFO     Train Epoch: 52 [1536/8000 (19%)]\tTotal Loss: 0.183017\n",
      "Reconstruction: 0.155066, Regularization: 0.027951\n",
      "2019-04-10 01:13:24,552 root         INFO     Train Epoch: 52 [2048/8000 (26%)]\tTotal Loss: 0.176108\n",
      "Reconstruction: 0.148244, Regularization: 0.027864\n",
      "2019-04-10 01:13:24,609 root         INFO     Train Epoch: 52 [2560/8000 (32%)]\tTotal Loss: 0.185277\n",
      "Reconstruction: 0.158324, Regularization: 0.026953\n",
      "2019-04-10 01:13:24,665 root         INFO     Train Epoch: 52 [3072/8000 (38%)]\tTotal Loss: 0.157464\n",
      "Reconstruction: 0.138669, Regularization: 0.018795\n",
      "2019-04-10 01:13:24,721 root         INFO     Train Epoch: 52 [3584/8000 (45%)]\tTotal Loss: 0.168521\n",
      "Reconstruction: 0.144991, Regularization: 0.023530\n",
      "2019-04-10 01:13:24,778 root         INFO     Train Epoch: 52 [4096/8000 (51%)]\tTotal Loss: 0.178152\n",
      "Reconstruction: 0.153988, Regularization: 0.024163\n",
      "2019-04-10 01:13:24,834 root         INFO     Train Epoch: 52 [4608/8000 (58%)]\tTotal Loss: 0.184865\n",
      "Reconstruction: 0.156514, Regularization: 0.028351\n",
      "2019-04-10 01:13:24,890 root         INFO     Train Epoch: 52 [5120/8000 (64%)]\tTotal Loss: 0.180029\n",
      "Reconstruction: 0.151988, Regularization: 0.028041\n",
      "2019-04-10 01:13:24,946 root         INFO     Train Epoch: 52 [5632/8000 (70%)]\tTotal Loss: 0.157690\n",
      "Reconstruction: 0.135676, Regularization: 0.022014\n",
      "2019-04-10 01:13:25,003 root         INFO     Train Epoch: 52 [6144/8000 (77%)]\tTotal Loss: 0.165414\n",
      "Reconstruction: 0.143008, Regularization: 0.022406\n",
      "2019-04-10 01:13:25,059 root         INFO     Train Epoch: 52 [6656/8000 (83%)]\tTotal Loss: 0.174300\n",
      "Reconstruction: 0.152999, Regularization: 0.021300\n",
      "2019-04-10 01:13:25,116 root         INFO     Train Epoch: 52 [7168/8000 (90%)]\tTotal Loss: 0.162252\n",
      "Reconstruction: 0.143566, Regularization: 0.018686\n",
      "2019-04-10 01:13:25,172 root         INFO     Train Epoch: 52 [7680/8000 (96%)]\tTotal Loss: 0.173735\n",
      "Reconstruction: 0.148907, Regularization: 0.024828\n",
      "2019-04-10 01:13:25,221 root         INFO     ====> Epoch: 52 Average loss: 0.1703\n",
      "2019-04-10 01:13:25,246 root         INFO     Train Epoch: 53 [0/8000 (0%)]\tTotal Loss: 0.161778\n",
      "Reconstruction: 0.142415, Regularization: 0.019363\n",
      "2019-04-10 01:13:25,302 root         INFO     Train Epoch: 53 [512/8000 (6%)]\tTotal Loss: 0.182014\n",
      "Reconstruction: 0.152835, Regularization: 0.029179\n",
      "2019-04-10 01:13:25,359 root         INFO     Train Epoch: 53 [1024/8000 (13%)]\tTotal Loss: 0.168636\n",
      "Reconstruction: 0.143956, Regularization: 0.024680\n",
      "2019-04-10 01:13:25,416 root         INFO     Train Epoch: 53 [1536/8000 (19%)]\tTotal Loss: 0.170526\n",
      "Reconstruction: 0.147903, Regularization: 0.022623\n",
      "2019-04-10 01:13:25,473 root         INFO     Train Epoch: 53 [2048/8000 (26%)]\tTotal Loss: 0.161330\n",
      "Reconstruction: 0.138893, Regularization: 0.022437\n",
      "2019-04-10 01:13:25,530 root         INFO     Train Epoch: 53 [2560/8000 (32%)]\tTotal Loss: 0.164166\n",
      "Reconstruction: 0.142744, Regularization: 0.021422\n",
      "2019-04-10 01:13:25,586 root         INFO     Train Epoch: 53 [3072/8000 (38%)]\tTotal Loss: 0.159768\n",
      "Reconstruction: 0.142283, Regularization: 0.017485\n",
      "2019-04-10 01:13:25,641 root         INFO     Train Epoch: 53 [3584/8000 (45%)]\tTotal Loss: 0.159145\n",
      "Reconstruction: 0.139444, Regularization: 0.019701\n",
      "2019-04-10 01:13:25,698 root         INFO     Train Epoch: 53 [4096/8000 (51%)]\tTotal Loss: 0.165608\n",
      "Reconstruction: 0.146949, Regularization: 0.018659\n",
      "2019-04-10 01:13:25,753 root         INFO     Train Epoch: 53 [4608/8000 (58%)]\tTotal Loss: 0.157943\n",
      "Reconstruction: 0.134719, Regularization: 0.023224\n",
      "2019-04-10 01:13:25,809 root         INFO     Train Epoch: 53 [5120/8000 (64%)]\tTotal Loss: 0.161697\n",
      "Reconstruction: 0.138756, Regularization: 0.022941\n",
      "2019-04-10 01:13:25,865 root         INFO     Train Epoch: 53 [5632/8000 (70%)]\tTotal Loss: 0.154386\n",
      "Reconstruction: 0.135737, Regularization: 0.018649\n",
      "2019-04-10 01:13:25,920 root         INFO     Train Epoch: 53 [6144/8000 (77%)]\tTotal Loss: 0.162468\n",
      "Reconstruction: 0.140627, Regularization: 0.021841\n",
      "2019-04-10 01:13:25,976 root         INFO     Train Epoch: 53 [6656/8000 (83%)]\tTotal Loss: 0.172549\n",
      "Reconstruction: 0.149510, Regularization: 0.023039\n",
      "2019-04-10 01:13:26,032 root         INFO     Train Epoch: 53 [7168/8000 (90%)]\tTotal Loss: 0.169595\n",
      "Reconstruction: 0.145411, Regularization: 0.024184\n",
      "2019-04-10 01:13:26,088 root         INFO     Train Epoch: 53 [7680/8000 (96%)]\tTotal Loss: 0.166480\n",
      "Reconstruction: 0.143999, Regularization: 0.022481\n",
      "2019-04-10 01:13:26,138 root         INFO     ====> Epoch: 53 Average loss: 0.1706\n",
      "2019-04-10 01:13:26,161 root         INFO     Train Epoch: 54 [0/8000 (0%)]\tTotal Loss: 0.198446\n",
      "Reconstruction: 0.171401, Regularization: 0.027044\n",
      "2019-04-10 01:13:26,218 root         INFO     Train Epoch: 54 [512/8000 (6%)]\tTotal Loss: 0.166452\n",
      "Reconstruction: 0.142929, Regularization: 0.023523\n",
      "2019-04-10 01:13:26,275 root         INFO     Train Epoch: 54 [1024/8000 (13%)]\tTotal Loss: 0.171837\n",
      "Reconstruction: 0.149232, Regularization: 0.022606\n",
      "2019-04-10 01:13:26,332 root         INFO     Train Epoch: 54 [1536/8000 (19%)]\tTotal Loss: 0.166179\n",
      "Reconstruction: 0.140460, Regularization: 0.025720\n",
      "2019-04-10 01:13:26,388 root         INFO     Train Epoch: 54 [2048/8000 (26%)]\tTotal Loss: 0.161270\n",
      "Reconstruction: 0.140439, Regularization: 0.020830\n",
      "2019-04-10 01:13:26,445 root         INFO     Train Epoch: 54 [2560/8000 (32%)]\tTotal Loss: 0.157629\n",
      "Reconstruction: 0.139228, Regularization: 0.018401\n",
      "2019-04-10 01:13:26,501 root         INFO     Train Epoch: 54 [3072/8000 (38%)]\tTotal Loss: 0.165585\n",
      "Reconstruction: 0.143551, Regularization: 0.022034\n",
      "2019-04-10 01:13:26,558 root         INFO     Train Epoch: 54 [3584/8000 (45%)]\tTotal Loss: 0.205500\n",
      "Reconstruction: 0.173801, Regularization: 0.031699\n",
      "2019-04-10 01:13:26,614 root         INFO     Train Epoch: 54 [4096/8000 (51%)]\tTotal Loss: 0.179514\n",
      "Reconstruction: 0.153748, Regularization: 0.025766\n",
      "2019-04-10 01:13:26,670 root         INFO     Train Epoch: 54 [4608/8000 (58%)]\tTotal Loss: 0.167439\n",
      "Reconstruction: 0.147119, Regularization: 0.020320\n",
      "2019-04-10 01:13:26,726 root         INFO     Train Epoch: 54 [5120/8000 (64%)]\tTotal Loss: 0.175857\n",
      "Reconstruction: 0.152048, Regularization: 0.023809\n",
      "2019-04-10 01:13:26,781 root         INFO     Train Epoch: 54 [5632/8000 (70%)]\tTotal Loss: 0.170774\n",
      "Reconstruction: 0.142228, Regularization: 0.028546\n",
      "2019-04-10 01:13:26,838 root         INFO     Train Epoch: 54 [6144/8000 (77%)]\tTotal Loss: 0.171686\n",
      "Reconstruction: 0.148676, Regularization: 0.023010\n",
      "2019-04-10 01:13:26,894 root         INFO     Train Epoch: 54 [6656/8000 (83%)]\tTotal Loss: 0.179629\n",
      "Reconstruction: 0.155220, Regularization: 0.024409\n",
      "2019-04-10 01:13:26,951 root         INFO     Train Epoch: 54 [7168/8000 (90%)]\tTotal Loss: 0.161344\n",
      "Reconstruction: 0.141256, Regularization: 0.020088\n",
      "2019-04-10 01:13:27,008 root         INFO     Train Epoch: 54 [7680/8000 (96%)]\tTotal Loss: 0.160588\n",
      "Reconstruction: 0.140744, Regularization: 0.019844\n",
      "2019-04-10 01:13:27,057 root         INFO     ====> Epoch: 54 Average loss: 0.1699\n",
      "2019-04-10 01:13:27,081 root         INFO     Train Epoch: 55 [0/8000 (0%)]\tTotal Loss: 0.170235\n",
      "Reconstruction: 0.148161, Regularization: 0.022074\n",
      "2019-04-10 01:13:27,139 root         INFO     Train Epoch: 55 [512/8000 (6%)]\tTotal Loss: 0.177491\n",
      "Reconstruction: 0.156860, Regularization: 0.020631\n",
      "2019-04-10 01:13:27,196 root         INFO     Train Epoch: 55 [1024/8000 (13%)]\tTotal Loss: 0.175566\n",
      "Reconstruction: 0.154674, Regularization: 0.020892\n",
      "2019-04-10 01:13:27,253 root         INFO     Train Epoch: 55 [1536/8000 (19%)]\tTotal Loss: 0.162373\n",
      "Reconstruction: 0.142115, Regularization: 0.020258\n",
      "2019-04-10 01:13:27,310 root         INFO     Train Epoch: 55 [2048/8000 (26%)]\tTotal Loss: 0.180566\n",
      "Reconstruction: 0.152842, Regularization: 0.027725\n",
      "2019-04-10 01:13:27,368 root         INFO     Train Epoch: 55 [2560/8000 (32%)]\tTotal Loss: 0.170640\n",
      "Reconstruction: 0.148148, Regularization: 0.022493\n",
      "2019-04-10 01:13:27,426 root         INFO     Train Epoch: 55 [3072/8000 (38%)]\tTotal Loss: 0.182645\n",
      "Reconstruction: 0.159098, Regularization: 0.023547\n",
      "2019-04-10 01:13:27,483 root         INFO     Train Epoch: 55 [3584/8000 (45%)]\tTotal Loss: 0.179965\n",
      "Reconstruction: 0.155593, Regularization: 0.024372\n",
      "2019-04-10 01:13:27,541 root         INFO     Train Epoch: 55 [4096/8000 (51%)]\tTotal Loss: 0.180113\n",
      "Reconstruction: 0.158924, Regularization: 0.021189\n",
      "2019-04-10 01:13:27,598 root         INFO     Train Epoch: 55 [4608/8000 (58%)]\tTotal Loss: 0.160536\n",
      "Reconstruction: 0.140188, Regularization: 0.020348\n",
      "2019-04-10 01:13:27,655 root         INFO     Train Epoch: 55 [5120/8000 (64%)]\tTotal Loss: 0.158264\n",
      "Reconstruction: 0.136976, Regularization: 0.021288\n",
      "2019-04-10 01:13:27,712 root         INFO     Train Epoch: 55 [5632/8000 (70%)]\tTotal Loss: 0.156909\n",
      "Reconstruction: 0.137741, Regularization: 0.019168\n",
      "2019-04-10 01:13:27,768 root         INFO     Train Epoch: 55 [6144/8000 (77%)]\tTotal Loss: 0.170345\n",
      "Reconstruction: 0.144653, Regularization: 0.025692\n",
      "2019-04-10 01:13:27,825 root         INFO     Train Epoch: 55 [6656/8000 (83%)]\tTotal Loss: 0.161105\n",
      "Reconstruction: 0.140773, Regularization: 0.020332\n",
      "2019-04-10 01:13:27,881 root         INFO     Train Epoch: 55 [7168/8000 (90%)]\tTotal Loss: 0.169454\n",
      "Reconstruction: 0.146297, Regularization: 0.023157\n",
      "2019-04-10 01:13:27,938 root         INFO     Train Epoch: 55 [7680/8000 (96%)]\tTotal Loss: 0.155260\n",
      "Reconstruction: 0.138701, Regularization: 0.016559\n",
      "2019-04-10 01:13:27,988 root         INFO     ====> Epoch: 55 Average loss: 0.1688\n",
      "2019-04-10 01:13:28,011 root         INFO     Train Epoch: 56 [0/8000 (0%)]\tTotal Loss: 0.165622\n",
      "Reconstruction: 0.145542, Regularization: 0.020080\n",
      "2019-04-10 01:13:28,068 root         INFO     Train Epoch: 56 [512/8000 (6%)]\tTotal Loss: 0.174738\n",
      "Reconstruction: 0.150874, Regularization: 0.023864\n",
      "2019-04-10 01:13:28,124 root         INFO     Train Epoch: 56 [1024/8000 (13%)]\tTotal Loss: 0.162384\n",
      "Reconstruction: 0.143771, Regularization: 0.018613\n",
      "2019-04-10 01:13:28,180 root         INFO     Train Epoch: 56 [1536/8000 (19%)]\tTotal Loss: 0.176240\n",
      "Reconstruction: 0.152880, Regularization: 0.023360\n",
      "2019-04-10 01:13:28,236 root         INFO     Train Epoch: 56 [2048/8000 (26%)]\tTotal Loss: 0.164780\n",
      "Reconstruction: 0.145294, Regularization: 0.019486\n",
      "2019-04-10 01:13:28,293 root         INFO     Train Epoch: 56 [2560/8000 (32%)]\tTotal Loss: 0.170656\n",
      "Reconstruction: 0.151118, Regularization: 0.019539\n",
      "2019-04-10 01:13:28,350 root         INFO     Train Epoch: 56 [3072/8000 (38%)]\tTotal Loss: 0.171284\n",
      "Reconstruction: 0.151493, Regularization: 0.019791\n",
      "2019-04-10 01:13:28,406 root         INFO     Train Epoch: 56 [3584/8000 (45%)]\tTotal Loss: 0.151991\n",
      "Reconstruction: 0.134699, Regularization: 0.017292\n",
      "2019-04-10 01:13:28,461 root         INFO     Train Epoch: 56 [4096/8000 (51%)]\tTotal Loss: 0.180667\n",
      "Reconstruction: 0.156690, Regularization: 0.023977\n",
      "2019-04-10 01:13:28,517 root         INFO     Train Epoch: 56 [4608/8000 (58%)]\tTotal Loss: 0.158336\n",
      "Reconstruction: 0.138437, Regularization: 0.019900\n",
      "2019-04-10 01:13:28,573 root         INFO     Train Epoch: 56 [5120/8000 (64%)]\tTotal Loss: 0.156656\n",
      "Reconstruction: 0.138022, Regularization: 0.018634\n",
      "2019-04-10 01:13:28,630 root         INFO     Train Epoch: 56 [5632/8000 (70%)]\tTotal Loss: 0.168491\n",
      "Reconstruction: 0.148369, Regularization: 0.020122\n",
      "2019-04-10 01:13:28,686 root         INFO     Train Epoch: 56 [6144/8000 (77%)]\tTotal Loss: 0.176596\n",
      "Reconstruction: 0.151179, Regularization: 0.025417\n",
      "2019-04-10 01:13:28,743 root         INFO     Train Epoch: 56 [6656/8000 (83%)]\tTotal Loss: 0.163825\n",
      "Reconstruction: 0.143410, Regularization: 0.020415\n",
      "2019-04-10 01:13:28,800 root         INFO     Train Epoch: 56 [7168/8000 (90%)]\tTotal Loss: 0.160987\n",
      "Reconstruction: 0.144335, Regularization: 0.016652\n",
      "2019-04-10 01:13:28,857 root         INFO     Train Epoch: 56 [7680/8000 (96%)]\tTotal Loss: 0.164033\n",
      "Reconstruction: 0.144611, Regularization: 0.019422\n",
      "2019-04-10 01:13:28,908 root         INFO     ====> Epoch: 56 Average loss: 0.1689\n",
      "2019-04-10 01:13:28,931 root         INFO     Train Epoch: 57 [0/8000 (0%)]\tTotal Loss: 0.152207\n",
      "Reconstruction: 0.132564, Regularization: 0.019643\n",
      "2019-04-10 01:13:28,989 root         INFO     Train Epoch: 57 [512/8000 (6%)]\tTotal Loss: 0.163460\n",
      "Reconstruction: 0.140930, Regularization: 0.022530\n",
      "2019-04-10 01:13:29,045 root         INFO     Train Epoch: 57 [1024/8000 (13%)]\tTotal Loss: 0.158940\n",
      "Reconstruction: 0.139550, Regularization: 0.019390\n",
      "2019-04-10 01:13:29,102 root         INFO     Train Epoch: 57 [1536/8000 (19%)]\tTotal Loss: 0.159505\n",
      "Reconstruction: 0.142892, Regularization: 0.016614\n",
      "2019-04-10 01:13:29,159 root         INFO     Train Epoch: 57 [2048/8000 (26%)]\tTotal Loss: 0.169639\n",
      "Reconstruction: 0.145631, Regularization: 0.024008\n",
      "2019-04-10 01:13:29,216 root         INFO     Train Epoch: 57 [2560/8000 (32%)]\tTotal Loss: 0.166725\n",
      "Reconstruction: 0.147753, Regularization: 0.018972\n",
      "2019-04-10 01:13:29,272 root         INFO     Train Epoch: 57 [3072/8000 (38%)]\tTotal Loss: 0.172750\n",
      "Reconstruction: 0.150144, Regularization: 0.022606\n",
      "2019-04-10 01:13:29,328 root         INFO     Train Epoch: 57 [3584/8000 (45%)]\tTotal Loss: 0.161607\n",
      "Reconstruction: 0.143995, Regularization: 0.017611\n",
      "2019-04-10 01:13:29,387 root         INFO     Train Epoch: 57 [4096/8000 (51%)]\tTotal Loss: 0.161705\n",
      "Reconstruction: 0.140531, Regularization: 0.021174\n",
      "2019-04-10 01:13:29,444 root         INFO     Train Epoch: 57 [4608/8000 (58%)]\tTotal Loss: 0.167511\n",
      "Reconstruction: 0.142480, Regularization: 0.025031\n",
      "2019-04-10 01:13:29,501 root         INFO     Train Epoch: 57 [5120/8000 (64%)]\tTotal Loss: 0.190995\n",
      "Reconstruction: 0.165070, Regularization: 0.025924\n",
      "2019-04-10 01:13:29,558 root         INFO     Train Epoch: 57 [5632/8000 (70%)]\tTotal Loss: 0.175412\n",
      "Reconstruction: 0.153882, Regularization: 0.021530\n",
      "2019-04-10 01:13:29,614 root         INFO     Train Epoch: 57 [6144/8000 (77%)]\tTotal Loss: 0.194142\n",
      "Reconstruction: 0.166988, Regularization: 0.027154\n",
      "2019-04-10 01:13:29,670 root         INFO     Train Epoch: 57 [6656/8000 (83%)]\tTotal Loss: 0.180573\n",
      "Reconstruction: 0.152535, Regularization: 0.028038\n",
      "2019-04-10 01:13:29,726 root         INFO     Train Epoch: 57 [7168/8000 (90%)]\tTotal Loss: 0.158840\n",
      "Reconstruction: 0.140038, Regularization: 0.018802\n",
      "2019-04-10 01:13:29,783 root         INFO     Train Epoch: 57 [7680/8000 (96%)]\tTotal Loss: 0.174822\n",
      "Reconstruction: 0.149304, Regularization: 0.025518\n",
      "2019-04-10 01:13:29,833 root         INFO     ====> Epoch: 57 Average loss: 0.1686\n",
      "2019-04-10 01:13:29,857 root         INFO     Train Epoch: 58 [0/8000 (0%)]\tTotal Loss: 0.162210\n",
      "Reconstruction: 0.139493, Regularization: 0.022718\n",
      "2019-04-10 01:13:29,914 root         INFO     Train Epoch: 58 [512/8000 (6%)]\tTotal Loss: 0.168968\n",
      "Reconstruction: 0.146671, Regularization: 0.022297\n",
      "2019-04-10 01:13:29,970 root         INFO     Train Epoch: 58 [1024/8000 (13%)]\tTotal Loss: 0.174911\n",
      "Reconstruction: 0.149045, Regularization: 0.025867\n",
      "2019-04-10 01:13:30,025 root         INFO     Train Epoch: 58 [1536/8000 (19%)]\tTotal Loss: 0.179972\n",
      "Reconstruction: 0.158075, Regularization: 0.021897\n",
      "2019-04-10 01:13:30,082 root         INFO     Train Epoch: 58 [2048/8000 (26%)]\tTotal Loss: 0.164309\n",
      "Reconstruction: 0.144216, Regularization: 0.020093\n",
      "2019-04-10 01:13:30,140 root         INFO     Train Epoch: 58 [2560/8000 (32%)]\tTotal Loss: 0.183562\n",
      "Reconstruction: 0.154774, Regularization: 0.028787\n",
      "2019-04-10 01:13:30,197 root         INFO     Train Epoch: 58 [3072/8000 (38%)]\tTotal Loss: 0.178754\n",
      "Reconstruction: 0.153948, Regularization: 0.024806\n",
      "2019-04-10 01:13:30,253 root         INFO     Train Epoch: 58 [3584/8000 (45%)]\tTotal Loss: 0.150953\n",
      "Reconstruction: 0.132784, Regularization: 0.018168\n",
      "2019-04-10 01:13:30,309 root         INFO     Train Epoch: 58 [4096/8000 (51%)]\tTotal Loss: 0.170317\n",
      "Reconstruction: 0.146241, Regularization: 0.024076\n",
      "2019-04-10 01:13:30,365 root         INFO     Train Epoch: 58 [4608/8000 (58%)]\tTotal Loss: 0.201587\n",
      "Reconstruction: 0.175099, Regularization: 0.026488\n",
      "2019-04-10 01:13:30,421 root         INFO     Train Epoch: 58 [5120/8000 (64%)]\tTotal Loss: 0.163515\n",
      "Reconstruction: 0.143037, Regularization: 0.020478\n",
      "2019-04-10 01:13:30,475 root         INFO     Train Epoch: 58 [5632/8000 (70%)]\tTotal Loss: 0.152191\n",
      "Reconstruction: 0.132669, Regularization: 0.019522\n",
      "2019-04-10 01:13:30,531 root         INFO     Train Epoch: 58 [6144/8000 (77%)]\tTotal Loss: 0.159487\n",
      "Reconstruction: 0.138117, Regularization: 0.021370\n",
      "2019-04-10 01:13:30,587 root         INFO     Train Epoch: 58 [6656/8000 (83%)]\tTotal Loss: 0.173277\n",
      "Reconstruction: 0.148234, Regularization: 0.025043\n",
      "2019-04-10 01:13:30,643 root         INFO     Train Epoch: 58 [7168/8000 (90%)]\tTotal Loss: 0.178756\n",
      "Reconstruction: 0.153199, Regularization: 0.025557\n",
      "2019-04-10 01:13:30,699 root         INFO     Train Epoch: 58 [7680/8000 (96%)]\tTotal Loss: 0.158412\n",
      "Reconstruction: 0.138367, Regularization: 0.020045\n",
      "2019-04-10 01:13:30,747 root         INFO     ====> Epoch: 58 Average loss: 0.1677\n",
      "2019-04-10 01:13:30,772 root         INFO     Train Epoch: 59 [0/8000 (0%)]\tTotal Loss: 0.161635\n",
      "Reconstruction: 0.140630, Regularization: 0.021005\n",
      "2019-04-10 01:13:30,829 root         INFO     Train Epoch: 59 [512/8000 (6%)]\tTotal Loss: 0.164758\n",
      "Reconstruction: 0.144837, Regularization: 0.019921\n",
      "2019-04-10 01:13:30,887 root         INFO     Train Epoch: 59 [1024/8000 (13%)]\tTotal Loss: 0.176023\n",
      "Reconstruction: 0.153926, Regularization: 0.022097\n",
      "2019-04-10 01:13:30,944 root         INFO     Train Epoch: 59 [1536/8000 (19%)]\tTotal Loss: 0.166201\n",
      "Reconstruction: 0.147765, Regularization: 0.018437\n",
      "2019-04-10 01:13:31,001 root         INFO     Train Epoch: 59 [2048/8000 (26%)]\tTotal Loss: 0.167221\n",
      "Reconstruction: 0.147529, Regularization: 0.019693\n",
      "2019-04-10 01:13:31,057 root         INFO     Train Epoch: 59 [2560/8000 (32%)]\tTotal Loss: 0.177407\n",
      "Reconstruction: 0.155183, Regularization: 0.022224\n",
      "2019-04-10 01:13:31,114 root         INFO     Train Epoch: 59 [3072/8000 (38%)]\tTotal Loss: 0.170754\n",
      "Reconstruction: 0.150764, Regularization: 0.019990\n",
      "2019-04-10 01:13:31,169 root         INFO     Train Epoch: 59 [3584/8000 (45%)]\tTotal Loss: 0.163974\n",
      "Reconstruction: 0.142550, Regularization: 0.021424\n",
      "2019-04-10 01:13:31,225 root         INFO     Train Epoch: 59 [4096/8000 (51%)]\tTotal Loss: 0.180978\n",
      "Reconstruction: 0.159673, Regularization: 0.021305\n",
      "2019-04-10 01:13:31,282 root         INFO     Train Epoch: 59 [4608/8000 (58%)]\tTotal Loss: 0.182545\n",
      "Reconstruction: 0.160377, Regularization: 0.022168\n",
      "2019-04-10 01:13:31,338 root         INFO     Train Epoch: 59 [5120/8000 (64%)]\tTotal Loss: 0.160380\n",
      "Reconstruction: 0.139805, Regularization: 0.020575\n",
      "2019-04-10 01:13:31,394 root         INFO     Train Epoch: 59 [5632/8000 (70%)]\tTotal Loss: 0.166364\n",
      "Reconstruction: 0.146401, Regularization: 0.019963\n",
      "2019-04-10 01:13:31,450 root         INFO     Train Epoch: 59 [6144/8000 (77%)]\tTotal Loss: 0.154974\n",
      "Reconstruction: 0.136856, Regularization: 0.018117\n",
      "2019-04-10 01:13:31,506 root         INFO     Train Epoch: 59 [6656/8000 (83%)]\tTotal Loss: 0.182984\n",
      "Reconstruction: 0.158319, Regularization: 0.024665\n",
      "2019-04-10 01:13:31,562 root         INFO     Train Epoch: 59 [7168/8000 (90%)]\tTotal Loss: 0.166662\n",
      "Reconstruction: 0.147706, Regularization: 0.018956\n",
      "2019-04-10 01:13:31,618 root         INFO     Train Epoch: 59 [7680/8000 (96%)]\tTotal Loss: 0.179633\n",
      "Reconstruction: 0.157775, Regularization: 0.021858\n",
      "2019-04-10 01:13:31,668 root         INFO     ====> Epoch: 59 Average loss: 0.1675\n",
      "2019-04-10 01:13:31,691 root         INFO     Train Epoch: 60 [0/8000 (0%)]\tTotal Loss: 0.169146\n",
      "Reconstruction: 0.148657, Regularization: 0.020489\n",
      "2019-04-10 01:13:31,748 root         INFO     Train Epoch: 60 [512/8000 (6%)]\tTotal Loss: 0.153233\n",
      "Reconstruction: 0.134931, Regularization: 0.018302\n",
      "2019-04-10 01:13:31,804 root         INFO     Train Epoch: 60 [1024/8000 (13%)]\tTotal Loss: 0.148120\n",
      "Reconstruction: 0.130719, Regularization: 0.017401\n",
      "2019-04-10 01:13:31,860 root         INFO     Train Epoch: 60 [1536/8000 (19%)]\tTotal Loss: 0.177241\n",
      "Reconstruction: 0.154687, Regularization: 0.022554\n",
      "2019-04-10 01:13:31,917 root         INFO     Train Epoch: 60 [2048/8000 (26%)]\tTotal Loss: 0.164340\n",
      "Reconstruction: 0.143756, Regularization: 0.020585\n",
      "2019-04-10 01:13:31,973 root         INFO     Train Epoch: 60 [2560/8000 (32%)]\tTotal Loss: 0.158379\n",
      "Reconstruction: 0.139598, Regularization: 0.018781\n",
      "2019-04-10 01:13:32,030 root         INFO     Train Epoch: 60 [3072/8000 (38%)]\tTotal Loss: 0.166371\n",
      "Reconstruction: 0.145387, Regularization: 0.020984\n",
      "2019-04-10 01:13:32,087 root         INFO     Train Epoch: 60 [3584/8000 (45%)]\tTotal Loss: 0.174561\n",
      "Reconstruction: 0.149543, Regularization: 0.025018\n",
      "2019-04-10 01:13:32,143 root         INFO     Train Epoch: 60 [4096/8000 (51%)]\tTotal Loss: 0.171274\n",
      "Reconstruction: 0.150799, Regularization: 0.020475\n",
      "2019-04-10 01:13:32,199 root         INFO     Train Epoch: 60 [4608/8000 (58%)]\tTotal Loss: 0.171882\n",
      "Reconstruction: 0.147065, Regularization: 0.024817\n",
      "2019-04-10 01:13:32,255 root         INFO     Train Epoch: 60 [5120/8000 (64%)]\tTotal Loss: 0.172429\n",
      "Reconstruction: 0.153557, Regularization: 0.018872\n",
      "2019-04-10 01:13:32,312 root         INFO     Train Epoch: 60 [5632/8000 (70%)]\tTotal Loss: 0.172617\n",
      "Reconstruction: 0.149570, Regularization: 0.023046\n",
      "2019-04-10 01:13:32,367 root         INFO     Train Epoch: 60 [6144/8000 (77%)]\tTotal Loss: 0.168177\n",
      "Reconstruction: 0.147758, Regularization: 0.020419\n",
      "2019-04-10 01:13:32,422 root         INFO     Train Epoch: 60 [6656/8000 (83%)]\tTotal Loss: 0.157738\n",
      "Reconstruction: 0.137808, Regularization: 0.019930\n",
      "2019-04-10 01:13:32,478 root         INFO     Train Epoch: 60 [7168/8000 (90%)]\tTotal Loss: 0.173704\n",
      "Reconstruction: 0.153050, Regularization: 0.020654\n",
      "2019-04-10 01:13:32,533 root         INFO     Train Epoch: 60 [7680/8000 (96%)]\tTotal Loss: 0.158661\n",
      "Reconstruction: 0.137959, Regularization: 0.020702\n",
      "2019-04-10 01:13:32,583 root         INFO     ====> Epoch: 60 Average loss: 0.1668\n",
      "2019-04-10 01:13:32,606 root         INFO     Train Epoch: 61 [0/8000 (0%)]\tTotal Loss: 0.153915\n",
      "Reconstruction: 0.135423, Regularization: 0.018492\n",
      "2019-04-10 01:13:32,662 root         INFO     Train Epoch: 61 [512/8000 (6%)]\tTotal Loss: 0.178044\n",
      "Reconstruction: 0.152779, Regularization: 0.025265\n",
      "2019-04-10 01:13:32,717 root         INFO     Train Epoch: 61 [1024/8000 (13%)]\tTotal Loss: 0.165411\n",
      "Reconstruction: 0.146989, Regularization: 0.018422\n",
      "2019-04-10 01:13:32,773 root         INFO     Train Epoch: 61 [1536/8000 (19%)]\tTotal Loss: 0.171014\n",
      "Reconstruction: 0.150557, Regularization: 0.020457\n",
      "2019-04-10 01:13:32,829 root         INFO     Train Epoch: 61 [2048/8000 (26%)]\tTotal Loss: 0.167831\n",
      "Reconstruction: 0.147461, Regularization: 0.020370\n",
      "2019-04-10 01:13:32,885 root         INFO     Train Epoch: 61 [2560/8000 (32%)]\tTotal Loss: 0.152121\n",
      "Reconstruction: 0.134809, Regularization: 0.017312\n",
      "2019-04-10 01:13:32,941 root         INFO     Train Epoch: 61 [3072/8000 (38%)]\tTotal Loss: 0.171764\n",
      "Reconstruction: 0.150663, Regularization: 0.021101\n",
      "2019-04-10 01:13:32,997 root         INFO     Train Epoch: 61 [3584/8000 (45%)]\tTotal Loss: 0.151019\n",
      "Reconstruction: 0.134214, Regularization: 0.016805\n",
      "2019-04-10 01:13:33,054 root         INFO     Train Epoch: 61 [4096/8000 (51%)]\tTotal Loss: 0.164099\n",
      "Reconstruction: 0.144425, Regularization: 0.019674\n",
      "2019-04-10 01:13:33,109 root         INFO     Train Epoch: 61 [4608/8000 (58%)]\tTotal Loss: 0.157592\n",
      "Reconstruction: 0.136228, Regularization: 0.021364\n",
      "2019-04-10 01:13:33,165 root         INFO     Train Epoch: 61 [5120/8000 (64%)]\tTotal Loss: 0.174892\n",
      "Reconstruction: 0.152799, Regularization: 0.022093\n",
      "2019-04-10 01:13:33,220 root         INFO     Train Epoch: 61 [5632/8000 (70%)]\tTotal Loss: 0.170560\n",
      "Reconstruction: 0.150586, Regularization: 0.019974\n",
      "2019-04-10 01:13:33,276 root         INFO     Train Epoch: 61 [6144/8000 (77%)]\tTotal Loss: 0.149518\n",
      "Reconstruction: 0.130963, Regularization: 0.018555\n",
      "2019-04-10 01:13:33,332 root         INFO     Train Epoch: 61 [6656/8000 (83%)]\tTotal Loss: 0.180327\n",
      "Reconstruction: 0.156988, Regularization: 0.023339\n",
      "2019-04-10 01:13:33,387 root         INFO     Train Epoch: 61 [7168/8000 (90%)]\tTotal Loss: 0.150260\n",
      "Reconstruction: 0.132352, Regularization: 0.017908\n",
      "2019-04-10 01:13:33,442 root         INFO     Train Epoch: 61 [7680/8000 (96%)]\tTotal Loss: 0.156396\n",
      "Reconstruction: 0.137293, Regularization: 0.019103\n",
      "2019-04-10 01:13:33,491 root         INFO     ====> Epoch: 61 Average loss: 0.1662\n",
      "2019-04-10 01:13:33,516 root         INFO     Train Epoch: 62 [0/8000 (0%)]\tTotal Loss: 0.173898\n",
      "Reconstruction: 0.151090, Regularization: 0.022807\n",
      "2019-04-10 01:13:33,573 root         INFO     Train Epoch: 62 [512/8000 (6%)]\tTotal Loss: 0.156364\n",
      "Reconstruction: 0.139277, Regularization: 0.017087\n",
      "2019-04-10 01:13:33,629 root         INFO     Train Epoch: 62 [1024/8000 (13%)]\tTotal Loss: 0.169094\n",
      "Reconstruction: 0.146831, Regularization: 0.022263\n",
      "2019-04-10 01:13:33,686 root         INFO     Train Epoch: 62 [1536/8000 (19%)]\tTotal Loss: 0.158284\n",
      "Reconstruction: 0.138853, Regularization: 0.019431\n",
      "2019-04-10 01:13:33,742 root         INFO     Train Epoch: 62 [2048/8000 (26%)]\tTotal Loss: 0.158267\n",
      "Reconstruction: 0.138679, Regularization: 0.019587\n",
      "2019-04-10 01:13:33,799 root         INFO     Train Epoch: 62 [2560/8000 (32%)]\tTotal Loss: 0.180988\n",
      "Reconstruction: 0.158602, Regularization: 0.022385\n",
      "2019-04-10 01:13:33,855 root         INFO     Train Epoch: 62 [3072/8000 (38%)]\tTotal Loss: 0.145225\n",
      "Reconstruction: 0.130542, Regularization: 0.014683\n",
      "2019-04-10 01:13:33,911 root         INFO     Train Epoch: 62 [3584/8000 (45%)]\tTotal Loss: 0.146779\n",
      "Reconstruction: 0.130638, Regularization: 0.016142\n",
      "2019-04-10 01:13:33,968 root         INFO     Train Epoch: 62 [4096/8000 (51%)]\tTotal Loss: 0.174401\n",
      "Reconstruction: 0.153401, Regularization: 0.021000\n",
      "2019-04-10 01:13:34,024 root         INFO     Train Epoch: 62 [4608/8000 (58%)]\tTotal Loss: 0.177778\n",
      "Reconstruction: 0.154842, Regularization: 0.022936\n",
      "2019-04-10 01:13:34,080 root         INFO     Train Epoch: 62 [5120/8000 (64%)]\tTotal Loss: 0.160055\n",
      "Reconstruction: 0.142304, Regularization: 0.017751\n",
      "2019-04-10 01:13:34,137 root         INFO     Train Epoch: 62 [5632/8000 (70%)]\tTotal Loss: 0.167720\n",
      "Reconstruction: 0.147514, Regularization: 0.020206\n",
      "2019-04-10 01:13:34,193 root         INFO     Train Epoch: 62 [6144/8000 (77%)]\tTotal Loss: 0.176862\n",
      "Reconstruction: 0.153024, Regularization: 0.023837\n",
      "2019-04-10 01:13:34,249 root         INFO     Train Epoch: 62 [6656/8000 (83%)]\tTotal Loss: 0.162122\n",
      "Reconstruction: 0.146379, Regularization: 0.015743\n",
      "2019-04-10 01:13:34,305 root         INFO     Train Epoch: 62 [7168/8000 (90%)]\tTotal Loss: 0.168329\n",
      "Reconstruction: 0.151439, Regularization: 0.016891\n",
      "2019-04-10 01:13:34,361 root         INFO     Train Epoch: 62 [7680/8000 (96%)]\tTotal Loss: 0.166535\n",
      "Reconstruction: 0.142873, Regularization: 0.023661\n",
      "2019-04-10 01:13:34,410 root         INFO     ====> Epoch: 62 Average loss: 0.1659\n",
      "2019-04-10 01:13:34,433 root         INFO     Train Epoch: 63 [0/8000 (0%)]\tTotal Loss: 0.165884\n",
      "Reconstruction: 0.143246, Regularization: 0.022638\n",
      "2019-04-10 01:13:34,490 root         INFO     Train Epoch: 63 [512/8000 (6%)]\tTotal Loss: 0.154188\n",
      "Reconstruction: 0.140558, Regularization: 0.013631\n",
      "2019-04-10 01:13:34,547 root         INFO     Train Epoch: 63 [1024/8000 (13%)]\tTotal Loss: 0.165991\n",
      "Reconstruction: 0.147739, Regularization: 0.018252\n",
      "2019-04-10 01:13:34,603 root         INFO     Train Epoch: 63 [1536/8000 (19%)]\tTotal Loss: 0.145743\n",
      "Reconstruction: 0.131301, Regularization: 0.014441\n",
      "2019-04-10 01:13:34,659 root         INFO     Train Epoch: 63 [2048/8000 (26%)]\tTotal Loss: 0.175882\n",
      "Reconstruction: 0.152250, Regularization: 0.023632\n",
      "2019-04-10 01:13:34,716 root         INFO     Train Epoch: 63 [2560/8000 (32%)]\tTotal Loss: 0.157088\n",
      "Reconstruction: 0.140855, Regularization: 0.016234\n",
      "2019-04-10 01:13:34,772 root         INFO     Train Epoch: 63 [3072/8000 (38%)]\tTotal Loss: 0.187041\n",
      "Reconstruction: 0.162259, Regularization: 0.024783\n",
      "2019-04-10 01:13:34,829 root         INFO     Train Epoch: 63 [3584/8000 (45%)]\tTotal Loss: 0.159336\n",
      "Reconstruction: 0.145201, Regularization: 0.014135\n",
      "2019-04-10 01:13:34,885 root         INFO     Train Epoch: 63 [4096/8000 (51%)]\tTotal Loss: 0.184117\n",
      "Reconstruction: 0.161355, Regularization: 0.022762\n",
      "2019-04-10 01:13:34,941 root         INFO     Train Epoch: 63 [4608/8000 (58%)]\tTotal Loss: 0.154427\n",
      "Reconstruction: 0.141255, Regularization: 0.013172\n",
      "2019-04-10 01:13:34,997 root         INFO     Train Epoch: 63 [5120/8000 (64%)]\tTotal Loss: 0.160730\n",
      "Reconstruction: 0.141250, Regularization: 0.019480\n",
      "2019-04-10 01:13:35,053 root         INFO     Train Epoch: 63 [5632/8000 (70%)]\tTotal Loss: 0.168000\n",
      "Reconstruction: 0.147374, Regularization: 0.020626\n",
      "2019-04-10 01:13:35,109 root         INFO     Train Epoch: 63 [6144/8000 (77%)]\tTotal Loss: 0.155515\n",
      "Reconstruction: 0.140873, Regularization: 0.014642\n",
      "2019-04-10 01:13:35,164 root         INFO     Train Epoch: 63 [6656/8000 (83%)]\tTotal Loss: 0.159644\n",
      "Reconstruction: 0.140949, Regularization: 0.018695\n",
      "2019-04-10 01:13:35,219 root         INFO     Train Epoch: 63 [7168/8000 (90%)]\tTotal Loss: 0.173215\n",
      "Reconstruction: 0.150788, Regularization: 0.022426\n",
      "2019-04-10 01:13:35,273 root         INFO     Train Epoch: 63 [7680/8000 (96%)]\tTotal Loss: 0.158809\n",
      "Reconstruction: 0.137860, Regularization: 0.020948\n",
      "2019-04-10 01:13:35,322 root         INFO     ====> Epoch: 63 Average loss: 0.1650\n",
      "2019-04-10 01:13:35,346 root         INFO     Train Epoch: 64 [0/8000 (0%)]\tTotal Loss: 0.165319\n",
      "Reconstruction: 0.143457, Regularization: 0.021862\n",
      "2019-04-10 01:13:35,402 root         INFO     Train Epoch: 64 [512/8000 (6%)]\tTotal Loss: 0.162138\n",
      "Reconstruction: 0.141048, Regularization: 0.021091\n",
      "2019-04-10 01:13:35,457 root         INFO     Train Epoch: 64 [1024/8000 (13%)]\tTotal Loss: 0.172290\n",
      "Reconstruction: 0.151428, Regularization: 0.020862\n",
      "2019-04-10 01:13:35,513 root         INFO     Train Epoch: 64 [1536/8000 (19%)]\tTotal Loss: 0.158249\n",
      "Reconstruction: 0.140203, Regularization: 0.018046\n",
      "2019-04-10 01:13:35,568 root         INFO     Train Epoch: 64 [2048/8000 (26%)]\tTotal Loss: 0.155033\n",
      "Reconstruction: 0.140537, Regularization: 0.014496\n",
      "2019-04-10 01:13:35,623 root         INFO     Train Epoch: 64 [2560/8000 (32%)]\tTotal Loss: 0.165402\n",
      "Reconstruction: 0.145135, Regularization: 0.020266\n",
      "2019-04-10 01:13:35,678 root         INFO     Train Epoch: 64 [3072/8000 (38%)]\tTotal Loss: 0.164191\n",
      "Reconstruction: 0.145821, Regularization: 0.018370\n",
      "2019-04-10 01:13:35,734 root         INFO     Train Epoch: 64 [3584/8000 (45%)]\tTotal Loss: 0.164095\n",
      "Reconstruction: 0.146357, Regularization: 0.017738\n",
      "2019-04-10 01:13:35,789 root         INFO     Train Epoch: 64 [4096/8000 (51%)]\tTotal Loss: 0.171444\n",
      "Reconstruction: 0.153781, Regularization: 0.017663\n",
      "2019-04-10 01:13:35,844 root         INFO     Train Epoch: 64 [4608/8000 (58%)]\tTotal Loss: 0.160027\n",
      "Reconstruction: 0.144379, Regularization: 0.015648\n",
      "2019-04-10 01:13:35,899 root         INFO     Train Epoch: 64 [5120/8000 (64%)]\tTotal Loss: 0.170338\n",
      "Reconstruction: 0.150020, Regularization: 0.020318\n",
      "2019-04-10 01:13:35,955 root         INFO     Train Epoch: 64 [5632/8000 (70%)]\tTotal Loss: 0.183006\n",
      "Reconstruction: 0.163014, Regularization: 0.019992\n",
      "2019-04-10 01:13:36,010 root         INFO     Train Epoch: 64 [6144/8000 (77%)]\tTotal Loss: 0.166390\n",
      "Reconstruction: 0.149418, Regularization: 0.016972\n",
      "2019-04-10 01:13:36,065 root         INFO     Train Epoch: 64 [6656/8000 (83%)]\tTotal Loss: 0.164999\n",
      "Reconstruction: 0.144984, Regularization: 0.020014\n",
      "2019-04-10 01:13:36,120 root         INFO     Train Epoch: 64 [7168/8000 (90%)]\tTotal Loss: 0.169382\n",
      "Reconstruction: 0.150745, Regularization: 0.018637\n",
      "2019-04-10 01:13:36,176 root         INFO     Train Epoch: 64 [7680/8000 (96%)]\tTotal Loss: 0.160173\n",
      "Reconstruction: 0.144310, Regularization: 0.015863\n",
      "2019-04-10 01:13:36,226 root         INFO     ====> Epoch: 64 Average loss: 0.1647\n",
      "2019-04-10 01:13:36,250 root         INFO     Train Epoch: 65 [0/8000 (0%)]\tTotal Loss: 0.161845\n",
      "Reconstruction: 0.144988, Regularization: 0.016857\n",
      "2019-04-10 01:13:36,305 root         INFO     Train Epoch: 65 [512/8000 (6%)]\tTotal Loss: 0.164716\n",
      "Reconstruction: 0.145355, Regularization: 0.019361\n",
      "2019-04-10 01:13:36,361 root         INFO     Train Epoch: 65 [1024/8000 (13%)]\tTotal Loss: 0.151007\n",
      "Reconstruction: 0.136058, Regularization: 0.014948\n",
      "2019-04-10 01:13:36,419 root         INFO     Train Epoch: 65 [1536/8000 (19%)]\tTotal Loss: 0.176816\n",
      "Reconstruction: 0.156013, Regularization: 0.020803\n",
      "2019-04-10 01:13:36,475 root         INFO     Train Epoch: 65 [2048/8000 (26%)]\tTotal Loss: 0.163712\n",
      "Reconstruction: 0.146674, Regularization: 0.017038\n",
      "2019-04-10 01:13:36,532 root         INFO     Train Epoch: 65 [2560/8000 (32%)]\tTotal Loss: 0.153461\n",
      "Reconstruction: 0.138846, Regularization: 0.014615\n",
      "2019-04-10 01:13:36,590 root         INFO     Train Epoch: 65 [3072/8000 (38%)]\tTotal Loss: 0.156895\n",
      "Reconstruction: 0.140974, Regularization: 0.015921\n",
      "2019-04-10 01:13:36,646 root         INFO     Train Epoch: 65 [3584/8000 (45%)]\tTotal Loss: 0.170376\n",
      "Reconstruction: 0.147652, Regularization: 0.022724\n",
      "2019-04-10 01:13:36,704 root         INFO     Train Epoch: 65 [4096/8000 (51%)]\tTotal Loss: 0.175212\n",
      "Reconstruction: 0.159114, Regularization: 0.016098\n",
      "2019-04-10 01:13:36,761 root         INFO     Train Epoch: 65 [4608/8000 (58%)]\tTotal Loss: 0.173244\n",
      "Reconstruction: 0.154070, Regularization: 0.019175\n",
      "2019-04-10 01:13:36,818 root         INFO     Train Epoch: 65 [5120/8000 (64%)]\tTotal Loss: 0.139958\n",
      "Reconstruction: 0.128231, Regularization: 0.011727\n",
      "2019-04-10 01:13:36,875 root         INFO     Train Epoch: 65 [5632/8000 (70%)]\tTotal Loss: 0.159789\n",
      "Reconstruction: 0.145559, Regularization: 0.014230\n",
      "2019-04-10 01:13:36,932 root         INFO     Train Epoch: 65 [6144/8000 (77%)]\tTotal Loss: 0.159337\n",
      "Reconstruction: 0.143102, Regularization: 0.016235\n",
      "2019-04-10 01:13:36,989 root         INFO     Train Epoch: 65 [6656/8000 (83%)]\tTotal Loss: 0.160018\n",
      "Reconstruction: 0.142467, Regularization: 0.017551\n",
      "2019-04-10 01:13:37,046 root         INFO     Train Epoch: 65 [7168/8000 (90%)]\tTotal Loss: 0.173429\n",
      "Reconstruction: 0.155138, Regularization: 0.018291\n",
      "2019-04-10 01:13:37,103 root         INFO     Train Epoch: 65 [7680/8000 (96%)]\tTotal Loss: 0.158521\n",
      "Reconstruction: 0.142126, Regularization: 0.016396\n",
      "2019-04-10 01:13:37,153 root         INFO     ====> Epoch: 65 Average loss: 0.1641\n",
      "2019-04-10 01:13:37,177 root         INFO     Train Epoch: 66 [0/8000 (0%)]\tTotal Loss: 0.158811\n",
      "Reconstruction: 0.140482, Regularization: 0.018329\n",
      "2019-04-10 01:13:37,235 root         INFO     Train Epoch: 66 [512/8000 (6%)]\tTotal Loss: 0.182088\n",
      "Reconstruction: 0.162915, Regularization: 0.019172\n",
      "2019-04-10 01:13:37,292 root         INFO     Train Epoch: 66 [1024/8000 (13%)]\tTotal Loss: 0.170371\n",
      "Reconstruction: 0.151405, Regularization: 0.018966\n",
      "2019-04-10 01:13:37,350 root         INFO     Train Epoch: 66 [1536/8000 (19%)]\tTotal Loss: 0.182246\n",
      "Reconstruction: 0.164340, Regularization: 0.017906\n",
      "2019-04-10 01:13:37,407 root         INFO     Train Epoch: 66 [2048/8000 (26%)]\tTotal Loss: 0.175342\n",
      "Reconstruction: 0.157465, Regularization: 0.017877\n",
      "2019-04-10 01:13:37,465 root         INFO     Train Epoch: 66 [2560/8000 (32%)]\tTotal Loss: 0.163575\n",
      "Reconstruction: 0.145429, Regularization: 0.018146\n",
      "2019-04-10 01:13:37,522 root         INFO     Train Epoch: 66 [3072/8000 (38%)]\tTotal Loss: 0.147548\n",
      "Reconstruction: 0.130774, Regularization: 0.016774\n",
      "2019-04-10 01:13:37,580 root         INFO     Train Epoch: 66 [3584/8000 (45%)]\tTotal Loss: 0.150243\n",
      "Reconstruction: 0.132841, Regularization: 0.017402\n",
      "2019-04-10 01:13:37,637 root         INFO     Train Epoch: 66 [4096/8000 (51%)]\tTotal Loss: 0.165648\n",
      "Reconstruction: 0.148386, Regularization: 0.017262\n",
      "2019-04-10 01:13:37,695 root         INFO     Train Epoch: 66 [4608/8000 (58%)]\tTotal Loss: 0.162593\n",
      "Reconstruction: 0.145407, Regularization: 0.017186\n",
      "2019-04-10 01:13:37,752 root         INFO     Train Epoch: 66 [5120/8000 (64%)]\tTotal Loss: 0.160042\n",
      "Reconstruction: 0.145509, Regularization: 0.014533\n",
      "2019-04-10 01:13:37,810 root         INFO     Train Epoch: 66 [5632/8000 (70%)]\tTotal Loss: 0.166760\n",
      "Reconstruction: 0.150867, Regularization: 0.015894\n",
      "2019-04-10 01:13:37,867 root         INFO     Train Epoch: 66 [6144/8000 (77%)]\tTotal Loss: 0.180402\n",
      "Reconstruction: 0.161179, Regularization: 0.019224\n",
      "2019-04-10 01:13:37,925 root         INFO     Train Epoch: 66 [6656/8000 (83%)]\tTotal Loss: 0.157504\n",
      "Reconstruction: 0.143246, Regularization: 0.014258\n",
      "2019-04-10 01:13:37,982 root         INFO     Train Epoch: 66 [7168/8000 (90%)]\tTotal Loss: 0.173879\n",
      "Reconstruction: 0.154644, Regularization: 0.019235\n",
      "2019-04-10 01:13:38,040 root         INFO     Train Epoch: 66 [7680/8000 (96%)]\tTotal Loss: 0.165686\n",
      "Reconstruction: 0.148177, Regularization: 0.017509\n",
      "2019-04-10 01:13:38,090 root         INFO     ====> Epoch: 66 Average loss: 0.1643\n",
      "2019-04-10 01:13:38,113 root         INFO     Train Epoch: 67 [0/8000 (0%)]\tTotal Loss: 0.151240\n",
      "Reconstruction: 0.137183, Regularization: 0.014056\n",
      "2019-04-10 01:13:38,170 root         INFO     Train Epoch: 67 [512/8000 (6%)]\tTotal Loss: 0.171218\n",
      "Reconstruction: 0.153577, Regularization: 0.017641\n",
      "2019-04-10 01:13:38,226 root         INFO     Train Epoch: 67 [1024/8000 (13%)]\tTotal Loss: 0.172467\n",
      "Reconstruction: 0.155851, Regularization: 0.016616\n",
      "2019-04-10 01:13:38,284 root         INFO     Train Epoch: 67 [1536/8000 (19%)]\tTotal Loss: 0.154713\n",
      "Reconstruction: 0.141862, Regularization: 0.012851\n",
      "2019-04-10 01:13:38,342 root         INFO     Train Epoch: 67 [2048/8000 (26%)]\tTotal Loss: 0.167799\n",
      "Reconstruction: 0.147023, Regularization: 0.020776\n",
      "2019-04-10 01:13:38,399 root         INFO     Train Epoch: 67 [2560/8000 (32%)]\tTotal Loss: 0.173403\n",
      "Reconstruction: 0.152967, Regularization: 0.020436\n",
      "2019-04-10 01:13:38,457 root         INFO     Train Epoch: 67 [3072/8000 (38%)]\tTotal Loss: 0.177085\n",
      "Reconstruction: 0.159328, Regularization: 0.017757\n",
      "2019-04-10 01:13:38,515 root         INFO     Train Epoch: 67 [3584/8000 (45%)]\tTotal Loss: 0.153562\n",
      "Reconstruction: 0.135758, Regularization: 0.017803\n",
      "2019-04-10 01:13:38,572 root         INFO     Train Epoch: 67 [4096/8000 (51%)]\tTotal Loss: 0.181256\n",
      "Reconstruction: 0.160241, Regularization: 0.021015\n",
      "2019-04-10 01:13:38,630 root         INFO     Train Epoch: 67 [4608/8000 (58%)]\tTotal Loss: 0.166570\n",
      "Reconstruction: 0.145942, Regularization: 0.020628\n",
      "2019-04-10 01:13:38,687 root         INFO     Train Epoch: 67 [5120/8000 (64%)]\tTotal Loss: 0.160155\n",
      "Reconstruction: 0.143362, Regularization: 0.016793\n",
      "2019-04-10 01:13:38,744 root         INFO     Train Epoch: 67 [5632/8000 (70%)]\tTotal Loss: 0.175394\n",
      "Reconstruction: 0.155734, Regularization: 0.019660\n",
      "2019-04-10 01:13:38,800 root         INFO     Train Epoch: 67 [6144/8000 (77%)]\tTotal Loss: 0.168501\n",
      "Reconstruction: 0.147774, Regularization: 0.020727\n",
      "2019-04-10 01:13:38,857 root         INFO     Train Epoch: 67 [6656/8000 (83%)]\tTotal Loss: 0.176210\n",
      "Reconstruction: 0.155464, Regularization: 0.020746\n",
      "2019-04-10 01:13:38,914 root         INFO     Train Epoch: 67 [7168/8000 (90%)]\tTotal Loss: 0.150933\n",
      "Reconstruction: 0.136154, Regularization: 0.014779\n",
      "2019-04-10 01:13:38,971 root         INFO     Train Epoch: 67 [7680/8000 (96%)]\tTotal Loss: 0.167985\n",
      "Reconstruction: 0.149485, Regularization: 0.018499\n",
      "2019-04-10 01:13:39,021 root         INFO     ====> Epoch: 67 Average loss: 0.1637\n",
      "2019-04-10 01:13:39,044 root         INFO     Train Epoch: 68 [0/8000 (0%)]\tTotal Loss: 0.153358\n",
      "Reconstruction: 0.137079, Regularization: 0.016278\n",
      "2019-04-10 01:13:39,102 root         INFO     Train Epoch: 68 [512/8000 (6%)]\tTotal Loss: 0.151897\n",
      "Reconstruction: 0.137227, Regularization: 0.014670\n",
      "2019-04-10 01:13:39,159 root         INFO     Train Epoch: 68 [1024/8000 (13%)]\tTotal Loss: 0.161988\n",
      "Reconstruction: 0.146439, Regularization: 0.015549\n",
      "2019-04-10 01:13:39,216 root         INFO     Train Epoch: 68 [1536/8000 (19%)]\tTotal Loss: 0.154485\n",
      "Reconstruction: 0.137038, Regularization: 0.017448\n",
      "2019-04-10 01:13:39,272 root         INFO     Train Epoch: 68 [2048/8000 (26%)]\tTotal Loss: 0.154894\n",
      "Reconstruction: 0.139540, Regularization: 0.015354\n",
      "2019-04-10 01:13:39,329 root         INFO     Train Epoch: 68 [2560/8000 (32%)]\tTotal Loss: 0.170623\n",
      "Reconstruction: 0.149265, Regularization: 0.021358\n",
      "2019-04-10 01:13:39,386 root         INFO     Train Epoch: 68 [3072/8000 (38%)]\tTotal Loss: 0.156338\n",
      "Reconstruction: 0.137525, Regularization: 0.018813\n",
      "2019-04-10 01:13:39,443 root         INFO     Train Epoch: 68 [3584/8000 (45%)]\tTotal Loss: 0.167575\n",
      "Reconstruction: 0.148811, Regularization: 0.018764\n",
      "2019-04-10 01:13:39,499 root         INFO     Train Epoch: 68 [4096/8000 (51%)]\tTotal Loss: 0.153935\n",
      "Reconstruction: 0.139249, Regularization: 0.014687\n",
      "2019-04-10 01:13:39,556 root         INFO     Train Epoch: 68 [4608/8000 (58%)]\tTotal Loss: 0.147350\n",
      "Reconstruction: 0.136634, Regularization: 0.010716\n",
      "2019-04-10 01:13:39,612 root         INFO     Train Epoch: 68 [5120/8000 (64%)]\tTotal Loss: 0.155947\n",
      "Reconstruction: 0.141657, Regularization: 0.014290\n",
      "2019-04-10 01:13:39,668 root         INFO     Train Epoch: 68 [5632/8000 (70%)]\tTotal Loss: 0.179093\n",
      "Reconstruction: 0.159832, Regularization: 0.019261\n",
      "2019-04-10 01:13:39,724 root         INFO     Train Epoch: 68 [6144/8000 (77%)]\tTotal Loss: 0.177493\n",
      "Reconstruction: 0.155760, Regularization: 0.021733\n",
      "2019-04-10 01:13:39,780 root         INFO     Train Epoch: 68 [6656/8000 (83%)]\tTotal Loss: 0.160106\n",
      "Reconstruction: 0.142789, Regularization: 0.017317\n",
      "2019-04-10 01:13:39,837 root         INFO     Train Epoch: 68 [7168/8000 (90%)]\tTotal Loss: 0.162262\n",
      "Reconstruction: 0.140110, Regularization: 0.022153\n",
      "2019-04-10 01:13:39,893 root         INFO     Train Epoch: 68 [7680/8000 (96%)]\tTotal Loss: 0.143546\n",
      "Reconstruction: 0.130317, Regularization: 0.013229\n",
      "2019-04-10 01:13:39,943 root         INFO     ====> Epoch: 68 Average loss: 0.1631\n",
      "2019-04-10 01:13:39,967 root         INFO     Train Epoch: 69 [0/8000 (0%)]\tTotal Loss: 0.153502\n",
      "Reconstruction: 0.135627, Regularization: 0.017876\n",
      "2019-04-10 01:13:40,024 root         INFO     Train Epoch: 69 [512/8000 (6%)]\tTotal Loss: 0.148147\n",
      "Reconstruction: 0.136634, Regularization: 0.011513\n",
      "2019-04-10 01:13:40,083 root         INFO     Train Epoch: 69 [1024/8000 (13%)]\tTotal Loss: 0.146723\n",
      "Reconstruction: 0.131233, Regularization: 0.015490\n",
      "2019-04-10 01:13:40,138 root         INFO     Train Epoch: 69 [1536/8000 (19%)]\tTotal Loss: 0.159064\n",
      "Reconstruction: 0.142852, Regularization: 0.016211\n",
      "2019-04-10 01:13:40,194 root         INFO     Train Epoch: 69 [2048/8000 (26%)]\tTotal Loss: 0.150941\n",
      "Reconstruction: 0.136947, Regularization: 0.013993\n",
      "2019-04-10 01:13:40,250 root         INFO     Train Epoch: 69 [2560/8000 (32%)]\tTotal Loss: 0.153061\n",
      "Reconstruction: 0.141195, Regularization: 0.011866\n",
      "2019-04-10 01:13:40,307 root         INFO     Train Epoch: 69 [3072/8000 (38%)]\tTotal Loss: 0.146458\n",
      "Reconstruction: 0.133530, Regularization: 0.012928\n",
      "2019-04-10 01:13:40,363 root         INFO     Train Epoch: 69 [3584/8000 (45%)]\tTotal Loss: 0.146986\n",
      "Reconstruction: 0.135922, Regularization: 0.011063\n",
      "2019-04-10 01:13:40,420 root         INFO     Train Epoch: 69 [4096/8000 (51%)]\tTotal Loss: 0.146070\n",
      "Reconstruction: 0.131176, Regularization: 0.014895\n",
      "2019-04-10 01:13:40,476 root         INFO     Train Epoch: 69 [4608/8000 (58%)]\tTotal Loss: 0.185887\n",
      "Reconstruction: 0.165585, Regularization: 0.020301\n",
      "2019-04-10 01:13:40,533 root         INFO     Train Epoch: 69 [5120/8000 (64%)]\tTotal Loss: 0.148452\n",
      "Reconstruction: 0.133979, Regularization: 0.014473\n",
      "2019-04-10 01:13:40,590 root         INFO     Train Epoch: 69 [5632/8000 (70%)]\tTotal Loss: 0.163073\n",
      "Reconstruction: 0.148402, Regularization: 0.014672\n",
      "2019-04-10 01:13:40,646 root         INFO     Train Epoch: 69 [6144/8000 (77%)]\tTotal Loss: 0.153200\n",
      "Reconstruction: 0.136852, Regularization: 0.016348\n",
      "2019-04-10 01:13:40,703 root         INFO     Train Epoch: 69 [6656/8000 (83%)]\tTotal Loss: 0.149418\n",
      "Reconstruction: 0.135472, Regularization: 0.013946\n",
      "2019-04-10 01:13:40,760 root         INFO     Train Epoch: 69 [7168/8000 (90%)]\tTotal Loss: 0.151189\n",
      "Reconstruction: 0.138482, Regularization: 0.012708\n",
      "2019-04-10 01:13:40,816 root         INFO     Train Epoch: 69 [7680/8000 (96%)]\tTotal Loss: 0.159728\n",
      "Reconstruction: 0.145541, Regularization: 0.014188\n",
      "2019-04-10 01:13:40,866 root         INFO     ====> Epoch: 69 Average loss: 0.1624\n",
      "2019-04-10 01:13:40,890 root         INFO     Train Epoch: 70 [0/8000 (0%)]\tTotal Loss: 0.163796\n",
      "Reconstruction: 0.146033, Regularization: 0.017763\n",
      "2019-04-10 01:13:40,948 root         INFO     Train Epoch: 70 [512/8000 (6%)]\tTotal Loss: 0.163567\n",
      "Reconstruction: 0.147155, Regularization: 0.016412\n",
      "2019-04-10 01:13:41,006 root         INFO     Train Epoch: 70 [1024/8000 (13%)]\tTotal Loss: 0.151907\n",
      "Reconstruction: 0.136859, Regularization: 0.015049\n",
      "2019-04-10 01:13:41,063 root         INFO     Train Epoch: 70 [1536/8000 (19%)]\tTotal Loss: 0.160397\n",
      "Reconstruction: 0.143584, Regularization: 0.016814\n",
      "2019-04-10 01:13:41,121 root         INFO     Train Epoch: 70 [2048/8000 (26%)]\tTotal Loss: 0.147979\n",
      "Reconstruction: 0.133823, Regularization: 0.014157\n",
      "2019-04-10 01:13:41,179 root         INFO     Train Epoch: 70 [2560/8000 (32%)]\tTotal Loss: 0.160199\n",
      "Reconstruction: 0.145205, Regularization: 0.014994\n",
      "2019-04-10 01:13:41,237 root         INFO     Train Epoch: 70 [3072/8000 (38%)]\tTotal Loss: 0.163377\n",
      "Reconstruction: 0.148113, Regularization: 0.015264\n",
      "2019-04-10 01:13:41,295 root         INFO     Train Epoch: 70 [3584/8000 (45%)]\tTotal Loss: 0.161675\n",
      "Reconstruction: 0.143507, Regularization: 0.018168\n",
      "2019-04-10 01:13:41,353 root         INFO     Train Epoch: 70 [4096/8000 (51%)]\tTotal Loss: 0.156999\n",
      "Reconstruction: 0.139579, Regularization: 0.017420\n",
      "2019-04-10 01:13:41,410 root         INFO     Train Epoch: 70 [4608/8000 (58%)]\tTotal Loss: 0.164521\n",
      "Reconstruction: 0.143562, Regularization: 0.020958\n",
      "2019-04-10 01:13:41,468 root         INFO     Train Epoch: 70 [5120/8000 (64%)]\tTotal Loss: 0.170201\n",
      "Reconstruction: 0.152706, Regularization: 0.017495\n",
      "2019-04-10 01:13:41,526 root         INFO     Train Epoch: 70 [5632/8000 (70%)]\tTotal Loss: 0.165663\n",
      "Reconstruction: 0.148185, Regularization: 0.017478\n",
      "2019-04-10 01:13:41,584 root         INFO     Train Epoch: 70 [6144/8000 (77%)]\tTotal Loss: 0.170639\n",
      "Reconstruction: 0.153837, Regularization: 0.016802\n",
      "2019-04-10 01:13:41,642 root         INFO     Train Epoch: 70 [6656/8000 (83%)]\tTotal Loss: 0.161278\n",
      "Reconstruction: 0.145659, Regularization: 0.015620\n",
      "2019-04-10 01:13:41,699 root         INFO     Train Epoch: 70 [7168/8000 (90%)]\tTotal Loss: 0.174034\n",
      "Reconstruction: 0.157939, Regularization: 0.016095\n",
      "2019-04-10 01:13:41,757 root         INFO     Train Epoch: 70 [7680/8000 (96%)]\tTotal Loss: 0.152784\n",
      "Reconstruction: 0.139810, Regularization: 0.012975\n",
      "2019-04-10 01:13:41,808 root         INFO     ====> Epoch: 70 Average loss: 0.1624\n",
      "2019-04-10 01:13:41,831 root         INFO     Train Epoch: 71 [0/8000 (0%)]\tTotal Loss: 0.148641\n",
      "Reconstruction: 0.133099, Regularization: 0.015542\n",
      "2019-04-10 01:13:41,889 root         INFO     Train Epoch: 71 [512/8000 (6%)]\tTotal Loss: 0.150042\n",
      "Reconstruction: 0.139683, Regularization: 0.010358\n",
      "2019-04-10 01:13:41,946 root         INFO     Train Epoch: 71 [1024/8000 (13%)]\tTotal Loss: 0.151174\n",
      "Reconstruction: 0.139914, Regularization: 0.011261\n",
      "2019-04-10 01:13:42,004 root         INFO     Train Epoch: 71 [1536/8000 (19%)]\tTotal Loss: 0.162801\n",
      "Reconstruction: 0.145889, Regularization: 0.016912\n",
      "2019-04-10 01:13:42,061 root         INFO     Train Epoch: 71 [2048/8000 (26%)]\tTotal Loss: 0.147053\n",
      "Reconstruction: 0.136286, Regularization: 0.010766\n",
      "2019-04-10 01:13:42,119 root         INFO     Train Epoch: 71 [2560/8000 (32%)]\tTotal Loss: 0.154969\n",
      "Reconstruction: 0.140218, Regularization: 0.014751\n",
      "2019-04-10 01:13:42,176 root         INFO     Train Epoch: 71 [3072/8000 (38%)]\tTotal Loss: 0.159941\n",
      "Reconstruction: 0.141933, Regularization: 0.018008\n",
      "2019-04-10 01:13:42,234 root         INFO     Train Epoch: 71 [3584/8000 (45%)]\tTotal Loss: 0.165160\n",
      "Reconstruction: 0.145218, Regularization: 0.019942\n",
      "2019-04-10 01:13:42,291 root         INFO     Train Epoch: 71 [4096/8000 (51%)]\tTotal Loss: 0.166277\n",
      "Reconstruction: 0.144854, Regularization: 0.021423\n",
      "2019-04-10 01:13:42,349 root         INFO     Train Epoch: 71 [4608/8000 (58%)]\tTotal Loss: 0.143885\n",
      "Reconstruction: 0.131837, Regularization: 0.012047\n",
      "2019-04-10 01:13:42,406 root         INFO     Train Epoch: 71 [5120/8000 (64%)]\tTotal Loss: 0.163563\n",
      "Reconstruction: 0.147579, Regularization: 0.015984\n",
      "2019-04-10 01:13:42,464 root         INFO     Train Epoch: 71 [5632/8000 (70%)]\tTotal Loss: 0.149917\n",
      "Reconstruction: 0.137848, Regularization: 0.012070\n",
      "2019-04-10 01:13:42,522 root         INFO     Train Epoch: 71 [6144/8000 (77%)]\tTotal Loss: 0.169852\n",
      "Reconstruction: 0.151348, Regularization: 0.018505\n",
      "2019-04-10 01:13:42,579 root         INFO     Train Epoch: 71 [6656/8000 (83%)]\tTotal Loss: 0.154131\n",
      "Reconstruction: 0.141499, Regularization: 0.012632\n",
      "2019-04-10 01:13:42,637 root         INFO     Train Epoch: 71 [7168/8000 (90%)]\tTotal Loss: 0.163781\n",
      "Reconstruction: 0.143817, Regularization: 0.019964\n",
      "2019-04-10 01:13:42,695 root         INFO     Train Epoch: 71 [7680/8000 (96%)]\tTotal Loss: 0.153922\n",
      "Reconstruction: 0.139608, Regularization: 0.014313\n",
      "2019-04-10 01:13:42,744 root         INFO     ====> Epoch: 71 Average loss: 0.1618\n",
      "2019-04-10 01:13:42,768 root         INFO     Train Epoch: 72 [0/8000 (0%)]\tTotal Loss: 0.163169\n",
      "Reconstruction: 0.146269, Regularization: 0.016900\n",
      "2019-04-10 01:13:42,823 root         INFO     Train Epoch: 72 [512/8000 (6%)]\tTotal Loss: 0.147422\n",
      "Reconstruction: 0.135537, Regularization: 0.011886\n",
      "2019-04-10 01:13:42,877 root         INFO     Train Epoch: 72 [1024/8000 (13%)]\tTotal Loss: 0.160890\n",
      "Reconstruction: 0.142802, Regularization: 0.018088\n",
      "2019-04-10 01:13:42,932 root         INFO     Train Epoch: 72 [1536/8000 (19%)]\tTotal Loss: 0.165679\n",
      "Reconstruction: 0.148406, Regularization: 0.017273\n",
      "2019-04-10 01:13:42,987 root         INFO     Train Epoch: 72 [2048/8000 (26%)]\tTotal Loss: 0.195116\n",
      "Reconstruction: 0.174782, Regularization: 0.020334\n",
      "2019-04-10 01:13:43,040 root         INFO     Train Epoch: 72 [2560/8000 (32%)]\tTotal Loss: 0.157782\n",
      "Reconstruction: 0.143979, Regularization: 0.013803\n",
      "2019-04-10 01:13:43,094 root         INFO     Train Epoch: 72 [3072/8000 (38%)]\tTotal Loss: 0.145645\n",
      "Reconstruction: 0.132047, Regularization: 0.013598\n",
      "2019-04-10 01:13:43,147 root         INFO     Train Epoch: 72 [3584/8000 (45%)]\tTotal Loss: 0.164915\n",
      "Reconstruction: 0.149776, Regularization: 0.015139\n",
      "2019-04-10 01:13:43,201 root         INFO     Train Epoch: 72 [4096/8000 (51%)]\tTotal Loss: 0.163144\n",
      "Reconstruction: 0.146950, Regularization: 0.016194\n",
      "2019-04-10 01:13:43,255 root         INFO     Train Epoch: 72 [4608/8000 (58%)]\tTotal Loss: 0.160096\n",
      "Reconstruction: 0.145558, Regularization: 0.014538\n",
      "2019-04-10 01:13:43,309 root         INFO     Train Epoch: 72 [5120/8000 (64%)]\tTotal Loss: 0.162337\n",
      "Reconstruction: 0.148346, Regularization: 0.013990\n",
      "2019-04-10 01:13:43,364 root         INFO     Train Epoch: 72 [5632/8000 (70%)]\tTotal Loss: 0.153915\n",
      "Reconstruction: 0.139592, Regularization: 0.014323\n",
      "2019-04-10 01:13:43,419 root         INFO     Train Epoch: 72 [6144/8000 (77%)]\tTotal Loss: 0.155398\n",
      "Reconstruction: 0.143020, Regularization: 0.012378\n",
      "2019-04-10 01:13:43,474 root         INFO     Train Epoch: 72 [6656/8000 (83%)]\tTotal Loss: 0.154060\n",
      "Reconstruction: 0.141145, Regularization: 0.012915\n",
      "2019-04-10 01:13:43,529 root         INFO     Train Epoch: 72 [7168/8000 (90%)]\tTotal Loss: 0.158755\n",
      "Reconstruction: 0.145000, Regularization: 0.013755\n",
      "2019-04-10 01:13:43,585 root         INFO     Train Epoch: 72 [7680/8000 (96%)]\tTotal Loss: 0.177259\n",
      "Reconstruction: 0.158831, Regularization: 0.018428\n",
      "2019-04-10 01:13:43,634 root         INFO     ====> Epoch: 72 Average loss: 0.1613\n",
      "2019-04-10 01:13:43,657 root         INFO     Train Epoch: 73 [0/8000 (0%)]\tTotal Loss: 0.154103\n",
      "Reconstruction: 0.139090, Regularization: 0.015013\n",
      "2019-04-10 01:13:43,713 root         INFO     Train Epoch: 73 [512/8000 (6%)]\tTotal Loss: 0.149234\n",
      "Reconstruction: 0.136818, Regularization: 0.012416\n",
      "2019-04-10 01:13:43,769 root         INFO     Train Epoch: 73 [1024/8000 (13%)]\tTotal Loss: 0.172147\n",
      "Reconstruction: 0.154725, Regularization: 0.017423\n",
      "2019-04-10 01:13:43,825 root         INFO     Train Epoch: 73 [1536/8000 (19%)]\tTotal Loss: 0.163038\n",
      "Reconstruction: 0.146723, Regularization: 0.016315\n",
      "2019-04-10 01:13:43,881 root         INFO     Train Epoch: 73 [2048/8000 (26%)]\tTotal Loss: 0.145996\n",
      "Reconstruction: 0.133822, Regularization: 0.012174\n",
      "2019-04-10 01:13:43,938 root         INFO     Train Epoch: 73 [2560/8000 (32%)]\tTotal Loss: 0.155149\n",
      "Reconstruction: 0.140397, Regularization: 0.014752\n",
      "2019-04-10 01:13:43,994 root         INFO     Train Epoch: 73 [3072/8000 (38%)]\tTotal Loss: 0.147041\n",
      "Reconstruction: 0.133951, Regularization: 0.013090\n",
      "2019-04-10 01:13:44,050 root         INFO     Train Epoch: 73 [3584/8000 (45%)]\tTotal Loss: 0.164650\n",
      "Reconstruction: 0.148414, Regularization: 0.016236\n",
      "2019-04-10 01:13:44,106 root         INFO     Train Epoch: 73 [4096/8000 (51%)]\tTotal Loss: 0.149059\n",
      "Reconstruction: 0.136261, Regularization: 0.012798\n",
      "2019-04-10 01:13:44,161 root         INFO     Train Epoch: 73 [4608/8000 (58%)]\tTotal Loss: 0.159367\n",
      "Reconstruction: 0.142095, Regularization: 0.017272\n",
      "2019-04-10 01:13:44,217 root         INFO     Train Epoch: 73 [5120/8000 (64%)]\tTotal Loss: 0.162780\n",
      "Reconstruction: 0.146638, Regularization: 0.016143\n",
      "2019-04-10 01:13:44,274 root         INFO     Train Epoch: 73 [5632/8000 (70%)]\tTotal Loss: 0.172189\n",
      "Reconstruction: 0.157744, Regularization: 0.014445\n",
      "2019-04-10 01:13:44,330 root         INFO     Train Epoch: 73 [6144/8000 (77%)]\tTotal Loss: 0.163414\n",
      "Reconstruction: 0.146523, Regularization: 0.016891\n",
      "2019-04-10 01:13:44,387 root         INFO     Train Epoch: 73 [6656/8000 (83%)]\tTotal Loss: 0.164932\n",
      "Reconstruction: 0.150775, Regularization: 0.014157\n",
      "2019-04-10 01:13:44,446 root         INFO     Train Epoch: 73 [7168/8000 (90%)]\tTotal Loss: 0.157407\n",
      "Reconstruction: 0.144224, Regularization: 0.013183\n",
      "2019-04-10 01:13:44,506 root         INFO     Train Epoch: 73 [7680/8000 (96%)]\tTotal Loss: 0.153195\n",
      "Reconstruction: 0.137490, Regularization: 0.015705\n",
      "2019-04-10 01:13:44,557 root         INFO     ====> Epoch: 73 Average loss: 0.1609\n",
      "2019-04-10 01:13:44,581 root         INFO     Train Epoch: 74 [0/8000 (0%)]\tTotal Loss: 0.167002\n",
      "Reconstruction: 0.149321, Regularization: 0.017681\n",
      "2019-04-10 01:13:44,637 root         INFO     Train Epoch: 74 [512/8000 (6%)]\tTotal Loss: 0.158274\n",
      "Reconstruction: 0.145214, Regularization: 0.013060\n",
      "2019-04-10 01:13:44,694 root         INFO     Train Epoch: 74 [1024/8000 (13%)]\tTotal Loss: 0.169022\n",
      "Reconstruction: 0.153037, Regularization: 0.015985\n",
      "2019-04-10 01:13:44,751 root         INFO     Train Epoch: 74 [1536/8000 (19%)]\tTotal Loss: 0.161570\n",
      "Reconstruction: 0.146899, Regularization: 0.014671\n",
      "2019-04-10 01:13:44,809 root         INFO     Train Epoch: 74 [2048/8000 (26%)]\tTotal Loss: 0.160463\n",
      "Reconstruction: 0.146584, Regularization: 0.013879\n",
      "2019-04-10 01:13:44,866 root         INFO     Train Epoch: 74 [2560/8000 (32%)]\tTotal Loss: 0.150550\n",
      "Reconstruction: 0.136937, Regularization: 0.013613\n",
      "2019-04-10 01:13:44,922 root         INFO     Train Epoch: 74 [3072/8000 (38%)]\tTotal Loss: 0.156744\n",
      "Reconstruction: 0.143863, Regularization: 0.012880\n",
      "2019-04-10 01:13:44,978 root         INFO     Train Epoch: 74 [3584/8000 (45%)]\tTotal Loss: 0.149973\n",
      "Reconstruction: 0.137492, Regularization: 0.012481\n",
      "2019-04-10 01:13:45,033 root         INFO     Train Epoch: 74 [4096/8000 (51%)]\tTotal Loss: 0.161396\n",
      "Reconstruction: 0.147221, Regularization: 0.014175\n",
      "2019-04-10 01:13:45,089 root         INFO     Train Epoch: 74 [4608/8000 (58%)]\tTotal Loss: 0.165631\n",
      "Reconstruction: 0.148768, Regularization: 0.016862\n",
      "2019-04-10 01:13:45,146 root         INFO     Train Epoch: 74 [5120/8000 (64%)]\tTotal Loss: 0.175967\n",
      "Reconstruction: 0.157119, Regularization: 0.018848\n",
      "2019-04-10 01:13:45,201 root         INFO     Train Epoch: 74 [5632/8000 (70%)]\tTotal Loss: 0.152504\n",
      "Reconstruction: 0.140359, Regularization: 0.012145\n",
      "2019-04-10 01:13:45,258 root         INFO     Train Epoch: 74 [6144/8000 (77%)]\tTotal Loss: 0.166077\n",
      "Reconstruction: 0.147647, Regularization: 0.018429\n",
      "2019-04-10 01:13:45,316 root         INFO     Train Epoch: 74 [6656/8000 (83%)]\tTotal Loss: 0.154342\n",
      "Reconstruction: 0.141053, Regularization: 0.013289\n",
      "2019-04-10 01:13:45,373 root         INFO     Train Epoch: 74 [7168/8000 (90%)]\tTotal Loss: 0.167577\n",
      "Reconstruction: 0.149212, Regularization: 0.018365\n",
      "2019-04-10 01:13:45,430 root         INFO     Train Epoch: 74 [7680/8000 (96%)]\tTotal Loss: 0.170804\n",
      "Reconstruction: 0.157738, Regularization: 0.013067\n",
      "2019-04-10 01:13:45,480 root         INFO     ====> Epoch: 74 Average loss: 0.1609\n",
      "2019-04-10 01:13:45,504 root         INFO     Train Epoch: 75 [0/8000 (0%)]\tTotal Loss: 0.155924\n",
      "Reconstruction: 0.141345, Regularization: 0.014579\n",
      "2019-04-10 01:13:45,561 root         INFO     Train Epoch: 75 [512/8000 (6%)]\tTotal Loss: 0.144516\n",
      "Reconstruction: 0.132461, Regularization: 0.012055\n",
      "2019-04-10 01:13:45,619 root         INFO     Train Epoch: 75 [1024/8000 (13%)]\tTotal Loss: 0.158925\n",
      "Reconstruction: 0.142755, Regularization: 0.016170\n",
      "2019-04-10 01:13:45,676 root         INFO     Train Epoch: 75 [1536/8000 (19%)]\tTotal Loss: 0.147123\n",
      "Reconstruction: 0.134918, Regularization: 0.012205\n",
      "2019-04-10 01:13:45,733 root         INFO     Train Epoch: 75 [2048/8000 (26%)]\tTotal Loss: 0.142136\n",
      "Reconstruction: 0.128779, Regularization: 0.013357\n",
      "2019-04-10 01:13:45,789 root         INFO     Train Epoch: 75 [2560/8000 (32%)]\tTotal Loss: 0.145809\n",
      "Reconstruction: 0.134836, Regularization: 0.010973\n",
      "2019-04-10 01:13:45,846 root         INFO     Train Epoch: 75 [3072/8000 (38%)]\tTotal Loss: 0.169324\n",
      "Reconstruction: 0.151971, Regularization: 0.017353\n",
      "2019-04-10 01:13:45,902 root         INFO     Train Epoch: 75 [3584/8000 (45%)]\tTotal Loss: 0.172427\n",
      "Reconstruction: 0.156042, Regularization: 0.016385\n",
      "2019-04-10 01:13:45,959 root         INFO     Train Epoch: 75 [4096/8000 (51%)]\tTotal Loss: 0.167220\n",
      "Reconstruction: 0.149079, Regularization: 0.018141\n",
      "2019-04-10 01:13:46,016 root         INFO     Train Epoch: 75 [4608/8000 (58%)]\tTotal Loss: 0.158810\n",
      "Reconstruction: 0.142873, Regularization: 0.015936\n",
      "2019-04-10 01:13:46,073 root         INFO     Train Epoch: 75 [5120/8000 (64%)]\tTotal Loss: 0.151622\n",
      "Reconstruction: 0.139310, Regularization: 0.012312\n",
      "2019-04-10 01:13:46,131 root         INFO     Train Epoch: 75 [5632/8000 (70%)]\tTotal Loss: 0.168050\n",
      "Reconstruction: 0.150871, Regularization: 0.017179\n",
      "2019-04-10 01:13:46,189 root         INFO     Train Epoch: 75 [6144/8000 (77%)]\tTotal Loss: 0.163779\n",
      "Reconstruction: 0.150354, Regularization: 0.013425\n",
      "2019-04-10 01:13:46,247 root         INFO     Train Epoch: 75 [6656/8000 (83%)]\tTotal Loss: 0.179781\n",
      "Reconstruction: 0.161791, Regularization: 0.017990\n",
      "2019-04-10 01:13:46,304 root         INFO     Train Epoch: 75 [7168/8000 (90%)]\tTotal Loss: 0.162848\n",
      "Reconstruction: 0.145466, Regularization: 0.017382\n",
      "2019-04-10 01:13:46,362 root         INFO     Train Epoch: 75 [7680/8000 (96%)]\tTotal Loss: 0.161605\n",
      "Reconstruction: 0.145195, Regularization: 0.016410\n",
      "2019-04-10 01:13:46,413 root         INFO     ====> Epoch: 75 Average loss: 0.1607\n",
      "2019-04-10 01:13:46,437 root         INFO     Train Epoch: 76 [0/8000 (0%)]\tTotal Loss: 0.169663\n",
      "Reconstruction: 0.152085, Regularization: 0.017578\n",
      "2019-04-10 01:13:46,494 root         INFO     Train Epoch: 76 [512/8000 (6%)]\tTotal Loss: 0.169845\n",
      "Reconstruction: 0.148764, Regularization: 0.021081\n",
      "2019-04-10 01:13:46,551 root         INFO     Train Epoch: 76 [1024/8000 (13%)]\tTotal Loss: 0.166236\n",
      "Reconstruction: 0.151874, Regularization: 0.014362\n",
      "2019-04-10 01:13:46,607 root         INFO     Train Epoch: 76 [1536/8000 (19%)]\tTotal Loss: 0.159526\n",
      "Reconstruction: 0.144355, Regularization: 0.015171\n",
      "2019-04-10 01:13:46,664 root         INFO     Train Epoch: 76 [2048/8000 (26%)]\tTotal Loss: 0.151446\n",
      "Reconstruction: 0.136977, Regularization: 0.014469\n",
      "2019-04-10 01:13:46,721 root         INFO     Train Epoch: 76 [2560/8000 (32%)]\tTotal Loss: 0.159125\n",
      "Reconstruction: 0.147191, Regularization: 0.011934\n",
      "2019-04-10 01:13:46,779 root         INFO     Train Epoch: 76 [3072/8000 (38%)]\tTotal Loss: 0.179198\n",
      "Reconstruction: 0.158316, Regularization: 0.020882\n",
      "2019-04-10 01:13:46,836 root         INFO     Train Epoch: 76 [3584/8000 (45%)]\tTotal Loss: 0.160680\n",
      "Reconstruction: 0.145986, Regularization: 0.014694\n",
      "2019-04-10 01:13:46,893 root         INFO     Train Epoch: 76 [4096/8000 (51%)]\tTotal Loss: 0.151208\n",
      "Reconstruction: 0.137937, Regularization: 0.013271\n",
      "2019-04-10 01:13:46,950 root         INFO     Train Epoch: 76 [4608/8000 (58%)]\tTotal Loss: 0.176138\n",
      "Reconstruction: 0.162756, Regularization: 0.013382\n",
      "2019-04-10 01:13:47,007 root         INFO     Train Epoch: 76 [5120/8000 (64%)]\tTotal Loss: 0.151685\n",
      "Reconstruction: 0.138673, Regularization: 0.013011\n",
      "2019-04-10 01:13:47,064 root         INFO     Train Epoch: 76 [5632/8000 (70%)]\tTotal Loss: 0.149397\n",
      "Reconstruction: 0.138007, Regularization: 0.011390\n",
      "2019-04-10 01:13:47,120 root         INFO     Train Epoch: 76 [6144/8000 (77%)]\tTotal Loss: 0.154524\n",
      "Reconstruction: 0.138408, Regularization: 0.016115\n",
      "2019-04-10 01:13:47,177 root         INFO     Train Epoch: 76 [6656/8000 (83%)]\tTotal Loss: 0.157486\n",
      "Reconstruction: 0.143257, Regularization: 0.014229\n",
      "2019-04-10 01:13:47,234 root         INFO     Train Epoch: 76 [7168/8000 (90%)]\tTotal Loss: 0.159707\n",
      "Reconstruction: 0.145869, Regularization: 0.013839\n",
      "2019-04-10 01:13:47,290 root         INFO     Train Epoch: 76 [7680/8000 (96%)]\tTotal Loss: 0.173997\n",
      "Reconstruction: 0.160206, Regularization: 0.013791\n",
      "2019-04-10 01:13:47,340 root         INFO     ====> Epoch: 76 Average loss: 0.1601\n",
      "2019-04-10 01:13:47,364 root         INFO     Train Epoch: 77 [0/8000 (0%)]\tTotal Loss: 0.164017\n",
      "Reconstruction: 0.151174, Regularization: 0.012843\n",
      "2019-04-10 01:13:47,420 root         INFO     Train Epoch: 77 [512/8000 (6%)]\tTotal Loss: 0.172799\n",
      "Reconstruction: 0.160019, Regularization: 0.012781\n",
      "2019-04-10 01:13:47,477 root         INFO     Train Epoch: 77 [1024/8000 (13%)]\tTotal Loss: 0.163649\n",
      "Reconstruction: 0.151208, Regularization: 0.012442\n",
      "2019-04-10 01:13:47,533 root         INFO     Train Epoch: 77 [1536/8000 (19%)]\tTotal Loss: 0.168184\n",
      "Reconstruction: 0.153716, Regularization: 0.014468\n",
      "2019-04-10 01:13:47,590 root         INFO     Train Epoch: 77 [2048/8000 (26%)]\tTotal Loss: 0.167629\n",
      "Reconstruction: 0.152244, Regularization: 0.015386\n",
      "2019-04-10 01:13:47,646 root         INFO     Train Epoch: 77 [2560/8000 (32%)]\tTotal Loss: 0.163809\n",
      "Reconstruction: 0.150760, Regularization: 0.013049\n",
      "2019-04-10 01:13:47,702 root         INFO     Train Epoch: 77 [3072/8000 (38%)]\tTotal Loss: 0.147285\n",
      "Reconstruction: 0.134351, Regularization: 0.012934\n",
      "2019-04-10 01:13:47,758 root         INFO     Train Epoch: 77 [3584/8000 (45%)]\tTotal Loss: 0.147807\n",
      "Reconstruction: 0.136273, Regularization: 0.011534\n",
      "2019-04-10 01:13:47,812 root         INFO     Train Epoch: 77 [4096/8000 (51%)]\tTotal Loss: 0.162232\n",
      "Reconstruction: 0.145408, Regularization: 0.016825\n",
      "2019-04-10 01:13:47,867 root         INFO     Train Epoch: 77 [4608/8000 (58%)]\tTotal Loss: 0.154456\n",
      "Reconstruction: 0.140876, Regularization: 0.013580\n",
      "2019-04-10 01:13:47,923 root         INFO     Train Epoch: 77 [5120/8000 (64%)]\tTotal Loss: 0.170852\n",
      "Reconstruction: 0.154815, Regularization: 0.016037\n",
      "2019-04-10 01:13:47,978 root         INFO     Train Epoch: 77 [5632/8000 (70%)]\tTotal Loss: 0.167832\n",
      "Reconstruction: 0.150284, Regularization: 0.017547\n",
      "2019-04-10 01:13:48,033 root         INFO     Train Epoch: 77 [6144/8000 (77%)]\tTotal Loss: 0.155092\n",
      "Reconstruction: 0.143467, Regularization: 0.011625\n",
      "2019-04-10 01:13:48,089 root         INFO     Train Epoch: 77 [6656/8000 (83%)]\tTotal Loss: 0.143198\n",
      "Reconstruction: 0.131391, Regularization: 0.011808\n",
      "2019-04-10 01:13:48,145 root         INFO     Train Epoch: 77 [7168/8000 (90%)]\tTotal Loss: 0.156013\n",
      "Reconstruction: 0.145772, Regularization: 0.010241\n",
      "2019-04-10 01:13:48,200 root         INFO     Train Epoch: 77 [7680/8000 (96%)]\tTotal Loss: 0.151631\n",
      "Reconstruction: 0.139504, Regularization: 0.012127\n",
      "2019-04-10 01:13:48,249 root         INFO     ====> Epoch: 77 Average loss: 0.1598\n",
      "2019-04-10 01:13:48,272 root         INFO     Train Epoch: 78 [0/8000 (0%)]\tTotal Loss: 0.147746\n",
      "Reconstruction: 0.136205, Regularization: 0.011541\n",
      "2019-04-10 01:13:48,329 root         INFO     Train Epoch: 78 [512/8000 (6%)]\tTotal Loss: 0.153188\n",
      "Reconstruction: 0.140295, Regularization: 0.012893\n",
      "2019-04-10 01:13:48,386 root         INFO     Train Epoch: 78 [1024/8000 (13%)]\tTotal Loss: 0.154599\n",
      "Reconstruction: 0.139669, Regularization: 0.014930\n",
      "2019-04-10 01:13:48,443 root         INFO     Train Epoch: 78 [1536/8000 (19%)]\tTotal Loss: 0.155094\n",
      "Reconstruction: 0.142239, Regularization: 0.012854\n",
      "2019-04-10 01:13:48,500 root         INFO     Train Epoch: 78 [2048/8000 (26%)]\tTotal Loss: 0.163708\n",
      "Reconstruction: 0.149145, Regularization: 0.014563\n",
      "2019-04-10 01:13:48,557 root         INFO     Train Epoch: 78 [2560/8000 (32%)]\tTotal Loss: 0.170313\n",
      "Reconstruction: 0.151764, Regularization: 0.018549\n",
      "2019-04-10 01:13:48,614 root         INFO     Train Epoch: 78 [3072/8000 (38%)]\tTotal Loss: 0.168258\n",
      "Reconstruction: 0.153969, Regularization: 0.014289\n",
      "2019-04-10 01:13:48,670 root         INFO     Train Epoch: 78 [3584/8000 (45%)]\tTotal Loss: 0.173734\n",
      "Reconstruction: 0.156741, Regularization: 0.016993\n",
      "2019-04-10 01:13:48,727 root         INFO     Train Epoch: 78 [4096/8000 (51%)]\tTotal Loss: 0.172290\n",
      "Reconstruction: 0.155386, Regularization: 0.016903\n",
      "2019-04-10 01:13:48,784 root         INFO     Train Epoch: 78 [4608/8000 (58%)]\tTotal Loss: 0.160539\n",
      "Reconstruction: 0.146451, Regularization: 0.014087\n",
      "2019-04-10 01:13:48,840 root         INFO     Train Epoch: 78 [5120/8000 (64%)]\tTotal Loss: 0.158547\n",
      "Reconstruction: 0.145322, Regularization: 0.013225\n",
      "2019-04-10 01:13:48,897 root         INFO     Train Epoch: 78 [5632/8000 (70%)]\tTotal Loss: 0.170598\n",
      "Reconstruction: 0.153835, Regularization: 0.016763\n",
      "2019-04-10 01:13:48,954 root         INFO     Train Epoch: 78 [6144/8000 (77%)]\tTotal Loss: 0.154028\n",
      "Reconstruction: 0.140816, Regularization: 0.013211\n",
      "2019-04-10 01:13:49,011 root         INFO     Train Epoch: 78 [6656/8000 (83%)]\tTotal Loss: 0.153001\n",
      "Reconstruction: 0.139593, Regularization: 0.013409\n",
      "2019-04-10 01:13:49,067 root         INFO     Train Epoch: 78 [7168/8000 (90%)]\tTotal Loss: 0.166488\n",
      "Reconstruction: 0.152174, Regularization: 0.014314\n",
      "2019-04-10 01:13:49,124 root         INFO     Train Epoch: 78 [7680/8000 (96%)]\tTotal Loss: 0.168981\n",
      "Reconstruction: 0.154464, Regularization: 0.014517\n",
      "2019-04-10 01:13:49,173 root         INFO     ====> Epoch: 78 Average loss: 0.1591\n",
      "2019-04-10 01:13:49,197 root         INFO     Train Epoch: 79 [0/8000 (0%)]\tTotal Loss: 0.161176\n",
      "Reconstruction: 0.148055, Regularization: 0.013121\n",
      "2019-04-10 01:13:49,253 root         INFO     Train Epoch: 79 [512/8000 (6%)]\tTotal Loss: 0.168608\n",
      "Reconstruction: 0.151711, Regularization: 0.016897\n",
      "2019-04-10 01:13:49,310 root         INFO     Train Epoch: 79 [1024/8000 (13%)]\tTotal Loss: 0.156104\n",
      "Reconstruction: 0.143365, Regularization: 0.012739\n",
      "2019-04-10 01:13:49,366 root         INFO     Train Epoch: 79 [1536/8000 (19%)]\tTotal Loss: 0.165145\n",
      "Reconstruction: 0.146715, Regularization: 0.018431\n",
      "2019-04-10 01:13:49,424 root         INFO     Train Epoch: 79 [2048/8000 (26%)]\tTotal Loss: 0.158442\n",
      "Reconstruction: 0.141606, Regularization: 0.016835\n",
      "2019-04-10 01:13:49,480 root         INFO     Train Epoch: 79 [2560/8000 (32%)]\tTotal Loss: 0.151402\n",
      "Reconstruction: 0.139201, Regularization: 0.012201\n",
      "2019-04-10 01:13:49,538 root         INFO     Train Epoch: 79 [3072/8000 (38%)]\tTotal Loss: 0.158688\n",
      "Reconstruction: 0.144105, Regularization: 0.014583\n",
      "2019-04-10 01:13:49,595 root         INFO     Train Epoch: 79 [3584/8000 (45%)]\tTotal Loss: 0.150310\n",
      "Reconstruction: 0.137460, Regularization: 0.012851\n",
      "2019-04-10 01:13:49,652 root         INFO     Train Epoch: 79 [4096/8000 (51%)]\tTotal Loss: 0.156407\n",
      "Reconstruction: 0.141249, Regularization: 0.015157\n",
      "2019-04-10 01:13:49,709 root         INFO     Train Epoch: 79 [4608/8000 (58%)]\tTotal Loss: 0.169299\n",
      "Reconstruction: 0.155748, Regularization: 0.013551\n",
      "2019-04-10 01:13:49,766 root         INFO     Train Epoch: 79 [5120/8000 (64%)]\tTotal Loss: 0.170870\n",
      "Reconstruction: 0.157097, Regularization: 0.013773\n",
      "2019-04-10 01:13:49,823 root         INFO     Train Epoch: 79 [5632/8000 (70%)]\tTotal Loss: 0.166860\n",
      "Reconstruction: 0.152533, Regularization: 0.014327\n",
      "2019-04-10 01:13:49,880 root         INFO     Train Epoch: 79 [6144/8000 (77%)]\tTotal Loss: 0.156164\n",
      "Reconstruction: 0.142416, Regularization: 0.013748\n",
      "2019-04-10 01:13:49,937 root         INFO     Train Epoch: 79 [6656/8000 (83%)]\tTotal Loss: 0.150815\n",
      "Reconstruction: 0.140534, Regularization: 0.010281\n",
      "2019-04-10 01:13:49,993 root         INFO     Train Epoch: 79 [7168/8000 (90%)]\tTotal Loss: 0.148003\n",
      "Reconstruction: 0.136812, Regularization: 0.011191\n",
      "2019-04-10 01:13:50,050 root         INFO     Train Epoch: 79 [7680/8000 (96%)]\tTotal Loss: 0.153446\n",
      "Reconstruction: 0.142436, Regularization: 0.011010\n",
      "2019-04-10 01:13:50,101 root         INFO     ====> Epoch: 79 Average loss: 0.1590\n",
      "2019-04-10 01:13:50,125 root         INFO     Train Epoch: 80 [0/8000 (0%)]\tTotal Loss: 0.158719\n",
      "Reconstruction: 0.146804, Regularization: 0.011914\n",
      "2019-04-10 01:13:50,182 root         INFO     Train Epoch: 80 [512/8000 (6%)]\tTotal Loss: 0.151379\n",
      "Reconstruction: 0.139336, Regularization: 0.012044\n",
      "2019-04-10 01:13:50,239 root         INFO     Train Epoch: 80 [1024/8000 (13%)]\tTotal Loss: 0.168266\n",
      "Reconstruction: 0.154084, Regularization: 0.014183\n",
      "2019-04-10 01:13:50,295 root         INFO     Train Epoch: 80 [1536/8000 (19%)]\tTotal Loss: 0.152044\n",
      "Reconstruction: 0.140119, Regularization: 0.011925\n",
      "2019-04-10 01:13:50,352 root         INFO     Train Epoch: 80 [2048/8000 (26%)]\tTotal Loss: 0.147489\n",
      "Reconstruction: 0.133860, Regularization: 0.013629\n",
      "2019-04-10 01:13:50,409 root         INFO     Train Epoch: 80 [2560/8000 (32%)]\tTotal Loss: 0.155980\n",
      "Reconstruction: 0.142468, Regularization: 0.013512\n",
      "2019-04-10 01:13:50,466 root         INFO     Train Epoch: 80 [3072/8000 (38%)]\tTotal Loss: 0.158632\n",
      "Reconstruction: 0.146084, Regularization: 0.012548\n",
      "2019-04-10 01:13:50,523 root         INFO     Train Epoch: 80 [3584/8000 (45%)]\tTotal Loss: 0.158047\n",
      "Reconstruction: 0.145530, Regularization: 0.012517\n",
      "2019-04-10 01:13:50,579 root         INFO     Train Epoch: 80 [4096/8000 (51%)]\tTotal Loss: 0.163961\n",
      "Reconstruction: 0.149608, Regularization: 0.014353\n",
      "2019-04-10 01:13:50,636 root         INFO     Train Epoch: 80 [4608/8000 (58%)]\tTotal Loss: 0.157011\n",
      "Reconstruction: 0.144308, Regularization: 0.012703\n",
      "2019-04-10 01:13:50,691 root         INFO     Train Epoch: 80 [5120/8000 (64%)]\tTotal Loss: 0.163926\n",
      "Reconstruction: 0.148326, Regularization: 0.015600\n",
      "2019-04-10 01:13:50,745 root         INFO     Train Epoch: 80 [5632/8000 (70%)]\tTotal Loss: 0.168048\n",
      "Reconstruction: 0.157739, Regularization: 0.010309\n",
      "2019-04-10 01:13:50,802 root         INFO     Train Epoch: 80 [6144/8000 (77%)]\tTotal Loss: 0.152186\n",
      "Reconstruction: 0.140290, Regularization: 0.011896\n",
      "2019-04-10 01:13:50,859 root         INFO     Train Epoch: 80 [6656/8000 (83%)]\tTotal Loss: 0.151838\n",
      "Reconstruction: 0.141552, Regularization: 0.010286\n",
      "2019-04-10 01:13:50,915 root         INFO     Train Epoch: 80 [7168/8000 (90%)]\tTotal Loss: 0.158717\n",
      "Reconstruction: 0.146163, Regularization: 0.012554\n",
      "2019-04-10 01:13:50,972 root         INFO     Train Epoch: 80 [7680/8000 (96%)]\tTotal Loss: 0.155460\n",
      "Reconstruction: 0.141388, Regularization: 0.014072\n",
      "2019-04-10 01:13:51,022 root         INFO     ====> Epoch: 80 Average loss: 0.1582\n",
      "2019-04-10 01:13:51,045 root         INFO     Train Epoch: 81 [0/8000 (0%)]\tTotal Loss: 0.183646\n",
      "Reconstruction: 0.170709, Regularization: 0.012936\n",
      "2019-04-10 01:13:51,102 root         INFO     Train Epoch: 81 [512/8000 (6%)]\tTotal Loss: 0.142566\n",
      "Reconstruction: 0.130523, Regularization: 0.012043\n",
      "2019-04-10 01:13:51,158 root         INFO     Train Epoch: 81 [1024/8000 (13%)]\tTotal Loss: 0.146653\n",
      "Reconstruction: 0.135402, Regularization: 0.011252\n",
      "2019-04-10 01:13:51,213 root         INFO     Train Epoch: 81 [1536/8000 (19%)]\tTotal Loss: 0.163024\n",
      "Reconstruction: 0.148931, Regularization: 0.014093\n",
      "2019-04-10 01:13:51,269 root         INFO     Train Epoch: 81 [2048/8000 (26%)]\tTotal Loss: 0.164284\n",
      "Reconstruction: 0.151451, Regularization: 0.012832\n",
      "2019-04-10 01:13:51,325 root         INFO     Train Epoch: 81 [2560/8000 (32%)]\tTotal Loss: 0.164443\n",
      "Reconstruction: 0.150263, Regularization: 0.014180\n",
      "2019-04-10 01:13:51,380 root         INFO     Train Epoch: 81 [3072/8000 (38%)]\tTotal Loss: 0.162945\n",
      "Reconstruction: 0.149335, Regularization: 0.013610\n",
      "2019-04-10 01:13:51,436 root         INFO     Train Epoch: 81 [3584/8000 (45%)]\tTotal Loss: 0.175030\n",
      "Reconstruction: 0.159335, Regularization: 0.015695\n",
      "2019-04-10 01:13:51,491 root         INFO     Train Epoch: 81 [4096/8000 (51%)]\tTotal Loss: 0.150385\n",
      "Reconstruction: 0.139262, Regularization: 0.011123\n",
      "2019-04-10 01:13:51,547 root         INFO     Train Epoch: 81 [4608/8000 (58%)]\tTotal Loss: 0.150873\n",
      "Reconstruction: 0.137500, Regularization: 0.013373\n",
      "2019-04-10 01:13:51,603 root         INFO     Train Epoch: 81 [5120/8000 (64%)]\tTotal Loss: 0.155221\n",
      "Reconstruction: 0.145977, Regularization: 0.009244\n",
      "2019-04-10 01:13:51,658 root         INFO     Train Epoch: 81 [5632/8000 (70%)]\tTotal Loss: 0.167984\n",
      "Reconstruction: 0.154849, Regularization: 0.013135\n",
      "2019-04-10 01:13:51,714 root         INFO     Train Epoch: 81 [6144/8000 (77%)]\tTotal Loss: 0.166875\n",
      "Reconstruction: 0.152626, Regularization: 0.014249\n",
      "2019-04-10 01:13:51,771 root         INFO     Train Epoch: 81 [6656/8000 (83%)]\tTotal Loss: 0.149172\n",
      "Reconstruction: 0.136916, Regularization: 0.012255\n",
      "2019-04-10 01:13:51,826 root         INFO     Train Epoch: 81 [7168/8000 (90%)]\tTotal Loss: 0.155608\n",
      "Reconstruction: 0.143265, Regularization: 0.012343\n",
      "2019-04-10 01:13:51,882 root         INFO     Train Epoch: 81 [7680/8000 (96%)]\tTotal Loss: 0.161401\n",
      "Reconstruction: 0.146907, Regularization: 0.014494\n",
      "2019-04-10 01:13:51,932 root         INFO     ====> Epoch: 81 Average loss: 0.1577\n",
      "2019-04-10 01:13:51,956 root         INFO     Train Epoch: 82 [0/8000 (0%)]\tTotal Loss: 0.154620\n",
      "Reconstruction: 0.144976, Regularization: 0.009644\n",
      "2019-04-10 01:13:52,013 root         INFO     Train Epoch: 82 [512/8000 (6%)]\tTotal Loss: 0.155243\n",
      "Reconstruction: 0.144450, Regularization: 0.010793\n",
      "2019-04-10 01:13:52,069 root         INFO     Train Epoch: 82 [1024/8000 (13%)]\tTotal Loss: 0.155514\n",
      "Reconstruction: 0.143045, Regularization: 0.012469\n",
      "2019-04-10 01:13:52,126 root         INFO     Train Epoch: 82 [1536/8000 (19%)]\tTotal Loss: 0.150325\n",
      "Reconstruction: 0.140995, Regularization: 0.009330\n",
      "2019-04-10 01:13:52,182 root         INFO     Train Epoch: 82 [2048/8000 (26%)]\tTotal Loss: 0.159023\n",
      "Reconstruction: 0.147905, Regularization: 0.011119\n",
      "2019-04-10 01:13:52,238 root         INFO     Train Epoch: 82 [2560/8000 (32%)]\tTotal Loss: 0.167600\n",
      "Reconstruction: 0.155433, Regularization: 0.012167\n",
      "2019-04-10 01:13:52,294 root         INFO     Train Epoch: 82 [3072/8000 (38%)]\tTotal Loss: 0.170173\n",
      "Reconstruction: 0.151559, Regularization: 0.018614\n",
      "2019-04-10 01:13:52,350 root         INFO     Train Epoch: 82 [3584/8000 (45%)]\tTotal Loss: 0.159653\n",
      "Reconstruction: 0.144764, Regularization: 0.014889\n",
      "2019-04-10 01:13:52,404 root         INFO     Train Epoch: 82 [4096/8000 (51%)]\tTotal Loss: 0.166585\n",
      "Reconstruction: 0.152491, Regularization: 0.014094\n",
      "2019-04-10 01:13:52,460 root         INFO     Train Epoch: 82 [4608/8000 (58%)]\tTotal Loss: 0.164362\n",
      "Reconstruction: 0.150990, Regularization: 0.013372\n",
      "2019-04-10 01:13:52,515 root         INFO     Train Epoch: 82 [5120/8000 (64%)]\tTotal Loss: 0.153509\n",
      "Reconstruction: 0.143079, Regularization: 0.010430\n",
      "2019-04-10 01:13:52,570 root         INFO     Train Epoch: 82 [5632/8000 (70%)]\tTotal Loss: 0.164613\n",
      "Reconstruction: 0.150894, Regularization: 0.013719\n",
      "2019-04-10 01:13:52,625 root         INFO     Train Epoch: 82 [6144/8000 (77%)]\tTotal Loss: 0.151066\n",
      "Reconstruction: 0.141176, Regularization: 0.009889\n",
      "2019-04-10 01:13:52,681 root         INFO     Train Epoch: 82 [6656/8000 (83%)]\tTotal Loss: 0.166297\n",
      "Reconstruction: 0.152311, Regularization: 0.013986\n",
      "2019-04-10 01:13:52,735 root         INFO     Train Epoch: 82 [7168/8000 (90%)]\tTotal Loss: 0.163850\n",
      "Reconstruction: 0.151292, Regularization: 0.012558\n",
      "2019-04-10 01:13:52,791 root         INFO     Train Epoch: 82 [7680/8000 (96%)]\tTotal Loss: 0.144222\n",
      "Reconstruction: 0.134787, Regularization: 0.009435\n",
      "2019-04-10 01:13:52,840 root         INFO     ====> Epoch: 82 Average loss: 0.1582\n",
      "2019-04-10 01:13:52,864 root         INFO     Train Epoch: 83 [0/8000 (0%)]\tTotal Loss: 0.147466\n",
      "Reconstruction: 0.136174, Regularization: 0.011292\n",
      "2019-04-10 01:13:52,922 root         INFO     Train Epoch: 83 [512/8000 (6%)]\tTotal Loss: 0.162505\n",
      "Reconstruction: 0.149955, Regularization: 0.012551\n",
      "2019-04-10 01:13:52,979 root         INFO     Train Epoch: 83 [1024/8000 (13%)]\tTotal Loss: 0.167081\n",
      "Reconstruction: 0.155062, Regularization: 0.012019\n",
      "2019-04-10 01:13:53,035 root         INFO     Train Epoch: 83 [1536/8000 (19%)]\tTotal Loss: 0.163067\n",
      "Reconstruction: 0.149035, Regularization: 0.014032\n",
      "2019-04-10 01:13:53,091 root         INFO     Train Epoch: 83 [2048/8000 (26%)]\tTotal Loss: 0.146682\n",
      "Reconstruction: 0.137046, Regularization: 0.009637\n",
      "2019-04-10 01:13:53,147 root         INFO     Train Epoch: 83 [2560/8000 (32%)]\tTotal Loss: 0.153003\n",
      "Reconstruction: 0.143438, Regularization: 0.009565\n",
      "2019-04-10 01:13:53,203 root         INFO     Train Epoch: 83 [3072/8000 (38%)]\tTotal Loss: 0.159454\n",
      "Reconstruction: 0.146772, Regularization: 0.012682\n",
      "2019-04-10 01:13:53,258 root         INFO     Train Epoch: 83 [3584/8000 (45%)]\tTotal Loss: 0.156387\n",
      "Reconstruction: 0.145044, Regularization: 0.011343\n",
      "2019-04-10 01:13:53,315 root         INFO     Train Epoch: 83 [4096/8000 (51%)]\tTotal Loss: 0.160746\n",
      "Reconstruction: 0.150035, Regularization: 0.010711\n",
      "2019-04-10 01:13:53,371 root         INFO     Train Epoch: 83 [4608/8000 (58%)]\tTotal Loss: 0.146534\n",
      "Reconstruction: 0.135726, Regularization: 0.010808\n",
      "2019-04-10 01:13:53,426 root         INFO     Train Epoch: 83 [5120/8000 (64%)]\tTotal Loss: 0.154290\n",
      "Reconstruction: 0.143025, Regularization: 0.011265\n",
      "2019-04-10 01:13:53,483 root         INFO     Train Epoch: 83 [5632/8000 (70%)]\tTotal Loss: 0.156943\n",
      "Reconstruction: 0.145447, Regularization: 0.011496\n",
      "2019-04-10 01:13:53,539 root         INFO     Train Epoch: 83 [6144/8000 (77%)]\tTotal Loss: 0.159942\n",
      "Reconstruction: 0.146780, Regularization: 0.013163\n",
      "2019-04-10 01:13:53,594 root         INFO     Train Epoch: 83 [6656/8000 (83%)]\tTotal Loss: 0.163085\n",
      "Reconstruction: 0.152178, Regularization: 0.010908\n",
      "2019-04-10 01:13:53,650 root         INFO     Train Epoch: 83 [7168/8000 (90%)]\tTotal Loss: 0.150426\n",
      "Reconstruction: 0.139909, Regularization: 0.010517\n",
      "2019-04-10 01:13:53,706 root         INFO     Train Epoch: 83 [7680/8000 (96%)]\tTotal Loss: 0.167434\n",
      "Reconstruction: 0.152206, Regularization: 0.015229\n",
      "2019-04-10 01:13:53,756 root         INFO     ====> Epoch: 83 Average loss: 0.1571\n",
      "2019-04-10 01:13:53,779 root         INFO     Train Epoch: 84 [0/8000 (0%)]\tTotal Loss: 0.163052\n",
      "Reconstruction: 0.151175, Regularization: 0.011877\n",
      "2019-04-10 01:13:53,835 root         INFO     Train Epoch: 84 [512/8000 (6%)]\tTotal Loss: 0.149250\n",
      "Reconstruction: 0.139395, Regularization: 0.009854\n",
      "2019-04-10 01:13:53,890 root         INFO     Train Epoch: 84 [1024/8000 (13%)]\tTotal Loss: 0.155583\n",
      "Reconstruction: 0.144277, Regularization: 0.011306\n",
      "2019-04-10 01:13:53,945 root         INFO     Train Epoch: 84 [1536/8000 (19%)]\tTotal Loss: 0.157374\n",
      "Reconstruction: 0.147269, Regularization: 0.010105\n",
      "2019-04-10 01:13:54,001 root         INFO     Train Epoch: 84 [2048/8000 (26%)]\tTotal Loss: 0.153077\n",
      "Reconstruction: 0.142944, Regularization: 0.010133\n",
      "2019-04-10 01:13:54,056 root         INFO     Train Epoch: 84 [2560/8000 (32%)]\tTotal Loss: 0.154126\n",
      "Reconstruction: 0.144320, Regularization: 0.009806\n",
      "2019-04-10 01:13:54,111 root         INFO     Train Epoch: 84 [3072/8000 (38%)]\tTotal Loss: 0.156080\n",
      "Reconstruction: 0.144969, Regularization: 0.011111\n",
      "2019-04-10 01:13:54,165 root         INFO     Train Epoch: 84 [3584/8000 (45%)]\tTotal Loss: 0.171145\n",
      "Reconstruction: 0.156757, Regularization: 0.014387\n",
      "2019-04-10 01:13:54,220 root         INFO     Train Epoch: 84 [4096/8000 (51%)]\tTotal Loss: 0.147713\n",
      "Reconstruction: 0.137208, Regularization: 0.010505\n",
      "2019-04-10 01:13:54,274 root         INFO     Train Epoch: 84 [4608/8000 (58%)]\tTotal Loss: 0.166071\n",
      "Reconstruction: 0.155358, Regularization: 0.010713\n",
      "2019-04-10 01:13:54,329 root         INFO     Train Epoch: 84 [5120/8000 (64%)]\tTotal Loss: 0.142333\n",
      "Reconstruction: 0.132967, Regularization: 0.009366\n",
      "2019-04-10 01:13:54,383 root         INFO     Train Epoch: 84 [5632/8000 (70%)]\tTotal Loss: 0.166116\n",
      "Reconstruction: 0.153276, Regularization: 0.012840\n",
      "2019-04-10 01:13:54,438 root         INFO     Train Epoch: 84 [6144/8000 (77%)]\tTotal Loss: 0.151124\n",
      "Reconstruction: 0.141094, Regularization: 0.010030\n",
      "2019-04-10 01:13:54,492 root         INFO     Train Epoch: 84 [6656/8000 (83%)]\tTotal Loss: 0.149451\n",
      "Reconstruction: 0.138869, Regularization: 0.010582\n",
      "2019-04-10 01:13:54,547 root         INFO     Train Epoch: 84 [7168/8000 (90%)]\tTotal Loss: 0.152365\n",
      "Reconstruction: 0.144159, Regularization: 0.008206\n",
      "2019-04-10 01:13:54,602 root         INFO     Train Epoch: 84 [7680/8000 (96%)]\tTotal Loss: 0.157475\n",
      "Reconstruction: 0.147165, Regularization: 0.010309\n",
      "2019-04-10 01:13:54,651 root         INFO     ====> Epoch: 84 Average loss: 0.1567\n",
      "2019-04-10 01:13:54,674 root         INFO     Train Epoch: 85 [0/8000 (0%)]\tTotal Loss: 0.168134\n",
      "Reconstruction: 0.156782, Regularization: 0.011352\n",
      "2019-04-10 01:13:54,731 root         INFO     Train Epoch: 85 [512/8000 (6%)]\tTotal Loss: 0.150846\n",
      "Reconstruction: 0.138877, Regularization: 0.011969\n",
      "2019-04-10 01:13:54,788 root         INFO     Train Epoch: 85 [1024/8000 (13%)]\tTotal Loss: 0.140155\n",
      "Reconstruction: 0.131238, Regularization: 0.008917\n",
      "2019-04-10 01:13:54,845 root         INFO     Train Epoch: 85 [1536/8000 (19%)]\tTotal Loss: 0.147218\n",
      "Reconstruction: 0.135113, Regularization: 0.012105\n",
      "2019-04-10 01:13:54,901 root         INFO     Train Epoch: 85 [2048/8000 (26%)]\tTotal Loss: 0.176757\n",
      "Reconstruction: 0.161792, Regularization: 0.014965\n",
      "2019-04-10 01:13:54,958 root         INFO     Train Epoch: 85 [2560/8000 (32%)]\tTotal Loss: 0.152482\n",
      "Reconstruction: 0.142368, Regularization: 0.010114\n",
      "2019-04-10 01:13:55,014 root         INFO     Train Epoch: 85 [3072/8000 (38%)]\tTotal Loss: 0.152814\n",
      "Reconstruction: 0.143699, Regularization: 0.009115\n",
      "2019-04-10 01:13:55,069 root         INFO     Train Epoch: 85 [3584/8000 (45%)]\tTotal Loss: 0.156120\n",
      "Reconstruction: 0.144500, Regularization: 0.011620\n",
      "2019-04-10 01:13:55,126 root         INFO     Train Epoch: 85 [4096/8000 (51%)]\tTotal Loss: 0.167824\n",
      "Reconstruction: 0.156900, Regularization: 0.010924\n",
      "2019-04-10 01:13:55,182 root         INFO     Train Epoch: 85 [4608/8000 (58%)]\tTotal Loss: 0.153346\n",
      "Reconstruction: 0.141877, Regularization: 0.011468\n",
      "2019-04-10 01:13:55,238 root         INFO     Train Epoch: 85 [5120/8000 (64%)]\tTotal Loss: 0.159393\n",
      "Reconstruction: 0.147780, Regularization: 0.011613\n",
      "2019-04-10 01:13:55,294 root         INFO     Train Epoch: 85 [5632/8000 (70%)]\tTotal Loss: 0.169621\n",
      "Reconstruction: 0.155273, Regularization: 0.014347\n",
      "2019-04-10 01:13:55,351 root         INFO     Train Epoch: 85 [6144/8000 (77%)]\tTotal Loss: 0.159227\n",
      "Reconstruction: 0.147904, Regularization: 0.011323\n",
      "2019-04-10 01:13:55,406 root         INFO     Train Epoch: 85 [6656/8000 (83%)]\tTotal Loss: 0.163517\n",
      "Reconstruction: 0.146863, Regularization: 0.016654\n",
      "2019-04-10 01:13:55,463 root         INFO     Train Epoch: 85 [7168/8000 (90%)]\tTotal Loss: 0.163215\n",
      "Reconstruction: 0.151164, Regularization: 0.012051\n",
      "2019-04-10 01:13:55,519 root         INFO     Train Epoch: 85 [7680/8000 (96%)]\tTotal Loss: 0.158377\n",
      "Reconstruction: 0.146861, Regularization: 0.011516\n",
      "2019-04-10 01:13:55,569 root         INFO     ====> Epoch: 85 Average loss: 0.1565\n",
      "2019-04-10 01:13:55,592 root         INFO     Train Epoch: 86 [0/8000 (0%)]\tTotal Loss: 0.161201\n",
      "Reconstruction: 0.149374, Regularization: 0.011827\n",
      "2019-04-10 01:13:55,649 root         INFO     Train Epoch: 86 [512/8000 (6%)]\tTotal Loss: 0.148499\n",
      "Reconstruction: 0.140124, Regularization: 0.008375\n",
      "2019-04-10 01:13:55,706 root         INFO     Train Epoch: 86 [1024/8000 (13%)]\tTotal Loss: 0.149649\n",
      "Reconstruction: 0.138409, Regularization: 0.011240\n",
      "2019-04-10 01:13:55,762 root         INFO     Train Epoch: 86 [1536/8000 (19%)]\tTotal Loss: 0.182744\n",
      "Reconstruction: 0.171602, Regularization: 0.011143\n",
      "2019-04-10 01:13:55,818 root         INFO     Train Epoch: 86 [2048/8000 (26%)]\tTotal Loss: 0.162664\n",
      "Reconstruction: 0.150118, Regularization: 0.012546\n",
      "2019-04-10 01:13:55,874 root         INFO     Train Epoch: 86 [2560/8000 (32%)]\tTotal Loss: 0.158230\n",
      "Reconstruction: 0.149806, Regularization: 0.008424\n",
      "2019-04-10 01:13:55,930 root         INFO     Train Epoch: 86 [3072/8000 (38%)]\tTotal Loss: 0.160741\n",
      "Reconstruction: 0.149421, Regularization: 0.011320\n",
      "2019-04-10 01:13:55,986 root         INFO     Train Epoch: 86 [3584/8000 (45%)]\tTotal Loss: 0.149433\n",
      "Reconstruction: 0.139720, Regularization: 0.009714\n",
      "2019-04-10 01:13:56,041 root         INFO     Train Epoch: 86 [4096/8000 (51%)]\tTotal Loss: 0.149657\n",
      "Reconstruction: 0.139716, Regularization: 0.009941\n",
      "2019-04-10 01:13:56,097 root         INFO     Train Epoch: 86 [4608/8000 (58%)]\tTotal Loss: 0.152336\n",
      "Reconstruction: 0.141514, Regularization: 0.010821\n",
      "2019-04-10 01:13:56,153 root         INFO     Train Epoch: 86 [5120/8000 (64%)]\tTotal Loss: 0.145622\n",
      "Reconstruction: 0.136178, Regularization: 0.009444\n",
      "2019-04-10 01:13:56,209 root         INFO     Train Epoch: 86 [5632/8000 (70%)]\tTotal Loss: 0.165091\n",
      "Reconstruction: 0.149987, Regularization: 0.015104\n",
      "2019-04-10 01:13:56,264 root         INFO     Train Epoch: 86 [6144/8000 (77%)]\tTotal Loss: 0.151389\n",
      "Reconstruction: 0.142020, Regularization: 0.009368\n",
      "2019-04-10 01:13:56,320 root         INFO     Train Epoch: 86 [6656/8000 (83%)]\tTotal Loss: 0.154131\n",
      "Reconstruction: 0.145056, Regularization: 0.009074\n",
      "2019-04-10 01:13:56,376 root         INFO     Train Epoch: 86 [7168/8000 (90%)]\tTotal Loss: 0.157698\n",
      "Reconstruction: 0.144218, Regularization: 0.013480\n",
      "2019-04-10 01:13:56,432 root         INFO     Train Epoch: 86 [7680/8000 (96%)]\tTotal Loss: 0.158882\n",
      "Reconstruction: 0.147603, Regularization: 0.011279\n",
      "2019-04-10 01:13:56,482 root         INFO     ====> Epoch: 86 Average loss: 0.1566\n",
      "2019-04-10 01:13:56,506 root         INFO     Train Epoch: 87 [0/8000 (0%)]\tTotal Loss: 0.133759\n",
      "Reconstruction: 0.126274, Regularization: 0.007485\n",
      "2019-04-10 01:13:56,563 root         INFO     Train Epoch: 87 [512/8000 (6%)]\tTotal Loss: 0.158032\n",
      "Reconstruction: 0.147094, Regularization: 0.010938\n",
      "2019-04-10 01:13:56,620 root         INFO     Train Epoch: 87 [1024/8000 (13%)]\tTotal Loss: 0.147244\n",
      "Reconstruction: 0.139535, Regularization: 0.007709\n",
      "2019-04-10 01:13:56,677 root         INFO     Train Epoch: 87 [1536/8000 (19%)]\tTotal Loss: 0.139332\n",
      "Reconstruction: 0.131799, Regularization: 0.007533\n",
      "2019-04-10 01:13:56,734 root         INFO     Train Epoch: 87 [2048/8000 (26%)]\tTotal Loss: 0.161895\n",
      "Reconstruction: 0.149789, Regularization: 0.012106\n",
      "2019-04-10 01:13:56,791 root         INFO     Train Epoch: 87 [2560/8000 (32%)]\tTotal Loss: 0.165298\n",
      "Reconstruction: 0.152185, Regularization: 0.013113\n",
      "2019-04-10 01:13:56,848 root         INFO     Train Epoch: 87 [3072/8000 (38%)]\tTotal Loss: 0.146450\n",
      "Reconstruction: 0.135977, Regularization: 0.010472\n",
      "2019-04-10 01:13:56,905 root         INFO     Train Epoch: 87 [3584/8000 (45%)]\tTotal Loss: 0.161665\n",
      "Reconstruction: 0.148484, Regularization: 0.013181\n",
      "2019-04-10 01:13:56,962 root         INFO     Train Epoch: 87 [4096/8000 (51%)]\tTotal Loss: 0.135563\n",
      "Reconstruction: 0.127733, Regularization: 0.007831\n",
      "2019-04-10 01:13:57,019 root         INFO     Train Epoch: 87 [4608/8000 (58%)]\tTotal Loss: 0.145791\n",
      "Reconstruction: 0.135334, Regularization: 0.010457\n",
      "2019-04-10 01:13:57,075 root         INFO     Train Epoch: 87 [5120/8000 (64%)]\tTotal Loss: 0.140539\n",
      "Reconstruction: 0.134292, Regularization: 0.006247\n",
      "2019-04-10 01:13:57,132 root         INFO     Train Epoch: 87 [5632/8000 (70%)]\tTotal Loss: 0.154660\n",
      "Reconstruction: 0.147191, Regularization: 0.007469\n",
      "2019-04-10 01:13:57,189 root         INFO     Train Epoch: 87 [6144/8000 (77%)]\tTotal Loss: 0.177974\n",
      "Reconstruction: 0.165356, Regularization: 0.012618\n",
      "2019-04-10 01:13:57,245 root         INFO     Train Epoch: 87 [6656/8000 (83%)]\tTotal Loss: 0.152434\n",
      "Reconstruction: 0.143459, Regularization: 0.008975\n",
      "2019-04-10 01:13:57,302 root         INFO     Train Epoch: 87 [7168/8000 (90%)]\tTotal Loss: 0.145057\n",
      "Reconstruction: 0.137173, Regularization: 0.007884\n",
      "2019-04-10 01:13:57,359 root         INFO     Train Epoch: 87 [7680/8000 (96%)]\tTotal Loss: 0.149819\n",
      "Reconstruction: 0.139051, Regularization: 0.010768\n",
      "2019-04-10 01:13:57,409 root         INFO     ====> Epoch: 87 Average loss: 0.1564\n",
      "2019-04-10 01:13:57,432 root         INFO     Train Epoch: 88 [0/8000 (0%)]\tTotal Loss: 0.137232\n",
      "Reconstruction: 0.130647, Regularization: 0.006585\n",
      "2019-04-10 01:13:57,489 root         INFO     Train Epoch: 88 [512/8000 (6%)]\tTotal Loss: 0.157823\n",
      "Reconstruction: 0.147322, Regularization: 0.010500\n",
      "2019-04-10 01:13:57,546 root         INFO     Train Epoch: 88 [1024/8000 (13%)]\tTotal Loss: 0.166914\n",
      "Reconstruction: 0.154250, Regularization: 0.012664\n",
      "2019-04-10 01:13:57,603 root         INFO     Train Epoch: 88 [1536/8000 (19%)]\tTotal Loss: 0.161377\n",
      "Reconstruction: 0.145974, Regularization: 0.015404\n",
      "2019-04-10 01:13:57,660 root         INFO     Train Epoch: 88 [2048/8000 (26%)]\tTotal Loss: 0.158629\n",
      "Reconstruction: 0.149098, Regularization: 0.009531\n",
      "2019-04-10 01:13:57,717 root         INFO     Train Epoch: 88 [2560/8000 (32%)]\tTotal Loss: 0.163703\n",
      "Reconstruction: 0.152960, Regularization: 0.010743\n",
      "2019-04-10 01:13:57,774 root         INFO     Train Epoch: 88 [3072/8000 (38%)]\tTotal Loss: 0.161446\n",
      "Reconstruction: 0.150162, Regularization: 0.011284\n",
      "2019-04-10 01:13:57,830 root         INFO     Train Epoch: 88 [3584/8000 (45%)]\tTotal Loss: 0.176903\n",
      "Reconstruction: 0.160562, Regularization: 0.016341\n",
      "2019-04-10 01:13:57,886 root         INFO     Train Epoch: 88 [4096/8000 (51%)]\tTotal Loss: 0.151082\n",
      "Reconstruction: 0.142493, Regularization: 0.008589\n",
      "2019-04-10 01:13:57,943 root         INFO     Train Epoch: 88 [4608/8000 (58%)]\tTotal Loss: 0.148818\n",
      "Reconstruction: 0.139035, Regularization: 0.009783\n",
      "2019-04-10 01:13:58,000 root         INFO     Train Epoch: 88 [5120/8000 (64%)]\tTotal Loss: 0.173775\n",
      "Reconstruction: 0.164822, Regularization: 0.008953\n",
      "2019-04-10 01:13:58,056 root         INFO     Train Epoch: 88 [5632/8000 (70%)]\tTotal Loss: 0.151570\n",
      "Reconstruction: 0.139471, Regularization: 0.012099\n",
      "2019-04-10 01:13:58,112 root         INFO     Train Epoch: 88 [6144/8000 (77%)]\tTotal Loss: 0.152088\n",
      "Reconstruction: 0.141228, Regularization: 0.010860\n",
      "2019-04-10 01:13:58,167 root         INFO     Train Epoch: 88 [6656/8000 (83%)]\tTotal Loss: 0.158783\n",
      "Reconstruction: 0.148403, Regularization: 0.010379\n",
      "2019-04-10 01:13:58,221 root         INFO     Train Epoch: 88 [7168/8000 (90%)]\tTotal Loss: 0.152032\n",
      "Reconstruction: 0.144662, Regularization: 0.007370\n",
      "2019-04-10 01:13:58,275 root         INFO     Train Epoch: 88 [7680/8000 (96%)]\tTotal Loss: 0.146639\n",
      "Reconstruction: 0.139699, Regularization: 0.006940\n",
      "2019-04-10 01:13:58,325 root         INFO     ====> Epoch: 88 Average loss: 0.1556\n",
      "2019-04-10 01:13:58,348 root         INFO     Train Epoch: 89 [0/8000 (0%)]\tTotal Loss: 0.142682\n",
      "Reconstruction: 0.133920, Regularization: 0.008762\n",
      "2019-04-10 01:13:58,406 root         INFO     Train Epoch: 89 [512/8000 (6%)]\tTotal Loss: 0.154251\n",
      "Reconstruction: 0.144178, Regularization: 0.010072\n",
      "2019-04-10 01:13:58,463 root         INFO     Train Epoch: 89 [1024/8000 (13%)]\tTotal Loss: 0.166562\n",
      "Reconstruction: 0.154066, Regularization: 0.012496\n",
      "2019-04-10 01:13:58,520 root         INFO     Train Epoch: 89 [1536/8000 (19%)]\tTotal Loss: 0.171440\n",
      "Reconstruction: 0.158307, Regularization: 0.013133\n",
      "2019-04-10 01:13:58,576 root         INFO     Train Epoch: 89 [2048/8000 (26%)]\tTotal Loss: 0.161267\n",
      "Reconstruction: 0.149230, Regularization: 0.012037\n",
      "2019-04-10 01:13:58,633 root         INFO     Train Epoch: 89 [2560/8000 (32%)]\tTotal Loss: 0.173482\n",
      "Reconstruction: 0.162828, Regularization: 0.010654\n",
      "2019-04-10 01:13:58,689 root         INFO     Train Epoch: 89 [3072/8000 (38%)]\tTotal Loss: 0.155598\n",
      "Reconstruction: 0.145497, Regularization: 0.010102\n",
      "2019-04-10 01:13:58,744 root         INFO     Train Epoch: 89 [3584/8000 (45%)]\tTotal Loss: 0.155710\n",
      "Reconstruction: 0.144525, Regularization: 0.011185\n",
      "2019-04-10 01:13:58,799 root         INFO     Train Epoch: 89 [4096/8000 (51%)]\tTotal Loss: 0.156581\n",
      "Reconstruction: 0.145659, Regularization: 0.010922\n",
      "2019-04-10 01:13:58,855 root         INFO     Train Epoch: 89 [4608/8000 (58%)]\tTotal Loss: 0.154383\n",
      "Reconstruction: 0.146078, Regularization: 0.008305\n",
      "2019-04-10 01:13:58,910 root         INFO     Train Epoch: 89 [5120/8000 (64%)]\tTotal Loss: 0.161033\n",
      "Reconstruction: 0.151955, Regularization: 0.009079\n",
      "2019-04-10 01:13:58,966 root         INFO     Train Epoch: 89 [5632/8000 (70%)]\tTotal Loss: 0.152717\n",
      "Reconstruction: 0.143037, Regularization: 0.009680\n",
      "2019-04-10 01:13:59,021 root         INFO     Train Epoch: 89 [6144/8000 (77%)]\tTotal Loss: 0.151614\n",
      "Reconstruction: 0.141016, Regularization: 0.010598\n",
      "2019-04-10 01:13:59,077 root         INFO     Train Epoch: 89 [6656/8000 (83%)]\tTotal Loss: 0.142230\n",
      "Reconstruction: 0.136026, Regularization: 0.006204\n",
      "2019-04-10 01:13:59,133 root         INFO     Train Epoch: 89 [7168/8000 (90%)]\tTotal Loss: 0.146480\n",
      "Reconstruction: 0.137846, Regularization: 0.008634\n",
      "2019-04-10 01:13:59,189 root         INFO     Train Epoch: 89 [7680/8000 (96%)]\tTotal Loss: 0.158945\n",
      "Reconstruction: 0.150487, Regularization: 0.008458\n",
      "2019-04-10 01:13:59,238 root         INFO     ====> Epoch: 89 Average loss: 0.1555\n",
      "2019-04-10 01:13:59,261 root         INFO     Train Epoch: 90 [0/8000 (0%)]\tTotal Loss: 0.152887\n",
      "Reconstruction: 0.143858, Regularization: 0.009028\n",
      "2019-04-10 01:13:59,318 root         INFO     Train Epoch: 90 [512/8000 (6%)]\tTotal Loss: 0.168916\n",
      "Reconstruction: 0.154351, Regularization: 0.014565\n",
      "2019-04-10 01:13:59,377 root         INFO     Train Epoch: 90 [1024/8000 (13%)]\tTotal Loss: 0.156968\n",
      "Reconstruction: 0.144827, Regularization: 0.012141\n",
      "2019-04-10 01:13:59,433 root         INFO     Train Epoch: 90 [1536/8000 (19%)]\tTotal Loss: 0.161781\n",
      "Reconstruction: 0.153003, Regularization: 0.008778\n",
      "2019-04-10 01:13:59,488 root         INFO     Train Epoch: 90 [2048/8000 (26%)]\tTotal Loss: 0.161349\n",
      "Reconstruction: 0.147936, Regularization: 0.013413\n",
      "2019-04-10 01:13:59,544 root         INFO     Train Epoch: 90 [2560/8000 (32%)]\tTotal Loss: 0.146796\n",
      "Reconstruction: 0.137531, Regularization: 0.009265\n",
      "2019-04-10 01:13:59,599 root         INFO     Train Epoch: 90 [3072/8000 (38%)]\tTotal Loss: 0.143904\n",
      "Reconstruction: 0.136262, Regularization: 0.007641\n",
      "2019-04-10 01:13:59,656 root         INFO     Train Epoch: 90 [3584/8000 (45%)]\tTotal Loss: 0.147455\n",
      "Reconstruction: 0.140287, Regularization: 0.007168\n",
      "2019-04-10 01:13:59,713 root         INFO     Train Epoch: 90 [4096/8000 (51%)]\tTotal Loss: 0.145057\n",
      "Reconstruction: 0.134517, Regularization: 0.010541\n",
      "2019-04-10 01:13:59,769 root         INFO     Train Epoch: 90 [4608/8000 (58%)]\tTotal Loss: 0.150676\n",
      "Reconstruction: 0.141792, Regularization: 0.008883\n",
      "2019-04-10 01:13:59,826 root         INFO     Train Epoch: 90 [5120/8000 (64%)]\tTotal Loss: 0.148557\n",
      "Reconstruction: 0.141284, Regularization: 0.007273\n",
      "2019-04-10 01:13:59,883 root         INFO     Train Epoch: 90 [5632/8000 (70%)]\tTotal Loss: 0.163123\n",
      "Reconstruction: 0.151648, Regularization: 0.011476\n",
      "2019-04-10 01:13:59,940 root         INFO     Train Epoch: 90 [6144/8000 (77%)]\tTotal Loss: 0.152171\n",
      "Reconstruction: 0.142551, Regularization: 0.009620\n",
      "2019-04-10 01:13:59,996 root         INFO     Train Epoch: 90 [6656/8000 (83%)]\tTotal Loss: 0.167798\n",
      "Reconstruction: 0.157852, Regularization: 0.009946\n",
      "2019-04-10 01:14:00,053 root         INFO     Train Epoch: 90 [7168/8000 (90%)]\tTotal Loss: 0.166409\n",
      "Reconstruction: 0.153301, Regularization: 0.013108\n",
      "2019-04-10 01:14:00,109 root         INFO     Train Epoch: 90 [7680/8000 (96%)]\tTotal Loss: 0.145048\n",
      "Reconstruction: 0.136591, Regularization: 0.008457\n",
      "2019-04-10 01:14:00,160 root         INFO     ====> Epoch: 90 Average loss: 0.1549\n",
      "2019-04-10 01:14:00,183 root         INFO     Train Epoch: 91 [0/8000 (0%)]\tTotal Loss: 0.157112\n",
      "Reconstruction: 0.148404, Regularization: 0.008707\n",
      "2019-04-10 01:14:00,240 root         INFO     Train Epoch: 91 [512/8000 (6%)]\tTotal Loss: 0.156425\n",
      "Reconstruction: 0.145494, Regularization: 0.010931\n",
      "2019-04-10 01:14:00,297 root         INFO     Train Epoch: 91 [1024/8000 (13%)]\tTotal Loss: 0.165569\n",
      "Reconstruction: 0.153930, Regularization: 0.011639\n",
      "2019-04-10 01:14:00,353 root         INFO     Train Epoch: 91 [1536/8000 (19%)]\tTotal Loss: 0.163396\n",
      "Reconstruction: 0.151352, Regularization: 0.012044\n",
      "2019-04-10 01:14:00,409 root         INFO     Train Epoch: 91 [2048/8000 (26%)]\tTotal Loss: 0.159558\n",
      "Reconstruction: 0.144029, Regularization: 0.015529\n",
      "2019-04-10 01:14:00,465 root         INFO     Train Epoch: 91 [2560/8000 (32%)]\tTotal Loss: 0.160972\n",
      "Reconstruction: 0.150620, Regularization: 0.010352\n",
      "2019-04-10 01:14:00,522 root         INFO     Train Epoch: 91 [3072/8000 (38%)]\tTotal Loss: 0.147042\n",
      "Reconstruction: 0.139930, Regularization: 0.007112\n",
      "2019-04-10 01:14:00,579 root         INFO     Train Epoch: 91 [3584/8000 (45%)]\tTotal Loss: 0.140607\n",
      "Reconstruction: 0.132562, Regularization: 0.008046\n",
      "2019-04-10 01:14:00,635 root         INFO     Train Epoch: 91 [4096/8000 (51%)]\tTotal Loss: 0.153283\n",
      "Reconstruction: 0.144322, Regularization: 0.008961\n",
      "2019-04-10 01:14:00,692 root         INFO     Train Epoch: 91 [4608/8000 (58%)]\tTotal Loss: 0.157240\n",
      "Reconstruction: 0.147369, Regularization: 0.009872\n",
      "2019-04-10 01:14:00,749 root         INFO     Train Epoch: 91 [5120/8000 (64%)]\tTotal Loss: 0.160513\n",
      "Reconstruction: 0.151904, Regularization: 0.008609\n",
      "2019-04-10 01:14:00,805 root         INFO     Train Epoch: 91 [5632/8000 (70%)]\tTotal Loss: 0.154743\n",
      "Reconstruction: 0.144188, Regularization: 0.010555\n",
      "2019-04-10 01:14:00,862 root         INFO     Train Epoch: 91 [6144/8000 (77%)]\tTotal Loss: 0.143307\n",
      "Reconstruction: 0.134755, Regularization: 0.008553\n",
      "2019-04-10 01:14:00,918 root         INFO     Train Epoch: 91 [6656/8000 (83%)]\tTotal Loss: 0.138377\n",
      "Reconstruction: 0.130692, Regularization: 0.007685\n",
      "2019-04-10 01:14:00,975 root         INFO     Train Epoch: 91 [7168/8000 (90%)]\tTotal Loss: 0.153612\n",
      "Reconstruction: 0.145362, Regularization: 0.008249\n",
      "2019-04-10 01:14:01,031 root         INFO     Train Epoch: 91 [7680/8000 (96%)]\tTotal Loss: 0.160479\n",
      "Reconstruction: 0.147814, Regularization: 0.012665\n",
      "2019-04-10 01:14:01,081 root         INFO     ====> Epoch: 91 Average loss: 0.1550\n",
      "2019-04-10 01:14:01,104 root         INFO     Train Epoch: 92 [0/8000 (0%)]\tTotal Loss: 0.155771\n",
      "Reconstruction: 0.145889, Regularization: 0.009882\n",
      "2019-04-10 01:14:01,161 root         INFO     Train Epoch: 92 [512/8000 (6%)]\tTotal Loss: 0.160546\n",
      "Reconstruction: 0.150249, Regularization: 0.010297\n",
      "2019-04-10 01:14:01,218 root         INFO     Train Epoch: 92 [1024/8000 (13%)]\tTotal Loss: 0.141030\n",
      "Reconstruction: 0.133467, Regularization: 0.007563\n",
      "2019-04-10 01:14:01,275 root         INFO     Train Epoch: 92 [1536/8000 (19%)]\tTotal Loss: 0.143098\n",
      "Reconstruction: 0.136344, Regularization: 0.006754\n",
      "2019-04-10 01:14:01,332 root         INFO     Train Epoch: 92 [2048/8000 (26%)]\tTotal Loss: 0.155577\n",
      "Reconstruction: 0.145859, Regularization: 0.009718\n",
      "2019-04-10 01:14:01,388 root         INFO     Train Epoch: 92 [2560/8000 (32%)]\tTotal Loss: 0.174280\n",
      "Reconstruction: 0.164711, Regularization: 0.009570\n",
      "2019-04-10 01:14:01,444 root         INFO     Train Epoch: 92 [3072/8000 (38%)]\tTotal Loss: 0.160453\n",
      "Reconstruction: 0.151223, Regularization: 0.009230\n",
      "2019-04-10 01:14:01,501 root         INFO     Train Epoch: 92 [3584/8000 (45%)]\tTotal Loss: 0.150132\n",
      "Reconstruction: 0.141570, Regularization: 0.008562\n",
      "2019-04-10 01:14:01,558 root         INFO     Train Epoch: 92 [4096/8000 (51%)]\tTotal Loss: 0.148939\n",
      "Reconstruction: 0.141353, Regularization: 0.007586\n",
      "2019-04-10 01:14:01,614 root         INFO     Train Epoch: 92 [4608/8000 (58%)]\tTotal Loss: 0.147534\n",
      "Reconstruction: 0.141114, Regularization: 0.006419\n",
      "2019-04-10 01:14:01,671 root         INFO     Train Epoch: 92 [5120/8000 (64%)]\tTotal Loss: 0.154293\n",
      "Reconstruction: 0.143082, Regularization: 0.011211\n",
      "2019-04-10 01:14:01,728 root         INFO     Train Epoch: 92 [5632/8000 (70%)]\tTotal Loss: 0.151447\n",
      "Reconstruction: 0.142821, Regularization: 0.008626\n",
      "2019-04-10 01:14:01,784 root         INFO     Train Epoch: 92 [6144/8000 (77%)]\tTotal Loss: 0.144560\n",
      "Reconstruction: 0.134394, Regularization: 0.010166\n",
      "2019-04-10 01:14:01,841 root         INFO     Train Epoch: 92 [6656/8000 (83%)]\tTotal Loss: 0.147167\n",
      "Reconstruction: 0.137470, Regularization: 0.009697\n",
      "2019-04-10 01:14:01,898 root         INFO     Train Epoch: 92 [7168/8000 (90%)]\tTotal Loss: 0.153263\n",
      "Reconstruction: 0.145262, Regularization: 0.008000\n",
      "2019-04-10 01:14:01,954 root         INFO     Train Epoch: 92 [7680/8000 (96%)]\tTotal Loss: 0.144521\n",
      "Reconstruction: 0.138061, Regularization: 0.006460\n",
      "2019-04-10 01:14:02,004 root         INFO     ====> Epoch: 92 Average loss: 0.1543\n",
      "2019-04-10 01:14:02,028 root         INFO     Train Epoch: 93 [0/8000 (0%)]\tTotal Loss: 0.162266\n",
      "Reconstruction: 0.149403, Regularization: 0.012863\n",
      "2019-04-10 01:14:02,085 root         INFO     Train Epoch: 93 [512/8000 (6%)]\tTotal Loss: 0.165441\n",
      "Reconstruction: 0.155426, Regularization: 0.010015\n",
      "2019-04-10 01:14:02,143 root         INFO     Train Epoch: 93 [1024/8000 (13%)]\tTotal Loss: 0.150623\n",
      "Reconstruction: 0.141878, Regularization: 0.008744\n",
      "2019-04-10 01:14:02,200 root         INFO     Train Epoch: 93 [1536/8000 (19%)]\tTotal Loss: 0.158446\n",
      "Reconstruction: 0.147737, Regularization: 0.010710\n",
      "2019-04-10 01:14:02,257 root         INFO     Train Epoch: 93 [2048/8000 (26%)]\tTotal Loss: 0.155567\n",
      "Reconstruction: 0.145460, Regularization: 0.010107\n",
      "2019-04-10 01:14:02,313 root         INFO     Train Epoch: 93 [2560/8000 (32%)]\tTotal Loss: 0.151034\n",
      "Reconstruction: 0.143337, Regularization: 0.007697\n",
      "2019-04-10 01:14:02,370 root         INFO     Train Epoch: 93 [3072/8000 (38%)]\tTotal Loss: 0.142435\n",
      "Reconstruction: 0.134938, Regularization: 0.007497\n",
      "2019-04-10 01:14:02,426 root         INFO     Train Epoch: 93 [3584/8000 (45%)]\tTotal Loss: 0.137445\n",
      "Reconstruction: 0.130670, Regularization: 0.006775\n",
      "2019-04-10 01:14:02,483 root         INFO     Train Epoch: 93 [4096/8000 (51%)]\tTotal Loss: 0.169769\n",
      "Reconstruction: 0.160489, Regularization: 0.009280\n",
      "2019-04-10 01:14:02,539 root         INFO     Train Epoch: 93 [4608/8000 (58%)]\tTotal Loss: 0.151135\n",
      "Reconstruction: 0.139694, Regularization: 0.011441\n",
      "2019-04-10 01:14:02,596 root         INFO     Train Epoch: 93 [5120/8000 (64%)]\tTotal Loss: 0.145038\n",
      "Reconstruction: 0.137488, Regularization: 0.007550\n",
      "2019-04-10 01:14:02,652 root         INFO     Train Epoch: 93 [5632/8000 (70%)]\tTotal Loss: 0.149624\n",
      "Reconstruction: 0.143168, Regularization: 0.006456\n",
      "2019-04-10 01:14:02,709 root         INFO     Train Epoch: 93 [6144/8000 (77%)]\tTotal Loss: 0.150585\n",
      "Reconstruction: 0.140566, Regularization: 0.010019\n",
      "2019-04-10 01:14:02,765 root         INFO     Train Epoch: 93 [6656/8000 (83%)]\tTotal Loss: 0.139208\n",
      "Reconstruction: 0.131758, Regularization: 0.007450\n",
      "2019-04-10 01:14:02,821 root         INFO     Train Epoch: 93 [7168/8000 (90%)]\tTotal Loss: 0.151233\n",
      "Reconstruction: 0.141395, Regularization: 0.009838\n",
      "2019-04-10 01:14:02,876 root         INFO     Train Epoch: 93 [7680/8000 (96%)]\tTotal Loss: 0.154986\n",
      "Reconstruction: 0.145264, Regularization: 0.009722\n",
      "2019-04-10 01:14:02,927 root         INFO     ====> Epoch: 93 Average loss: 0.1544\n",
      "2019-04-10 01:14:02,950 root         INFO     Train Epoch: 94 [0/8000 (0%)]\tTotal Loss: 0.153623\n",
      "Reconstruction: 0.145222, Regularization: 0.008400\n",
      "2019-04-10 01:14:03,008 root         INFO     Train Epoch: 94 [512/8000 (6%)]\tTotal Loss: 0.148313\n",
      "Reconstruction: 0.140430, Regularization: 0.007882\n",
      "2019-04-10 01:14:03,065 root         INFO     Train Epoch: 94 [1024/8000 (13%)]\tTotal Loss: 0.159991\n",
      "Reconstruction: 0.147072, Regularization: 0.012920\n",
      "2019-04-10 01:14:03,123 root         INFO     Train Epoch: 94 [1536/8000 (19%)]\tTotal Loss: 0.146582\n",
      "Reconstruction: 0.139656, Regularization: 0.006925\n",
      "2019-04-10 01:14:03,180 root         INFO     Train Epoch: 94 [2048/8000 (26%)]\tTotal Loss: 0.153516\n",
      "Reconstruction: 0.141702, Regularization: 0.011813\n",
      "2019-04-10 01:14:03,237 root         INFO     Train Epoch: 94 [2560/8000 (32%)]\tTotal Loss: 0.168905\n",
      "Reconstruction: 0.157780, Regularization: 0.011126\n",
      "2019-04-10 01:14:03,295 root         INFO     Train Epoch: 94 [3072/8000 (38%)]\tTotal Loss: 0.163560\n",
      "Reconstruction: 0.151709, Regularization: 0.011851\n",
      "2019-04-10 01:14:03,352 root         INFO     Train Epoch: 94 [3584/8000 (45%)]\tTotal Loss: 0.166281\n",
      "Reconstruction: 0.156165, Regularization: 0.010116\n",
      "2019-04-10 01:14:03,410 root         INFO     Train Epoch: 94 [4096/8000 (51%)]\tTotal Loss: 0.144632\n",
      "Reconstruction: 0.136161, Regularization: 0.008472\n",
      "2019-04-10 01:14:03,467 root         INFO     Train Epoch: 94 [4608/8000 (58%)]\tTotal Loss: 0.143201\n",
      "Reconstruction: 0.137233, Regularization: 0.005968\n",
      "2019-04-10 01:14:03,525 root         INFO     Train Epoch: 94 [5120/8000 (64%)]\tTotal Loss: 0.159542\n",
      "Reconstruction: 0.150860, Regularization: 0.008682\n",
      "2019-04-10 01:14:03,582 root         INFO     Train Epoch: 94 [5632/8000 (70%)]\tTotal Loss: 0.155688\n",
      "Reconstruction: 0.146053, Regularization: 0.009635\n",
      "2019-04-10 01:14:03,639 root         INFO     Train Epoch: 94 [6144/8000 (77%)]\tTotal Loss: 0.143918\n",
      "Reconstruction: 0.136239, Regularization: 0.007679\n",
      "2019-04-10 01:14:03,696 root         INFO     Train Epoch: 94 [6656/8000 (83%)]\tTotal Loss: 0.161701\n",
      "Reconstruction: 0.151028, Regularization: 0.010673\n",
      "2019-04-10 01:14:03,754 root         INFO     Train Epoch: 94 [7168/8000 (90%)]\tTotal Loss: 0.149752\n",
      "Reconstruction: 0.140950, Regularization: 0.008801\n",
      "2019-04-10 01:14:03,811 root         INFO     Train Epoch: 94 [7680/8000 (96%)]\tTotal Loss: 0.156237\n",
      "Reconstruction: 0.147738, Regularization: 0.008498\n",
      "2019-04-10 01:14:03,861 root         INFO     ====> Epoch: 94 Average loss: 0.1541\n",
      "2019-04-10 01:14:03,885 root         INFO     Train Epoch: 95 [0/8000 (0%)]\tTotal Loss: 0.163555\n",
      "Reconstruction: 0.153496, Regularization: 0.010059\n",
      "2019-04-10 01:14:03,943 root         INFO     Train Epoch: 95 [512/8000 (6%)]\tTotal Loss: 0.168842\n",
      "Reconstruction: 0.157632, Regularization: 0.011210\n",
      "2019-04-10 01:14:04,000 root         INFO     Train Epoch: 95 [1024/8000 (13%)]\tTotal Loss: 0.143917\n",
      "Reconstruction: 0.135537, Regularization: 0.008380\n",
      "2019-04-10 01:14:04,057 root         INFO     Train Epoch: 95 [1536/8000 (19%)]\tTotal Loss: 0.143519\n",
      "Reconstruction: 0.136473, Regularization: 0.007046\n",
      "2019-04-10 01:14:04,114 root         INFO     Train Epoch: 95 [2048/8000 (26%)]\tTotal Loss: 0.152841\n",
      "Reconstruction: 0.144500, Regularization: 0.008341\n",
      "2019-04-10 01:14:04,171 root         INFO     Train Epoch: 95 [2560/8000 (32%)]\tTotal Loss: 0.159914\n",
      "Reconstruction: 0.148374, Regularization: 0.011540\n",
      "2019-04-10 01:14:04,228 root         INFO     Train Epoch: 95 [3072/8000 (38%)]\tTotal Loss: 0.144524\n",
      "Reconstruction: 0.134039, Regularization: 0.010485\n",
      "2019-04-10 01:14:04,285 root         INFO     Train Epoch: 95 [3584/8000 (45%)]\tTotal Loss: 0.148874\n",
      "Reconstruction: 0.142666, Regularization: 0.006208\n",
      "2019-04-10 01:14:04,342 root         INFO     Train Epoch: 95 [4096/8000 (51%)]\tTotal Loss: 0.160497\n",
      "Reconstruction: 0.153866, Regularization: 0.006632\n",
      "2019-04-10 01:14:04,400 root         INFO     Train Epoch: 95 [4608/8000 (58%)]\tTotal Loss: 0.174508\n",
      "Reconstruction: 0.157750, Regularization: 0.016757\n",
      "2019-04-10 01:14:04,457 root         INFO     Train Epoch: 95 [5120/8000 (64%)]\tTotal Loss: 0.153745\n",
      "Reconstruction: 0.143473, Regularization: 0.010272\n",
      "2019-04-10 01:14:04,514 root         INFO     Train Epoch: 95 [5632/8000 (70%)]\tTotal Loss: 0.155895\n",
      "Reconstruction: 0.149228, Regularization: 0.006666\n",
      "2019-04-10 01:14:04,571 root         INFO     Train Epoch: 95 [6144/8000 (77%)]\tTotal Loss: 0.167688\n",
      "Reconstruction: 0.154997, Regularization: 0.012691\n",
      "2019-04-10 01:14:04,628 root         INFO     Train Epoch: 95 [6656/8000 (83%)]\tTotal Loss: 0.154819\n",
      "Reconstruction: 0.144221, Regularization: 0.010598\n",
      "2019-04-10 01:14:04,686 root         INFO     Train Epoch: 95 [7168/8000 (90%)]\tTotal Loss: 0.154446\n",
      "Reconstruction: 0.144684, Regularization: 0.009762\n",
      "2019-04-10 01:14:04,743 root         INFO     Train Epoch: 95 [7680/8000 (96%)]\tTotal Loss: 0.162663\n",
      "Reconstruction: 0.153431, Regularization: 0.009232\n",
      "2019-04-10 01:14:04,793 root         INFO     ====> Epoch: 95 Average loss: 0.1539\n",
      "2019-04-10 01:14:04,817 root         INFO     Train Epoch: 96 [0/8000 (0%)]\tTotal Loss: 0.150447\n",
      "Reconstruction: 0.143863, Regularization: 0.006583\n",
      "2019-04-10 01:14:04,874 root         INFO     Train Epoch: 96 [512/8000 (6%)]\tTotal Loss: 0.158775\n",
      "Reconstruction: 0.150402, Regularization: 0.008373\n",
      "2019-04-10 01:14:04,932 root         INFO     Train Epoch: 96 [1024/8000 (13%)]\tTotal Loss: 0.154039\n",
      "Reconstruction: 0.145757, Regularization: 0.008282\n",
      "2019-04-10 01:14:04,989 root         INFO     Train Epoch: 96 [1536/8000 (19%)]\tTotal Loss: 0.148646\n",
      "Reconstruction: 0.141147, Regularization: 0.007499\n",
      "2019-04-10 01:14:05,047 root         INFO     Train Epoch: 96 [2048/8000 (26%)]\tTotal Loss: 0.154940\n",
      "Reconstruction: 0.144436, Regularization: 0.010504\n",
      "2019-04-10 01:14:05,104 root         INFO     Train Epoch: 96 [2560/8000 (32%)]\tTotal Loss: 0.154105\n",
      "Reconstruction: 0.145903, Regularization: 0.008203\n",
      "2019-04-10 01:14:05,161 root         INFO     Train Epoch: 96 [3072/8000 (38%)]\tTotal Loss: 0.164350\n",
      "Reconstruction: 0.155122, Regularization: 0.009228\n",
      "2019-04-10 01:14:05,219 root         INFO     Train Epoch: 96 [3584/8000 (45%)]\tTotal Loss: 0.161834\n",
      "Reconstruction: 0.151778, Regularization: 0.010056\n",
      "2019-04-10 01:14:05,276 root         INFO     Train Epoch: 96 [4096/8000 (51%)]\tTotal Loss: 0.154274\n",
      "Reconstruction: 0.144980, Regularization: 0.009294\n",
      "2019-04-10 01:14:05,334 root         INFO     Train Epoch: 96 [4608/8000 (58%)]\tTotal Loss: 0.152239\n",
      "Reconstruction: 0.143873, Regularization: 0.008366\n",
      "2019-04-10 01:14:05,391 root         INFO     Train Epoch: 96 [5120/8000 (64%)]\tTotal Loss: 0.165240\n",
      "Reconstruction: 0.155818, Regularization: 0.009423\n",
      "2019-04-10 01:14:05,449 root         INFO     Train Epoch: 96 [5632/8000 (70%)]\tTotal Loss: 0.151597\n",
      "Reconstruction: 0.142641, Regularization: 0.008957\n",
      "2019-04-10 01:14:05,505 root         INFO     Train Epoch: 96 [6144/8000 (77%)]\tTotal Loss: 0.144325\n",
      "Reconstruction: 0.137937, Regularization: 0.006388\n",
      "2019-04-10 01:14:05,562 root         INFO     Train Epoch: 96 [6656/8000 (83%)]\tTotal Loss: 0.148042\n",
      "Reconstruction: 0.141002, Regularization: 0.007039\n",
      "2019-04-10 01:14:05,617 root         INFO     Train Epoch: 96 [7168/8000 (90%)]\tTotal Loss: 0.149308\n",
      "Reconstruction: 0.139747, Regularization: 0.009561\n",
      "2019-04-10 01:14:05,673 root         INFO     Train Epoch: 96 [7680/8000 (96%)]\tTotal Loss: 0.187807\n",
      "Reconstruction: 0.177418, Regularization: 0.010389\n",
      "2019-04-10 01:14:05,723 root         INFO     ====> Epoch: 96 Average loss: 0.1540\n",
      "2019-04-10 01:14:05,747 root         INFO     Train Epoch: 97 [0/8000 (0%)]\tTotal Loss: 0.141766\n",
      "Reconstruction: 0.136553, Regularization: 0.005213\n",
      "2019-04-10 01:14:05,804 root         INFO     Train Epoch: 97 [512/8000 (6%)]\tTotal Loss: 0.155767\n",
      "Reconstruction: 0.147875, Regularization: 0.007891\n",
      "2019-04-10 01:14:05,861 root         INFO     Train Epoch: 97 [1024/8000 (13%)]\tTotal Loss: 0.154026\n",
      "Reconstruction: 0.146794, Regularization: 0.007232\n",
      "2019-04-10 01:14:05,918 root         INFO     Train Epoch: 97 [1536/8000 (19%)]\tTotal Loss: 0.142860\n",
      "Reconstruction: 0.135881, Regularization: 0.006979\n",
      "2019-04-10 01:14:05,975 root         INFO     Train Epoch: 97 [2048/8000 (26%)]\tTotal Loss: 0.151508\n",
      "Reconstruction: 0.143563, Regularization: 0.007944\n",
      "2019-04-10 01:14:06,033 root         INFO     Train Epoch: 97 [2560/8000 (32%)]\tTotal Loss: 0.165078\n",
      "Reconstruction: 0.153130, Regularization: 0.011948\n",
      "2019-04-10 01:14:06,090 root         INFO     Train Epoch: 97 [3072/8000 (38%)]\tTotal Loss: 0.147070\n",
      "Reconstruction: 0.139861, Regularization: 0.007209\n",
      "2019-04-10 01:14:06,147 root         INFO     Train Epoch: 97 [3584/8000 (45%)]\tTotal Loss: 0.156214\n",
      "Reconstruction: 0.149121, Regularization: 0.007094\n",
      "2019-04-10 01:14:06,204 root         INFO     Train Epoch: 97 [4096/8000 (51%)]\tTotal Loss: 0.155819\n",
      "Reconstruction: 0.147540, Regularization: 0.008279\n",
      "2019-04-10 01:14:06,261 root         INFO     Train Epoch: 97 [4608/8000 (58%)]\tTotal Loss: 0.161460\n",
      "Reconstruction: 0.148654, Regularization: 0.012806\n",
      "2019-04-10 01:14:06,318 root         INFO     Train Epoch: 97 [5120/8000 (64%)]\tTotal Loss: 0.153242\n",
      "Reconstruction: 0.146131, Regularization: 0.007111\n",
      "2019-04-10 01:14:06,375 root         INFO     Train Epoch: 97 [5632/8000 (70%)]\tTotal Loss: 0.150972\n",
      "Reconstruction: 0.142317, Regularization: 0.008655\n",
      "2019-04-10 01:14:06,433 root         INFO     Train Epoch: 97 [6144/8000 (77%)]\tTotal Loss: 0.159371\n",
      "Reconstruction: 0.147504, Regularization: 0.011867\n",
      "2019-04-10 01:14:06,490 root         INFO     Train Epoch: 97 [6656/8000 (83%)]\tTotal Loss: 0.173545\n",
      "Reconstruction: 0.160113, Regularization: 0.013432\n",
      "2019-04-10 01:14:06,547 root         INFO     Train Epoch: 97 [7168/8000 (90%)]\tTotal Loss: 0.174562\n",
      "Reconstruction: 0.164248, Regularization: 0.010315\n",
      "2019-04-10 01:14:06,602 root         INFO     Train Epoch: 97 [7680/8000 (96%)]\tTotal Loss: 0.139283\n",
      "Reconstruction: 0.133301, Regularization: 0.005981\n",
      "2019-04-10 01:14:06,651 root         INFO     ====> Epoch: 97 Average loss: 0.1538\n",
      "2019-04-10 01:14:06,675 root         INFO     Train Epoch: 98 [0/8000 (0%)]\tTotal Loss: 0.158079\n",
      "Reconstruction: 0.148428, Regularization: 0.009651\n",
      "2019-04-10 01:14:06,732 root         INFO     Train Epoch: 98 [512/8000 (6%)]\tTotal Loss: 0.152106\n",
      "Reconstruction: 0.145971, Regularization: 0.006134\n",
      "2019-04-10 01:14:06,789 root         INFO     Train Epoch: 98 [1024/8000 (13%)]\tTotal Loss: 0.146233\n",
      "Reconstruction: 0.139531, Regularization: 0.006702\n",
      "2019-04-10 01:14:06,846 root         INFO     Train Epoch: 98 [1536/8000 (19%)]\tTotal Loss: 0.154064\n",
      "Reconstruction: 0.143980, Regularization: 0.010084\n",
      "2019-04-10 01:14:06,902 root         INFO     Train Epoch: 98 [2048/8000 (26%)]\tTotal Loss: 0.153258\n",
      "Reconstruction: 0.143884, Regularization: 0.009374\n",
      "2019-04-10 01:14:06,959 root         INFO     Train Epoch: 98 [2560/8000 (32%)]\tTotal Loss: 0.153230\n",
      "Reconstruction: 0.144795, Regularization: 0.008435\n",
      "2019-04-10 01:14:07,015 root         INFO     Train Epoch: 98 [3072/8000 (38%)]\tTotal Loss: 0.142775\n",
      "Reconstruction: 0.136818, Regularization: 0.005958\n",
      "2019-04-10 01:14:07,070 root         INFO     Train Epoch: 98 [3584/8000 (45%)]\tTotal Loss: 0.155548\n",
      "Reconstruction: 0.145430, Regularization: 0.010119\n",
      "2019-04-10 01:14:07,125 root         INFO     Train Epoch: 98 [4096/8000 (51%)]\tTotal Loss: 0.155747\n",
      "Reconstruction: 0.145708, Regularization: 0.010039\n",
      "2019-04-10 01:14:07,181 root         INFO     Train Epoch: 98 [4608/8000 (58%)]\tTotal Loss: 0.136988\n",
      "Reconstruction: 0.130789, Regularization: 0.006199\n",
      "2019-04-10 01:14:07,236 root         INFO     Train Epoch: 98 [5120/8000 (64%)]\tTotal Loss: 0.147180\n",
      "Reconstruction: 0.140770, Regularization: 0.006410\n",
      "2019-04-10 01:14:07,291 root         INFO     Train Epoch: 98 [5632/8000 (70%)]\tTotal Loss: 0.149486\n",
      "Reconstruction: 0.142413, Regularization: 0.007073\n",
      "2019-04-10 01:14:07,346 root         INFO     Train Epoch: 98 [6144/8000 (77%)]\tTotal Loss: 0.154364\n",
      "Reconstruction: 0.145818, Regularization: 0.008546\n",
      "2019-04-10 01:14:07,401 root         INFO     Train Epoch: 98 [6656/8000 (83%)]\tTotal Loss: 0.155502\n",
      "Reconstruction: 0.148594, Regularization: 0.006908\n",
      "2019-04-10 01:14:07,456 root         INFO     Train Epoch: 98 [7168/8000 (90%)]\tTotal Loss: 0.151269\n",
      "Reconstruction: 0.141679, Regularization: 0.009590\n",
      "2019-04-10 01:14:07,511 root         INFO     Train Epoch: 98 [7680/8000 (96%)]\tTotal Loss: 0.149617\n",
      "Reconstruction: 0.141476, Regularization: 0.008141\n",
      "2019-04-10 01:14:07,560 root         INFO     ====> Epoch: 98 Average loss: 0.1531\n",
      "2019-04-10 01:14:07,584 root         INFO     Train Epoch: 99 [0/8000 (0%)]\tTotal Loss: 0.162206\n",
      "Reconstruction: 0.154820, Regularization: 0.007386\n",
      "2019-04-10 01:14:07,640 root         INFO     Train Epoch: 99 [512/8000 (6%)]\tTotal Loss: 0.148472\n",
      "Reconstruction: 0.141233, Regularization: 0.007238\n",
      "2019-04-10 01:14:07,697 root         INFO     Train Epoch: 99 [1024/8000 (13%)]\tTotal Loss: 0.145963\n",
      "Reconstruction: 0.139329, Regularization: 0.006634\n",
      "2019-04-10 01:14:07,754 root         INFO     Train Epoch: 99 [1536/8000 (19%)]\tTotal Loss: 0.148278\n",
      "Reconstruction: 0.142391, Regularization: 0.005887\n",
      "2019-04-10 01:14:07,810 root         INFO     Train Epoch: 99 [2048/8000 (26%)]\tTotal Loss: 0.160096\n",
      "Reconstruction: 0.150623, Regularization: 0.009473\n",
      "2019-04-10 01:14:07,867 root         INFO     Train Epoch: 99 [2560/8000 (32%)]\tTotal Loss: 0.149255\n",
      "Reconstruction: 0.142807, Regularization: 0.006448\n",
      "2019-04-10 01:14:07,923 root         INFO     Train Epoch: 99 [3072/8000 (38%)]\tTotal Loss: 0.156825\n",
      "Reconstruction: 0.146170, Regularization: 0.010656\n",
      "2019-04-10 01:14:07,980 root         INFO     Train Epoch: 99 [3584/8000 (45%)]\tTotal Loss: 0.145481\n",
      "Reconstruction: 0.138310, Regularization: 0.007171\n",
      "2019-04-10 01:14:08,036 root         INFO     Train Epoch: 99 [4096/8000 (51%)]\tTotal Loss: 0.158420\n",
      "Reconstruction: 0.148202, Regularization: 0.010218\n",
      "2019-04-10 01:14:08,091 root         INFO     Train Epoch: 99 [4608/8000 (58%)]\tTotal Loss: 0.166209\n",
      "Reconstruction: 0.155986, Regularization: 0.010223\n",
      "2019-04-10 01:14:08,146 root         INFO     Train Epoch: 99 [5120/8000 (64%)]\tTotal Loss: 0.151436\n",
      "Reconstruction: 0.142203, Regularization: 0.009233\n",
      "2019-04-10 01:14:08,201 root         INFO     Train Epoch: 99 [5632/8000 (70%)]\tTotal Loss: 0.158484\n",
      "Reconstruction: 0.152095, Regularization: 0.006390\n",
      "2019-04-10 01:14:08,256 root         INFO     Train Epoch: 99 [6144/8000 (77%)]\tTotal Loss: 0.152384\n",
      "Reconstruction: 0.145671, Regularization: 0.006713\n",
      "2019-04-10 01:14:08,311 root         INFO     Train Epoch: 99 [6656/8000 (83%)]\tTotal Loss: 0.151352\n",
      "Reconstruction: 0.144681, Regularization: 0.006671\n",
      "2019-04-10 01:14:08,367 root         INFO     Train Epoch: 99 [7168/8000 (90%)]\tTotal Loss: 0.153229\n",
      "Reconstruction: 0.147391, Regularization: 0.005838\n",
      "2019-04-10 01:14:08,422 root         INFO     Train Epoch: 99 [7680/8000 (96%)]\tTotal Loss: 0.145292\n",
      "Reconstruction: 0.137769, Regularization: 0.007523\n",
      "2019-04-10 01:14:08,471 root         INFO     ====> Epoch: 99 Average loss: 0.1533\n",
      "2019-04-10 01:14:08,495 root         INFO     Train Epoch: 100 [0/8000 (0%)]\tTotal Loss: 0.149875\n",
      "Reconstruction: 0.142555, Regularization: 0.007320\n",
      "2019-04-10 01:14:08,551 root         INFO     Train Epoch: 100 [512/8000 (6%)]\tTotal Loss: 0.146490\n",
      "Reconstruction: 0.140027, Regularization: 0.006462\n",
      "2019-04-10 01:14:08,607 root         INFO     Train Epoch: 100 [1024/8000 (13%)]\tTotal Loss: 0.154761\n",
      "Reconstruction: 0.144651, Regularization: 0.010111\n",
      "2019-04-10 01:14:08,662 root         INFO     Train Epoch: 100 [1536/8000 (19%)]\tTotal Loss: 0.141274\n",
      "Reconstruction: 0.136164, Regularization: 0.005110\n",
      "2019-04-10 01:14:08,718 root         INFO     Train Epoch: 100 [2048/8000 (26%)]\tTotal Loss: 0.141170\n",
      "Reconstruction: 0.135341, Regularization: 0.005829\n",
      "2019-04-10 01:14:08,773 root         INFO     Train Epoch: 100 [2560/8000 (32%)]\tTotal Loss: 0.146757\n",
      "Reconstruction: 0.138903, Regularization: 0.007855\n",
      "2019-04-10 01:14:08,828 root         INFO     Train Epoch: 100 [3072/8000 (38%)]\tTotal Loss: 0.149419\n",
      "Reconstruction: 0.141330, Regularization: 0.008089\n",
      "2019-04-10 01:14:08,883 root         INFO     Train Epoch: 100 [3584/8000 (45%)]\tTotal Loss: 0.154313\n",
      "Reconstruction: 0.146479, Regularization: 0.007834\n",
      "2019-04-10 01:14:08,939 root         INFO     Train Epoch: 100 [4096/8000 (51%)]\tTotal Loss: 0.136472\n",
      "Reconstruction: 0.131677, Regularization: 0.004794\n",
      "2019-04-10 01:14:08,994 root         INFO     Train Epoch: 100 [4608/8000 (58%)]\tTotal Loss: 0.131032\n",
      "Reconstruction: 0.125846, Regularization: 0.005185\n",
      "2019-04-10 01:14:09,049 root         INFO     Train Epoch: 100 [5120/8000 (64%)]\tTotal Loss: 0.149935\n",
      "Reconstruction: 0.142655, Regularization: 0.007280\n",
      "2019-04-10 01:14:09,105 root         INFO     Train Epoch: 100 [5632/8000 (70%)]\tTotal Loss: 0.161816\n",
      "Reconstruction: 0.152802, Regularization: 0.009014\n",
      "2019-04-10 01:14:09,160 root         INFO     Train Epoch: 100 [6144/8000 (77%)]\tTotal Loss: 0.166482\n",
      "Reconstruction: 0.155634, Regularization: 0.010848\n",
      "2019-04-10 01:14:09,216 root         INFO     Train Epoch: 100 [6656/8000 (83%)]\tTotal Loss: 0.161509\n",
      "Reconstruction: 0.153458, Regularization: 0.008051\n",
      "2019-04-10 01:14:09,271 root         INFO     Train Epoch: 100 [7168/8000 (90%)]\tTotal Loss: 0.153223\n",
      "Reconstruction: 0.147096, Regularization: 0.006128\n",
      "2019-04-10 01:14:09,327 root         INFO     Train Epoch: 100 [7680/8000 (96%)]\tTotal Loss: 0.153267\n",
      "Reconstruction: 0.145190, Regularization: 0.008077\n",
      "2019-04-10 01:14:09,377 root         INFO     ====> Epoch: 100 Average loss: 0.1529\n",
      "2019-04-10 01:14:09,400 root         INFO     Train Epoch: 101 [0/8000 (0%)]\tTotal Loss: 0.147314\n",
      "Reconstruction: 0.137708, Regularization: 0.009607\n",
      "2019-04-10 01:14:09,457 root         INFO     Train Epoch: 101 [512/8000 (6%)]\tTotal Loss: 0.154190\n",
      "Reconstruction: 0.145131, Regularization: 0.009059\n",
      "2019-04-10 01:14:09,514 root         INFO     Train Epoch: 101 [1024/8000 (13%)]\tTotal Loss: 0.159092\n",
      "Reconstruction: 0.150689, Regularization: 0.008403\n",
      "2019-04-10 01:14:09,570 root         INFO     Train Epoch: 101 [1536/8000 (19%)]\tTotal Loss: 0.149509\n",
      "Reconstruction: 0.144001, Regularization: 0.005508\n",
      "2019-04-10 01:14:09,627 root         INFO     Train Epoch: 101 [2048/8000 (26%)]\tTotal Loss: 0.168010\n",
      "Reconstruction: 0.159113, Regularization: 0.008897\n",
      "2019-04-10 01:14:09,684 root         INFO     Train Epoch: 101 [2560/8000 (32%)]\tTotal Loss: 0.164544\n",
      "Reconstruction: 0.156488, Regularization: 0.008056\n",
      "2019-04-10 01:14:09,740 root         INFO     Train Epoch: 101 [3072/8000 (38%)]\tTotal Loss: 0.152903\n",
      "Reconstruction: 0.145175, Regularization: 0.007728\n",
      "2019-04-10 01:14:09,796 root         INFO     Train Epoch: 101 [3584/8000 (45%)]\tTotal Loss: 0.171604\n",
      "Reconstruction: 0.160171, Regularization: 0.011433\n",
      "2019-04-10 01:14:09,852 root         INFO     Train Epoch: 101 [4096/8000 (51%)]\tTotal Loss: 0.154809\n",
      "Reconstruction: 0.146973, Regularization: 0.007836\n",
      "2019-04-10 01:14:09,908 root         INFO     Train Epoch: 101 [4608/8000 (58%)]\tTotal Loss: 0.149318\n",
      "Reconstruction: 0.139161, Regularization: 0.010157\n",
      "2019-04-10 01:14:09,963 root         INFO     Train Epoch: 101 [5120/8000 (64%)]\tTotal Loss: 0.145695\n",
      "Reconstruction: 0.138953, Regularization: 0.006742\n",
      "2019-04-10 01:14:10,019 root         INFO     Train Epoch: 101 [5632/8000 (70%)]\tTotal Loss: 0.168295\n",
      "Reconstruction: 0.155911, Regularization: 0.012384\n",
      "2019-04-10 01:14:10,074 root         INFO     Train Epoch: 101 [6144/8000 (77%)]\tTotal Loss: 0.132384\n",
      "Reconstruction: 0.126822, Regularization: 0.005561\n",
      "2019-04-10 01:14:10,129 root         INFO     Train Epoch: 101 [6656/8000 (83%)]\tTotal Loss: 0.145155\n",
      "Reconstruction: 0.138873, Regularization: 0.006282\n",
      "2019-04-10 01:14:10,185 root         INFO     Train Epoch: 101 [7168/8000 (90%)]\tTotal Loss: 0.151920\n",
      "Reconstruction: 0.142646, Regularization: 0.009274\n",
      "2019-04-10 01:14:10,240 root         INFO     Train Epoch: 101 [7680/8000 (96%)]\tTotal Loss: 0.160837\n",
      "Reconstruction: 0.150934, Regularization: 0.009903\n",
      "2019-04-10 01:14:10,289 root         INFO     ====> Epoch: 101 Average loss: 0.1529\n",
      "2019-04-10 01:14:10,313 root         INFO     Train Epoch: 102 [0/8000 (0%)]\tTotal Loss: 0.153994\n",
      "Reconstruction: 0.147463, Regularization: 0.006531\n",
      "2019-04-10 01:14:10,370 root         INFO     Train Epoch: 102 [512/8000 (6%)]\tTotal Loss: 0.158748\n",
      "Reconstruction: 0.151497, Regularization: 0.007251\n",
      "2019-04-10 01:14:10,426 root         INFO     Train Epoch: 102 [1024/8000 (13%)]\tTotal Loss: 0.158465\n",
      "Reconstruction: 0.150928, Regularization: 0.007537\n",
      "2019-04-10 01:14:10,483 root         INFO     Train Epoch: 102 [1536/8000 (19%)]\tTotal Loss: 0.154620\n",
      "Reconstruction: 0.147964, Regularization: 0.006656\n",
      "2019-04-10 01:14:10,539 root         INFO     Train Epoch: 102 [2048/8000 (26%)]\tTotal Loss: 0.144084\n",
      "Reconstruction: 0.138208, Regularization: 0.005876\n",
      "2019-04-10 01:14:10,596 root         INFO     Train Epoch: 102 [2560/8000 (32%)]\tTotal Loss: 0.156178\n",
      "Reconstruction: 0.146663, Regularization: 0.009515\n",
      "2019-04-10 01:14:10,652 root         INFO     Train Epoch: 102 [3072/8000 (38%)]\tTotal Loss: 0.140330\n",
      "Reconstruction: 0.134821, Regularization: 0.005509\n",
      "2019-04-10 01:14:10,708 root         INFO     Train Epoch: 102 [3584/8000 (45%)]\tTotal Loss: 0.140098\n",
      "Reconstruction: 0.135711, Regularization: 0.004387\n",
      "2019-04-10 01:14:10,764 root         INFO     Train Epoch: 102 [4096/8000 (51%)]\tTotal Loss: 0.165225\n",
      "Reconstruction: 0.154430, Regularization: 0.010795\n",
      "2019-04-10 01:14:10,821 root         INFO     Train Epoch: 102 [4608/8000 (58%)]\tTotal Loss: 0.176426\n",
      "Reconstruction: 0.167337, Regularization: 0.009089\n",
      "2019-04-10 01:14:10,877 root         INFO     Train Epoch: 102 [5120/8000 (64%)]\tTotal Loss: 0.149311\n",
      "Reconstruction: 0.140946, Regularization: 0.008366\n",
      "2019-04-10 01:14:10,933 root         INFO     Train Epoch: 102 [5632/8000 (70%)]\tTotal Loss: 0.149229\n",
      "Reconstruction: 0.141112, Regularization: 0.008117\n",
      "2019-04-10 01:14:10,990 root         INFO     Train Epoch: 102 [6144/8000 (77%)]\tTotal Loss: 0.166815\n",
      "Reconstruction: 0.159393, Regularization: 0.007422\n",
      "2019-04-10 01:14:11,046 root         INFO     Train Epoch: 102 [6656/8000 (83%)]\tTotal Loss: 0.152069\n",
      "Reconstruction: 0.145369, Regularization: 0.006700\n",
      "2019-04-10 01:14:11,102 root         INFO     Train Epoch: 102 [7168/8000 (90%)]\tTotal Loss: 0.137504\n",
      "Reconstruction: 0.132987, Regularization: 0.004517\n",
      "2019-04-10 01:14:11,159 root         INFO     Train Epoch: 102 [7680/8000 (96%)]\tTotal Loss: 0.135324\n",
      "Reconstruction: 0.130098, Regularization: 0.005227\n",
      "2019-04-10 01:14:11,209 root         INFO     ====> Epoch: 102 Average loss: 0.1524\n",
      "2019-04-10 01:14:11,232 root         INFO     Train Epoch: 103 [0/8000 (0%)]\tTotal Loss: 0.154995\n",
      "Reconstruction: 0.145182, Regularization: 0.009813\n",
      "2019-04-10 01:14:11,289 root         INFO     Train Epoch: 103 [512/8000 (6%)]\tTotal Loss: 0.152175\n",
      "Reconstruction: 0.143143, Regularization: 0.009033\n",
      "2019-04-10 01:14:11,345 root         INFO     Train Epoch: 103 [1024/8000 (13%)]\tTotal Loss: 0.149573\n",
      "Reconstruction: 0.144024, Regularization: 0.005549\n",
      "2019-04-10 01:14:11,401 root         INFO     Train Epoch: 103 [1536/8000 (19%)]\tTotal Loss: 0.140808\n",
      "Reconstruction: 0.134968, Regularization: 0.005841\n",
      "2019-04-10 01:14:11,458 root         INFO     Train Epoch: 103 [2048/8000 (26%)]\tTotal Loss: 0.149966\n",
      "Reconstruction: 0.142027, Regularization: 0.007939\n",
      "2019-04-10 01:14:11,515 root         INFO     Train Epoch: 103 [2560/8000 (32%)]\tTotal Loss: 0.131587\n",
      "Reconstruction: 0.128130, Regularization: 0.003457\n",
      "2019-04-10 01:14:11,572 root         INFO     Train Epoch: 103 [3072/8000 (38%)]\tTotal Loss: 0.144124\n",
      "Reconstruction: 0.138676, Regularization: 0.005448\n",
      "2019-04-10 01:14:11,631 root         INFO     Train Epoch: 103 [3584/8000 (45%)]\tTotal Loss: 0.163136\n",
      "Reconstruction: 0.155819, Regularization: 0.007317\n",
      "2019-04-10 01:14:11,690 root         INFO     Train Epoch: 103 [4096/8000 (51%)]\tTotal Loss: 0.143319\n",
      "Reconstruction: 0.135709, Regularization: 0.007610\n",
      "2019-04-10 01:14:11,749 root         INFO     Train Epoch: 103 [4608/8000 (58%)]\tTotal Loss: 0.161366\n",
      "Reconstruction: 0.154032, Regularization: 0.007334\n",
      "2019-04-10 01:14:11,808 root         INFO     Train Epoch: 103 [5120/8000 (64%)]\tTotal Loss: 0.150239\n",
      "Reconstruction: 0.140904, Regularization: 0.009335\n",
      "2019-04-10 01:14:11,867 root         INFO     Train Epoch: 103 [5632/8000 (70%)]\tTotal Loss: 0.150207\n",
      "Reconstruction: 0.139909, Regularization: 0.010298\n",
      "2019-04-10 01:14:11,926 root         INFO     Train Epoch: 103 [6144/8000 (77%)]\tTotal Loss: 0.152554\n",
      "Reconstruction: 0.145255, Regularization: 0.007299\n",
      "2019-04-10 01:14:11,984 root         INFO     Train Epoch: 103 [6656/8000 (83%)]\tTotal Loss: 0.155102\n",
      "Reconstruction: 0.149337, Regularization: 0.005765\n",
      "2019-04-10 01:14:12,041 root         INFO     Train Epoch: 103 [7168/8000 (90%)]\tTotal Loss: 0.151815\n",
      "Reconstruction: 0.145721, Regularization: 0.006093\n",
      "2019-04-10 01:14:12,097 root         INFO     Train Epoch: 103 [7680/8000 (96%)]\tTotal Loss: 0.166762\n",
      "Reconstruction: 0.154861, Regularization: 0.011901\n",
      "2019-04-10 01:14:12,146 root         INFO     ====> Epoch: 103 Average loss: 0.1518\n",
      "2019-04-10 01:14:12,170 root         INFO     Train Epoch: 104 [0/8000 (0%)]\tTotal Loss: 0.143848\n",
      "Reconstruction: 0.137331, Regularization: 0.006517\n",
      "2019-04-10 01:14:12,227 root         INFO     Train Epoch: 104 [512/8000 (6%)]\tTotal Loss: 0.141231\n",
      "Reconstruction: 0.135770, Regularization: 0.005461\n",
      "2019-04-10 01:14:12,284 root         INFO     Train Epoch: 104 [1024/8000 (13%)]\tTotal Loss: 0.153590\n",
      "Reconstruction: 0.146044, Regularization: 0.007545\n",
      "2019-04-10 01:14:12,340 root         INFO     Train Epoch: 104 [1536/8000 (19%)]\tTotal Loss: 0.167468\n",
      "Reconstruction: 0.158705, Regularization: 0.008763\n",
      "2019-04-10 01:14:12,397 root         INFO     Train Epoch: 104 [2048/8000 (26%)]\tTotal Loss: 0.144536\n",
      "Reconstruction: 0.136592, Regularization: 0.007944\n",
      "2019-04-10 01:14:12,453 root         INFO     Train Epoch: 104 [2560/8000 (32%)]\tTotal Loss: 0.154901\n",
      "Reconstruction: 0.148599, Regularization: 0.006302\n",
      "2019-04-10 01:14:12,510 root         INFO     Train Epoch: 104 [3072/8000 (38%)]\tTotal Loss: 0.138927\n",
      "Reconstruction: 0.132702, Regularization: 0.006225\n",
      "2019-04-10 01:14:12,566 root         INFO     Train Epoch: 104 [3584/8000 (45%)]\tTotal Loss: 0.166196\n",
      "Reconstruction: 0.157222, Regularization: 0.008975\n",
      "2019-04-10 01:14:12,623 root         INFO     Train Epoch: 104 [4096/8000 (51%)]\tTotal Loss: 0.146462\n",
      "Reconstruction: 0.140446, Regularization: 0.006015\n",
      "2019-04-10 01:14:12,680 root         INFO     Train Epoch: 104 [4608/8000 (58%)]\tTotal Loss: 0.157507\n",
      "Reconstruction: 0.149762, Regularization: 0.007745\n",
      "2019-04-10 01:14:12,736 root         INFO     Train Epoch: 104 [5120/8000 (64%)]\tTotal Loss: 0.161490\n",
      "Reconstruction: 0.155299, Regularization: 0.006192\n",
      "2019-04-10 01:14:12,791 root         INFO     Train Epoch: 104 [5632/8000 (70%)]\tTotal Loss: 0.154166\n",
      "Reconstruction: 0.143883, Regularization: 0.010283\n",
      "2019-04-10 01:14:12,846 root         INFO     Train Epoch: 104 [6144/8000 (77%)]\tTotal Loss: 0.151081\n",
      "Reconstruction: 0.144472, Regularization: 0.006609\n",
      "2019-04-10 01:14:12,901 root         INFO     Train Epoch: 104 [6656/8000 (83%)]\tTotal Loss: 0.156101\n",
      "Reconstruction: 0.149408, Regularization: 0.006693\n",
      "2019-04-10 01:14:12,957 root         INFO     Train Epoch: 104 [7168/8000 (90%)]\tTotal Loss: 0.147590\n",
      "Reconstruction: 0.142977, Regularization: 0.004613\n",
      "2019-04-10 01:14:13,012 root         INFO     Train Epoch: 104 [7680/8000 (96%)]\tTotal Loss: 0.168687\n",
      "Reconstruction: 0.159846, Regularization: 0.008841\n",
      "2019-04-10 01:14:13,061 root         INFO     ====> Epoch: 104 Average loss: 0.1521\n",
      "2019-04-10 01:14:13,085 root         INFO     Train Epoch: 105 [0/8000 (0%)]\tTotal Loss: 0.160686\n",
      "Reconstruction: 0.152191, Regularization: 0.008495\n",
      "2019-04-10 01:14:13,142 root         INFO     Train Epoch: 105 [512/8000 (6%)]\tTotal Loss: 0.152662\n",
      "Reconstruction: 0.144712, Regularization: 0.007949\n",
      "2019-04-10 01:14:13,199 root         INFO     Train Epoch: 105 [1024/8000 (13%)]\tTotal Loss: 0.163084\n",
      "Reconstruction: 0.154431, Regularization: 0.008653\n",
      "2019-04-10 01:14:13,255 root         INFO     Train Epoch: 105 [1536/8000 (19%)]\tTotal Loss: 0.146076\n",
      "Reconstruction: 0.140286, Regularization: 0.005791\n",
      "2019-04-10 01:14:13,312 root         INFO     Train Epoch: 105 [2048/8000 (26%)]\tTotal Loss: 0.159013\n",
      "Reconstruction: 0.148719, Regularization: 0.010294\n",
      "2019-04-10 01:14:13,368 root         INFO     Train Epoch: 105 [2560/8000 (32%)]\tTotal Loss: 0.134467\n",
      "Reconstruction: 0.130503, Regularization: 0.003964\n",
      "2019-04-10 01:14:13,424 root         INFO     Train Epoch: 105 [3072/8000 (38%)]\tTotal Loss: 0.176751\n",
      "Reconstruction: 0.167270, Regularization: 0.009481\n",
      "2019-04-10 01:14:13,481 root         INFO     Train Epoch: 105 [3584/8000 (45%)]\tTotal Loss: 0.156423\n",
      "Reconstruction: 0.147533, Regularization: 0.008890\n",
      "2019-04-10 01:14:13,537 root         INFO     Train Epoch: 105 [4096/8000 (51%)]\tTotal Loss: 0.159903\n",
      "Reconstruction: 0.150101, Regularization: 0.009802\n",
      "2019-04-10 01:14:13,593 root         INFO     Train Epoch: 105 [4608/8000 (58%)]\tTotal Loss: 0.140825\n",
      "Reconstruction: 0.135388, Regularization: 0.005437\n",
      "2019-04-10 01:14:13,649 root         INFO     Train Epoch: 105 [5120/8000 (64%)]\tTotal Loss: 0.159490\n",
      "Reconstruction: 0.152776, Regularization: 0.006713\n",
      "2019-04-10 01:14:13,706 root         INFO     Train Epoch: 105 [5632/8000 (70%)]\tTotal Loss: 0.156387\n",
      "Reconstruction: 0.147510, Regularization: 0.008877\n",
      "2019-04-10 01:14:13,763 root         INFO     Train Epoch: 105 [6144/8000 (77%)]\tTotal Loss: 0.165017\n",
      "Reconstruction: 0.158209, Regularization: 0.006808\n",
      "2019-04-10 01:14:13,818 root         INFO     Train Epoch: 105 [6656/8000 (83%)]\tTotal Loss: 0.136499\n",
      "Reconstruction: 0.132229, Regularization: 0.004271\n",
      "2019-04-10 01:14:13,874 root         INFO     Train Epoch: 105 [7168/8000 (90%)]\tTotal Loss: 0.137276\n",
      "Reconstruction: 0.133394, Regularization: 0.003882\n",
      "2019-04-10 01:14:13,930 root         INFO     Train Epoch: 105 [7680/8000 (96%)]\tTotal Loss: 0.160417\n",
      "Reconstruction: 0.150555, Regularization: 0.009862\n",
      "2019-04-10 01:14:13,980 root         INFO     ====> Epoch: 105 Average loss: 0.1518\n",
      "2019-04-10 01:14:14,003 root         INFO     Train Epoch: 106 [0/8000 (0%)]\tTotal Loss: 0.157990\n",
      "Reconstruction: 0.151113, Regularization: 0.006878\n",
      "2019-04-10 01:14:14,061 root         INFO     Train Epoch: 106 [512/8000 (6%)]\tTotal Loss: 0.159967\n",
      "Reconstruction: 0.154689, Regularization: 0.005278\n",
      "2019-04-10 01:14:14,118 root         INFO     Train Epoch: 106 [1024/8000 (13%)]\tTotal Loss: 0.146067\n",
      "Reconstruction: 0.140578, Regularization: 0.005489\n",
      "2019-04-10 01:14:14,176 root         INFO     Train Epoch: 106 [1536/8000 (19%)]\tTotal Loss: 0.144882\n",
      "Reconstruction: 0.136371, Regularization: 0.008511\n",
      "2019-04-10 01:14:14,233 root         INFO     Train Epoch: 106 [2048/8000 (26%)]\tTotal Loss: 0.149613\n",
      "Reconstruction: 0.144127, Regularization: 0.005487\n",
      "2019-04-10 01:14:14,288 root         INFO     Train Epoch: 106 [2560/8000 (32%)]\tTotal Loss: 0.143712\n",
      "Reconstruction: 0.135724, Regularization: 0.007988\n",
      "2019-04-10 01:14:14,343 root         INFO     Train Epoch: 106 [3072/8000 (38%)]\tTotal Loss: 0.137748\n",
      "Reconstruction: 0.131207, Regularization: 0.006541\n",
      "2019-04-10 01:14:14,400 root         INFO     Train Epoch: 106 [3584/8000 (45%)]\tTotal Loss: 0.155576\n",
      "Reconstruction: 0.147700, Regularization: 0.007876\n",
      "2019-04-10 01:14:14,457 root         INFO     Train Epoch: 106 [4096/8000 (51%)]\tTotal Loss: 0.151070\n",
      "Reconstruction: 0.145094, Regularization: 0.005976\n",
      "2019-04-10 01:14:14,513 root         INFO     Train Epoch: 106 [4608/8000 (58%)]\tTotal Loss: 0.149008\n",
      "Reconstruction: 0.142632, Regularization: 0.006376\n",
      "2019-04-10 01:14:14,568 root         INFO     Train Epoch: 106 [5120/8000 (64%)]\tTotal Loss: 0.149821\n",
      "Reconstruction: 0.143847, Regularization: 0.005974\n",
      "2019-04-10 01:14:14,624 root         INFO     Train Epoch: 106 [5632/8000 (70%)]\tTotal Loss: 0.165761\n",
      "Reconstruction: 0.159106, Regularization: 0.006655\n",
      "2019-04-10 01:14:14,679 root         INFO     Train Epoch: 106 [6144/8000 (77%)]\tTotal Loss: 0.150872\n",
      "Reconstruction: 0.144156, Regularization: 0.006716\n",
      "2019-04-10 01:14:14,735 root         INFO     Train Epoch: 106 [6656/8000 (83%)]\tTotal Loss: 0.152378\n",
      "Reconstruction: 0.145461, Regularization: 0.006917\n",
      "2019-04-10 01:14:14,790 root         INFO     Train Epoch: 106 [7168/8000 (90%)]\tTotal Loss: 0.149123\n",
      "Reconstruction: 0.143583, Regularization: 0.005539\n",
      "2019-04-10 01:14:14,846 root         INFO     Train Epoch: 106 [7680/8000 (96%)]\tTotal Loss: 0.138622\n",
      "Reconstruction: 0.134128, Regularization: 0.004494\n",
      "2019-04-10 01:14:14,896 root         INFO     ====> Epoch: 106 Average loss: 0.1515\n",
      "2019-04-10 01:14:14,920 root         INFO     Train Epoch: 107 [0/8000 (0%)]\tTotal Loss: 0.158372\n",
      "Reconstruction: 0.152610, Regularization: 0.005762\n",
      "2019-04-10 01:14:14,977 root         INFO     Train Epoch: 107 [512/8000 (6%)]\tTotal Loss: 0.151441\n",
      "Reconstruction: 0.145425, Regularization: 0.006016\n",
      "2019-04-10 01:14:15,034 root         INFO     Train Epoch: 107 [1024/8000 (13%)]\tTotal Loss: 0.149303\n",
      "Reconstruction: 0.141613, Regularization: 0.007691\n",
      "2019-04-10 01:14:15,091 root         INFO     Train Epoch: 107 [1536/8000 (19%)]\tTotal Loss: 0.143073\n",
      "Reconstruction: 0.136555, Regularization: 0.006518\n",
      "2019-04-10 01:14:15,148 root         INFO     Train Epoch: 107 [2048/8000 (26%)]\tTotal Loss: 0.152173\n",
      "Reconstruction: 0.146690, Regularization: 0.005483\n",
      "2019-04-10 01:14:15,204 root         INFO     Train Epoch: 107 [2560/8000 (32%)]\tTotal Loss: 0.147870\n",
      "Reconstruction: 0.138538, Regularization: 0.009332\n",
      "2019-04-10 01:14:15,261 root         INFO     Train Epoch: 107 [3072/8000 (38%)]\tTotal Loss: 0.142120\n",
      "Reconstruction: 0.136161, Regularization: 0.005959\n",
      "2019-04-10 01:14:15,318 root         INFO     Train Epoch: 107 [3584/8000 (45%)]\tTotal Loss: 0.150019\n",
      "Reconstruction: 0.143591, Regularization: 0.006428\n",
      "2019-04-10 01:14:15,375 root         INFO     Train Epoch: 107 [4096/8000 (51%)]\tTotal Loss: 0.129878\n",
      "Reconstruction: 0.125893, Regularization: 0.003985\n",
      "2019-04-10 01:14:15,432 root         INFO     Train Epoch: 107 [4608/8000 (58%)]\tTotal Loss: 0.150074\n",
      "Reconstruction: 0.140088, Regularization: 0.009986\n",
      "2019-04-10 01:14:15,488 root         INFO     Train Epoch: 107 [5120/8000 (64%)]\tTotal Loss: 0.143992\n",
      "Reconstruction: 0.138202, Regularization: 0.005791\n",
      "2019-04-10 01:14:15,545 root         INFO     Train Epoch: 107 [5632/8000 (70%)]\tTotal Loss: 0.141106\n",
      "Reconstruction: 0.136931, Regularization: 0.004175\n",
      "2019-04-10 01:14:15,602 root         INFO     Train Epoch: 107 [6144/8000 (77%)]\tTotal Loss: 0.140637\n",
      "Reconstruction: 0.133807, Regularization: 0.006830\n",
      "2019-04-10 01:14:15,659 root         INFO     Train Epoch: 107 [6656/8000 (83%)]\tTotal Loss: 0.153849\n",
      "Reconstruction: 0.146115, Regularization: 0.007734\n",
      "2019-04-10 01:14:15,715 root         INFO     Train Epoch: 107 [7168/8000 (90%)]\tTotal Loss: 0.146434\n",
      "Reconstruction: 0.142151, Regularization: 0.004282\n",
      "2019-04-10 01:14:15,771 root         INFO     Train Epoch: 107 [7680/8000 (96%)]\tTotal Loss: 0.156562\n",
      "Reconstruction: 0.150978, Regularization: 0.005584\n",
      "2019-04-10 01:14:15,821 root         INFO     ====> Epoch: 107 Average loss: 0.1510\n",
      "2019-04-10 01:14:15,845 root         INFO     Train Epoch: 108 [0/8000 (0%)]\tTotal Loss: 0.136327\n",
      "Reconstruction: 0.131966, Regularization: 0.004361\n",
      "2019-04-10 01:14:15,902 root         INFO     Train Epoch: 108 [512/8000 (6%)]\tTotal Loss: 0.158749\n",
      "Reconstruction: 0.150837, Regularization: 0.007912\n",
      "2019-04-10 01:14:15,959 root         INFO     Train Epoch: 108 [1024/8000 (13%)]\tTotal Loss: 0.141769\n",
      "Reconstruction: 0.134909, Regularization: 0.006859\n",
      "2019-04-10 01:14:16,016 root         INFO     Train Epoch: 108 [1536/8000 (19%)]\tTotal Loss: 0.153677\n",
      "Reconstruction: 0.147956, Regularization: 0.005721\n",
      "2019-04-10 01:14:16,072 root         INFO     Train Epoch: 108 [2048/8000 (26%)]\tTotal Loss: 0.161644\n",
      "Reconstruction: 0.152858, Regularization: 0.008786\n",
      "2019-04-10 01:14:16,128 root         INFO     Train Epoch: 108 [2560/8000 (32%)]\tTotal Loss: 0.158066\n",
      "Reconstruction: 0.152560, Regularization: 0.005507\n",
      "2019-04-10 01:14:16,185 root         INFO     Train Epoch: 108 [3072/8000 (38%)]\tTotal Loss: 0.154124\n",
      "Reconstruction: 0.146672, Regularization: 0.007451\n",
      "2019-04-10 01:14:16,241 root         INFO     Train Epoch: 108 [3584/8000 (45%)]\tTotal Loss: 0.158721\n",
      "Reconstruction: 0.153274, Regularization: 0.005447\n",
      "2019-04-10 01:14:16,298 root         INFO     Train Epoch: 108 [4096/8000 (51%)]\tTotal Loss: 0.169921\n",
      "Reconstruction: 0.157653, Regularization: 0.012267\n",
      "2019-04-10 01:14:16,355 root         INFO     Train Epoch: 108 [4608/8000 (58%)]\tTotal Loss: 0.151135\n",
      "Reconstruction: 0.145460, Regularization: 0.005675\n",
      "2019-04-10 01:14:16,412 root         INFO     Train Epoch: 108 [5120/8000 (64%)]\tTotal Loss: 0.140138\n",
      "Reconstruction: 0.134522, Regularization: 0.005616\n",
      "2019-04-10 01:14:16,467 root         INFO     Train Epoch: 108 [5632/8000 (70%)]\tTotal Loss: 0.162343\n",
      "Reconstruction: 0.153036, Regularization: 0.009308\n",
      "2019-04-10 01:14:16,523 root         INFO     Train Epoch: 108 [6144/8000 (77%)]\tTotal Loss: 0.155666\n",
      "Reconstruction: 0.148560, Regularization: 0.007105\n",
      "2019-04-10 01:14:16,579 root         INFO     Train Epoch: 108 [6656/8000 (83%)]\tTotal Loss: 0.141361\n",
      "Reconstruction: 0.136662, Regularization: 0.004699\n",
      "2019-04-10 01:14:16,635 root         INFO     Train Epoch: 108 [7168/8000 (90%)]\tTotal Loss: 0.153905\n",
      "Reconstruction: 0.146511, Regularization: 0.007394\n",
      "2019-04-10 01:14:16,692 root         INFO     Train Epoch: 108 [7680/8000 (96%)]\tTotal Loss: 0.149707\n",
      "Reconstruction: 0.143481, Regularization: 0.006226\n",
      "2019-04-10 01:14:16,742 root         INFO     ====> Epoch: 108 Average loss: 0.1513\n",
      "2019-04-10 01:14:16,766 root         INFO     Train Epoch: 109 [0/8000 (0%)]\tTotal Loss: 0.158554\n",
      "Reconstruction: 0.148821, Regularization: 0.009733\n",
      "2019-04-10 01:14:16,823 root         INFO     Train Epoch: 109 [512/8000 (6%)]\tTotal Loss: 0.141062\n",
      "Reconstruction: 0.136524, Regularization: 0.004539\n",
      "2019-04-10 01:14:16,880 root         INFO     Train Epoch: 109 [1024/8000 (13%)]\tTotal Loss: 0.147864\n",
      "Reconstruction: 0.140062, Regularization: 0.007802\n",
      "2019-04-10 01:14:16,937 root         INFO     Train Epoch: 109 [1536/8000 (19%)]\tTotal Loss: 0.144858\n",
      "Reconstruction: 0.139551, Regularization: 0.005307\n",
      "2019-04-10 01:14:16,994 root         INFO     Train Epoch: 109 [2048/8000 (26%)]\tTotal Loss: 0.145183\n",
      "Reconstruction: 0.139493, Regularization: 0.005690\n",
      "2019-04-10 01:14:17,051 root         INFO     Train Epoch: 109 [2560/8000 (32%)]\tTotal Loss: 0.139459\n",
      "Reconstruction: 0.135905, Regularization: 0.003555\n",
      "2019-04-10 01:14:17,109 root         INFO     Train Epoch: 109 [3072/8000 (38%)]\tTotal Loss: 0.154871\n",
      "Reconstruction: 0.145794, Regularization: 0.009078\n",
      "2019-04-10 01:14:17,166 root         INFO     Train Epoch: 109 [3584/8000 (45%)]\tTotal Loss: 0.152216\n",
      "Reconstruction: 0.147204, Regularization: 0.005012\n",
      "2019-04-10 01:14:17,223 root         INFO     Train Epoch: 109 [4096/8000 (51%)]\tTotal Loss: 0.154400\n",
      "Reconstruction: 0.147608, Regularization: 0.006792\n",
      "2019-04-10 01:14:17,280 root         INFO     Train Epoch: 109 [4608/8000 (58%)]\tTotal Loss: 0.142847\n",
      "Reconstruction: 0.136568, Regularization: 0.006279\n",
      "2019-04-10 01:14:17,335 root         INFO     Train Epoch: 109 [5120/8000 (64%)]\tTotal Loss: 0.147372\n",
      "Reconstruction: 0.142406, Regularization: 0.004965\n",
      "2019-04-10 01:14:17,392 root         INFO     Train Epoch: 109 [5632/8000 (70%)]\tTotal Loss: 0.139314\n",
      "Reconstruction: 0.134275, Regularization: 0.005039\n",
      "2019-04-10 01:14:17,449 root         INFO     Train Epoch: 109 [6144/8000 (77%)]\tTotal Loss: 0.160826\n",
      "Reconstruction: 0.155718, Regularization: 0.005107\n",
      "2019-04-10 01:14:17,507 root         INFO     Train Epoch: 109 [6656/8000 (83%)]\tTotal Loss: 0.147596\n",
      "Reconstruction: 0.142885, Regularization: 0.004711\n",
      "2019-04-10 01:14:17,564 root         INFO     Train Epoch: 109 [7168/8000 (90%)]\tTotal Loss: 0.159185\n",
      "Reconstruction: 0.150983, Regularization: 0.008202\n",
      "2019-04-10 01:14:17,620 root         INFO     Train Epoch: 109 [7680/8000 (96%)]\tTotal Loss: 0.162190\n",
      "Reconstruction: 0.154245, Regularization: 0.007945\n",
      "2019-04-10 01:14:17,670 root         INFO     ====> Epoch: 109 Average loss: 0.1515\n",
      "2019-04-10 01:14:17,695 root         INFO     Train Epoch: 110 [0/8000 (0%)]\tTotal Loss: 0.142654\n",
      "Reconstruction: 0.138310, Regularization: 0.004344\n",
      "2019-04-10 01:14:17,751 root         INFO     Train Epoch: 110 [512/8000 (6%)]\tTotal Loss: 0.158673\n",
      "Reconstruction: 0.151167, Regularization: 0.007506\n",
      "2019-04-10 01:14:17,806 root         INFO     Train Epoch: 110 [1024/8000 (13%)]\tTotal Loss: 0.141634\n",
      "Reconstruction: 0.135587, Regularization: 0.006047\n",
      "2019-04-10 01:14:17,861 root         INFO     Train Epoch: 110 [1536/8000 (19%)]\tTotal Loss: 0.148533\n",
      "Reconstruction: 0.141499, Regularization: 0.007035\n",
      "2019-04-10 01:14:17,918 root         INFO     Train Epoch: 110 [2048/8000 (26%)]\tTotal Loss: 0.144800\n",
      "Reconstruction: 0.140365, Regularization: 0.004435\n",
      "2019-04-10 01:14:17,974 root         INFO     Train Epoch: 110 [2560/8000 (32%)]\tTotal Loss: 0.145286\n",
      "Reconstruction: 0.137152, Regularization: 0.008134\n",
      "2019-04-10 01:14:18,031 root         INFO     Train Epoch: 110 [3072/8000 (38%)]\tTotal Loss: 0.147946\n",
      "Reconstruction: 0.143539, Regularization: 0.004407\n",
      "2019-04-10 01:14:18,087 root         INFO     Train Epoch: 110 [3584/8000 (45%)]\tTotal Loss: 0.153648\n",
      "Reconstruction: 0.146881, Regularization: 0.006767\n",
      "2019-04-10 01:14:18,143 root         INFO     Train Epoch: 110 [4096/8000 (51%)]\tTotal Loss: 0.170374\n",
      "Reconstruction: 0.159088, Regularization: 0.011286\n",
      "2019-04-10 01:14:18,199 root         INFO     Train Epoch: 110 [4608/8000 (58%)]\tTotal Loss: 0.150476\n",
      "Reconstruction: 0.145707, Regularization: 0.004769\n",
      "2019-04-10 01:14:18,256 root         INFO     Train Epoch: 110 [5120/8000 (64%)]\tTotal Loss: 0.152502\n",
      "Reconstruction: 0.145580, Regularization: 0.006922\n",
      "2019-04-10 01:14:18,313 root         INFO     Train Epoch: 110 [5632/8000 (70%)]\tTotal Loss: 0.154869\n",
      "Reconstruction: 0.147715, Regularization: 0.007154\n",
      "2019-04-10 01:14:18,370 root         INFO     Train Epoch: 110 [6144/8000 (77%)]\tTotal Loss: 0.152050\n",
      "Reconstruction: 0.146952, Regularization: 0.005098\n",
      "2019-04-10 01:14:18,427 root         INFO     Train Epoch: 110 [6656/8000 (83%)]\tTotal Loss: 0.162939\n",
      "Reconstruction: 0.153467, Regularization: 0.009472\n",
      "2019-04-10 01:14:18,484 root         INFO     Train Epoch: 110 [7168/8000 (90%)]\tTotal Loss: 0.146222\n",
      "Reconstruction: 0.141329, Regularization: 0.004894\n",
      "2019-04-10 01:14:18,540 root         INFO     Train Epoch: 110 [7680/8000 (96%)]\tTotal Loss: 0.147133\n",
      "Reconstruction: 0.143344, Regularization: 0.003789\n",
      "2019-04-10 01:14:18,591 root         INFO     ====> Epoch: 110 Average loss: 0.1513\n",
      "2019-04-10 01:14:18,614 root         INFO     Train Epoch: 111 [0/8000 (0%)]\tTotal Loss: 0.160855\n",
      "Reconstruction: 0.153717, Regularization: 0.007138\n",
      "2019-04-10 01:14:18,671 root         INFO     Train Epoch: 111 [512/8000 (6%)]\tTotal Loss: 0.152034\n",
      "Reconstruction: 0.145993, Regularization: 0.006041\n",
      "2019-04-10 01:14:18,728 root         INFO     Train Epoch: 111 [1024/8000 (13%)]\tTotal Loss: 0.155296\n",
      "Reconstruction: 0.147039, Regularization: 0.008257\n",
      "2019-04-10 01:14:18,786 root         INFO     Train Epoch: 111 [1536/8000 (19%)]\tTotal Loss: 0.149120\n",
      "Reconstruction: 0.143432, Regularization: 0.005688\n",
      "2019-04-10 01:14:18,843 root         INFO     Train Epoch: 111 [2048/8000 (26%)]\tTotal Loss: 0.130322\n",
      "Reconstruction: 0.126747, Regularization: 0.003575\n",
      "2019-04-10 01:14:18,900 root         INFO     Train Epoch: 111 [2560/8000 (32%)]\tTotal Loss: 0.142405\n",
      "Reconstruction: 0.137318, Regularization: 0.005086\n",
      "2019-04-10 01:14:18,957 root         INFO     Train Epoch: 111 [3072/8000 (38%)]\tTotal Loss: 0.156075\n",
      "Reconstruction: 0.150582, Regularization: 0.005493\n",
      "2019-04-10 01:14:19,014 root         INFO     Train Epoch: 111 [3584/8000 (45%)]\tTotal Loss: 0.156424\n",
      "Reconstruction: 0.150227, Regularization: 0.006196\n",
      "2019-04-10 01:14:19,071 root         INFO     Train Epoch: 111 [4096/8000 (51%)]\tTotal Loss: 0.153462\n",
      "Reconstruction: 0.146041, Regularization: 0.007421\n",
      "2019-04-10 01:14:19,129 root         INFO     Train Epoch: 111 [4608/8000 (58%)]\tTotal Loss: 0.167002\n",
      "Reconstruction: 0.157723, Regularization: 0.009280\n",
      "2019-04-10 01:14:19,186 root         INFO     Train Epoch: 111 [5120/8000 (64%)]\tTotal Loss: 0.155453\n",
      "Reconstruction: 0.148667, Regularization: 0.006786\n",
      "2019-04-10 01:14:19,242 root         INFO     Train Epoch: 111 [5632/8000 (70%)]\tTotal Loss: 0.145494\n",
      "Reconstruction: 0.140545, Regularization: 0.004949\n",
      "2019-04-10 01:14:19,298 root         INFO     Train Epoch: 111 [6144/8000 (77%)]\tTotal Loss: 0.144481\n",
      "Reconstruction: 0.139446, Regularization: 0.005035\n",
      "2019-04-10 01:14:19,354 root         INFO     Train Epoch: 111 [6656/8000 (83%)]\tTotal Loss: 0.151781\n",
      "Reconstruction: 0.145590, Regularization: 0.006192\n",
      "2019-04-10 01:14:19,409 root         INFO     Train Epoch: 111 [7168/8000 (90%)]\tTotal Loss: 0.147109\n",
      "Reconstruction: 0.141529, Regularization: 0.005580\n",
      "2019-04-10 01:14:19,465 root         INFO     Train Epoch: 111 [7680/8000 (96%)]\tTotal Loss: 0.155922\n",
      "Reconstruction: 0.148737, Regularization: 0.007185\n",
      "2019-04-10 01:14:19,514 root         INFO     ====> Epoch: 111 Average loss: 0.1509\n",
      "2019-04-10 01:14:19,538 root         INFO     Train Epoch: 112 [0/8000 (0%)]\tTotal Loss: 0.151642\n",
      "Reconstruction: 0.145421, Regularization: 0.006221\n",
      "2019-04-10 01:14:19,594 root         INFO     Train Epoch: 112 [512/8000 (6%)]\tTotal Loss: 0.156757\n",
      "Reconstruction: 0.149789, Regularization: 0.006968\n",
      "2019-04-10 01:14:19,650 root         INFO     Train Epoch: 112 [1024/8000 (13%)]\tTotal Loss: 0.140874\n",
      "Reconstruction: 0.136361, Regularization: 0.004513\n",
      "2019-04-10 01:14:19,705 root         INFO     Train Epoch: 112 [1536/8000 (19%)]\tTotal Loss: 0.151131\n",
      "Reconstruction: 0.142578, Regularization: 0.008553\n",
      "2019-04-10 01:14:19,762 root         INFO     Train Epoch: 112 [2048/8000 (26%)]\tTotal Loss: 0.149546\n",
      "Reconstruction: 0.142275, Regularization: 0.007271\n",
      "2019-04-10 01:14:19,819 root         INFO     Train Epoch: 112 [2560/8000 (32%)]\tTotal Loss: 0.145224\n",
      "Reconstruction: 0.139482, Regularization: 0.005742\n",
      "2019-04-10 01:14:19,877 root         INFO     Train Epoch: 112 [3072/8000 (38%)]\tTotal Loss: 0.149376\n",
      "Reconstruction: 0.143248, Regularization: 0.006128\n",
      "2019-04-10 01:14:19,936 root         INFO     Train Epoch: 112 [3584/8000 (45%)]\tTotal Loss: 0.159299\n",
      "Reconstruction: 0.151836, Regularization: 0.007464\n",
      "2019-04-10 01:14:19,994 root         INFO     Train Epoch: 112 [4096/8000 (51%)]\tTotal Loss: 0.140194\n",
      "Reconstruction: 0.133384, Regularization: 0.006810\n",
      "2019-04-10 01:14:20,052 root         INFO     Train Epoch: 112 [4608/8000 (58%)]\tTotal Loss: 0.164300\n",
      "Reconstruction: 0.156468, Regularization: 0.007832\n",
      "2019-04-10 01:14:20,110 root         INFO     Train Epoch: 112 [5120/8000 (64%)]\tTotal Loss: 0.166736\n",
      "Reconstruction: 0.155507, Regularization: 0.011229\n",
      "2019-04-10 01:14:20,169 root         INFO     Train Epoch: 112 [5632/8000 (70%)]\tTotal Loss: 0.145912\n",
      "Reconstruction: 0.139564, Regularization: 0.006347\n",
      "2019-04-10 01:14:20,227 root         INFO     Train Epoch: 112 [6144/8000 (77%)]\tTotal Loss: 0.154960\n",
      "Reconstruction: 0.149019, Regularization: 0.005941\n",
      "2019-04-10 01:14:20,285 root         INFO     Train Epoch: 112 [6656/8000 (83%)]\tTotal Loss: 0.162282\n",
      "Reconstruction: 0.155643, Regularization: 0.006639\n",
      "2019-04-10 01:14:20,343 root         INFO     Train Epoch: 112 [7168/8000 (90%)]\tTotal Loss: 0.149853\n",
      "Reconstruction: 0.143904, Regularization: 0.005949\n",
      "2019-04-10 01:14:20,400 root         INFO     Train Epoch: 112 [7680/8000 (96%)]\tTotal Loss: 0.157845\n",
      "Reconstruction: 0.150718, Regularization: 0.007127\n",
      "2019-04-10 01:14:20,450 root         INFO     ====> Epoch: 112 Average loss: 0.1508\n",
      "2019-04-10 01:14:20,474 root         INFO     Train Epoch: 113 [0/8000 (0%)]\tTotal Loss: 0.142470\n",
      "Reconstruction: 0.137062, Regularization: 0.005408\n",
      "2019-04-10 01:14:20,532 root         INFO     Train Epoch: 113 [512/8000 (6%)]\tTotal Loss: 0.132253\n",
      "Reconstruction: 0.128073, Regularization: 0.004180\n",
      "2019-04-10 01:14:20,588 root         INFO     Train Epoch: 113 [1024/8000 (13%)]\tTotal Loss: 0.169077\n",
      "Reconstruction: 0.158836, Regularization: 0.010241\n",
      "2019-04-10 01:14:20,645 root         INFO     Train Epoch: 113 [1536/8000 (19%)]\tTotal Loss: 0.166751\n",
      "Reconstruction: 0.160196, Regularization: 0.006555\n",
      "2019-04-10 01:14:20,702 root         INFO     Train Epoch: 113 [2048/8000 (26%)]\tTotal Loss: 0.148816\n",
      "Reconstruction: 0.145083, Regularization: 0.003733\n",
      "2019-04-10 01:14:20,759 root         INFO     Train Epoch: 113 [2560/8000 (32%)]\tTotal Loss: 0.148402\n",
      "Reconstruction: 0.140590, Regularization: 0.007812\n",
      "2019-04-10 01:14:20,815 root         INFO     Train Epoch: 113 [3072/8000 (38%)]\tTotal Loss: 0.147400\n",
      "Reconstruction: 0.143323, Regularization: 0.004078\n",
      "2019-04-10 01:14:20,871 root         INFO     Train Epoch: 113 [3584/8000 (45%)]\tTotal Loss: 0.147261\n",
      "Reconstruction: 0.139579, Regularization: 0.007682\n",
      "2019-04-10 01:14:20,928 root         INFO     Train Epoch: 113 [4096/8000 (51%)]\tTotal Loss: 0.133913\n",
      "Reconstruction: 0.127843, Regularization: 0.006070\n",
      "2019-04-10 01:14:20,983 root         INFO     Train Epoch: 113 [4608/8000 (58%)]\tTotal Loss: 0.149858\n",
      "Reconstruction: 0.145477, Regularization: 0.004381\n",
      "2019-04-10 01:14:21,038 root         INFO     Train Epoch: 113 [5120/8000 (64%)]\tTotal Loss: 0.141580\n",
      "Reconstruction: 0.136843, Regularization: 0.004737\n",
      "2019-04-10 01:14:21,093 root         INFO     Train Epoch: 113 [5632/8000 (70%)]\tTotal Loss: 0.159983\n",
      "Reconstruction: 0.151086, Regularization: 0.008897\n",
      "2019-04-10 01:14:21,148 root         INFO     Train Epoch: 113 [6144/8000 (77%)]\tTotal Loss: 0.138503\n",
      "Reconstruction: 0.133774, Regularization: 0.004729\n",
      "2019-04-10 01:14:21,203 root         INFO     Train Epoch: 113 [6656/8000 (83%)]\tTotal Loss: 0.153334\n",
      "Reconstruction: 0.146036, Regularization: 0.007299\n",
      "2019-04-10 01:14:21,258 root         INFO     Train Epoch: 113 [7168/8000 (90%)]\tTotal Loss: 0.164593\n",
      "Reconstruction: 0.150106, Regularization: 0.014487\n",
      "2019-04-10 01:14:21,314 root         INFO     Train Epoch: 113 [7680/8000 (96%)]\tTotal Loss: 0.143104\n",
      "Reconstruction: 0.136581, Regularization: 0.006523\n",
      "2019-04-10 01:14:21,364 root         INFO     ====> Epoch: 113 Average loss: 0.1508\n",
      "2019-04-10 01:14:21,388 root         INFO     Train Epoch: 114 [0/8000 (0%)]\tTotal Loss: 0.140616\n",
      "Reconstruction: 0.136121, Regularization: 0.004495\n",
      "2019-04-10 01:14:21,444 root         INFO     Train Epoch: 114 [512/8000 (6%)]\tTotal Loss: 0.162553\n",
      "Reconstruction: 0.157396, Regularization: 0.005157\n",
      "2019-04-10 01:14:21,501 root         INFO     Train Epoch: 114 [1024/8000 (13%)]\tTotal Loss: 0.145835\n",
      "Reconstruction: 0.141268, Regularization: 0.004567\n",
      "2019-04-10 01:14:21,558 root         INFO     Train Epoch: 114 [1536/8000 (19%)]\tTotal Loss: 0.178451\n",
      "Reconstruction: 0.171556, Regularization: 0.006896\n",
      "2019-04-10 01:14:21,615 root         INFO     Train Epoch: 114 [2048/8000 (26%)]\tTotal Loss: 0.145201\n",
      "Reconstruction: 0.140252, Regularization: 0.004949\n",
      "2019-04-10 01:14:21,671 root         INFO     Train Epoch: 114 [2560/8000 (32%)]\tTotal Loss: 0.142830\n",
      "Reconstruction: 0.136218, Regularization: 0.006612\n",
      "2019-04-10 01:14:21,728 root         INFO     Train Epoch: 114 [3072/8000 (38%)]\tTotal Loss: 0.145880\n",
      "Reconstruction: 0.142134, Regularization: 0.003746\n",
      "2019-04-10 01:14:21,785 root         INFO     Train Epoch: 114 [3584/8000 (45%)]\tTotal Loss: 0.146715\n",
      "Reconstruction: 0.139071, Regularization: 0.007644\n",
      "2019-04-10 01:14:21,841 root         INFO     Train Epoch: 114 [4096/8000 (51%)]\tTotal Loss: 0.153616\n",
      "Reconstruction: 0.149421, Regularization: 0.004195\n",
      "2019-04-10 01:14:21,897 root         INFO     Train Epoch: 114 [4608/8000 (58%)]\tTotal Loss: 0.139446\n",
      "Reconstruction: 0.134077, Regularization: 0.005369\n",
      "2019-04-10 01:14:21,954 root         INFO     Train Epoch: 114 [5120/8000 (64%)]\tTotal Loss: 0.149608\n",
      "Reconstruction: 0.145015, Regularization: 0.004593\n",
      "2019-04-10 01:14:22,011 root         INFO     Train Epoch: 114 [5632/8000 (70%)]\tTotal Loss: 0.153680\n",
      "Reconstruction: 0.148887, Regularization: 0.004793\n",
      "2019-04-10 01:14:22,067 root         INFO     Train Epoch: 114 [6144/8000 (77%)]\tTotal Loss: 0.145934\n",
      "Reconstruction: 0.141627, Regularization: 0.004307\n",
      "2019-04-10 01:14:22,124 root         INFO     Train Epoch: 114 [6656/8000 (83%)]\tTotal Loss: 0.152118\n",
      "Reconstruction: 0.144165, Regularization: 0.007952\n",
      "2019-04-10 01:14:22,180 root         INFO     Train Epoch: 114 [7168/8000 (90%)]\tTotal Loss: 0.151575\n",
      "Reconstruction: 0.143821, Regularization: 0.007754\n",
      "2019-04-10 01:14:22,236 root         INFO     Train Epoch: 114 [7680/8000 (96%)]\tTotal Loss: 0.154151\n",
      "Reconstruction: 0.148145, Regularization: 0.006006\n",
      "2019-04-10 01:14:22,286 root         INFO     ====> Epoch: 114 Average loss: 0.1513\n",
      "2019-04-10 01:14:22,309 root         INFO     Train Epoch: 115 [0/8000 (0%)]\tTotal Loss: 0.155138\n",
      "Reconstruction: 0.148022, Regularization: 0.007117\n",
      "2019-04-10 01:14:22,366 root         INFO     Train Epoch: 115 [512/8000 (6%)]\tTotal Loss: 0.155548\n",
      "Reconstruction: 0.145833, Regularization: 0.009714\n",
      "2019-04-10 01:14:22,423 root         INFO     Train Epoch: 115 [1024/8000 (13%)]\tTotal Loss: 0.142522\n",
      "Reconstruction: 0.137776, Regularization: 0.004746\n",
      "2019-04-10 01:14:22,480 root         INFO     Train Epoch: 115 [1536/8000 (19%)]\tTotal Loss: 0.144063\n",
      "Reconstruction: 0.139458, Regularization: 0.004605\n",
      "2019-04-10 01:14:22,537 root         INFO     Train Epoch: 115 [2048/8000 (26%)]\tTotal Loss: 0.157270\n",
      "Reconstruction: 0.150826, Regularization: 0.006443\n",
      "2019-04-10 01:14:22,592 root         INFO     Train Epoch: 115 [2560/8000 (32%)]\tTotal Loss: 0.141742\n",
      "Reconstruction: 0.136572, Regularization: 0.005170\n",
      "2019-04-10 01:14:22,647 root         INFO     Train Epoch: 115 [3072/8000 (38%)]\tTotal Loss: 0.146793\n",
      "Reconstruction: 0.139241, Regularization: 0.007552\n",
      "2019-04-10 01:14:22,703 root         INFO     Train Epoch: 115 [3584/8000 (45%)]\tTotal Loss: 0.153441\n",
      "Reconstruction: 0.144382, Regularization: 0.009059\n",
      "2019-04-10 01:14:22,759 root         INFO     Train Epoch: 115 [4096/8000 (51%)]\tTotal Loss: 0.158516\n",
      "Reconstruction: 0.153266, Regularization: 0.005250\n",
      "2019-04-10 01:14:22,815 root         INFO     Train Epoch: 115 [4608/8000 (58%)]\tTotal Loss: 0.144331\n",
      "Reconstruction: 0.138732, Regularization: 0.005599\n",
      "2019-04-10 01:14:22,871 root         INFO     Train Epoch: 115 [5120/8000 (64%)]\tTotal Loss: 0.142413\n",
      "Reconstruction: 0.135997, Regularization: 0.006417\n",
      "2019-04-10 01:14:22,926 root         INFO     Train Epoch: 115 [5632/8000 (70%)]\tTotal Loss: 0.155476\n",
      "Reconstruction: 0.150156, Regularization: 0.005320\n",
      "2019-04-10 01:14:22,982 root         INFO     Train Epoch: 115 [6144/8000 (77%)]\tTotal Loss: 0.157509\n",
      "Reconstruction: 0.147402, Regularization: 0.010107\n",
      "2019-04-10 01:14:23,039 root         INFO     Train Epoch: 115 [6656/8000 (83%)]\tTotal Loss: 0.157421\n",
      "Reconstruction: 0.148955, Regularization: 0.008466\n",
      "2019-04-10 01:14:23,095 root         INFO     Train Epoch: 115 [7168/8000 (90%)]\tTotal Loss: 0.153010\n",
      "Reconstruction: 0.146351, Regularization: 0.006659\n",
      "2019-04-10 01:14:23,152 root         INFO     Train Epoch: 115 [7680/8000 (96%)]\tTotal Loss: 0.166174\n",
      "Reconstruction: 0.159796, Regularization: 0.006379\n",
      "2019-04-10 01:14:23,202 root         INFO     ====> Epoch: 115 Average loss: 0.1507\n",
      "2019-04-10 01:14:23,225 root         INFO     Train Epoch: 116 [0/8000 (0%)]\tTotal Loss: 0.145079\n",
      "Reconstruction: 0.140342, Regularization: 0.004737\n",
      "2019-04-10 01:14:23,281 root         INFO     Train Epoch: 116 [512/8000 (6%)]\tTotal Loss: 0.153046\n",
      "Reconstruction: 0.147793, Regularization: 0.005253\n",
      "2019-04-10 01:14:23,336 root         INFO     Train Epoch: 116 [1024/8000 (13%)]\tTotal Loss: 0.152882\n",
      "Reconstruction: 0.148780, Regularization: 0.004102\n",
      "2019-04-10 01:14:23,392 root         INFO     Train Epoch: 116 [1536/8000 (19%)]\tTotal Loss: 0.151427\n",
      "Reconstruction: 0.146222, Regularization: 0.005205\n",
      "2019-04-10 01:14:23,447 root         INFO     Train Epoch: 116 [2048/8000 (26%)]\tTotal Loss: 0.149632\n",
      "Reconstruction: 0.145871, Regularization: 0.003761\n",
      "2019-04-10 01:14:23,503 root         INFO     Train Epoch: 116 [2560/8000 (32%)]\tTotal Loss: 0.156071\n",
      "Reconstruction: 0.151056, Regularization: 0.005015\n",
      "2019-04-10 01:14:23,558 root         INFO     Train Epoch: 116 [3072/8000 (38%)]\tTotal Loss: 0.141954\n",
      "Reconstruction: 0.136255, Regularization: 0.005699\n",
      "2019-04-10 01:14:23,613 root         INFO     Train Epoch: 116 [3584/8000 (45%)]\tTotal Loss: 0.159040\n",
      "Reconstruction: 0.149061, Regularization: 0.009979\n",
      "2019-04-10 01:14:23,668 root         INFO     Train Epoch: 116 [4096/8000 (51%)]\tTotal Loss: 0.135776\n",
      "Reconstruction: 0.131322, Regularization: 0.004455\n",
      "2019-04-10 01:14:23,722 root         INFO     Train Epoch: 116 [4608/8000 (58%)]\tTotal Loss: 0.144930\n",
      "Reconstruction: 0.140384, Regularization: 0.004545\n",
      "2019-04-10 01:14:23,777 root         INFO     Train Epoch: 116 [5120/8000 (64%)]\tTotal Loss: 0.156795\n",
      "Reconstruction: 0.151329, Regularization: 0.005466\n",
      "2019-04-10 01:14:23,831 root         INFO     Train Epoch: 116 [5632/8000 (70%)]\tTotal Loss: 0.151714\n",
      "Reconstruction: 0.144847, Regularization: 0.006867\n",
      "2019-04-10 01:14:23,886 root         INFO     Train Epoch: 116 [6144/8000 (77%)]\tTotal Loss: 0.157189\n",
      "Reconstruction: 0.148691, Regularization: 0.008498\n",
      "2019-04-10 01:14:23,941 root         INFO     Train Epoch: 116 [6656/8000 (83%)]\tTotal Loss: 0.161133\n",
      "Reconstruction: 0.152476, Regularization: 0.008658\n",
      "2019-04-10 01:14:23,996 root         INFO     Train Epoch: 116 [7168/8000 (90%)]\tTotal Loss: 0.161767\n",
      "Reconstruction: 0.154722, Regularization: 0.007045\n",
      "2019-04-10 01:14:24,050 root         INFO     Train Epoch: 116 [7680/8000 (96%)]\tTotal Loss: 0.154372\n",
      "Reconstruction: 0.149283, Regularization: 0.005090\n",
      "2019-04-10 01:14:24,100 root         INFO     ====> Epoch: 116 Average loss: 0.1505\n",
      "2019-04-10 01:14:24,124 root         INFO     Train Epoch: 117 [0/8000 (0%)]\tTotal Loss: 0.149335\n",
      "Reconstruction: 0.143763, Regularization: 0.005572\n",
      "2019-04-10 01:14:24,181 root         INFO     Train Epoch: 117 [512/8000 (6%)]\tTotal Loss: 0.140917\n",
      "Reconstruction: 0.135770, Regularization: 0.005146\n",
      "2019-04-10 01:14:24,237 root         INFO     Train Epoch: 117 [1024/8000 (13%)]\tTotal Loss: 0.140984\n",
      "Reconstruction: 0.136077, Regularization: 0.004907\n",
      "2019-04-10 01:14:24,292 root         INFO     Train Epoch: 117 [1536/8000 (19%)]\tTotal Loss: 0.140507\n",
      "Reconstruction: 0.134822, Regularization: 0.005686\n",
      "2019-04-10 01:14:24,348 root         INFO     Train Epoch: 117 [2048/8000 (26%)]\tTotal Loss: 0.156691\n",
      "Reconstruction: 0.149633, Regularization: 0.007059\n",
      "2019-04-10 01:14:24,403 root         INFO     Train Epoch: 117 [2560/8000 (32%)]\tTotal Loss: 0.159673\n",
      "Reconstruction: 0.149948, Regularization: 0.009725\n",
      "2019-04-10 01:14:24,459 root         INFO     Train Epoch: 117 [3072/8000 (38%)]\tTotal Loss: 0.145764\n",
      "Reconstruction: 0.140981, Regularization: 0.004783\n",
      "2019-04-10 01:14:24,514 root         INFO     Train Epoch: 117 [3584/8000 (45%)]\tTotal Loss: 0.137799\n",
      "Reconstruction: 0.134196, Regularization: 0.003603\n",
      "2019-04-10 01:14:24,569 root         INFO     Train Epoch: 117 [4096/8000 (51%)]\tTotal Loss: 0.162205\n",
      "Reconstruction: 0.154855, Regularization: 0.007350\n",
      "2019-04-10 01:14:24,624 root         INFO     Train Epoch: 117 [4608/8000 (58%)]\tTotal Loss: 0.151257\n",
      "Reconstruction: 0.145007, Regularization: 0.006250\n",
      "2019-04-10 01:14:24,680 root         INFO     Train Epoch: 117 [5120/8000 (64%)]\tTotal Loss: 0.159541\n",
      "Reconstruction: 0.153183, Regularization: 0.006358\n",
      "2019-04-10 01:14:24,735 root         INFO     Train Epoch: 117 [5632/8000 (70%)]\tTotal Loss: 0.158280\n",
      "Reconstruction: 0.150946, Regularization: 0.007334\n",
      "2019-04-10 01:14:24,790 root         INFO     Train Epoch: 117 [6144/8000 (77%)]\tTotal Loss: 0.155652\n",
      "Reconstruction: 0.147301, Regularization: 0.008351\n",
      "2019-04-10 01:14:24,846 root         INFO     Train Epoch: 117 [6656/8000 (83%)]\tTotal Loss: 0.158461\n",
      "Reconstruction: 0.151026, Regularization: 0.007435\n",
      "2019-04-10 01:14:24,901 root         INFO     Train Epoch: 117 [7168/8000 (90%)]\tTotal Loss: 0.141844\n",
      "Reconstruction: 0.136823, Regularization: 0.005020\n",
      "2019-04-10 01:14:24,956 root         INFO     Train Epoch: 117 [7680/8000 (96%)]\tTotal Loss: 0.141615\n",
      "Reconstruction: 0.134642, Regularization: 0.006972\n",
      "2019-04-10 01:14:25,006 root         INFO     ====> Epoch: 117 Average loss: 0.1505\n",
      "2019-04-10 01:14:25,029 root         INFO     Train Epoch: 118 [0/8000 (0%)]\tTotal Loss: 0.153987\n",
      "Reconstruction: 0.146487, Regularization: 0.007500\n",
      "2019-04-10 01:14:25,087 root         INFO     Train Epoch: 118 [512/8000 (6%)]\tTotal Loss: 0.137411\n",
      "Reconstruction: 0.132233, Regularization: 0.005178\n",
      "2019-04-10 01:14:25,144 root         INFO     Train Epoch: 118 [1024/8000 (13%)]\tTotal Loss: 0.148242\n",
      "Reconstruction: 0.142691, Regularization: 0.005551\n",
      "2019-04-10 01:14:25,201 root         INFO     Train Epoch: 118 [1536/8000 (19%)]\tTotal Loss: 0.142719\n",
      "Reconstruction: 0.137639, Regularization: 0.005081\n",
      "2019-04-10 01:14:25,258 root         INFO     Train Epoch: 118 [2048/8000 (26%)]\tTotal Loss: 0.163113\n",
      "Reconstruction: 0.154844, Regularization: 0.008269\n",
      "2019-04-10 01:14:25,314 root         INFO     Train Epoch: 118 [2560/8000 (32%)]\tTotal Loss: 0.153499\n",
      "Reconstruction: 0.147487, Regularization: 0.006011\n",
      "2019-04-10 01:14:25,371 root         INFO     Train Epoch: 118 [3072/8000 (38%)]\tTotal Loss: 0.148729\n",
      "Reconstruction: 0.143177, Regularization: 0.005552\n",
      "2019-04-10 01:14:25,428 root         INFO     Train Epoch: 118 [3584/8000 (45%)]\tTotal Loss: 0.149308\n",
      "Reconstruction: 0.140792, Regularization: 0.008516\n",
      "2019-04-10 01:14:25,485 root         INFO     Train Epoch: 118 [4096/8000 (51%)]\tTotal Loss: 0.162252\n",
      "Reconstruction: 0.155586, Regularization: 0.006666\n",
      "2019-04-10 01:14:25,541 root         INFO     Train Epoch: 118 [4608/8000 (58%)]\tTotal Loss: 0.163957\n",
      "Reconstruction: 0.156384, Regularization: 0.007573\n",
      "2019-04-10 01:14:25,597 root         INFO     Train Epoch: 118 [5120/8000 (64%)]\tTotal Loss: 0.141012\n",
      "Reconstruction: 0.136809, Regularization: 0.004203\n",
      "2019-04-10 01:14:25,653 root         INFO     Train Epoch: 118 [5632/8000 (70%)]\tTotal Loss: 0.155788\n",
      "Reconstruction: 0.148514, Regularization: 0.007274\n",
      "2019-04-10 01:14:25,709 root         INFO     Train Epoch: 118 [6144/8000 (77%)]\tTotal Loss: 0.144688\n",
      "Reconstruction: 0.137493, Regularization: 0.007194\n",
      "2019-04-10 01:14:25,766 root         INFO     Train Epoch: 118 [6656/8000 (83%)]\tTotal Loss: 0.146129\n",
      "Reconstruction: 0.141048, Regularization: 0.005081\n",
      "2019-04-10 01:14:25,822 root         INFO     Train Epoch: 118 [7168/8000 (90%)]\tTotal Loss: 0.159643\n",
      "Reconstruction: 0.151451, Regularization: 0.008192\n",
      "2019-04-10 01:14:25,878 root         INFO     Train Epoch: 118 [7680/8000 (96%)]\tTotal Loss: 0.157880\n",
      "Reconstruction: 0.151514, Regularization: 0.006366\n",
      "2019-04-10 01:14:25,926 root         INFO     ====> Epoch: 118 Average loss: 0.1505\n",
      "2019-04-10 01:14:25,950 root         INFO     Train Epoch: 119 [0/8000 (0%)]\tTotal Loss: 0.159980\n",
      "Reconstruction: 0.154570, Regularization: 0.005410\n",
      "2019-04-10 01:14:26,008 root         INFO     Train Epoch: 119 [512/8000 (6%)]\tTotal Loss: 0.153747\n",
      "Reconstruction: 0.147774, Regularization: 0.005973\n",
      "2019-04-10 01:14:26,065 root         INFO     Train Epoch: 119 [1024/8000 (13%)]\tTotal Loss: 0.146016\n",
      "Reconstruction: 0.140963, Regularization: 0.005053\n",
      "2019-04-10 01:14:26,120 root         INFO     Train Epoch: 119 [1536/8000 (19%)]\tTotal Loss: 0.138626\n",
      "Reconstruction: 0.135218, Regularization: 0.003408\n",
      "2019-04-10 01:14:26,176 root         INFO     Train Epoch: 119 [2048/8000 (26%)]\tTotal Loss: 0.161117\n",
      "Reconstruction: 0.150621, Regularization: 0.010496\n",
      "2019-04-10 01:14:26,231 root         INFO     Train Epoch: 119 [2560/8000 (32%)]\tTotal Loss: 0.150549\n",
      "Reconstruction: 0.145259, Regularization: 0.005289\n",
      "2019-04-10 01:14:26,287 root         INFO     Train Epoch: 119 [3072/8000 (38%)]\tTotal Loss: 0.160092\n",
      "Reconstruction: 0.149947, Regularization: 0.010146\n",
      "2019-04-10 01:14:26,342 root         INFO     Train Epoch: 119 [3584/8000 (45%)]\tTotal Loss: 0.148018\n",
      "Reconstruction: 0.141898, Regularization: 0.006120\n",
      "2019-04-10 01:14:26,398 root         INFO     Train Epoch: 119 [4096/8000 (51%)]\tTotal Loss: 0.145530\n",
      "Reconstruction: 0.138988, Regularization: 0.006543\n",
      "2019-04-10 01:14:26,453 root         INFO     Train Epoch: 119 [4608/8000 (58%)]\tTotal Loss: 0.150748\n",
      "Reconstruction: 0.145881, Regularization: 0.004867\n",
      "2019-04-10 01:14:26,509 root         INFO     Train Epoch: 119 [5120/8000 (64%)]\tTotal Loss: 0.144486\n",
      "Reconstruction: 0.139719, Regularization: 0.004767\n",
      "2019-04-10 01:14:26,565 root         INFO     Train Epoch: 119 [5632/8000 (70%)]\tTotal Loss: 0.147573\n",
      "Reconstruction: 0.143924, Regularization: 0.003648\n",
      "2019-04-10 01:14:26,620 root         INFO     Train Epoch: 119 [6144/8000 (77%)]\tTotal Loss: 0.153347\n",
      "Reconstruction: 0.148634, Regularization: 0.004713\n",
      "2019-04-10 01:14:26,676 root         INFO     Train Epoch: 119 [6656/8000 (83%)]\tTotal Loss: 0.140666\n",
      "Reconstruction: 0.136626, Regularization: 0.004040\n",
      "2019-04-10 01:14:26,732 root         INFO     Train Epoch: 119 [7168/8000 (90%)]\tTotal Loss: 0.147178\n",
      "Reconstruction: 0.140797, Regularization: 0.006381\n",
      "2019-04-10 01:14:26,788 root         INFO     Train Epoch: 119 [7680/8000 (96%)]\tTotal Loss: 0.145971\n",
      "Reconstruction: 0.139495, Regularization: 0.006476\n",
      "2019-04-10 01:14:26,837 root         INFO     ====> Epoch: 119 Average loss: 0.1507\n",
      "2019-04-10 01:14:26,861 root         INFO     Train Epoch: 120 [0/8000 (0%)]\tTotal Loss: 0.143734\n",
      "Reconstruction: 0.137798, Regularization: 0.005936\n",
      "2019-04-10 01:14:26,918 root         INFO     Train Epoch: 120 [512/8000 (6%)]\tTotal Loss: 0.152520\n",
      "Reconstruction: 0.145157, Regularization: 0.007363\n",
      "2019-04-10 01:14:26,976 root         INFO     Train Epoch: 120 [1024/8000 (13%)]\tTotal Loss: 0.143425\n",
      "Reconstruction: 0.135753, Regularization: 0.007672\n",
      "2019-04-10 01:14:27,033 root         INFO     Train Epoch: 120 [1536/8000 (19%)]\tTotal Loss: 0.156608\n",
      "Reconstruction: 0.150598, Regularization: 0.006011\n",
      "2019-04-10 01:14:27,090 root         INFO     Train Epoch: 120 [2048/8000 (26%)]\tTotal Loss: 0.152353\n",
      "Reconstruction: 0.147626, Regularization: 0.004727\n",
      "2019-04-10 01:14:27,147 root         INFO     Train Epoch: 120 [2560/8000 (32%)]\tTotal Loss: 0.149200\n",
      "Reconstruction: 0.143347, Regularization: 0.005854\n",
      "2019-04-10 01:14:27,202 root         INFO     Train Epoch: 120 [3072/8000 (38%)]\tTotal Loss: 0.153889\n",
      "Reconstruction: 0.146228, Regularization: 0.007662\n",
      "2019-04-10 01:14:27,258 root         INFO     Train Epoch: 120 [3584/8000 (45%)]\tTotal Loss: 0.161251\n",
      "Reconstruction: 0.155185, Regularization: 0.006066\n",
      "2019-04-10 01:14:27,313 root         INFO     Train Epoch: 120 [4096/8000 (51%)]\tTotal Loss: 0.141577\n",
      "Reconstruction: 0.137025, Regularization: 0.004551\n",
      "2019-04-10 01:14:27,369 root         INFO     Train Epoch: 120 [4608/8000 (58%)]\tTotal Loss: 0.155148\n",
      "Reconstruction: 0.149905, Regularization: 0.005243\n",
      "2019-04-10 01:14:27,424 root         INFO     Train Epoch: 120 [5120/8000 (64%)]\tTotal Loss: 0.146830\n",
      "Reconstruction: 0.141303, Regularization: 0.005527\n",
      "2019-04-10 01:14:27,479 root         INFO     Train Epoch: 120 [5632/8000 (70%)]\tTotal Loss: 0.163459\n",
      "Reconstruction: 0.153857, Regularization: 0.009602\n",
      "2019-04-10 01:14:27,534 root         INFO     Train Epoch: 120 [6144/8000 (77%)]\tTotal Loss: 0.150200\n",
      "Reconstruction: 0.144152, Regularization: 0.006048\n",
      "2019-04-10 01:14:27,590 root         INFO     Train Epoch: 120 [6656/8000 (83%)]\tTotal Loss: 0.149317\n",
      "Reconstruction: 0.142266, Regularization: 0.007051\n",
      "2019-04-10 01:14:27,646 root         INFO     Train Epoch: 120 [7168/8000 (90%)]\tTotal Loss: 0.157496\n",
      "Reconstruction: 0.153090, Regularization: 0.004406\n",
      "2019-04-10 01:14:27,701 root         INFO     Train Epoch: 120 [7680/8000 (96%)]\tTotal Loss: 0.154402\n",
      "Reconstruction: 0.143995, Regularization: 0.010407\n",
      "2019-04-10 01:14:27,750 root         INFO     ====> Epoch: 120 Average loss: 0.1504\n",
      "2019-04-10 01:14:27,773 root         INFO     Train Epoch: 121 [0/8000 (0%)]\tTotal Loss: 0.161631\n",
      "Reconstruction: 0.154941, Regularization: 0.006690\n",
      "2019-04-10 01:14:27,830 root         INFO     Train Epoch: 121 [512/8000 (6%)]\tTotal Loss: 0.154223\n",
      "Reconstruction: 0.149775, Regularization: 0.004448\n",
      "2019-04-10 01:14:27,887 root         INFO     Train Epoch: 121 [1024/8000 (13%)]\tTotal Loss: 0.144881\n",
      "Reconstruction: 0.138356, Regularization: 0.006526\n",
      "2019-04-10 01:14:27,944 root         INFO     Train Epoch: 121 [1536/8000 (19%)]\tTotal Loss: 0.151643\n",
      "Reconstruction: 0.145577, Regularization: 0.006066\n",
      "2019-04-10 01:14:28,001 root         INFO     Train Epoch: 121 [2048/8000 (26%)]\tTotal Loss: 0.144558\n",
      "Reconstruction: 0.139783, Regularization: 0.004776\n",
      "2019-04-10 01:14:28,059 root         INFO     Train Epoch: 121 [2560/8000 (32%)]\tTotal Loss: 0.148562\n",
      "Reconstruction: 0.141197, Regularization: 0.007364\n",
      "2019-04-10 01:14:28,116 root         INFO     Train Epoch: 121 [3072/8000 (38%)]\tTotal Loss: 0.182349\n",
      "Reconstruction: 0.175179, Regularization: 0.007170\n",
      "2019-04-10 01:14:28,173 root         INFO     Train Epoch: 121 [3584/8000 (45%)]\tTotal Loss: 0.146037\n",
      "Reconstruction: 0.142263, Regularization: 0.003774\n",
      "2019-04-10 01:14:28,230 root         INFO     Train Epoch: 121 [4096/8000 (51%)]\tTotal Loss: 0.148246\n",
      "Reconstruction: 0.142122, Regularization: 0.006124\n",
      "2019-04-10 01:14:28,287 root         INFO     Train Epoch: 121 [4608/8000 (58%)]\tTotal Loss: 0.142187\n",
      "Reconstruction: 0.137480, Regularization: 0.004706\n",
      "2019-04-10 01:14:28,344 root         INFO     Train Epoch: 121 [5120/8000 (64%)]\tTotal Loss: 0.148788\n",
      "Reconstruction: 0.143085, Regularization: 0.005703\n",
      "2019-04-10 01:14:28,400 root         INFO     Train Epoch: 121 [5632/8000 (70%)]\tTotal Loss: 0.147710\n",
      "Reconstruction: 0.144810, Regularization: 0.002900\n",
      "2019-04-10 01:14:28,455 root         INFO     Train Epoch: 121 [6144/8000 (77%)]\tTotal Loss: 0.158669\n",
      "Reconstruction: 0.150833, Regularization: 0.007836\n",
      "2019-04-10 01:14:28,510 root         INFO     Train Epoch: 121 [6656/8000 (83%)]\tTotal Loss: 0.158543\n",
      "Reconstruction: 0.150723, Regularization: 0.007820\n",
      "2019-04-10 01:14:28,566 root         INFO     Train Epoch: 121 [7168/8000 (90%)]\tTotal Loss: 0.139372\n",
      "Reconstruction: 0.134033, Regularization: 0.005338\n",
      "2019-04-10 01:14:28,621 root         INFO     Train Epoch: 121 [7680/8000 (96%)]\tTotal Loss: 0.162060\n",
      "Reconstruction: 0.155585, Regularization: 0.006475\n",
      "2019-04-10 01:14:28,671 root         INFO     ====> Epoch: 121 Average loss: 0.1503\n",
      "2019-04-10 01:14:28,694 root         INFO     Train Epoch: 122 [0/8000 (0%)]\tTotal Loss: 0.152448\n",
      "Reconstruction: 0.146979, Regularization: 0.005469\n",
      "2019-04-10 01:14:28,752 root         INFO     Train Epoch: 122 [512/8000 (6%)]\tTotal Loss: 0.158216\n",
      "Reconstruction: 0.147633, Regularization: 0.010583\n",
      "2019-04-10 01:14:28,810 root         INFO     Train Epoch: 122 [1024/8000 (13%)]\tTotal Loss: 0.142866\n",
      "Reconstruction: 0.137186, Regularization: 0.005680\n",
      "2019-04-10 01:14:28,867 root         INFO     Train Epoch: 122 [1536/8000 (19%)]\tTotal Loss: 0.153731\n",
      "Reconstruction: 0.147341, Regularization: 0.006390\n",
      "2019-04-10 01:14:28,924 root         INFO     Train Epoch: 122 [2048/8000 (26%)]\tTotal Loss: 0.149385\n",
      "Reconstruction: 0.143982, Regularization: 0.005403\n",
      "2019-04-10 01:14:28,982 root         INFO     Train Epoch: 122 [2560/8000 (32%)]\tTotal Loss: 0.165422\n",
      "Reconstruction: 0.156882, Regularization: 0.008540\n",
      "2019-04-10 01:14:29,040 root         INFO     Train Epoch: 122 [3072/8000 (38%)]\tTotal Loss: 0.148371\n",
      "Reconstruction: 0.139605, Regularization: 0.008767\n",
      "2019-04-10 01:14:29,097 root         INFO     Train Epoch: 122 [3584/8000 (45%)]\tTotal Loss: 0.147230\n",
      "Reconstruction: 0.142311, Regularization: 0.004919\n",
      "2019-04-10 01:14:29,155 root         INFO     Train Epoch: 122 [4096/8000 (51%)]\tTotal Loss: 0.143200\n",
      "Reconstruction: 0.138939, Regularization: 0.004261\n",
      "2019-04-10 01:14:29,212 root         INFO     Train Epoch: 122 [4608/8000 (58%)]\tTotal Loss: 0.155729\n",
      "Reconstruction: 0.150809, Regularization: 0.004919\n",
      "2019-04-10 01:14:29,268 root         INFO     Train Epoch: 122 [5120/8000 (64%)]\tTotal Loss: 0.153420\n",
      "Reconstruction: 0.145503, Regularization: 0.007917\n",
      "2019-04-10 01:14:29,324 root         INFO     Train Epoch: 122 [5632/8000 (70%)]\tTotal Loss: 0.134199\n",
      "Reconstruction: 0.131055, Regularization: 0.003145\n",
      "2019-04-10 01:14:29,381 root         INFO     Train Epoch: 122 [6144/8000 (77%)]\tTotal Loss: 0.152149\n",
      "Reconstruction: 0.144998, Regularization: 0.007151\n",
      "2019-04-10 01:14:29,438 root         INFO     Train Epoch: 122 [6656/8000 (83%)]\tTotal Loss: 0.147924\n",
      "Reconstruction: 0.144030, Regularization: 0.003894\n",
      "2019-04-10 01:14:29,493 root         INFO     Train Epoch: 122 [7168/8000 (90%)]\tTotal Loss: 0.167801\n",
      "Reconstruction: 0.161600, Regularization: 0.006201\n",
      "2019-04-10 01:14:29,550 root         INFO     Train Epoch: 122 [7680/8000 (96%)]\tTotal Loss: 0.150308\n",
      "Reconstruction: 0.146238, Regularization: 0.004070\n",
      "2019-04-10 01:14:29,600 root         INFO     ====> Epoch: 122 Average loss: 0.1504\n",
      "2019-04-10 01:14:29,623 root         INFO     Train Epoch: 123 [0/8000 (0%)]\tTotal Loss: 0.168227\n",
      "Reconstruction: 0.159340, Regularization: 0.008887\n",
      "2019-04-10 01:14:29,680 root         INFO     Train Epoch: 123 [512/8000 (6%)]\tTotal Loss: 0.139366\n",
      "Reconstruction: 0.135076, Regularization: 0.004290\n",
      "2019-04-10 01:14:29,737 root         INFO     Train Epoch: 123 [1024/8000 (13%)]\tTotal Loss: 0.161318\n",
      "Reconstruction: 0.153272, Regularization: 0.008046\n",
      "2019-04-10 01:14:29,794 root         INFO     Train Epoch: 123 [1536/8000 (19%)]\tTotal Loss: 0.169138\n",
      "Reconstruction: 0.161378, Regularization: 0.007760\n",
      "2019-04-10 01:14:29,851 root         INFO     Train Epoch: 123 [2048/8000 (26%)]\tTotal Loss: 0.145416\n",
      "Reconstruction: 0.140524, Regularization: 0.004891\n",
      "2019-04-10 01:14:29,908 root         INFO     Train Epoch: 123 [2560/8000 (32%)]\tTotal Loss: 0.152319\n",
      "Reconstruction: 0.147819, Regularization: 0.004499\n",
      "2019-04-10 01:14:29,964 root         INFO     Train Epoch: 123 [3072/8000 (38%)]\tTotal Loss: 0.150383\n",
      "Reconstruction: 0.141501, Regularization: 0.008881\n",
      "2019-04-10 01:14:30,021 root         INFO     Train Epoch: 123 [3584/8000 (45%)]\tTotal Loss: 0.153419\n",
      "Reconstruction: 0.146598, Regularization: 0.006821\n",
      "2019-04-10 01:14:30,078 root         INFO     Train Epoch: 123 [4096/8000 (51%)]\tTotal Loss: 0.149752\n",
      "Reconstruction: 0.145865, Regularization: 0.003887\n",
      "2019-04-10 01:14:30,135 root         INFO     Train Epoch: 123 [4608/8000 (58%)]\tTotal Loss: 0.167820\n",
      "Reconstruction: 0.160234, Regularization: 0.007586\n",
      "2019-04-10 01:14:30,191 root         INFO     Train Epoch: 123 [5120/8000 (64%)]\tTotal Loss: 0.138691\n",
      "Reconstruction: 0.134325, Regularization: 0.004366\n",
      "2019-04-10 01:14:30,248 root         INFO     Train Epoch: 123 [5632/8000 (70%)]\tTotal Loss: 0.151991\n",
      "Reconstruction: 0.148015, Regularization: 0.003975\n",
      "2019-04-10 01:14:30,304 root         INFO     Train Epoch: 123 [6144/8000 (77%)]\tTotal Loss: 0.145498\n",
      "Reconstruction: 0.141293, Regularization: 0.004205\n",
      "2019-04-10 01:14:30,361 root         INFO     Train Epoch: 123 [6656/8000 (83%)]\tTotal Loss: 0.144315\n",
      "Reconstruction: 0.138544, Regularization: 0.005772\n",
      "2019-04-10 01:14:30,418 root         INFO     Train Epoch: 123 [7168/8000 (90%)]\tTotal Loss: 0.157133\n",
      "Reconstruction: 0.151462, Regularization: 0.005672\n",
      "2019-04-10 01:14:30,475 root         INFO     Train Epoch: 123 [7680/8000 (96%)]\tTotal Loss: 0.141058\n",
      "Reconstruction: 0.135639, Regularization: 0.005419\n",
      "2019-04-10 01:14:30,525 root         INFO     ====> Epoch: 123 Average loss: 0.1507\n",
      "2019-04-10 01:14:30,548 root         INFO     Train Epoch: 124 [0/8000 (0%)]\tTotal Loss: 0.146979\n",
      "Reconstruction: 0.143440, Regularization: 0.003540\n",
      "2019-04-10 01:14:30,606 root         INFO     Train Epoch: 124 [512/8000 (6%)]\tTotal Loss: 0.153284\n",
      "Reconstruction: 0.147400, Regularization: 0.005883\n",
      "2019-04-10 01:14:30,663 root         INFO     Train Epoch: 124 [1024/8000 (13%)]\tTotal Loss: 0.147537\n",
      "Reconstruction: 0.140115, Regularization: 0.007422\n",
      "2019-04-10 01:14:30,720 root         INFO     Train Epoch: 124 [1536/8000 (19%)]\tTotal Loss: 0.156346\n",
      "Reconstruction: 0.149572, Regularization: 0.006774\n",
      "2019-04-10 01:14:30,777 root         INFO     Train Epoch: 124 [2048/8000 (26%)]\tTotal Loss: 0.146296\n",
      "Reconstruction: 0.140500, Regularization: 0.005796\n",
      "2019-04-10 01:14:30,833 root         INFO     Train Epoch: 124 [2560/8000 (32%)]\tTotal Loss: 0.151258\n",
      "Reconstruction: 0.145638, Regularization: 0.005620\n",
      "2019-04-10 01:14:30,890 root         INFO     Train Epoch: 124 [3072/8000 (38%)]\tTotal Loss: 0.143217\n",
      "Reconstruction: 0.137904, Regularization: 0.005313\n",
      "2019-04-10 01:14:30,947 root         INFO     Train Epoch: 124 [3584/8000 (45%)]\tTotal Loss: 0.154665\n",
      "Reconstruction: 0.150229, Regularization: 0.004436\n",
      "2019-04-10 01:14:31,003 root         INFO     Train Epoch: 124 [4096/8000 (51%)]\tTotal Loss: 0.162679\n",
      "Reconstruction: 0.156026, Regularization: 0.006653\n",
      "2019-04-10 01:14:31,060 root         INFO     Train Epoch: 124 [4608/8000 (58%)]\tTotal Loss: 0.154622\n",
      "Reconstruction: 0.149161, Regularization: 0.005461\n",
      "2019-04-10 01:14:31,117 root         INFO     Train Epoch: 124 [5120/8000 (64%)]\tTotal Loss: 0.153285\n",
      "Reconstruction: 0.145534, Regularization: 0.007751\n",
      "2019-04-10 01:14:31,173 root         INFO     Train Epoch: 124 [5632/8000 (70%)]\tTotal Loss: 0.154529\n",
      "Reconstruction: 0.147000, Regularization: 0.007529\n",
      "2019-04-10 01:14:31,230 root         INFO     Train Epoch: 124 [6144/8000 (77%)]\tTotal Loss: 0.146786\n",
      "Reconstruction: 0.142170, Regularization: 0.004616\n",
      "2019-04-10 01:14:31,287 root         INFO     Train Epoch: 124 [6656/8000 (83%)]\tTotal Loss: 0.144804\n",
      "Reconstruction: 0.138960, Regularization: 0.005843\n",
      "2019-04-10 01:14:31,343 root         INFO     Train Epoch: 124 [7168/8000 (90%)]\tTotal Loss: 0.162156\n",
      "Reconstruction: 0.152016, Regularization: 0.010140\n",
      "2019-04-10 01:14:31,400 root         INFO     Train Epoch: 124 [7680/8000 (96%)]\tTotal Loss: 0.162536\n",
      "Reconstruction: 0.156432, Regularization: 0.006104\n",
      "2019-04-10 01:14:31,450 root         INFO     ====> Epoch: 124 Average loss: 0.1500\n",
      "2019-04-10 01:14:31,473 root         INFO     Train Epoch: 125 [0/8000 (0%)]\tTotal Loss: 0.145179\n",
      "Reconstruction: 0.138547, Regularization: 0.006631\n",
      "2019-04-10 01:14:31,531 root         INFO     Train Epoch: 125 [512/8000 (6%)]\tTotal Loss: 0.158524\n",
      "Reconstruction: 0.153979, Regularization: 0.004544\n",
      "2019-04-10 01:14:31,589 root         INFO     Train Epoch: 125 [1024/8000 (13%)]\tTotal Loss: 0.150533\n",
      "Reconstruction: 0.145630, Regularization: 0.004903\n",
      "2019-04-10 01:14:31,646 root         INFO     Train Epoch: 125 [1536/8000 (19%)]\tTotal Loss: 0.157071\n",
      "Reconstruction: 0.150233, Regularization: 0.006838\n",
      "2019-04-10 01:14:31,703 root         INFO     Train Epoch: 125 [2048/8000 (26%)]\tTotal Loss: 0.136161\n",
      "Reconstruction: 0.132250, Regularization: 0.003911\n",
      "2019-04-10 01:14:31,759 root         INFO     Train Epoch: 125 [2560/8000 (32%)]\tTotal Loss: 0.150350\n",
      "Reconstruction: 0.144303, Regularization: 0.006047\n",
      "2019-04-10 01:14:31,815 root         INFO     Train Epoch: 125 [3072/8000 (38%)]\tTotal Loss: 0.151135\n",
      "Reconstruction: 0.145940, Regularization: 0.005195\n",
      "2019-04-10 01:14:31,871 root         INFO     Train Epoch: 125 [3584/8000 (45%)]\tTotal Loss: 0.148217\n",
      "Reconstruction: 0.142192, Regularization: 0.006025\n",
      "2019-04-10 01:14:31,927 root         INFO     Train Epoch: 125 [4096/8000 (51%)]\tTotal Loss: 0.146070\n",
      "Reconstruction: 0.140150, Regularization: 0.005920\n",
      "2019-04-10 01:14:31,984 root         INFO     Train Epoch: 125 [4608/8000 (58%)]\tTotal Loss: 0.146129\n",
      "Reconstruction: 0.138859, Regularization: 0.007270\n",
      "2019-04-10 01:14:32,041 root         INFO     Train Epoch: 125 [5120/8000 (64%)]\tTotal Loss: 0.143563\n",
      "Reconstruction: 0.138171, Regularization: 0.005393\n",
      "2019-04-10 01:14:32,098 root         INFO     Train Epoch: 125 [5632/8000 (70%)]\tTotal Loss: 0.136223\n",
      "Reconstruction: 0.132634, Regularization: 0.003589\n",
      "2019-04-10 01:14:32,155 root         INFO     Train Epoch: 125 [6144/8000 (77%)]\tTotal Loss: 0.138545\n",
      "Reconstruction: 0.134171, Regularization: 0.004374\n",
      "2019-04-10 01:14:32,212 root         INFO     Train Epoch: 125 [6656/8000 (83%)]\tTotal Loss: 0.150071\n",
      "Reconstruction: 0.146053, Regularization: 0.004019\n",
      "2019-04-10 01:14:32,270 root         INFO     Train Epoch: 125 [7168/8000 (90%)]\tTotal Loss: 0.151925\n",
      "Reconstruction: 0.145472, Regularization: 0.006454\n",
      "2019-04-10 01:14:32,327 root         INFO     Train Epoch: 125 [7680/8000 (96%)]\tTotal Loss: 0.144718\n",
      "Reconstruction: 0.141298, Regularization: 0.003420\n",
      "2019-04-10 01:14:32,379 root         INFO     ====> Epoch: 125 Average loss: 0.1500\n",
      "2019-04-10 01:14:32,402 root         INFO     Train Epoch: 126 [0/8000 (0%)]\tTotal Loss: 0.145641\n",
      "Reconstruction: 0.141421, Regularization: 0.004220\n",
      "2019-04-10 01:14:32,459 root         INFO     Train Epoch: 126 [512/8000 (6%)]\tTotal Loss: 0.148236\n",
      "Reconstruction: 0.143519, Regularization: 0.004717\n",
      "2019-04-10 01:14:32,516 root         INFO     Train Epoch: 126 [1024/8000 (13%)]\tTotal Loss: 0.150874\n",
      "Reconstruction: 0.144990, Regularization: 0.005884\n",
      "2019-04-10 01:14:32,573 root         INFO     Train Epoch: 126 [1536/8000 (19%)]\tTotal Loss: 0.151434\n",
      "Reconstruction: 0.146370, Regularization: 0.005064\n",
      "2019-04-10 01:14:32,630 root         INFO     Train Epoch: 126 [2048/8000 (26%)]\tTotal Loss: 0.146690\n",
      "Reconstruction: 0.142367, Regularization: 0.004323\n",
      "2019-04-10 01:14:32,686 root         INFO     Train Epoch: 126 [2560/8000 (32%)]\tTotal Loss: 0.159215\n",
      "Reconstruction: 0.151717, Regularization: 0.007498\n",
      "2019-04-10 01:14:32,743 root         INFO     Train Epoch: 126 [3072/8000 (38%)]\tTotal Loss: 0.158668\n",
      "Reconstruction: 0.150265, Regularization: 0.008403\n",
      "2019-04-10 01:14:32,800 root         INFO     Train Epoch: 126 [3584/8000 (45%)]\tTotal Loss: 0.153011\n",
      "Reconstruction: 0.147595, Regularization: 0.005416\n",
      "2019-04-10 01:14:32,857 root         INFO     Train Epoch: 126 [4096/8000 (51%)]\tTotal Loss: 0.151096\n",
      "Reconstruction: 0.144312, Regularization: 0.006783\n",
      "2019-04-10 01:14:32,913 root         INFO     Train Epoch: 126 [4608/8000 (58%)]\tTotal Loss: 0.139979\n",
      "Reconstruction: 0.135142, Regularization: 0.004837\n",
      "2019-04-10 01:14:32,970 root         INFO     Train Epoch: 126 [5120/8000 (64%)]\tTotal Loss: 0.140653\n",
      "Reconstruction: 0.136936, Regularization: 0.003718\n",
      "2019-04-10 01:14:33,027 root         INFO     Train Epoch: 126 [5632/8000 (70%)]\tTotal Loss: 0.146467\n",
      "Reconstruction: 0.141271, Regularization: 0.005197\n",
      "2019-04-10 01:14:33,083 root         INFO     Train Epoch: 126 [6144/8000 (77%)]\tTotal Loss: 0.158513\n",
      "Reconstruction: 0.149510, Regularization: 0.009004\n",
      "2019-04-10 01:14:33,140 root         INFO     Train Epoch: 126 [6656/8000 (83%)]\tTotal Loss: 0.153866\n",
      "Reconstruction: 0.146115, Regularization: 0.007750\n",
      "2019-04-10 01:14:33,197 root         INFO     Train Epoch: 126 [7168/8000 (90%)]\tTotal Loss: 0.146245\n",
      "Reconstruction: 0.140771, Regularization: 0.005475\n",
      "2019-04-10 01:14:33,254 root         INFO     Train Epoch: 126 [7680/8000 (96%)]\tTotal Loss: 0.145003\n",
      "Reconstruction: 0.141896, Regularization: 0.003107\n",
      "2019-04-10 01:14:33,304 root         INFO     ====> Epoch: 126 Average loss: 0.1499\n",
      "2019-04-10 01:14:33,327 root         INFO     Train Epoch: 127 [0/8000 (0%)]\tTotal Loss: 0.155092\n",
      "Reconstruction: 0.149195, Regularization: 0.005896\n",
      "2019-04-10 01:14:33,384 root         INFO     Train Epoch: 127 [512/8000 (6%)]\tTotal Loss: 0.145899\n",
      "Reconstruction: 0.140866, Regularization: 0.005033\n",
      "2019-04-10 01:14:33,441 root         INFO     Train Epoch: 127 [1024/8000 (13%)]\tTotal Loss: 0.149779\n",
      "Reconstruction: 0.144299, Regularization: 0.005479\n",
      "2019-04-10 01:14:33,497 root         INFO     Train Epoch: 127 [1536/8000 (19%)]\tTotal Loss: 0.148093\n",
      "Reconstruction: 0.142193, Regularization: 0.005900\n",
      "2019-04-10 01:14:33,554 root         INFO     Train Epoch: 127 [2048/8000 (26%)]\tTotal Loss: 0.140770\n",
      "Reconstruction: 0.135633, Regularization: 0.005136\n",
      "2019-04-10 01:14:33,610 root         INFO     Train Epoch: 127 [2560/8000 (32%)]\tTotal Loss: 0.141253\n",
      "Reconstruction: 0.137613, Regularization: 0.003640\n",
      "2019-04-10 01:14:33,668 root         INFO     Train Epoch: 127 [3072/8000 (38%)]\tTotal Loss: 0.157172\n",
      "Reconstruction: 0.149725, Regularization: 0.007447\n",
      "2019-04-10 01:14:33,724 root         INFO     Train Epoch: 127 [3584/8000 (45%)]\tTotal Loss: 0.145549\n",
      "Reconstruction: 0.141865, Regularization: 0.003684\n",
      "2019-04-10 01:14:33,781 root         INFO     Train Epoch: 127 [4096/8000 (51%)]\tTotal Loss: 0.157214\n",
      "Reconstruction: 0.150376, Regularization: 0.006838\n",
      "2019-04-10 01:14:33,837 root         INFO     Train Epoch: 127 [4608/8000 (58%)]\tTotal Loss: 0.147173\n",
      "Reconstruction: 0.141911, Regularization: 0.005262\n",
      "2019-04-10 01:14:33,893 root         INFO     Train Epoch: 127 [5120/8000 (64%)]\tTotal Loss: 0.150024\n",
      "Reconstruction: 0.142253, Regularization: 0.007771\n",
      "2019-04-10 01:14:33,950 root         INFO     Train Epoch: 127 [5632/8000 (70%)]\tTotal Loss: 0.154861\n",
      "Reconstruction: 0.146694, Regularization: 0.008167\n",
      "2019-04-10 01:14:34,006 root         INFO     Train Epoch: 127 [6144/8000 (77%)]\tTotal Loss: 0.153376\n",
      "Reconstruction: 0.146605, Regularization: 0.006771\n",
      "2019-04-10 01:14:34,063 root         INFO     Train Epoch: 127 [6656/8000 (83%)]\tTotal Loss: 0.161320\n",
      "Reconstruction: 0.155164, Regularization: 0.006156\n",
      "2019-04-10 01:14:34,119 root         INFO     Train Epoch: 127 [7168/8000 (90%)]\tTotal Loss: 0.140292\n",
      "Reconstruction: 0.137012, Regularization: 0.003280\n",
      "2019-04-10 01:14:34,174 root         INFO     Train Epoch: 127 [7680/8000 (96%)]\tTotal Loss: 0.155073\n",
      "Reconstruction: 0.148468, Regularization: 0.006605\n",
      "2019-04-10 01:14:34,225 root         INFO     ====> Epoch: 127 Average loss: 0.1500\n",
      "2019-04-10 01:14:34,248 root         INFO     Train Epoch: 128 [0/8000 (0%)]\tTotal Loss: 0.140492\n",
      "Reconstruction: 0.135594, Regularization: 0.004898\n",
      "2019-04-10 01:14:34,305 root         INFO     Train Epoch: 128 [512/8000 (6%)]\tTotal Loss: 0.157555\n",
      "Reconstruction: 0.153233, Regularization: 0.004322\n",
      "2019-04-10 01:14:34,363 root         INFO     Train Epoch: 128 [1024/8000 (13%)]\tTotal Loss: 0.157865\n",
      "Reconstruction: 0.150777, Regularization: 0.007088\n",
      "2019-04-10 01:14:34,420 root         INFO     Train Epoch: 128 [1536/8000 (19%)]\tTotal Loss: 0.149899\n",
      "Reconstruction: 0.146513, Regularization: 0.003387\n",
      "2019-04-10 01:14:34,477 root         INFO     Train Epoch: 128 [2048/8000 (26%)]\tTotal Loss: 0.144037\n",
      "Reconstruction: 0.137030, Regularization: 0.007006\n",
      "2019-04-10 01:14:34,533 root         INFO     Train Epoch: 128 [2560/8000 (32%)]\tTotal Loss: 0.153027\n",
      "Reconstruction: 0.147291, Regularization: 0.005736\n",
      "2019-04-10 01:14:34,590 root         INFO     Train Epoch: 128 [3072/8000 (38%)]\tTotal Loss: 0.153362\n",
      "Reconstruction: 0.144375, Regularization: 0.008987\n",
      "2019-04-10 01:14:34,647 root         INFO     Train Epoch: 128 [3584/8000 (45%)]\tTotal Loss: 0.153285\n",
      "Reconstruction: 0.146943, Regularization: 0.006341\n",
      "2019-04-10 01:14:34,704 root         INFO     Train Epoch: 128 [4096/8000 (51%)]\tTotal Loss: 0.143794\n",
      "Reconstruction: 0.138679, Regularization: 0.005115\n",
      "2019-04-10 01:14:34,761 root         INFO     Train Epoch: 128 [4608/8000 (58%)]\tTotal Loss: 0.155800\n",
      "Reconstruction: 0.146524, Regularization: 0.009276\n",
      "2019-04-10 01:14:34,818 root         INFO     Train Epoch: 128 [5120/8000 (64%)]\tTotal Loss: 0.140500\n",
      "Reconstruction: 0.134253, Regularization: 0.006247\n",
      "2019-04-10 01:14:34,874 root         INFO     Train Epoch: 128 [5632/8000 (70%)]\tTotal Loss: 0.151163\n",
      "Reconstruction: 0.145247, Regularization: 0.005917\n",
      "2019-04-10 01:14:34,931 root         INFO     Train Epoch: 128 [6144/8000 (77%)]\tTotal Loss: 0.146514\n",
      "Reconstruction: 0.142458, Regularization: 0.004056\n",
      "2019-04-10 01:14:34,986 root         INFO     Train Epoch: 128 [6656/8000 (83%)]\tTotal Loss: 0.158391\n",
      "Reconstruction: 0.151673, Regularization: 0.006718\n",
      "2019-04-10 01:14:35,042 root         INFO     Train Epoch: 128 [7168/8000 (90%)]\tTotal Loss: 0.150610\n",
      "Reconstruction: 0.146176, Regularization: 0.004434\n",
      "2019-04-10 01:14:35,096 root         INFO     Train Epoch: 128 [7680/8000 (96%)]\tTotal Loss: 0.155420\n",
      "Reconstruction: 0.146331, Regularization: 0.009089\n",
      "2019-04-10 01:14:35,146 root         INFO     ====> Epoch: 128 Average loss: 0.1502\n",
      "2019-04-10 01:14:35,169 root         INFO     Train Epoch: 129 [0/8000 (0%)]\tTotal Loss: 0.147469\n",
      "Reconstruction: 0.141580, Regularization: 0.005889\n",
      "2019-04-10 01:14:35,225 root         INFO     Train Epoch: 129 [512/8000 (6%)]\tTotal Loss: 0.150965\n",
      "Reconstruction: 0.142599, Regularization: 0.008367\n",
      "2019-04-10 01:14:35,280 root         INFO     Train Epoch: 129 [1024/8000 (13%)]\tTotal Loss: 0.139742\n",
      "Reconstruction: 0.135236, Regularization: 0.004506\n",
      "2019-04-10 01:14:35,335 root         INFO     Train Epoch: 129 [1536/8000 (19%)]\tTotal Loss: 0.147579\n",
      "Reconstruction: 0.143382, Regularization: 0.004197\n",
      "2019-04-10 01:14:35,390 root         INFO     Train Epoch: 129 [2048/8000 (26%)]\tTotal Loss: 0.148804\n",
      "Reconstruction: 0.144911, Regularization: 0.003892\n",
      "2019-04-10 01:14:35,445 root         INFO     Train Epoch: 129 [2560/8000 (32%)]\tTotal Loss: 0.141327\n",
      "Reconstruction: 0.134538, Regularization: 0.006789\n",
      "2019-04-10 01:14:35,499 root         INFO     Train Epoch: 129 [3072/8000 (38%)]\tTotal Loss: 0.140799\n",
      "Reconstruction: 0.136007, Regularization: 0.004792\n",
      "2019-04-10 01:14:35,555 root         INFO     Train Epoch: 129 [3584/8000 (45%)]\tTotal Loss: 0.156613\n",
      "Reconstruction: 0.152215, Regularization: 0.004398\n",
      "2019-04-10 01:14:35,610 root         INFO     Train Epoch: 129 [4096/8000 (51%)]\tTotal Loss: 0.149514\n",
      "Reconstruction: 0.144493, Regularization: 0.005021\n",
      "2019-04-10 01:14:35,665 root         INFO     Train Epoch: 129 [4608/8000 (58%)]\tTotal Loss: 0.146485\n",
      "Reconstruction: 0.136375, Regularization: 0.010109\n",
      "2019-04-10 01:14:35,719 root         INFO     Train Epoch: 129 [5120/8000 (64%)]\tTotal Loss: 0.160750\n",
      "Reconstruction: 0.151258, Regularization: 0.009492\n",
      "2019-04-10 01:14:35,775 root         INFO     Train Epoch: 129 [5632/8000 (70%)]\tTotal Loss: 0.156109\n",
      "Reconstruction: 0.150981, Regularization: 0.005128\n",
      "2019-04-10 01:14:35,830 root         INFO     Train Epoch: 129 [6144/8000 (77%)]\tTotal Loss: 0.160565\n",
      "Reconstruction: 0.155018, Regularization: 0.005547\n",
      "2019-04-10 01:14:35,886 root         INFO     Train Epoch: 129 [6656/8000 (83%)]\tTotal Loss: 0.137546\n",
      "Reconstruction: 0.133450, Regularization: 0.004096\n",
      "2019-04-10 01:14:35,941 root         INFO     Train Epoch: 129 [7168/8000 (90%)]\tTotal Loss: 0.148703\n",
      "Reconstruction: 0.145305, Regularization: 0.003399\n",
      "2019-04-10 01:14:35,996 root         INFO     Train Epoch: 129 [7680/8000 (96%)]\tTotal Loss: 0.147945\n",
      "Reconstruction: 0.143927, Regularization: 0.004018\n",
      "2019-04-10 01:14:36,045 root         INFO     ====> Epoch: 129 Average loss: 0.1498\n",
      "2019-04-10 01:14:36,068 root         INFO     Train Epoch: 130 [0/8000 (0%)]\tTotal Loss: 0.139781\n",
      "Reconstruction: 0.136115, Regularization: 0.003667\n",
      "2019-04-10 01:14:36,125 root         INFO     Train Epoch: 130 [512/8000 (6%)]\tTotal Loss: 0.164789\n",
      "Reconstruction: 0.156074, Regularization: 0.008714\n",
      "2019-04-10 01:14:36,180 root         INFO     Train Epoch: 130 [1024/8000 (13%)]\tTotal Loss: 0.140619\n",
      "Reconstruction: 0.135415, Regularization: 0.005204\n",
      "2019-04-10 01:14:36,236 root         INFO     Train Epoch: 130 [1536/8000 (19%)]\tTotal Loss: 0.151205\n",
      "Reconstruction: 0.145218, Regularization: 0.005987\n",
      "2019-04-10 01:14:36,292 root         INFO     Train Epoch: 130 [2048/8000 (26%)]\tTotal Loss: 0.165023\n",
      "Reconstruction: 0.155814, Regularization: 0.009209\n",
      "2019-04-10 01:14:36,348 root         INFO     Train Epoch: 130 [2560/8000 (32%)]\tTotal Loss: 0.141513\n",
      "Reconstruction: 0.137551, Regularization: 0.003962\n",
      "2019-04-10 01:14:36,403 root         INFO     Train Epoch: 130 [3072/8000 (38%)]\tTotal Loss: 0.147346\n",
      "Reconstruction: 0.141739, Regularization: 0.005606\n",
      "2019-04-10 01:14:36,459 root         INFO     Train Epoch: 130 [3584/8000 (45%)]\tTotal Loss: 0.141620\n",
      "Reconstruction: 0.136987, Regularization: 0.004633\n",
      "2019-04-10 01:14:36,514 root         INFO     Train Epoch: 130 [4096/8000 (51%)]\tTotal Loss: 0.143050\n",
      "Reconstruction: 0.138738, Regularization: 0.004312\n",
      "2019-04-10 01:14:36,571 root         INFO     Train Epoch: 130 [4608/8000 (58%)]\tTotal Loss: 0.146364\n",
      "Reconstruction: 0.141767, Regularization: 0.004597\n",
      "2019-04-10 01:14:36,627 root         INFO     Train Epoch: 130 [5120/8000 (64%)]\tTotal Loss: 0.153478\n",
      "Reconstruction: 0.147043, Regularization: 0.006435\n",
      "2019-04-10 01:14:36,683 root         INFO     Train Epoch: 130 [5632/8000 (70%)]\tTotal Loss: 0.157457\n",
      "Reconstruction: 0.149890, Regularization: 0.007567\n",
      "2019-04-10 01:14:36,739 root         INFO     Train Epoch: 130 [6144/8000 (77%)]\tTotal Loss: 0.152861\n",
      "Reconstruction: 0.145751, Regularization: 0.007110\n",
      "2019-04-10 01:14:36,795 root         INFO     Train Epoch: 130 [6656/8000 (83%)]\tTotal Loss: 0.163210\n",
      "Reconstruction: 0.156186, Regularization: 0.007023\n",
      "2019-04-10 01:14:36,851 root         INFO     Train Epoch: 130 [7168/8000 (90%)]\tTotal Loss: 0.161220\n",
      "Reconstruction: 0.150293, Regularization: 0.010928\n",
      "2019-04-10 01:14:36,907 root         INFO     Train Epoch: 130 [7680/8000 (96%)]\tTotal Loss: 0.151544\n",
      "Reconstruction: 0.147402, Regularization: 0.004142\n",
      "2019-04-10 01:14:36,957 root         INFO     ====> Epoch: 130 Average loss: 0.1500\n",
      "2019-04-10 01:14:36,980 root         INFO     Train Epoch: 131 [0/8000 (0%)]\tTotal Loss: 0.157502\n",
      "Reconstruction: 0.147126, Regularization: 0.010376\n",
      "2019-04-10 01:14:37,037 root         INFO     Train Epoch: 131 [512/8000 (6%)]\tTotal Loss: 0.155399\n",
      "Reconstruction: 0.148605, Regularization: 0.006794\n",
      "2019-04-10 01:14:37,094 root         INFO     Train Epoch: 131 [1024/8000 (13%)]\tTotal Loss: 0.155518\n",
      "Reconstruction: 0.148018, Regularization: 0.007500\n",
      "2019-04-10 01:14:37,151 root         INFO     Train Epoch: 131 [1536/8000 (19%)]\tTotal Loss: 0.149498\n",
      "Reconstruction: 0.144580, Regularization: 0.004918\n",
      "2019-04-10 01:14:37,207 root         INFO     Train Epoch: 131 [2048/8000 (26%)]\tTotal Loss: 0.169470\n",
      "Reconstruction: 0.162902, Regularization: 0.006568\n",
      "2019-04-10 01:14:37,264 root         INFO     Train Epoch: 131 [2560/8000 (32%)]\tTotal Loss: 0.164155\n",
      "Reconstruction: 0.158733, Regularization: 0.005423\n",
      "2019-04-10 01:14:37,321 root         INFO     Train Epoch: 131 [3072/8000 (38%)]\tTotal Loss: 0.157945\n",
      "Reconstruction: 0.151743, Regularization: 0.006202\n",
      "2019-04-10 01:14:37,378 root         INFO     Train Epoch: 131 [3584/8000 (45%)]\tTotal Loss: 0.180020\n",
      "Reconstruction: 0.170263, Regularization: 0.009757\n",
      "2019-04-10 01:14:37,435 root         INFO     Train Epoch: 131 [4096/8000 (51%)]\tTotal Loss: 0.149595\n",
      "Reconstruction: 0.144961, Regularization: 0.004634\n",
      "2019-04-10 01:14:37,492 root         INFO     Train Epoch: 131 [4608/8000 (58%)]\tTotal Loss: 0.159160\n",
      "Reconstruction: 0.154245, Regularization: 0.004915\n",
      "2019-04-10 01:14:37,548 root         INFO     Train Epoch: 131 [5120/8000 (64%)]\tTotal Loss: 0.146334\n",
      "Reconstruction: 0.142444, Regularization: 0.003890\n",
      "2019-04-10 01:14:37,605 root         INFO     Train Epoch: 131 [5632/8000 (70%)]\tTotal Loss: 0.141601\n",
      "Reconstruction: 0.137446, Regularization: 0.004156\n",
      "2019-04-10 01:14:37,662 root         INFO     Train Epoch: 131 [6144/8000 (77%)]\tTotal Loss: 0.151700\n",
      "Reconstruction: 0.144473, Regularization: 0.007227\n",
      "2019-04-10 01:14:37,719 root         INFO     Train Epoch: 131 [6656/8000 (83%)]\tTotal Loss: 0.141854\n",
      "Reconstruction: 0.138189, Regularization: 0.003665\n",
      "2019-04-10 01:14:37,776 root         INFO     Train Epoch: 131 [7168/8000 (90%)]\tTotal Loss: 0.177262\n",
      "Reconstruction: 0.166175, Regularization: 0.011087\n",
      "2019-04-10 01:14:37,833 root         INFO     Train Epoch: 131 [7680/8000 (96%)]\tTotal Loss: 0.143117\n",
      "Reconstruction: 0.137358, Regularization: 0.005759\n",
      "2019-04-10 01:14:37,884 root         INFO     ====> Epoch: 131 Average loss: 0.1496\n",
      "2019-04-10 01:14:37,907 root         INFO     Train Epoch: 132 [0/8000 (0%)]\tTotal Loss: 0.161922\n",
      "Reconstruction: 0.156120, Regularization: 0.005802\n",
      "2019-04-10 01:14:37,964 root         INFO     Train Epoch: 132 [512/8000 (6%)]\tTotal Loss: 0.152498\n",
      "Reconstruction: 0.144552, Regularization: 0.007946\n",
      "2019-04-10 01:14:38,020 root         INFO     Train Epoch: 132 [1024/8000 (13%)]\tTotal Loss: 0.141532\n",
      "Reconstruction: 0.135698, Regularization: 0.005834\n",
      "2019-04-10 01:14:38,076 root         INFO     Train Epoch: 132 [1536/8000 (19%)]\tTotal Loss: 0.146121\n",
      "Reconstruction: 0.140808, Regularization: 0.005313\n",
      "2019-04-10 01:14:38,131 root         INFO     Train Epoch: 132 [2048/8000 (26%)]\tTotal Loss: 0.156681\n",
      "Reconstruction: 0.150452, Regularization: 0.006228\n",
      "2019-04-10 01:14:38,186 root         INFO     Train Epoch: 132 [2560/8000 (32%)]\tTotal Loss: 0.139439\n",
      "Reconstruction: 0.136020, Regularization: 0.003419\n",
      "2019-04-10 01:14:38,241 root         INFO     Train Epoch: 132 [3072/8000 (38%)]\tTotal Loss: 0.151418\n",
      "Reconstruction: 0.147008, Regularization: 0.004410\n",
      "2019-04-10 01:14:38,296 root         INFO     Train Epoch: 132 [3584/8000 (45%)]\tTotal Loss: 0.151801\n",
      "Reconstruction: 0.146256, Regularization: 0.005545\n",
      "2019-04-10 01:14:38,351 root         INFO     Train Epoch: 132 [4096/8000 (51%)]\tTotal Loss: 0.139690\n",
      "Reconstruction: 0.134459, Regularization: 0.005231\n",
      "2019-04-10 01:14:38,407 root         INFO     Train Epoch: 132 [4608/8000 (58%)]\tTotal Loss: 0.139916\n",
      "Reconstruction: 0.135825, Regularization: 0.004091\n",
      "2019-04-10 01:14:38,462 root         INFO     Train Epoch: 132 [5120/8000 (64%)]\tTotal Loss: 0.150751\n",
      "Reconstruction: 0.142761, Regularization: 0.007990\n",
      "2019-04-10 01:14:38,517 root         INFO     Train Epoch: 132 [5632/8000 (70%)]\tTotal Loss: 0.146997\n",
      "Reconstruction: 0.142412, Regularization: 0.004585\n",
      "2019-04-10 01:14:38,572 root         INFO     Train Epoch: 132 [6144/8000 (77%)]\tTotal Loss: 0.136288\n",
      "Reconstruction: 0.132341, Regularization: 0.003947\n",
      "2019-04-10 01:14:38,627 root         INFO     Train Epoch: 132 [6656/8000 (83%)]\tTotal Loss: 0.144315\n",
      "Reconstruction: 0.139326, Regularization: 0.004989\n",
      "2019-04-10 01:14:38,683 root         INFO     Train Epoch: 132 [7168/8000 (90%)]\tTotal Loss: 0.156037\n",
      "Reconstruction: 0.149816, Regularization: 0.006221\n",
      "2019-04-10 01:14:38,738 root         INFO     Train Epoch: 132 [7680/8000 (96%)]\tTotal Loss: 0.150083\n",
      "Reconstruction: 0.145569, Regularization: 0.004514\n",
      "2019-04-10 01:14:38,787 root         INFO     ====> Epoch: 132 Average loss: 0.1500\n",
      "2019-04-10 01:14:38,810 root         INFO     Train Epoch: 133 [0/8000 (0%)]\tTotal Loss: 0.146654\n",
      "Reconstruction: 0.143071, Regularization: 0.003583\n",
      "2019-04-10 01:14:38,868 root         INFO     Train Epoch: 133 [512/8000 (6%)]\tTotal Loss: 0.162427\n",
      "Reconstruction: 0.154463, Regularization: 0.007963\n",
      "2019-04-10 01:14:38,924 root         INFO     Train Epoch: 133 [1024/8000 (13%)]\tTotal Loss: 0.149283\n",
      "Reconstruction: 0.144826, Regularization: 0.004457\n",
      "2019-04-10 01:14:38,979 root         INFO     Train Epoch: 133 [1536/8000 (19%)]\tTotal Loss: 0.155038\n",
      "Reconstruction: 0.148956, Regularization: 0.006081\n",
      "2019-04-10 01:14:39,034 root         INFO     Train Epoch: 133 [2048/8000 (26%)]\tTotal Loss: 0.156914\n",
      "Reconstruction: 0.149023, Regularization: 0.007892\n",
      "2019-04-10 01:14:39,090 root         INFO     Train Epoch: 133 [2560/8000 (32%)]\tTotal Loss: 0.141132\n",
      "Reconstruction: 0.137433, Regularization: 0.003699\n",
      "2019-04-10 01:14:39,145 root         INFO     Train Epoch: 133 [3072/8000 (38%)]\tTotal Loss: 0.150087\n",
      "Reconstruction: 0.145572, Regularization: 0.004515\n",
      "2019-04-10 01:14:39,200 root         INFO     Train Epoch: 133 [3584/8000 (45%)]\tTotal Loss: 0.157606\n",
      "Reconstruction: 0.149255, Regularization: 0.008351\n",
      "2019-04-10 01:14:39,256 root         INFO     Train Epoch: 133 [4096/8000 (51%)]\tTotal Loss: 0.146878\n",
      "Reconstruction: 0.142207, Regularization: 0.004671\n",
      "2019-04-10 01:14:39,311 root         INFO     Train Epoch: 133 [4608/8000 (58%)]\tTotal Loss: 0.148612\n",
      "Reconstruction: 0.142843, Regularization: 0.005769\n",
      "2019-04-10 01:14:39,366 root         INFO     Train Epoch: 133 [5120/8000 (64%)]\tTotal Loss: 0.158284\n",
      "Reconstruction: 0.150825, Regularization: 0.007458\n",
      "2019-04-10 01:14:39,422 root         INFO     Train Epoch: 133 [5632/8000 (70%)]\tTotal Loss: 0.161042\n",
      "Reconstruction: 0.153509, Regularization: 0.007532\n",
      "2019-04-10 01:14:39,477 root         INFO     Train Epoch: 133 [6144/8000 (77%)]\tTotal Loss: 0.159811\n",
      "Reconstruction: 0.154822, Regularization: 0.004988\n",
      "2019-04-10 01:14:39,532 root         INFO     Train Epoch: 133 [6656/8000 (83%)]\tTotal Loss: 0.139116\n",
      "Reconstruction: 0.134484, Regularization: 0.004632\n",
      "2019-04-10 01:14:39,588 root         INFO     Train Epoch: 133 [7168/8000 (90%)]\tTotal Loss: 0.147965\n",
      "Reconstruction: 0.143761, Regularization: 0.004205\n",
      "2019-04-10 01:14:39,643 root         INFO     Train Epoch: 133 [7680/8000 (96%)]\tTotal Loss: 0.138623\n",
      "Reconstruction: 0.132437, Regularization: 0.006186\n",
      "2019-04-10 01:14:39,692 root         INFO     ====> Epoch: 133 Average loss: 0.1502\n",
      "2019-04-10 01:14:39,716 root         INFO     Train Epoch: 134 [0/8000 (0%)]\tTotal Loss: 0.152199\n",
      "Reconstruction: 0.144254, Regularization: 0.007945\n",
      "2019-04-10 01:14:39,772 root         INFO     Train Epoch: 134 [512/8000 (6%)]\tTotal Loss: 0.148648\n",
      "Reconstruction: 0.141888, Regularization: 0.006760\n",
      "2019-04-10 01:14:39,828 root         INFO     Train Epoch: 134 [1024/8000 (13%)]\tTotal Loss: 0.145436\n",
      "Reconstruction: 0.141175, Regularization: 0.004261\n",
      "2019-04-10 01:14:39,884 root         INFO     Train Epoch: 134 [1536/8000 (19%)]\tTotal Loss: 0.146001\n",
      "Reconstruction: 0.140193, Regularization: 0.005808\n",
      "2019-04-10 01:14:39,940 root         INFO     Train Epoch: 134 [2048/8000 (26%)]\tTotal Loss: 0.153972\n",
      "Reconstruction: 0.150149, Regularization: 0.003823\n",
      "2019-04-10 01:14:39,996 root         INFO     Train Epoch: 134 [2560/8000 (32%)]\tTotal Loss: 0.147049\n",
      "Reconstruction: 0.141313, Regularization: 0.005735\n",
      "2019-04-10 01:14:40,052 root         INFO     Train Epoch: 134 [3072/8000 (38%)]\tTotal Loss: 0.159494\n",
      "Reconstruction: 0.152130, Regularization: 0.007363\n",
      "2019-04-10 01:14:40,107 root         INFO     Train Epoch: 134 [3584/8000 (45%)]\tTotal Loss: 0.143028\n",
      "Reconstruction: 0.138533, Regularization: 0.004495\n",
      "2019-04-10 01:14:40,162 root         INFO     Train Epoch: 134 [4096/8000 (51%)]\tTotal Loss: 0.146632\n",
      "Reconstruction: 0.140210, Regularization: 0.006422\n",
      "2019-04-10 01:14:40,217 root         INFO     Train Epoch: 134 [4608/8000 (58%)]\tTotal Loss: 0.146308\n",
      "Reconstruction: 0.142043, Regularization: 0.004265\n",
      "2019-04-10 01:14:40,271 root         INFO     Train Epoch: 134 [5120/8000 (64%)]\tTotal Loss: 0.148280\n",
      "Reconstruction: 0.143572, Regularization: 0.004708\n",
      "2019-04-10 01:14:40,327 root         INFO     Train Epoch: 134 [5632/8000 (70%)]\tTotal Loss: 0.167834\n",
      "Reconstruction: 0.160696, Regularization: 0.007138\n",
      "2019-04-10 01:14:40,382 root         INFO     Train Epoch: 134 [6144/8000 (77%)]\tTotal Loss: 0.152936\n",
      "Reconstruction: 0.148232, Regularization: 0.004704\n",
      "2019-04-10 01:14:40,437 root         INFO     Train Epoch: 134 [6656/8000 (83%)]\tTotal Loss: 0.136288\n",
      "Reconstruction: 0.131131, Regularization: 0.005157\n",
      "2019-04-10 01:14:40,492 root         INFO     Train Epoch: 134 [7168/8000 (90%)]\tTotal Loss: 0.151511\n",
      "Reconstruction: 0.142898, Regularization: 0.008613\n",
      "2019-04-10 01:14:40,547 root         INFO     Train Epoch: 134 [7680/8000 (96%)]\tTotal Loss: 0.154203\n",
      "Reconstruction: 0.147731, Regularization: 0.006471\n",
      "2019-04-10 01:14:40,596 root         INFO     ====> Epoch: 134 Average loss: 0.1502\n",
      "2019-04-10 01:14:40,620 root         INFO     Train Epoch: 135 [0/8000 (0%)]\tTotal Loss: 0.149210\n",
      "Reconstruction: 0.143488, Regularization: 0.005722\n",
      "2019-04-10 01:14:40,675 root         INFO     Train Epoch: 135 [512/8000 (6%)]\tTotal Loss: 0.146312\n",
      "Reconstruction: 0.141368, Regularization: 0.004944\n",
      "2019-04-10 01:14:40,730 root         INFO     Train Epoch: 135 [1024/8000 (13%)]\tTotal Loss: 0.146514\n",
      "Reconstruction: 0.141732, Regularization: 0.004782\n",
      "2019-04-10 01:14:40,786 root         INFO     Train Epoch: 135 [1536/8000 (19%)]\tTotal Loss: 0.141707\n",
      "Reconstruction: 0.137672, Regularization: 0.004036\n",
      "2019-04-10 01:14:40,841 root         INFO     Train Epoch: 135 [2048/8000 (26%)]\tTotal Loss: 0.144220\n",
      "Reconstruction: 0.139617, Regularization: 0.004603\n",
      "2019-04-10 01:14:40,895 root         INFO     Train Epoch: 135 [2560/8000 (32%)]\tTotal Loss: 0.152451\n",
      "Reconstruction: 0.145983, Regularization: 0.006468\n",
      "2019-04-10 01:14:40,950 root         INFO     Train Epoch: 135 [3072/8000 (38%)]\tTotal Loss: 0.153508\n",
      "Reconstruction: 0.148594, Regularization: 0.004914\n",
      "2019-04-10 01:14:41,005 root         INFO     Train Epoch: 135 [3584/8000 (45%)]\tTotal Loss: 0.160668\n",
      "Reconstruction: 0.151947, Regularization: 0.008722\n",
      "2019-04-10 01:14:41,059 root         INFO     Train Epoch: 135 [4096/8000 (51%)]\tTotal Loss: 0.142460\n",
      "Reconstruction: 0.137845, Regularization: 0.004615\n",
      "2019-04-10 01:14:41,113 root         INFO     Train Epoch: 135 [4608/8000 (58%)]\tTotal Loss: 0.156562\n",
      "Reconstruction: 0.149449, Regularization: 0.007113\n",
      "2019-04-10 01:14:41,167 root         INFO     Train Epoch: 135 [5120/8000 (64%)]\tTotal Loss: 0.152534\n",
      "Reconstruction: 0.148475, Regularization: 0.004058\n",
      "2019-04-10 01:14:41,222 root         INFO     Train Epoch: 135 [5632/8000 (70%)]\tTotal Loss: 0.149424\n",
      "Reconstruction: 0.143458, Regularization: 0.005966\n",
      "2019-04-10 01:14:41,276 root         INFO     Train Epoch: 135 [6144/8000 (77%)]\tTotal Loss: 0.170631\n",
      "Reconstruction: 0.163776, Regularization: 0.006854\n",
      "2019-04-10 01:14:41,330 root         INFO     Train Epoch: 135 [6656/8000 (83%)]\tTotal Loss: 0.147401\n",
      "Reconstruction: 0.139805, Regularization: 0.007596\n",
      "2019-04-10 01:14:41,385 root         INFO     Train Epoch: 135 [7168/8000 (90%)]\tTotal Loss: 0.172794\n",
      "Reconstruction: 0.163510, Regularization: 0.009284\n",
      "2019-04-10 01:14:41,439 root         INFO     Train Epoch: 135 [7680/8000 (96%)]\tTotal Loss: 0.151112\n",
      "Reconstruction: 0.144321, Regularization: 0.006791\n",
      "2019-04-10 01:14:41,488 root         INFO     ====> Epoch: 135 Average loss: 0.1503\n",
      "2019-04-10 01:14:41,512 root         INFO     Train Epoch: 136 [0/8000 (0%)]\tTotal Loss: 0.151437\n",
      "Reconstruction: 0.145998, Regularization: 0.005439\n",
      "2019-04-10 01:14:41,569 root         INFO     Train Epoch: 136 [512/8000 (6%)]\tTotal Loss: 0.140896\n",
      "Reconstruction: 0.135986, Regularization: 0.004910\n",
      "2019-04-10 01:14:41,626 root         INFO     Train Epoch: 136 [1024/8000 (13%)]\tTotal Loss: 0.151053\n",
      "Reconstruction: 0.145426, Regularization: 0.005628\n",
      "2019-04-10 01:14:41,683 root         INFO     Train Epoch: 136 [1536/8000 (19%)]\tTotal Loss: 0.137816\n",
      "Reconstruction: 0.134284, Regularization: 0.003532\n",
      "2019-04-10 01:14:41,739 root         INFO     Train Epoch: 136 [2048/8000 (26%)]\tTotal Loss: 0.139434\n",
      "Reconstruction: 0.134663, Regularization: 0.004771\n",
      "2019-04-10 01:14:41,796 root         INFO     Train Epoch: 136 [2560/8000 (32%)]\tTotal Loss: 0.149143\n",
      "Reconstruction: 0.141097, Regularization: 0.008045\n",
      "2019-04-10 01:14:41,853 root         INFO     Train Epoch: 136 [3072/8000 (38%)]\tTotal Loss: 0.143141\n",
      "Reconstruction: 0.139635, Regularization: 0.003505\n",
      "2019-04-10 01:14:41,909 root         INFO     Train Epoch: 136 [3584/8000 (45%)]\tTotal Loss: 0.148185\n",
      "Reconstruction: 0.144058, Regularization: 0.004127\n",
      "2019-04-10 01:14:41,966 root         INFO     Train Epoch: 136 [4096/8000 (51%)]\tTotal Loss: 0.155822\n",
      "Reconstruction: 0.149070, Regularization: 0.006752\n",
      "2019-04-10 01:14:42,023 root         INFO     Train Epoch: 136 [4608/8000 (58%)]\tTotal Loss: 0.144205\n",
      "Reconstruction: 0.138941, Regularization: 0.005263\n",
      "2019-04-10 01:14:42,079 root         INFO     Train Epoch: 136 [5120/8000 (64%)]\tTotal Loss: 0.143885\n",
      "Reconstruction: 0.139376, Regularization: 0.004509\n",
      "2019-04-10 01:14:42,136 root         INFO     Train Epoch: 136 [5632/8000 (70%)]\tTotal Loss: 0.154310\n",
      "Reconstruction: 0.146038, Regularization: 0.008271\n",
      "2019-04-10 01:14:42,193 root         INFO     Train Epoch: 136 [6144/8000 (77%)]\tTotal Loss: 0.136147\n",
      "Reconstruction: 0.131706, Regularization: 0.004441\n",
      "2019-04-10 01:14:42,249 root         INFO     Train Epoch: 136 [6656/8000 (83%)]\tTotal Loss: 0.142536\n",
      "Reconstruction: 0.136056, Regularization: 0.006480\n",
      "2019-04-10 01:14:42,306 root         INFO     Train Epoch: 136 [7168/8000 (90%)]\tTotal Loss: 0.156533\n",
      "Reconstruction: 0.146452, Regularization: 0.010080\n",
      "2019-04-10 01:14:42,363 root         INFO     Train Epoch: 136 [7680/8000 (96%)]\tTotal Loss: 0.142852\n",
      "Reconstruction: 0.136794, Regularization: 0.006058\n",
      "2019-04-10 01:14:42,413 root         INFO     ====> Epoch: 136 Average loss: 0.1495\n",
      "2019-04-10 01:14:42,437 root         INFO     Train Epoch: 137 [0/8000 (0%)]\tTotal Loss: 0.145661\n",
      "Reconstruction: 0.141462, Regularization: 0.004199\n",
      "2019-04-10 01:14:42,494 root         INFO     Train Epoch: 137 [512/8000 (6%)]\tTotal Loss: 0.146933\n",
      "Reconstruction: 0.141699, Regularization: 0.005234\n",
      "2019-04-10 01:14:42,550 root         INFO     Train Epoch: 137 [1024/8000 (13%)]\tTotal Loss: 0.153942\n",
      "Reconstruction: 0.148629, Regularization: 0.005313\n",
      "2019-04-10 01:14:42,606 root         INFO     Train Epoch: 137 [1536/8000 (19%)]\tTotal Loss: 0.150705\n",
      "Reconstruction: 0.143577, Regularization: 0.007128\n",
      "2019-04-10 01:14:42,664 root         INFO     Train Epoch: 137 [2048/8000 (26%)]\tTotal Loss: 0.144043\n",
      "Reconstruction: 0.137513, Regularization: 0.006530\n",
      "2019-04-10 01:14:42,721 root         INFO     Train Epoch: 137 [2560/8000 (32%)]\tTotal Loss: 0.155859\n",
      "Reconstruction: 0.150742, Regularization: 0.005117\n",
      "2019-04-10 01:14:42,778 root         INFO     Train Epoch: 137 [3072/8000 (38%)]\tTotal Loss: 0.151316\n",
      "Reconstruction: 0.144371, Regularization: 0.006946\n",
      "2019-04-10 01:14:42,836 root         INFO     Train Epoch: 137 [3584/8000 (45%)]\tTotal Loss: 0.137146\n",
      "Reconstruction: 0.132477, Regularization: 0.004669\n",
      "2019-04-10 01:14:42,893 root         INFO     Train Epoch: 137 [4096/8000 (51%)]\tTotal Loss: 0.141803\n",
      "Reconstruction: 0.138629, Regularization: 0.003174\n",
      "2019-04-10 01:14:42,951 root         INFO     Train Epoch: 137 [4608/8000 (58%)]\tTotal Loss: 0.165689\n",
      "Reconstruction: 0.156303, Regularization: 0.009386\n",
      "2019-04-10 01:14:43,009 root         INFO     Train Epoch: 137 [5120/8000 (64%)]\tTotal Loss: 0.147772\n",
      "Reconstruction: 0.141373, Regularization: 0.006399\n",
      "2019-04-10 01:14:43,066 root         INFO     Train Epoch: 137 [5632/8000 (70%)]\tTotal Loss: 0.141238\n",
      "Reconstruction: 0.136615, Regularization: 0.004623\n",
      "2019-04-10 01:14:43,124 root         INFO     Train Epoch: 137 [6144/8000 (77%)]\tTotal Loss: 0.134551\n",
      "Reconstruction: 0.130990, Regularization: 0.003561\n",
      "2019-04-10 01:14:43,181 root         INFO     Train Epoch: 137 [6656/8000 (83%)]\tTotal Loss: 0.154801\n",
      "Reconstruction: 0.147403, Regularization: 0.007399\n",
      "2019-04-10 01:14:43,238 root         INFO     Train Epoch: 137 [7168/8000 (90%)]\tTotal Loss: 0.143109\n",
      "Reconstruction: 0.138817, Regularization: 0.004293\n",
      "2019-04-10 01:14:43,296 root         INFO     Train Epoch: 137 [7680/8000 (96%)]\tTotal Loss: 0.167993\n",
      "Reconstruction: 0.163077, Regularization: 0.004916\n",
      "2019-04-10 01:14:43,346 root         INFO     ====> Epoch: 137 Average loss: 0.1495\n",
      "2019-04-10 01:14:43,370 root         INFO     Train Epoch: 138 [0/8000 (0%)]\tTotal Loss: 0.149696\n",
      "Reconstruction: 0.144848, Regularization: 0.004848\n",
      "2019-04-10 01:14:43,427 root         INFO     Train Epoch: 138 [512/8000 (6%)]\tTotal Loss: 0.145293\n",
      "Reconstruction: 0.139247, Regularization: 0.006046\n",
      "2019-04-10 01:14:43,485 root         INFO     Train Epoch: 138 [1024/8000 (13%)]\tTotal Loss: 0.157020\n",
      "Reconstruction: 0.149566, Regularization: 0.007454\n",
      "2019-04-10 01:14:43,542 root         INFO     Train Epoch: 138 [1536/8000 (19%)]\tTotal Loss: 0.145010\n",
      "Reconstruction: 0.140131, Regularization: 0.004878\n",
      "2019-04-10 01:14:43,599 root         INFO     Train Epoch: 138 [2048/8000 (26%)]\tTotal Loss: 0.138872\n",
      "Reconstruction: 0.134779, Regularization: 0.004093\n",
      "2019-04-10 01:14:43,656 root         INFO     Train Epoch: 138 [2560/8000 (32%)]\tTotal Loss: 0.142124\n",
      "Reconstruction: 0.138608, Regularization: 0.003517\n",
      "2019-04-10 01:14:43,713 root         INFO     Train Epoch: 138 [3072/8000 (38%)]\tTotal Loss: 0.155114\n",
      "Reconstruction: 0.144468, Regularization: 0.010646\n",
      "2019-04-10 01:14:43,770 root         INFO     Train Epoch: 138 [3584/8000 (45%)]\tTotal Loss: 0.146638\n",
      "Reconstruction: 0.141592, Regularization: 0.005046\n",
      "2019-04-10 01:14:43,827 root         INFO     Train Epoch: 138 [4096/8000 (51%)]\tTotal Loss: 0.147379\n",
      "Reconstruction: 0.141025, Regularization: 0.006353\n",
      "2019-04-10 01:14:43,884 root         INFO     Train Epoch: 138 [4608/8000 (58%)]\tTotal Loss: 0.144033\n",
      "Reconstruction: 0.139219, Regularization: 0.004814\n",
      "2019-04-10 01:14:43,941 root         INFO     Train Epoch: 138 [5120/8000 (64%)]\tTotal Loss: 0.152327\n",
      "Reconstruction: 0.148044, Regularization: 0.004284\n",
      "2019-04-10 01:14:43,997 root         INFO     Train Epoch: 138 [5632/8000 (70%)]\tTotal Loss: 0.151189\n",
      "Reconstruction: 0.147816, Regularization: 0.003373\n",
      "2019-04-10 01:14:44,053 root         INFO     Train Epoch: 138 [6144/8000 (77%)]\tTotal Loss: 0.145081\n",
      "Reconstruction: 0.141831, Regularization: 0.003250\n",
      "2019-04-10 01:14:44,110 root         INFO     Train Epoch: 138 [6656/8000 (83%)]\tTotal Loss: 0.144709\n",
      "Reconstruction: 0.139219, Regularization: 0.005489\n",
      "2019-04-10 01:14:44,167 root         INFO     Train Epoch: 138 [7168/8000 (90%)]\tTotal Loss: 0.155098\n",
      "Reconstruction: 0.146270, Regularization: 0.008828\n",
      "2019-04-10 01:14:44,223 root         INFO     Train Epoch: 138 [7680/8000 (96%)]\tTotal Loss: 0.150359\n",
      "Reconstruction: 0.146087, Regularization: 0.004271\n",
      "2019-04-10 01:14:44,273 root         INFO     ====> Epoch: 138 Average loss: 0.1499\n",
      "2019-04-10 01:14:44,297 root         INFO     Train Epoch: 139 [0/8000 (0%)]\tTotal Loss: 0.149860\n",
      "Reconstruction: 0.146023, Regularization: 0.003838\n",
      "2019-04-10 01:14:44,354 root         INFO     Train Epoch: 139 [512/8000 (6%)]\tTotal Loss: 0.152093\n",
      "Reconstruction: 0.147813, Regularization: 0.004280\n",
      "2019-04-10 01:14:44,411 root         INFO     Train Epoch: 139 [1024/8000 (13%)]\tTotal Loss: 0.147524\n",
      "Reconstruction: 0.141876, Regularization: 0.005648\n",
      "2019-04-10 01:14:44,466 root         INFO     Train Epoch: 139 [1536/8000 (19%)]\tTotal Loss: 0.137680\n",
      "Reconstruction: 0.132831, Regularization: 0.004849\n",
      "2019-04-10 01:14:44,521 root         INFO     Train Epoch: 139 [2048/8000 (26%)]\tTotal Loss: 0.155295\n",
      "Reconstruction: 0.150572, Regularization: 0.004724\n",
      "2019-04-10 01:14:44,576 root         INFO     Train Epoch: 139 [2560/8000 (32%)]\tTotal Loss: 0.154076\n",
      "Reconstruction: 0.147330, Regularization: 0.006746\n",
      "2019-04-10 01:14:44,631 root         INFO     Train Epoch: 139 [3072/8000 (38%)]\tTotal Loss: 0.149753\n",
      "Reconstruction: 0.144275, Regularization: 0.005478\n",
      "2019-04-10 01:14:44,686 root         INFO     Train Epoch: 139 [3584/8000 (45%)]\tTotal Loss: 0.140899\n",
      "Reconstruction: 0.136199, Regularization: 0.004700\n",
      "2019-04-10 01:14:44,741 root         INFO     Train Epoch: 139 [4096/8000 (51%)]\tTotal Loss: 0.141773\n",
      "Reconstruction: 0.137870, Regularization: 0.003903\n",
      "2019-04-10 01:14:44,797 root         INFO     Train Epoch: 139 [4608/8000 (58%)]\tTotal Loss: 0.141881\n",
      "Reconstruction: 0.136830, Regularization: 0.005051\n",
      "2019-04-10 01:14:44,851 root         INFO     Train Epoch: 139 [5120/8000 (64%)]\tTotal Loss: 0.147321\n",
      "Reconstruction: 0.141258, Regularization: 0.006064\n",
      "2019-04-10 01:14:44,906 root         INFO     Train Epoch: 139 [5632/8000 (70%)]\tTotal Loss: 0.140995\n",
      "Reconstruction: 0.136009, Regularization: 0.004987\n",
      "2019-04-10 01:14:44,962 root         INFO     Train Epoch: 139 [6144/8000 (77%)]\tTotal Loss: 0.168000\n",
      "Reconstruction: 0.161554, Regularization: 0.006446\n",
      "2019-04-10 01:14:45,018 root         INFO     Train Epoch: 139 [6656/8000 (83%)]\tTotal Loss: 0.148173\n",
      "Reconstruction: 0.142902, Regularization: 0.005270\n",
      "2019-04-10 01:14:45,073 root         INFO     Train Epoch: 139 [7168/8000 (90%)]\tTotal Loss: 0.152263\n",
      "Reconstruction: 0.146545, Regularization: 0.005718\n",
      "2019-04-10 01:14:45,128 root         INFO     Train Epoch: 139 [7680/8000 (96%)]\tTotal Loss: 0.150702\n",
      "Reconstruction: 0.143344, Regularization: 0.007359\n",
      "2019-04-10 01:14:45,178 root         INFO     ====> Epoch: 139 Average loss: 0.1497\n",
      "2019-04-10 01:14:45,202 root         INFO     Train Epoch: 140 [0/8000 (0%)]\tTotal Loss: 0.164209\n",
      "Reconstruction: 0.155944, Regularization: 0.008266\n",
      "2019-04-10 01:14:45,258 root         INFO     Train Epoch: 140 [512/8000 (6%)]\tTotal Loss: 0.153911\n",
      "Reconstruction: 0.147558, Regularization: 0.006353\n",
      "2019-04-10 01:14:45,313 root         INFO     Train Epoch: 140 [1024/8000 (13%)]\tTotal Loss: 0.156745\n",
      "Reconstruction: 0.146006, Regularization: 0.010738\n",
      "2019-04-10 01:14:45,368 root         INFO     Train Epoch: 140 [1536/8000 (19%)]\tTotal Loss: 0.137809\n",
      "Reconstruction: 0.133073, Regularization: 0.004736\n",
      "2019-04-10 01:14:45,424 root         INFO     Train Epoch: 140 [2048/8000 (26%)]\tTotal Loss: 0.148083\n",
      "Reconstruction: 0.144676, Regularization: 0.003407\n",
      "2019-04-10 01:14:45,479 root         INFO     Train Epoch: 140 [2560/8000 (32%)]\tTotal Loss: 0.146611\n",
      "Reconstruction: 0.139315, Regularization: 0.007296\n",
      "2019-04-10 01:14:45,535 root         INFO     Train Epoch: 140 [3072/8000 (38%)]\tTotal Loss: 0.158552\n",
      "Reconstruction: 0.152191, Regularization: 0.006361\n",
      "2019-04-10 01:14:45,590 root         INFO     Train Epoch: 140 [3584/8000 (45%)]\tTotal Loss: 0.142709\n",
      "Reconstruction: 0.136754, Regularization: 0.005955\n",
      "2019-04-10 01:14:45,646 root         INFO     Train Epoch: 140 [4096/8000 (51%)]\tTotal Loss: 0.145072\n",
      "Reconstruction: 0.138532, Regularization: 0.006540\n",
      "2019-04-10 01:14:45,702 root         INFO     Train Epoch: 140 [4608/8000 (58%)]\tTotal Loss: 0.144045\n",
      "Reconstruction: 0.139846, Regularization: 0.004199\n",
      "2019-04-10 01:14:45,757 root         INFO     Train Epoch: 140 [5120/8000 (64%)]\tTotal Loss: 0.137950\n",
      "Reconstruction: 0.133237, Regularization: 0.004713\n",
      "2019-04-10 01:14:45,813 root         INFO     Train Epoch: 140 [5632/8000 (70%)]\tTotal Loss: 0.146073\n",
      "Reconstruction: 0.141745, Regularization: 0.004328\n",
      "2019-04-10 01:14:45,869 root         INFO     Train Epoch: 140 [6144/8000 (77%)]\tTotal Loss: 0.154983\n",
      "Reconstruction: 0.149479, Regularization: 0.005503\n",
      "2019-04-10 01:14:45,924 root         INFO     Train Epoch: 140 [6656/8000 (83%)]\tTotal Loss: 0.148038\n",
      "Reconstruction: 0.145034, Regularization: 0.003005\n",
      "2019-04-10 01:14:45,980 root         INFO     Train Epoch: 140 [7168/8000 (90%)]\tTotal Loss: 0.146295\n",
      "Reconstruction: 0.142339, Regularization: 0.003956\n",
      "2019-04-10 01:14:46,036 root         INFO     Train Epoch: 140 [7680/8000 (96%)]\tTotal Loss: 0.155978\n",
      "Reconstruction: 0.150653, Regularization: 0.005324\n",
      "2019-04-10 01:14:46,085 root         INFO     ====> Epoch: 140 Average loss: 0.1495\n",
      "2019-04-10 01:14:46,108 root         INFO     Train Epoch: 141 [0/8000 (0%)]\tTotal Loss: 0.140170\n",
      "Reconstruction: 0.134253, Regularization: 0.005917\n",
      "2019-04-10 01:14:46,164 root         INFO     Train Epoch: 141 [512/8000 (6%)]\tTotal Loss: 0.136996\n",
      "Reconstruction: 0.131904, Regularization: 0.005092\n",
      "2019-04-10 01:14:46,219 root         INFO     Train Epoch: 141 [1024/8000 (13%)]\tTotal Loss: 0.141180\n",
      "Reconstruction: 0.133055, Regularization: 0.008124\n",
      "2019-04-10 01:14:46,275 root         INFO     Train Epoch: 141 [1536/8000 (19%)]\tTotal Loss: 0.148039\n",
      "Reconstruction: 0.143797, Regularization: 0.004242\n",
      "2019-04-10 01:14:46,331 root         INFO     Train Epoch: 141 [2048/8000 (26%)]\tTotal Loss: 0.169999\n",
      "Reconstruction: 0.163651, Regularization: 0.006348\n",
      "2019-04-10 01:14:46,388 root         INFO     Train Epoch: 141 [2560/8000 (32%)]\tTotal Loss: 0.162193\n",
      "Reconstruction: 0.156179, Regularization: 0.006014\n",
      "2019-04-10 01:14:46,443 root         INFO     Train Epoch: 141 [3072/8000 (38%)]\tTotal Loss: 0.154117\n",
      "Reconstruction: 0.148882, Regularization: 0.005235\n",
      "2019-04-10 01:14:46,499 root         INFO     Train Epoch: 141 [3584/8000 (45%)]\tTotal Loss: 0.163442\n",
      "Reconstruction: 0.157064, Regularization: 0.006378\n",
      "2019-04-10 01:14:46,555 root         INFO     Train Epoch: 141 [4096/8000 (51%)]\tTotal Loss: 0.143582\n",
      "Reconstruction: 0.140260, Regularization: 0.003322\n",
      "2019-04-10 01:14:46,611 root         INFO     Train Epoch: 141 [4608/8000 (58%)]\tTotal Loss: 0.138694\n",
      "Reconstruction: 0.135420, Regularization: 0.003274\n",
      "2019-04-10 01:14:46,668 root         INFO     Train Epoch: 141 [5120/8000 (64%)]\tTotal Loss: 0.149344\n",
      "Reconstruction: 0.141976, Regularization: 0.007367\n",
      "2019-04-10 01:14:46,724 root         INFO     Train Epoch: 141 [5632/8000 (70%)]\tTotal Loss: 0.153429\n",
      "Reconstruction: 0.149395, Regularization: 0.004034\n",
      "2019-04-10 01:14:46,780 root         INFO     Train Epoch: 141 [6144/8000 (77%)]\tTotal Loss: 0.164517\n",
      "Reconstruction: 0.157498, Regularization: 0.007019\n",
      "2019-04-10 01:14:46,836 root         INFO     Train Epoch: 141 [6656/8000 (83%)]\tTotal Loss: 0.137928\n",
      "Reconstruction: 0.133458, Regularization: 0.004470\n",
      "2019-04-10 01:14:46,892 root         INFO     Train Epoch: 141 [7168/8000 (90%)]\tTotal Loss: 0.151149\n",
      "Reconstruction: 0.146367, Regularization: 0.004782\n",
      "2019-04-10 01:14:46,949 root         INFO     Train Epoch: 141 [7680/8000 (96%)]\tTotal Loss: 0.161096\n",
      "Reconstruction: 0.151898, Regularization: 0.009198\n",
      "2019-04-10 01:14:47,000 root         INFO     ====> Epoch: 141 Average loss: 0.1500\n",
      "2019-04-10 01:14:47,023 root         INFO     Train Epoch: 142 [0/8000 (0%)]\tTotal Loss: 0.145702\n",
      "Reconstruction: 0.140273, Regularization: 0.005429\n",
      "2019-04-10 01:14:47,080 root         INFO     Train Epoch: 142 [512/8000 (6%)]\tTotal Loss: 0.142896\n",
      "Reconstruction: 0.139081, Regularization: 0.003815\n",
      "2019-04-10 01:14:47,136 root         INFO     Train Epoch: 142 [1024/8000 (13%)]\tTotal Loss: 0.143677\n",
      "Reconstruction: 0.137618, Regularization: 0.006059\n",
      "2019-04-10 01:14:47,193 root         INFO     Train Epoch: 142 [1536/8000 (19%)]\tTotal Loss: 0.146949\n",
      "Reconstruction: 0.142209, Regularization: 0.004740\n",
      "2019-04-10 01:14:47,250 root         INFO     Train Epoch: 142 [2048/8000 (26%)]\tTotal Loss: 0.147724\n",
      "Reconstruction: 0.141837, Regularization: 0.005887\n",
      "2019-04-10 01:14:47,306 root         INFO     Train Epoch: 142 [2560/8000 (32%)]\tTotal Loss: 0.144515\n",
      "Reconstruction: 0.139666, Regularization: 0.004849\n",
      "2019-04-10 01:14:47,363 root         INFO     Train Epoch: 142 [3072/8000 (38%)]\tTotal Loss: 0.141066\n",
      "Reconstruction: 0.136033, Regularization: 0.005033\n",
      "2019-04-10 01:14:47,419 root         INFO     Train Epoch: 142 [3584/8000 (45%)]\tTotal Loss: 0.146561\n",
      "Reconstruction: 0.140577, Regularization: 0.005984\n",
      "2019-04-10 01:14:47,475 root         INFO     Train Epoch: 142 [4096/8000 (51%)]\tTotal Loss: 0.142648\n",
      "Reconstruction: 0.138979, Regularization: 0.003669\n",
      "2019-04-10 01:14:47,532 root         INFO     Train Epoch: 142 [4608/8000 (58%)]\tTotal Loss: 0.139865\n",
      "Reconstruction: 0.134193, Regularization: 0.005672\n",
      "2019-04-10 01:14:47,588 root         INFO     Train Epoch: 142 [5120/8000 (64%)]\tTotal Loss: 0.137629\n",
      "Reconstruction: 0.132569, Regularization: 0.005061\n",
      "2019-04-10 01:14:47,644 root         INFO     Train Epoch: 142 [5632/8000 (70%)]\tTotal Loss: 0.167104\n",
      "Reconstruction: 0.154358, Regularization: 0.012746\n",
      "2019-04-10 01:14:47,701 root         INFO     Train Epoch: 142 [6144/8000 (77%)]\tTotal Loss: 0.148380\n",
      "Reconstruction: 0.144376, Regularization: 0.004004\n",
      "2019-04-10 01:14:47,757 root         INFO     Train Epoch: 142 [6656/8000 (83%)]\tTotal Loss: 0.156905\n",
      "Reconstruction: 0.148344, Regularization: 0.008561\n",
      "2019-04-10 01:14:47,814 root         INFO     Train Epoch: 142 [7168/8000 (90%)]\tTotal Loss: 0.146189\n",
      "Reconstruction: 0.140395, Regularization: 0.005794\n",
      "2019-04-10 01:14:47,870 root         INFO     Train Epoch: 142 [7680/8000 (96%)]\tTotal Loss: 0.143578\n",
      "Reconstruction: 0.136597, Regularization: 0.006981\n",
      "2019-04-10 01:14:47,921 root         INFO     ====> Epoch: 142 Average loss: 0.1500\n",
      "2019-04-10 01:14:47,944 root         INFO     Train Epoch: 143 [0/8000 (0%)]\tTotal Loss: 0.146325\n",
      "Reconstruction: 0.140362, Regularization: 0.005963\n",
      "2019-04-10 01:14:48,000 root         INFO     Train Epoch: 143 [512/8000 (6%)]\tTotal Loss: 0.139257\n",
      "Reconstruction: 0.136425, Regularization: 0.002832\n",
      "2019-04-10 01:14:48,057 root         INFO     Train Epoch: 143 [1024/8000 (13%)]\tTotal Loss: 0.163000\n",
      "Reconstruction: 0.157450, Regularization: 0.005550\n",
      "2019-04-10 01:14:48,113 root         INFO     Train Epoch: 143 [1536/8000 (19%)]\tTotal Loss: 0.158230\n",
      "Reconstruction: 0.150960, Regularization: 0.007270\n",
      "2019-04-10 01:14:48,170 root         INFO     Train Epoch: 143 [2048/8000 (26%)]\tTotal Loss: 0.156027\n",
      "Reconstruction: 0.149575, Regularization: 0.006452\n",
      "2019-04-10 01:14:48,227 root         INFO     Train Epoch: 143 [2560/8000 (32%)]\tTotal Loss: 0.151215\n",
      "Reconstruction: 0.147678, Regularization: 0.003537\n",
      "2019-04-10 01:14:48,283 root         INFO     Train Epoch: 143 [3072/8000 (38%)]\tTotal Loss: 0.148590\n",
      "Reconstruction: 0.141204, Regularization: 0.007387\n",
      "2019-04-10 01:14:48,340 root         INFO     Train Epoch: 143 [3584/8000 (45%)]\tTotal Loss: 0.148206\n",
      "Reconstruction: 0.144312, Regularization: 0.003894\n",
      "2019-04-10 01:14:48,397 root         INFO     Train Epoch: 143 [4096/8000 (51%)]\tTotal Loss: 0.145640\n",
      "Reconstruction: 0.141168, Regularization: 0.004471\n",
      "2019-04-10 01:14:48,453 root         INFO     Train Epoch: 143 [4608/8000 (58%)]\tTotal Loss: 0.143472\n",
      "Reconstruction: 0.138371, Regularization: 0.005100\n",
      "2019-04-10 01:14:48,509 root         INFO     Train Epoch: 143 [5120/8000 (64%)]\tTotal Loss: 0.152528\n",
      "Reconstruction: 0.143313, Regularization: 0.009214\n",
      "2019-04-10 01:14:48,564 root         INFO     Train Epoch: 143 [5632/8000 (70%)]\tTotal Loss: 0.135526\n",
      "Reconstruction: 0.130978, Regularization: 0.004548\n",
      "2019-04-10 01:14:48,620 root         INFO     Train Epoch: 143 [6144/8000 (77%)]\tTotal Loss: 0.150870\n",
      "Reconstruction: 0.143008, Regularization: 0.007863\n",
      "2019-04-10 01:14:48,675 root         INFO     Train Epoch: 143 [6656/8000 (83%)]\tTotal Loss: 0.154225\n",
      "Reconstruction: 0.146790, Regularization: 0.007435\n",
      "2019-04-10 01:14:48,730 root         INFO     Train Epoch: 143 [7168/8000 (90%)]\tTotal Loss: 0.140742\n",
      "Reconstruction: 0.135234, Regularization: 0.005508\n",
      "2019-04-10 01:14:48,786 root         INFO     Train Epoch: 143 [7680/8000 (96%)]\tTotal Loss: 0.142804\n",
      "Reconstruction: 0.138086, Regularization: 0.004717\n",
      "2019-04-10 01:14:48,835 root         INFO     ====> Epoch: 143 Average loss: 0.1498\n",
      "2019-04-10 01:14:48,859 root         INFO     Train Epoch: 144 [0/8000 (0%)]\tTotal Loss: 0.154081\n",
      "Reconstruction: 0.149760, Regularization: 0.004321\n",
      "2019-04-10 01:14:48,914 root         INFO     Train Epoch: 144 [512/8000 (6%)]\tTotal Loss: 0.145274\n",
      "Reconstruction: 0.140506, Regularization: 0.004768\n",
      "2019-04-10 01:14:48,970 root         INFO     Train Epoch: 144 [1024/8000 (13%)]\tTotal Loss: 0.154572\n",
      "Reconstruction: 0.148177, Regularization: 0.006395\n",
      "2019-04-10 01:14:49,026 root         INFO     Train Epoch: 144 [1536/8000 (19%)]\tTotal Loss: 0.156567\n",
      "Reconstruction: 0.149565, Regularization: 0.007003\n",
      "2019-04-10 01:14:49,081 root         INFO     Train Epoch: 144 [2048/8000 (26%)]\tTotal Loss: 0.149417\n",
      "Reconstruction: 0.143068, Regularization: 0.006349\n",
      "2019-04-10 01:14:49,137 root         INFO     Train Epoch: 144 [2560/8000 (32%)]\tTotal Loss: 0.148376\n",
      "Reconstruction: 0.141838, Regularization: 0.006538\n",
      "2019-04-10 01:14:49,193 root         INFO     Train Epoch: 144 [3072/8000 (38%)]\tTotal Loss: 0.155732\n",
      "Reconstruction: 0.146567, Regularization: 0.009164\n",
      "2019-04-10 01:14:49,248 root         INFO     Train Epoch: 144 [3584/8000 (45%)]\tTotal Loss: 0.148481\n",
      "Reconstruction: 0.143831, Regularization: 0.004650\n",
      "2019-04-10 01:14:49,304 root         INFO     Train Epoch: 144 [4096/8000 (51%)]\tTotal Loss: 0.149370\n",
      "Reconstruction: 0.145445, Regularization: 0.003925\n",
      "2019-04-10 01:14:49,360 root         INFO     Train Epoch: 144 [4608/8000 (58%)]\tTotal Loss: 0.144761\n",
      "Reconstruction: 0.140491, Regularization: 0.004270\n",
      "2019-04-10 01:14:49,415 root         INFO     Train Epoch: 144 [5120/8000 (64%)]\tTotal Loss: 0.146153\n",
      "Reconstruction: 0.140464, Regularization: 0.005690\n",
      "2019-04-10 01:14:49,471 root         INFO     Train Epoch: 144 [5632/8000 (70%)]\tTotal Loss: 0.150893\n",
      "Reconstruction: 0.141823, Regularization: 0.009070\n",
      "2019-04-10 01:14:49,527 root         INFO     Train Epoch: 144 [6144/8000 (77%)]\tTotal Loss: 0.144491\n",
      "Reconstruction: 0.140079, Regularization: 0.004412\n",
      "2019-04-10 01:14:49,583 root         INFO     Train Epoch: 144 [6656/8000 (83%)]\tTotal Loss: 0.151230\n",
      "Reconstruction: 0.146541, Regularization: 0.004689\n",
      "2019-04-10 01:14:49,638 root         INFO     Train Epoch: 144 [7168/8000 (90%)]\tTotal Loss: 0.148471\n",
      "Reconstruction: 0.144313, Regularization: 0.004158\n",
      "2019-04-10 01:14:49,694 root         INFO     Train Epoch: 144 [7680/8000 (96%)]\tTotal Loss: 0.149442\n",
      "Reconstruction: 0.145030, Regularization: 0.004412\n",
      "2019-04-10 01:14:49,744 root         INFO     ====> Epoch: 144 Average loss: 0.1495\n",
      "2019-04-10 01:14:49,768 root         INFO     Train Epoch: 145 [0/8000 (0%)]\tTotal Loss: 0.145098\n",
      "Reconstruction: 0.139765, Regularization: 0.005333\n",
      "2019-04-10 01:14:49,824 root         INFO     Train Epoch: 145 [512/8000 (6%)]\tTotal Loss: 0.138595\n",
      "Reconstruction: 0.134136, Regularization: 0.004459\n",
      "2019-04-10 01:14:49,880 root         INFO     Train Epoch: 145 [1024/8000 (13%)]\tTotal Loss: 0.158870\n",
      "Reconstruction: 0.154383, Regularization: 0.004487\n",
      "2019-04-10 01:14:49,936 root         INFO     Train Epoch: 145 [1536/8000 (19%)]\tTotal Loss: 0.143771\n",
      "Reconstruction: 0.138904, Regularization: 0.004866\n",
      "2019-04-10 01:14:49,992 root         INFO     Train Epoch: 145 [2048/8000 (26%)]\tTotal Loss: 0.153459\n",
      "Reconstruction: 0.146173, Regularization: 0.007285\n",
      "2019-04-10 01:14:50,048 root         INFO     Train Epoch: 145 [2560/8000 (32%)]\tTotal Loss: 0.163268\n",
      "Reconstruction: 0.158383, Regularization: 0.004885\n",
      "2019-04-10 01:14:50,103 root         INFO     Train Epoch: 145 [3072/8000 (38%)]\tTotal Loss: 0.146512\n",
      "Reconstruction: 0.141755, Regularization: 0.004757\n",
      "2019-04-10 01:14:50,159 root         INFO     Train Epoch: 145 [3584/8000 (45%)]\tTotal Loss: 0.138994\n",
      "Reconstruction: 0.135080, Regularization: 0.003914\n",
      "2019-04-10 01:14:50,215 root         INFO     Train Epoch: 145 [4096/8000 (51%)]\tTotal Loss: 0.168177\n",
      "Reconstruction: 0.157250, Regularization: 0.010927\n",
      "2019-04-10 01:14:50,271 root         INFO     Train Epoch: 145 [4608/8000 (58%)]\tTotal Loss: 0.172299\n",
      "Reconstruction: 0.159743, Regularization: 0.012555\n",
      "2019-04-10 01:14:50,326 root         INFO     Train Epoch: 145 [5120/8000 (64%)]\tTotal Loss: 0.142432\n",
      "Reconstruction: 0.136429, Regularization: 0.006003\n",
      "2019-04-10 01:14:50,382 root         INFO     Train Epoch: 145 [5632/8000 (70%)]\tTotal Loss: 0.150921\n",
      "Reconstruction: 0.144989, Regularization: 0.005932\n",
      "2019-04-10 01:14:50,438 root         INFO     Train Epoch: 145 [6144/8000 (77%)]\tTotal Loss: 0.142765\n",
      "Reconstruction: 0.136540, Regularization: 0.006225\n",
      "2019-04-10 01:14:50,494 root         INFO     Train Epoch: 145 [6656/8000 (83%)]\tTotal Loss: 0.158960\n",
      "Reconstruction: 0.154297, Regularization: 0.004663\n",
      "2019-04-10 01:14:50,550 root         INFO     Train Epoch: 145 [7168/8000 (90%)]\tTotal Loss: 0.139105\n",
      "Reconstruction: 0.134297, Regularization: 0.004808\n",
      "2019-04-10 01:14:50,606 root         INFO     Train Epoch: 145 [7680/8000 (96%)]\tTotal Loss: 0.139383\n",
      "Reconstruction: 0.134235, Regularization: 0.005148\n",
      "2019-04-10 01:14:50,655 root         INFO     ====> Epoch: 145 Average loss: 0.1495\n",
      "2019-04-10 01:14:50,679 root         INFO     Train Epoch: 146 [0/8000 (0%)]\tTotal Loss: 0.141083\n",
      "Reconstruction: 0.136246, Regularization: 0.004837\n",
      "2019-04-10 01:14:50,734 root         INFO     Train Epoch: 146 [512/8000 (6%)]\tTotal Loss: 0.162078\n",
      "Reconstruction: 0.157823, Regularization: 0.004255\n",
      "2019-04-10 01:14:50,789 root         INFO     Train Epoch: 146 [1024/8000 (13%)]\tTotal Loss: 0.158173\n",
      "Reconstruction: 0.150958, Regularization: 0.007215\n",
      "2019-04-10 01:14:50,844 root         INFO     Train Epoch: 146 [1536/8000 (19%)]\tTotal Loss: 0.139493\n",
      "Reconstruction: 0.134448, Regularization: 0.005045\n",
      "2019-04-10 01:14:50,899 root         INFO     Train Epoch: 146 [2048/8000 (26%)]\tTotal Loss: 0.147894\n",
      "Reconstruction: 0.142246, Regularization: 0.005648\n",
      "2019-04-10 01:14:50,955 root         INFO     Train Epoch: 146 [2560/8000 (32%)]\tTotal Loss: 0.164295\n",
      "Reconstruction: 0.155688, Regularization: 0.008608\n",
      "2019-04-10 01:14:51,011 root         INFO     Train Epoch: 146 [3072/8000 (38%)]\tTotal Loss: 0.132448\n",
      "Reconstruction: 0.128080, Regularization: 0.004368\n",
      "2019-04-10 01:14:51,068 root         INFO     Train Epoch: 146 [3584/8000 (45%)]\tTotal Loss: 0.143815\n",
      "Reconstruction: 0.139702, Regularization: 0.004113\n",
      "2019-04-10 01:14:51,124 root         INFO     Train Epoch: 146 [4096/8000 (51%)]\tTotal Loss: 0.149084\n",
      "Reconstruction: 0.144371, Regularization: 0.004713\n",
      "2019-04-10 01:14:51,180 root         INFO     Train Epoch: 146 [4608/8000 (58%)]\tTotal Loss: 0.138127\n",
      "Reconstruction: 0.134023, Regularization: 0.004104\n",
      "2019-04-10 01:14:51,236 root         INFO     Train Epoch: 146 [5120/8000 (64%)]\tTotal Loss: 0.146044\n",
      "Reconstruction: 0.141013, Regularization: 0.005031\n",
      "2019-04-10 01:14:51,292 root         INFO     Train Epoch: 146 [5632/8000 (70%)]\tTotal Loss: 0.145068\n",
      "Reconstruction: 0.140588, Regularization: 0.004480\n",
      "2019-04-10 01:14:51,348 root         INFO     Train Epoch: 146 [6144/8000 (77%)]\tTotal Loss: 0.133837\n",
      "Reconstruction: 0.129858, Regularization: 0.003979\n",
      "2019-04-10 01:14:51,404 root         INFO     Train Epoch: 146 [6656/8000 (83%)]\tTotal Loss: 0.151196\n",
      "Reconstruction: 0.144941, Regularization: 0.006255\n",
      "2019-04-10 01:14:51,460 root         INFO     Train Epoch: 146 [7168/8000 (90%)]\tTotal Loss: 0.154032\n",
      "Reconstruction: 0.147707, Regularization: 0.006325\n",
      "2019-04-10 01:14:51,516 root         INFO     Train Epoch: 146 [7680/8000 (96%)]\tTotal Loss: 0.167165\n",
      "Reconstruction: 0.154417, Regularization: 0.012748\n",
      "2019-04-10 01:14:51,566 root         INFO     ====> Epoch: 146 Average loss: 0.1500\n",
      "2019-04-10 01:14:51,590 root         INFO     Train Epoch: 147 [0/8000 (0%)]\tTotal Loss: 0.154909\n",
      "Reconstruction: 0.148454, Regularization: 0.006455\n",
      "2019-04-10 01:14:51,647 root         INFO     Train Epoch: 147 [512/8000 (6%)]\tTotal Loss: 0.140414\n",
      "Reconstruction: 0.135792, Regularization: 0.004622\n",
      "2019-04-10 01:14:51,703 root         INFO     Train Epoch: 147 [1024/8000 (13%)]\tTotal Loss: 0.143111\n",
      "Reconstruction: 0.136838, Regularization: 0.006273\n",
      "2019-04-10 01:14:51,759 root         INFO     Train Epoch: 147 [1536/8000 (19%)]\tTotal Loss: 0.149870\n",
      "Reconstruction: 0.141847, Regularization: 0.008023\n",
      "2019-04-10 01:14:51,814 root         INFO     Train Epoch: 147 [2048/8000 (26%)]\tTotal Loss: 0.148949\n",
      "Reconstruction: 0.142568, Regularization: 0.006381\n",
      "2019-04-10 01:14:51,871 root         INFO     Train Epoch: 147 [2560/8000 (32%)]\tTotal Loss: 0.141032\n",
      "Reconstruction: 0.135821, Regularization: 0.005211\n",
      "2019-04-10 01:14:51,927 root         INFO     Train Epoch: 147 [3072/8000 (38%)]\tTotal Loss: 0.146328\n",
      "Reconstruction: 0.142930, Regularization: 0.003398\n",
      "2019-04-10 01:14:51,983 root         INFO     Train Epoch: 147 [3584/8000 (45%)]\tTotal Loss: 0.150532\n",
      "Reconstruction: 0.144925, Regularization: 0.005607\n",
      "2019-04-10 01:14:52,039 root         INFO     Train Epoch: 147 [4096/8000 (51%)]\tTotal Loss: 0.151832\n",
      "Reconstruction: 0.143352, Regularization: 0.008480\n",
      "2019-04-10 01:14:52,095 root         INFO     Train Epoch: 147 [4608/8000 (58%)]\tTotal Loss: 0.140225\n",
      "Reconstruction: 0.134699, Regularization: 0.005526\n",
      "2019-04-10 01:14:52,151 root         INFO     Train Epoch: 147 [5120/8000 (64%)]\tTotal Loss: 0.152257\n",
      "Reconstruction: 0.146466, Regularization: 0.005790\n",
      "2019-04-10 01:14:52,207 root         INFO     Train Epoch: 147 [5632/8000 (70%)]\tTotal Loss: 0.147735\n",
      "Reconstruction: 0.143722, Regularization: 0.004013\n",
      "2019-04-10 01:14:52,263 root         INFO     Train Epoch: 147 [6144/8000 (77%)]\tTotal Loss: 0.168580\n",
      "Reconstruction: 0.158588, Regularization: 0.009993\n",
      "2019-04-10 01:14:52,319 root         INFO     Train Epoch: 147 [6656/8000 (83%)]\tTotal Loss: 0.145486\n",
      "Reconstruction: 0.142507, Regularization: 0.002979\n",
      "2019-04-10 01:14:52,375 root         INFO     Train Epoch: 147 [7168/8000 (90%)]\tTotal Loss: 0.154876\n",
      "Reconstruction: 0.146970, Regularization: 0.007906\n",
      "2019-04-10 01:14:52,431 root         INFO     Train Epoch: 147 [7680/8000 (96%)]\tTotal Loss: 0.167923\n",
      "Reconstruction: 0.161595, Regularization: 0.006328\n",
      "2019-04-10 01:14:52,481 root         INFO     ====> Epoch: 147 Average loss: 0.1500\n",
      "2019-04-10 01:14:52,505 root         INFO     Train Epoch: 148 [0/8000 (0%)]\tTotal Loss: 0.157299\n",
      "Reconstruction: 0.152406, Regularization: 0.004893\n",
      "2019-04-10 01:14:52,561 root         INFO     Train Epoch: 148 [512/8000 (6%)]\tTotal Loss: 0.139483\n",
      "Reconstruction: 0.134558, Regularization: 0.004925\n",
      "2019-04-10 01:14:52,618 root         INFO     Train Epoch: 148 [1024/8000 (13%)]\tTotal Loss: 0.138806\n",
      "Reconstruction: 0.134594, Regularization: 0.004213\n",
      "2019-04-10 01:14:52,674 root         INFO     Train Epoch: 148 [1536/8000 (19%)]\tTotal Loss: 0.152908\n",
      "Reconstruction: 0.147615, Regularization: 0.005292\n",
      "2019-04-10 01:14:52,730 root         INFO     Train Epoch: 148 [2048/8000 (26%)]\tTotal Loss: 0.154315\n",
      "Reconstruction: 0.148589, Regularization: 0.005726\n",
      "2019-04-10 01:14:52,787 root         INFO     Train Epoch: 148 [2560/8000 (32%)]\tTotal Loss: 0.152319\n",
      "Reconstruction: 0.146345, Regularization: 0.005975\n",
      "2019-04-10 01:14:52,844 root         INFO     Train Epoch: 148 [3072/8000 (38%)]\tTotal Loss: 0.148608\n",
      "Reconstruction: 0.142574, Regularization: 0.006035\n",
      "2019-04-10 01:14:52,901 root         INFO     Train Epoch: 148 [3584/8000 (45%)]\tTotal Loss: 0.144236\n",
      "Reconstruction: 0.139734, Regularization: 0.004502\n",
      "2019-04-10 01:14:52,958 root         INFO     Train Epoch: 148 [4096/8000 (51%)]\tTotal Loss: 0.145132\n",
      "Reconstruction: 0.139109, Regularization: 0.006023\n",
      "2019-04-10 01:14:53,015 root         INFO     Train Epoch: 148 [4608/8000 (58%)]\tTotal Loss: 0.151533\n",
      "Reconstruction: 0.144445, Regularization: 0.007088\n",
      "2019-04-10 01:14:53,072 root         INFO     Train Epoch: 148 [5120/8000 (64%)]\tTotal Loss: 0.141411\n",
      "Reconstruction: 0.135527, Regularization: 0.005885\n",
      "2019-04-10 01:14:53,130 root         INFO     Train Epoch: 148 [5632/8000 (70%)]\tTotal Loss: 0.149331\n",
      "Reconstruction: 0.142362, Regularization: 0.006969\n",
      "2019-04-10 01:14:53,187 root         INFO     Train Epoch: 148 [6144/8000 (77%)]\tTotal Loss: 0.153724\n",
      "Reconstruction: 0.146561, Regularization: 0.007163\n",
      "2019-04-10 01:14:53,244 root         INFO     Train Epoch: 148 [6656/8000 (83%)]\tTotal Loss: 0.145192\n",
      "Reconstruction: 0.139225, Regularization: 0.005967\n",
      "2019-04-10 01:14:53,302 root         INFO     Train Epoch: 148 [7168/8000 (90%)]\tTotal Loss: 0.154026\n",
      "Reconstruction: 0.149232, Regularization: 0.004794\n",
      "2019-04-10 01:14:53,359 root         INFO     Train Epoch: 148 [7680/8000 (96%)]\tTotal Loss: 0.144732\n",
      "Reconstruction: 0.140989, Regularization: 0.003744\n",
      "2019-04-10 01:14:53,409 root         INFO     ====> Epoch: 148 Average loss: 0.1497\n",
      "2019-04-10 01:14:53,433 root         INFO     Train Epoch: 149 [0/8000 (0%)]\tTotal Loss: 0.172819\n",
      "Reconstruction: 0.167426, Regularization: 0.005393\n",
      "2019-04-10 01:14:53,489 root         INFO     Train Epoch: 149 [512/8000 (6%)]\tTotal Loss: 0.155128\n",
      "Reconstruction: 0.149856, Regularization: 0.005271\n",
      "2019-04-10 01:14:53,547 root         INFO     Train Epoch: 149 [1024/8000 (13%)]\tTotal Loss: 0.150344\n",
      "Reconstruction: 0.144590, Regularization: 0.005754\n",
      "2019-04-10 01:14:53,604 root         INFO     Train Epoch: 149 [1536/8000 (19%)]\tTotal Loss: 0.141963\n",
      "Reconstruction: 0.136777, Regularization: 0.005187\n",
      "2019-04-10 01:14:53,662 root         INFO     Train Epoch: 149 [2048/8000 (26%)]\tTotal Loss: 0.148488\n",
      "Reconstruction: 0.141564, Regularization: 0.006924\n",
      "2019-04-10 01:14:53,719 root         INFO     Train Epoch: 149 [2560/8000 (32%)]\tTotal Loss: 0.147647\n",
      "Reconstruction: 0.143526, Regularization: 0.004121\n",
      "2019-04-10 01:14:53,776 root         INFO     Train Epoch: 149 [3072/8000 (38%)]\tTotal Loss: 0.145521\n",
      "Reconstruction: 0.140860, Regularization: 0.004661\n",
      "2019-04-10 01:14:53,834 root         INFO     Train Epoch: 149 [3584/8000 (45%)]\tTotal Loss: 0.165732\n",
      "Reconstruction: 0.160104, Regularization: 0.005628\n",
      "2019-04-10 01:14:53,892 root         INFO     Train Epoch: 149 [4096/8000 (51%)]\tTotal Loss: 0.146513\n",
      "Reconstruction: 0.142009, Regularization: 0.004504\n",
      "2019-04-10 01:14:53,951 root         INFO     Train Epoch: 149 [4608/8000 (58%)]\tTotal Loss: 0.147535\n",
      "Reconstruction: 0.143153, Regularization: 0.004382\n",
      "2019-04-10 01:14:54,009 root         INFO     Train Epoch: 149 [5120/8000 (64%)]\tTotal Loss: 0.145226\n",
      "Reconstruction: 0.139739, Regularization: 0.005487\n",
      "2019-04-10 01:14:54,067 root         INFO     Train Epoch: 149 [5632/8000 (70%)]\tTotal Loss: 0.151059\n",
      "Reconstruction: 0.147092, Regularization: 0.003967\n",
      "2019-04-10 01:14:54,126 root         INFO     Train Epoch: 149 [6144/8000 (77%)]\tTotal Loss: 0.145012\n",
      "Reconstruction: 0.139897, Regularization: 0.005115\n",
      "2019-04-10 01:14:54,184 root         INFO     Train Epoch: 149 [6656/8000 (83%)]\tTotal Loss: 0.160693\n",
      "Reconstruction: 0.151947, Regularization: 0.008746\n",
      "2019-04-10 01:14:54,243 root         INFO     Train Epoch: 149 [7168/8000 (90%)]\tTotal Loss: 0.151611\n",
      "Reconstruction: 0.145231, Regularization: 0.006380\n",
      "2019-04-10 01:14:54,301 root         INFO     Train Epoch: 149 [7680/8000 (96%)]\tTotal Loss: 0.147963\n",
      "Reconstruction: 0.141472, Regularization: 0.006491\n",
      "2019-04-10 01:14:54,352 root         INFO     ====> Epoch: 149 Average loss: 0.1498\n",
      "2019-04-10 01:14:54,375 root         INFO     Train Epoch: 150 [0/8000 (0%)]\tTotal Loss: 0.156935\n",
      "Reconstruction: 0.147034, Regularization: 0.009902\n",
      "2019-04-10 01:14:54,434 root         INFO     Train Epoch: 150 [512/8000 (6%)]\tTotal Loss: 0.145709\n",
      "Reconstruction: 0.140081, Regularization: 0.005629\n",
      "2019-04-10 01:14:54,491 root         INFO     Train Epoch: 150 [1024/8000 (13%)]\tTotal Loss: 0.153025\n",
      "Reconstruction: 0.148694, Regularization: 0.004331\n",
      "2019-04-10 01:14:54,549 root         INFO     Train Epoch: 150 [1536/8000 (19%)]\tTotal Loss: 0.147370\n",
      "Reconstruction: 0.143149, Regularization: 0.004222\n",
      "2019-04-10 01:14:54,607 root         INFO     Train Epoch: 150 [2048/8000 (26%)]\tTotal Loss: 0.163998\n",
      "Reconstruction: 0.151872, Regularization: 0.012126\n",
      "2019-04-10 01:14:54,665 root         INFO     Train Epoch: 150 [2560/8000 (32%)]\tTotal Loss: 0.155056\n",
      "Reconstruction: 0.147917, Regularization: 0.007139\n",
      "2019-04-10 01:14:54,723 root         INFO     Train Epoch: 150 [3072/8000 (38%)]\tTotal Loss: 0.162091\n",
      "Reconstruction: 0.152122, Regularization: 0.009970\n",
      "2019-04-10 01:14:54,781 root         INFO     Train Epoch: 150 [3584/8000 (45%)]\tTotal Loss: 0.152159\n",
      "Reconstruction: 0.146359, Regularization: 0.005800\n",
      "2019-04-10 01:14:54,838 root         INFO     Train Epoch: 150 [4096/8000 (51%)]\tTotal Loss: 0.148662\n",
      "Reconstruction: 0.144617, Regularization: 0.004045\n",
      "2019-04-10 01:14:54,896 root         INFO     Train Epoch: 150 [4608/8000 (58%)]\tTotal Loss: 0.157681\n",
      "Reconstruction: 0.151233, Regularization: 0.006448\n",
      "2019-04-10 01:14:54,953 root         INFO     Train Epoch: 150 [5120/8000 (64%)]\tTotal Loss: 0.146395\n",
      "Reconstruction: 0.141430, Regularization: 0.004965\n",
      "2019-04-10 01:14:55,011 root         INFO     Train Epoch: 150 [5632/8000 (70%)]\tTotal Loss: 0.154863\n",
      "Reconstruction: 0.149814, Regularization: 0.005049\n",
      "2019-04-10 01:14:55,068 root         INFO     Train Epoch: 150 [6144/8000 (77%)]\tTotal Loss: 0.142913\n",
      "Reconstruction: 0.137986, Regularization: 0.004927\n",
      "2019-04-10 01:14:55,125 root         INFO     Train Epoch: 150 [6656/8000 (83%)]\tTotal Loss: 0.168649\n",
      "Reconstruction: 0.161514, Regularization: 0.007134\n",
      "2019-04-10 01:14:55,182 root         INFO     Train Epoch: 150 [7168/8000 (90%)]\tTotal Loss: 0.151203\n",
      "Reconstruction: 0.144634, Regularization: 0.006569\n",
      "2019-04-10 01:14:55,239 root         INFO     Train Epoch: 150 [7680/8000 (96%)]\tTotal Loss: 0.146578\n",
      "Reconstruction: 0.141177, Regularization: 0.005401\n",
      "2019-04-10 01:14:55,289 root         INFO     ====> Epoch: 150 Average loss: 0.1497\n",
      "2019-04-10 01:14:55,313 root         INFO     Train Epoch: 151 [0/8000 (0%)]\tTotal Loss: 0.150175\n",
      "Reconstruction: 0.143247, Regularization: 0.006928\n",
      "2019-04-10 01:14:55,370 root         INFO     Train Epoch: 151 [512/8000 (6%)]\tTotal Loss: 0.149282\n",
      "Reconstruction: 0.144558, Regularization: 0.004724\n",
      "2019-04-10 01:14:55,428 root         INFO     Train Epoch: 151 [1024/8000 (13%)]\tTotal Loss: 0.145987\n",
      "Reconstruction: 0.140363, Regularization: 0.005624\n",
      "2019-04-10 01:14:55,485 root         INFO     Train Epoch: 151 [1536/8000 (19%)]\tTotal Loss: 0.147521\n",
      "Reconstruction: 0.142197, Regularization: 0.005324\n",
      "2019-04-10 01:14:55,542 root         INFO     Train Epoch: 151 [2048/8000 (26%)]\tTotal Loss: 0.139019\n",
      "Reconstruction: 0.131890, Regularization: 0.007129\n",
      "2019-04-10 01:14:55,600 root         INFO     Train Epoch: 151 [2560/8000 (32%)]\tTotal Loss: 0.144987\n",
      "Reconstruction: 0.138167, Regularization: 0.006820\n",
      "2019-04-10 01:14:55,655 root         INFO     Train Epoch: 151 [3072/8000 (38%)]\tTotal Loss: 0.156720\n",
      "Reconstruction: 0.148559, Regularization: 0.008162\n",
      "2019-04-10 01:14:55,711 root         INFO     Train Epoch: 151 [3584/8000 (45%)]\tTotal Loss: 0.147297\n",
      "Reconstruction: 0.142336, Regularization: 0.004961\n",
      "2019-04-10 01:14:55,766 root         INFO     Train Epoch: 151 [4096/8000 (51%)]\tTotal Loss: 0.155408\n",
      "Reconstruction: 0.147733, Regularization: 0.007675\n",
      "2019-04-10 01:14:55,821 root         INFO     Train Epoch: 151 [4608/8000 (58%)]\tTotal Loss: 0.149257\n",
      "Reconstruction: 0.143308, Regularization: 0.005949\n",
      "2019-04-10 01:14:55,876 root         INFO     Train Epoch: 151 [5120/8000 (64%)]\tTotal Loss: 0.156554\n",
      "Reconstruction: 0.149402, Regularization: 0.007152\n",
      "2019-04-10 01:14:55,931 root         INFO     Train Epoch: 151 [5632/8000 (70%)]\tTotal Loss: 0.161443\n",
      "Reconstruction: 0.152785, Regularization: 0.008658\n",
      "2019-04-10 01:14:55,986 root         INFO     Train Epoch: 151 [6144/8000 (77%)]\tTotal Loss: 0.156789\n",
      "Reconstruction: 0.149928, Regularization: 0.006861\n",
      "2019-04-10 01:14:56,041 root         INFO     Train Epoch: 151 [6656/8000 (83%)]\tTotal Loss: 0.152021\n",
      "Reconstruction: 0.146801, Regularization: 0.005220\n",
      "2019-04-10 01:14:56,097 root         INFO     Train Epoch: 151 [7168/8000 (90%)]\tTotal Loss: 0.140492\n",
      "Reconstruction: 0.135168, Regularization: 0.005324\n",
      "2019-04-10 01:14:56,152 root         INFO     Train Epoch: 151 [7680/8000 (96%)]\tTotal Loss: 0.143184\n",
      "Reconstruction: 0.137296, Regularization: 0.005887\n",
      "2019-04-10 01:14:56,202 root         INFO     ====> Epoch: 151 Average loss: 0.1497\n",
      "2019-04-10 01:14:56,226 root         INFO     Train Epoch: 152 [0/8000 (0%)]\tTotal Loss: 0.136977\n",
      "Reconstruction: 0.132883, Regularization: 0.004094\n",
      "2019-04-10 01:14:56,283 root         INFO     Train Epoch: 152 [512/8000 (6%)]\tTotal Loss: 0.147603\n",
      "Reconstruction: 0.140870, Regularization: 0.006733\n",
      "2019-04-10 01:14:56,340 root         INFO     Train Epoch: 152 [1024/8000 (13%)]\tTotal Loss: 0.168489\n",
      "Reconstruction: 0.156365, Regularization: 0.012123\n",
      "2019-04-10 01:14:56,397 root         INFO     Train Epoch: 152 [1536/8000 (19%)]\tTotal Loss: 0.144280\n",
      "Reconstruction: 0.139305, Regularization: 0.004975\n",
      "2019-04-10 01:14:56,454 root         INFO     Train Epoch: 152 [2048/8000 (26%)]\tTotal Loss: 0.152949\n",
      "Reconstruction: 0.144813, Regularization: 0.008136\n",
      "2019-04-10 01:14:56,511 root         INFO     Train Epoch: 152 [2560/8000 (32%)]\tTotal Loss: 0.154981\n",
      "Reconstruction: 0.147596, Regularization: 0.007385\n",
      "2019-04-10 01:14:56,568 root         INFO     Train Epoch: 152 [3072/8000 (38%)]\tTotal Loss: 0.145752\n",
      "Reconstruction: 0.139004, Regularization: 0.006749\n",
      "2019-04-10 01:14:56,625 root         INFO     Train Epoch: 152 [3584/8000 (45%)]\tTotal Loss: 0.160378\n",
      "Reconstruction: 0.151986, Regularization: 0.008391\n",
      "2019-04-10 01:14:56,682 root         INFO     Train Epoch: 152 [4096/8000 (51%)]\tTotal Loss: 0.157993\n",
      "Reconstruction: 0.152377, Regularization: 0.005616\n",
      "2019-04-10 01:14:56,739 root         INFO     Train Epoch: 152 [4608/8000 (58%)]\tTotal Loss: 0.152267\n",
      "Reconstruction: 0.147282, Regularization: 0.004985\n",
      "2019-04-10 01:14:56,795 root         INFO     Train Epoch: 152 [5120/8000 (64%)]\tTotal Loss: 0.167817\n",
      "Reconstruction: 0.155053, Regularization: 0.012764\n",
      "2019-04-10 01:14:56,850 root         INFO     Train Epoch: 152 [5632/8000 (70%)]\tTotal Loss: 0.135345\n",
      "Reconstruction: 0.130610, Regularization: 0.004735\n",
      "2019-04-10 01:14:56,906 root         INFO     Train Epoch: 152 [6144/8000 (77%)]\tTotal Loss: 0.145855\n",
      "Reconstruction: 0.141592, Regularization: 0.004263\n",
      "2019-04-10 01:14:56,962 root         INFO     Train Epoch: 152 [6656/8000 (83%)]\tTotal Loss: 0.141636\n",
      "Reconstruction: 0.135978, Regularization: 0.005659\n",
      "2019-04-10 01:14:57,018 root         INFO     Train Epoch: 152 [7168/8000 (90%)]\tTotal Loss: 0.154302\n",
      "Reconstruction: 0.146521, Regularization: 0.007781\n",
      "2019-04-10 01:14:57,074 root         INFO     Train Epoch: 152 [7680/8000 (96%)]\tTotal Loss: 0.155740\n",
      "Reconstruction: 0.149190, Regularization: 0.006550\n",
      "2019-04-10 01:14:57,124 root         INFO     ====> Epoch: 152 Average loss: 0.1497\n",
      "2019-04-10 01:14:57,147 root         INFO     Train Epoch: 153 [0/8000 (0%)]\tTotal Loss: 0.145993\n",
      "Reconstruction: 0.137257, Regularization: 0.008736\n",
      "2019-04-10 01:14:57,204 root         INFO     Train Epoch: 153 [512/8000 (6%)]\tTotal Loss: 0.139291\n",
      "Reconstruction: 0.131414, Regularization: 0.007877\n",
      "2019-04-10 01:14:57,261 root         INFO     Train Epoch: 153 [1024/8000 (13%)]\tTotal Loss: 0.154671\n",
      "Reconstruction: 0.148737, Regularization: 0.005934\n",
      "2019-04-10 01:14:57,318 root         INFO     Train Epoch: 153 [1536/8000 (19%)]\tTotal Loss: 0.152730\n",
      "Reconstruction: 0.144973, Regularization: 0.007757\n",
      "2019-04-10 01:14:57,374 root         INFO     Train Epoch: 153 [2048/8000 (26%)]\tTotal Loss: 0.154522\n",
      "Reconstruction: 0.148167, Regularization: 0.006355\n",
      "2019-04-10 01:14:57,431 root         INFO     Train Epoch: 153 [2560/8000 (32%)]\tTotal Loss: 0.151413\n",
      "Reconstruction: 0.146131, Regularization: 0.005282\n",
      "2019-04-10 01:14:57,487 root         INFO     Train Epoch: 153 [3072/8000 (38%)]\tTotal Loss: 0.144640\n",
      "Reconstruction: 0.138668, Regularization: 0.005971\n",
      "2019-04-10 01:14:57,543 root         INFO     Train Epoch: 153 [3584/8000 (45%)]\tTotal Loss: 0.134582\n",
      "Reconstruction: 0.130284, Regularization: 0.004298\n",
      "2019-04-10 01:14:57,600 root         INFO     Train Epoch: 153 [4096/8000 (51%)]\tTotal Loss: 0.168387\n",
      "Reconstruction: 0.160783, Regularization: 0.007604\n",
      "2019-04-10 01:14:57,657 root         INFO     Train Epoch: 153 [4608/8000 (58%)]\tTotal Loss: 0.148666\n",
      "Reconstruction: 0.144067, Regularization: 0.004599\n",
      "2019-04-10 01:14:57,714 root         INFO     Train Epoch: 153 [5120/8000 (64%)]\tTotal Loss: 0.145932\n",
      "Reconstruction: 0.142306, Regularization: 0.003626\n",
      "2019-04-10 01:14:57,770 root         INFO     Train Epoch: 153 [5632/8000 (70%)]\tTotal Loss: 0.151923\n",
      "Reconstruction: 0.148237, Regularization: 0.003686\n",
      "2019-04-10 01:14:57,827 root         INFO     Train Epoch: 153 [6144/8000 (77%)]\tTotal Loss: 0.144169\n",
      "Reconstruction: 0.139322, Regularization: 0.004846\n",
      "2019-04-10 01:14:57,884 root         INFO     Train Epoch: 153 [6656/8000 (83%)]\tTotal Loss: 0.162283\n",
      "Reconstruction: 0.154256, Regularization: 0.008027\n",
      "2019-04-10 01:14:57,941 root         INFO     Train Epoch: 153 [7168/8000 (90%)]\tTotal Loss: 0.152862\n",
      "Reconstruction: 0.146379, Regularization: 0.006483\n",
      "2019-04-10 01:14:57,998 root         INFO     Train Epoch: 153 [7680/8000 (96%)]\tTotal Loss: 0.151575\n",
      "Reconstruction: 0.144704, Regularization: 0.006871\n",
      "2019-04-10 01:14:58,048 root         INFO     ====> Epoch: 153 Average loss: 0.1496\n",
      "2019-04-10 01:14:58,072 root         INFO     Train Epoch: 154 [0/8000 (0%)]\tTotal Loss: 0.145636\n",
      "Reconstruction: 0.141913, Regularization: 0.003723\n",
      "2019-04-10 01:14:58,129 root         INFO     Train Epoch: 154 [512/8000 (6%)]\tTotal Loss: 0.149209\n",
      "Reconstruction: 0.144704, Regularization: 0.004505\n",
      "2019-04-10 01:14:58,185 root         INFO     Train Epoch: 154 [1024/8000 (13%)]\tTotal Loss: 0.158517\n",
      "Reconstruction: 0.152703, Regularization: 0.005815\n",
      "2019-04-10 01:14:58,241 root         INFO     Train Epoch: 154 [1536/8000 (19%)]\tTotal Loss: 0.144350\n",
      "Reconstruction: 0.138844, Regularization: 0.005505\n",
      "2019-04-10 01:14:58,298 root         INFO     Train Epoch: 154 [2048/8000 (26%)]\tTotal Loss: 0.142447\n",
      "Reconstruction: 0.137242, Regularization: 0.005205\n",
      "2019-04-10 01:14:58,354 root         INFO     Train Epoch: 154 [2560/8000 (32%)]\tTotal Loss: 0.146066\n",
      "Reconstruction: 0.138388, Regularization: 0.007678\n",
      "2019-04-10 01:14:58,411 root         INFO     Train Epoch: 154 [3072/8000 (38%)]\tTotal Loss: 0.151356\n",
      "Reconstruction: 0.144816, Regularization: 0.006540\n",
      "2019-04-10 01:14:58,467 root         INFO     Train Epoch: 154 [3584/8000 (45%)]\tTotal Loss: 0.149678\n",
      "Reconstruction: 0.146883, Regularization: 0.002795\n",
      "2019-04-10 01:14:58,524 root         INFO     Train Epoch: 154 [4096/8000 (51%)]\tTotal Loss: 0.133262\n",
      "Reconstruction: 0.129712, Regularization: 0.003550\n",
      "2019-04-10 01:14:58,581 root         INFO     Train Epoch: 154 [4608/8000 (58%)]\tTotal Loss: 0.159743\n",
      "Reconstruction: 0.150479, Regularization: 0.009264\n",
      "2019-04-10 01:14:58,637 root         INFO     Train Epoch: 154 [5120/8000 (64%)]\tTotal Loss: 0.153361\n",
      "Reconstruction: 0.144884, Regularization: 0.008477\n",
      "2019-04-10 01:14:58,694 root         INFO     Train Epoch: 154 [5632/8000 (70%)]\tTotal Loss: 0.147035\n",
      "Reconstruction: 0.142058, Regularization: 0.004977\n",
      "2019-04-10 01:14:58,751 root         INFO     Train Epoch: 154 [6144/8000 (77%)]\tTotal Loss: 0.157576\n",
      "Reconstruction: 0.150629, Regularization: 0.006947\n",
      "2019-04-10 01:14:58,808 root         INFO     Train Epoch: 154 [6656/8000 (83%)]\tTotal Loss: 0.151939\n",
      "Reconstruction: 0.145549, Regularization: 0.006389\n",
      "2019-04-10 01:14:58,865 root         INFO     Train Epoch: 154 [7168/8000 (90%)]\tTotal Loss: 0.154484\n",
      "Reconstruction: 0.147637, Regularization: 0.006847\n",
      "2019-04-10 01:14:58,921 root         INFO     Train Epoch: 154 [7680/8000 (96%)]\tTotal Loss: 0.143877\n",
      "Reconstruction: 0.138526, Regularization: 0.005352\n",
      "2019-04-10 01:14:58,971 root         INFO     ====> Epoch: 154 Average loss: 0.1500\n",
      "2019-04-10 01:14:58,995 root         INFO     Train Epoch: 155 [0/8000 (0%)]\tTotal Loss: 0.145368\n",
      "Reconstruction: 0.140285, Regularization: 0.005083\n",
      "2019-04-10 01:14:59,052 root         INFO     Train Epoch: 155 [512/8000 (6%)]\tTotal Loss: 0.147278\n",
      "Reconstruction: 0.141175, Regularization: 0.006103\n",
      "2019-04-10 01:14:59,109 root         INFO     Train Epoch: 155 [1024/8000 (13%)]\tTotal Loss: 0.143374\n",
      "Reconstruction: 0.137212, Regularization: 0.006162\n",
      "2019-04-10 01:14:59,166 root         INFO     Train Epoch: 155 [1536/8000 (19%)]\tTotal Loss: 0.150149\n",
      "Reconstruction: 0.144861, Regularization: 0.005288\n",
      "2019-04-10 01:14:59,222 root         INFO     Train Epoch: 155 [2048/8000 (26%)]\tTotal Loss: 0.144949\n",
      "Reconstruction: 0.138417, Regularization: 0.006532\n",
      "2019-04-10 01:14:59,279 root         INFO     Train Epoch: 155 [2560/8000 (32%)]\tTotal Loss: 0.149892\n",
      "Reconstruction: 0.146608, Regularization: 0.003284\n",
      "2019-04-10 01:14:59,335 root         INFO     Train Epoch: 155 [3072/8000 (38%)]\tTotal Loss: 0.160172\n",
      "Reconstruction: 0.151687, Regularization: 0.008485\n",
      "2019-04-10 01:14:59,393 root         INFO     Train Epoch: 155 [3584/8000 (45%)]\tTotal Loss: 0.138779\n",
      "Reconstruction: 0.133559, Regularization: 0.005220\n",
      "2019-04-10 01:14:59,448 root         INFO     Train Epoch: 155 [4096/8000 (51%)]\tTotal Loss: 0.150094\n",
      "Reconstruction: 0.145396, Regularization: 0.004698\n",
      "2019-04-10 01:14:59,503 root         INFO     Train Epoch: 155 [4608/8000 (58%)]\tTotal Loss: 0.156493\n",
      "Reconstruction: 0.150480, Regularization: 0.006013\n",
      "2019-04-10 01:14:59,558 root         INFO     Train Epoch: 155 [5120/8000 (64%)]\tTotal Loss: 0.138983\n",
      "Reconstruction: 0.135899, Regularization: 0.003084\n",
      "2019-04-10 01:14:59,613 root         INFO     Train Epoch: 155 [5632/8000 (70%)]\tTotal Loss: 0.148350\n",
      "Reconstruction: 0.142109, Regularization: 0.006241\n",
      "2019-04-10 01:14:59,668 root         INFO     Train Epoch: 155 [6144/8000 (77%)]\tTotal Loss: 0.144131\n",
      "Reconstruction: 0.138512, Regularization: 0.005619\n",
      "2019-04-10 01:14:59,723 root         INFO     Train Epoch: 155 [6656/8000 (83%)]\tTotal Loss: 0.152159\n",
      "Reconstruction: 0.144843, Regularization: 0.007316\n",
      "2019-04-10 01:14:59,778 root         INFO     Train Epoch: 155 [7168/8000 (90%)]\tTotal Loss: 0.147231\n",
      "Reconstruction: 0.142329, Regularization: 0.004901\n",
      "2019-04-10 01:14:59,834 root         INFO     Train Epoch: 155 [7680/8000 (96%)]\tTotal Loss: 0.138298\n",
      "Reconstruction: 0.130575, Regularization: 0.007724\n",
      "2019-04-10 01:14:59,883 root         INFO     ====> Epoch: 155 Average loss: 0.1498\n",
      "2019-04-10 01:14:59,907 root         INFO     Train Epoch: 156 [0/8000 (0%)]\tTotal Loss: 0.147341\n",
      "Reconstruction: 0.140488, Regularization: 0.006854\n",
      "2019-04-10 01:14:59,964 root         INFO     Train Epoch: 156 [512/8000 (6%)]\tTotal Loss: 0.149326\n",
      "Reconstruction: 0.140632, Regularization: 0.008694\n",
      "2019-04-10 01:15:00,021 root         INFO     Train Epoch: 156 [1024/8000 (13%)]\tTotal Loss: 0.142466\n",
      "Reconstruction: 0.137766, Regularization: 0.004700\n",
      "2019-04-10 01:15:00,078 root         INFO     Train Epoch: 156 [1536/8000 (19%)]\tTotal Loss: 0.149573\n",
      "Reconstruction: 0.143081, Regularization: 0.006492\n",
      "2019-04-10 01:15:00,135 root         INFO     Train Epoch: 156 [2048/8000 (26%)]\tTotal Loss: 0.146799\n",
      "Reconstruction: 0.138905, Regularization: 0.007894\n",
      "2019-04-10 01:15:00,192 root         INFO     Train Epoch: 156 [2560/8000 (32%)]\tTotal Loss: 0.163709\n",
      "Reconstruction: 0.158386, Regularization: 0.005323\n",
      "2019-04-10 01:15:00,249 root         INFO     Train Epoch: 156 [3072/8000 (38%)]\tTotal Loss: 0.151596\n",
      "Reconstruction: 0.145776, Regularization: 0.005820\n",
      "2019-04-10 01:15:00,306 root         INFO     Train Epoch: 156 [3584/8000 (45%)]\tTotal Loss: 0.138372\n",
      "Reconstruction: 0.134269, Regularization: 0.004103\n",
      "2019-04-10 01:15:00,363 root         INFO     Train Epoch: 156 [4096/8000 (51%)]\tTotal Loss: 0.159884\n",
      "Reconstruction: 0.147686, Regularization: 0.012198\n",
      "2019-04-10 01:15:00,421 root         INFO     Train Epoch: 156 [4608/8000 (58%)]\tTotal Loss: 0.152478\n",
      "Reconstruction: 0.144233, Regularization: 0.008245\n",
      "2019-04-10 01:15:00,478 root         INFO     Train Epoch: 156 [5120/8000 (64%)]\tTotal Loss: 0.151434\n",
      "Reconstruction: 0.144647, Regularization: 0.006787\n",
      "2019-04-10 01:15:00,534 root         INFO     Train Epoch: 156 [5632/8000 (70%)]\tTotal Loss: 0.139695\n",
      "Reconstruction: 0.132558, Regularization: 0.007137\n",
      "2019-04-10 01:15:00,591 root         INFO     Train Epoch: 156 [6144/8000 (77%)]\tTotal Loss: 0.150626\n",
      "Reconstruction: 0.142705, Regularization: 0.007922\n",
      "2019-04-10 01:15:00,648 root         INFO     Train Epoch: 156 [6656/8000 (83%)]\tTotal Loss: 0.142387\n",
      "Reconstruction: 0.136770, Regularization: 0.005617\n",
      "2019-04-10 01:15:00,705 root         INFO     Train Epoch: 156 [7168/8000 (90%)]\tTotal Loss: 0.141909\n",
      "Reconstruction: 0.136513, Regularization: 0.005397\n",
      "2019-04-10 01:15:00,763 root         INFO     Train Epoch: 156 [7680/8000 (96%)]\tTotal Loss: 0.143953\n",
      "Reconstruction: 0.140009, Regularization: 0.003944\n",
      "2019-04-10 01:15:00,813 root         INFO     ====> Epoch: 156 Average loss: 0.1495\n",
      "2019-04-10 01:15:00,837 root         INFO     Train Epoch: 157 [0/8000 (0%)]\tTotal Loss: 0.140500\n",
      "Reconstruction: 0.133843, Regularization: 0.006657\n",
      "2019-04-10 01:15:00,895 root         INFO     Train Epoch: 157 [512/8000 (6%)]\tTotal Loss: 0.166647\n",
      "Reconstruction: 0.158768, Regularization: 0.007879\n",
      "2019-04-10 01:15:00,953 root         INFO     Train Epoch: 157 [1024/8000 (13%)]\tTotal Loss: 0.156572\n",
      "Reconstruction: 0.151622, Regularization: 0.004950\n",
      "2019-04-10 01:15:01,011 root         INFO     Train Epoch: 157 [1536/8000 (19%)]\tTotal Loss: 0.146040\n",
      "Reconstruction: 0.142423, Regularization: 0.003616\n",
      "2019-04-10 01:15:01,068 root         INFO     Train Epoch: 157 [2048/8000 (26%)]\tTotal Loss: 0.144975\n",
      "Reconstruction: 0.140040, Regularization: 0.004935\n",
      "2019-04-10 01:15:01,127 root         INFO     Train Epoch: 157 [2560/8000 (32%)]\tTotal Loss: 0.142266\n",
      "Reconstruction: 0.136942, Regularization: 0.005324\n",
      "2019-04-10 01:15:01,188 root         INFO     Train Epoch: 157 [3072/8000 (38%)]\tTotal Loss: 0.157050\n",
      "Reconstruction: 0.149929, Regularization: 0.007121\n",
      "2019-04-10 01:15:01,247 root         INFO     Train Epoch: 157 [3584/8000 (45%)]\tTotal Loss: 0.160409\n",
      "Reconstruction: 0.153287, Regularization: 0.007122\n",
      "2019-04-10 01:15:01,304 root         INFO     Train Epoch: 157 [4096/8000 (51%)]\tTotal Loss: 0.160880\n",
      "Reconstruction: 0.153830, Regularization: 0.007050\n",
      "2019-04-10 01:15:01,361 root         INFO     Train Epoch: 157 [4608/8000 (58%)]\tTotal Loss: 0.149850\n",
      "Reconstruction: 0.145087, Regularization: 0.004763\n",
      "2019-04-10 01:15:01,419 root         INFO     Train Epoch: 157 [5120/8000 (64%)]\tTotal Loss: 0.143272\n",
      "Reconstruction: 0.138165, Regularization: 0.005106\n",
      "2019-04-10 01:15:01,477 root         INFO     Train Epoch: 157 [5632/8000 (70%)]\tTotal Loss: 0.148990\n",
      "Reconstruction: 0.140686, Regularization: 0.008305\n",
      "2019-04-10 01:15:01,533 root         INFO     Train Epoch: 157 [6144/8000 (77%)]\tTotal Loss: 0.154102\n",
      "Reconstruction: 0.147432, Regularization: 0.006670\n",
      "2019-04-10 01:15:01,588 root         INFO     Train Epoch: 157 [6656/8000 (83%)]\tTotal Loss: 0.167909\n",
      "Reconstruction: 0.156634, Regularization: 0.011275\n",
      "2019-04-10 01:15:01,644 root         INFO     Train Epoch: 157 [7168/8000 (90%)]\tTotal Loss: 0.146398\n",
      "Reconstruction: 0.140236, Regularization: 0.006161\n",
      "2019-04-10 01:15:01,700 root         INFO     Train Epoch: 157 [7680/8000 (96%)]\tTotal Loss: 0.143610\n",
      "Reconstruction: 0.140196, Regularization: 0.003414\n",
      "2019-04-10 01:15:01,749 root         INFO     ====> Epoch: 157 Average loss: 0.1499\n",
      "2019-04-10 01:15:01,772 root         INFO     Train Epoch: 158 [0/8000 (0%)]\tTotal Loss: 0.157150\n",
      "Reconstruction: 0.149921, Regularization: 0.007229\n",
      "2019-04-10 01:15:01,829 root         INFO     Train Epoch: 158 [512/8000 (6%)]\tTotal Loss: 0.139724\n",
      "Reconstruction: 0.135018, Regularization: 0.004706\n",
      "2019-04-10 01:15:01,887 root         INFO     Train Epoch: 158 [1024/8000 (13%)]\tTotal Loss: 0.138035\n",
      "Reconstruction: 0.133266, Regularization: 0.004769\n",
      "2019-04-10 01:15:01,945 root         INFO     Train Epoch: 158 [1536/8000 (19%)]\tTotal Loss: 0.137658\n",
      "Reconstruction: 0.132233, Regularization: 0.005425\n",
      "2019-04-10 01:15:02,002 root         INFO     Train Epoch: 158 [2048/8000 (26%)]\tTotal Loss: 0.144955\n",
      "Reconstruction: 0.140735, Regularization: 0.004220\n",
      "2019-04-10 01:15:02,056 root         INFO     Train Epoch: 158 [2560/8000 (32%)]\tTotal Loss: 0.158771\n",
      "Reconstruction: 0.155081, Regularization: 0.003691\n",
      "2019-04-10 01:15:02,110 root         INFO     Train Epoch: 158 [3072/8000 (38%)]\tTotal Loss: 0.161136\n",
      "Reconstruction: 0.157187, Regularization: 0.003949\n",
      "2019-04-10 01:15:02,165 root         INFO     Train Epoch: 158 [3584/8000 (45%)]\tTotal Loss: 0.150844\n",
      "Reconstruction: 0.142864, Regularization: 0.007980\n",
      "2019-04-10 01:15:02,219 root         INFO     Train Epoch: 158 [4096/8000 (51%)]\tTotal Loss: 0.141924\n",
      "Reconstruction: 0.135378, Regularization: 0.006546\n",
      "2019-04-10 01:15:02,273 root         INFO     Train Epoch: 158 [4608/8000 (58%)]\tTotal Loss: 0.164784\n",
      "Reconstruction: 0.159151, Regularization: 0.005634\n",
      "2019-04-10 01:15:02,327 root         INFO     Train Epoch: 158 [5120/8000 (64%)]\tTotal Loss: 0.141917\n",
      "Reconstruction: 0.137000, Regularization: 0.004917\n",
      "2019-04-10 01:15:02,381 root         INFO     Train Epoch: 158 [5632/8000 (70%)]\tTotal Loss: 0.146305\n",
      "Reconstruction: 0.139427, Regularization: 0.006879\n",
      "2019-04-10 01:15:02,436 root         INFO     Train Epoch: 158 [6144/8000 (77%)]\tTotal Loss: 0.153649\n",
      "Reconstruction: 0.149771, Regularization: 0.003878\n",
      "2019-04-10 01:15:02,490 root         INFO     Train Epoch: 158 [6656/8000 (83%)]\tTotal Loss: 0.159548\n",
      "Reconstruction: 0.154898, Regularization: 0.004650\n",
      "2019-04-10 01:15:02,545 root         INFO     Train Epoch: 158 [7168/8000 (90%)]\tTotal Loss: 0.158035\n",
      "Reconstruction: 0.151251, Regularization: 0.006785\n",
      "2019-04-10 01:15:02,601 root         INFO     Train Epoch: 158 [7680/8000 (96%)]\tTotal Loss: 0.155283\n",
      "Reconstruction: 0.148710, Regularization: 0.006573\n",
      "2019-04-10 01:15:02,651 root         INFO     ====> Epoch: 158 Average loss: 0.1494\n",
      "2019-04-10 01:15:02,674 root         INFO     Train Epoch: 159 [0/8000 (0%)]\tTotal Loss: 0.152321\n",
      "Reconstruction: 0.145526, Regularization: 0.006796\n",
      "2019-04-10 01:15:02,731 root         INFO     Train Epoch: 159 [512/8000 (6%)]\tTotal Loss: 0.138930\n",
      "Reconstruction: 0.134336, Regularization: 0.004594\n",
      "2019-04-10 01:15:02,789 root         INFO     Train Epoch: 159 [1024/8000 (13%)]\tTotal Loss: 0.146524\n",
      "Reconstruction: 0.137775, Regularization: 0.008749\n",
      "2019-04-10 01:15:02,845 root         INFO     Train Epoch: 159 [1536/8000 (19%)]\tTotal Loss: 0.149544\n",
      "Reconstruction: 0.140590, Regularization: 0.008955\n",
      "2019-04-10 01:15:02,900 root         INFO     Train Epoch: 159 [2048/8000 (26%)]\tTotal Loss: 0.148683\n",
      "Reconstruction: 0.141373, Regularization: 0.007309\n",
      "2019-04-10 01:15:02,957 root         INFO     Train Epoch: 159 [2560/8000 (32%)]\tTotal Loss: 0.142479\n",
      "Reconstruction: 0.138434, Regularization: 0.004045\n",
      "2019-04-10 01:15:03,014 root         INFO     Train Epoch: 159 [3072/8000 (38%)]\tTotal Loss: 0.166487\n",
      "Reconstruction: 0.159840, Regularization: 0.006648\n",
      "2019-04-10 01:15:03,071 root         INFO     Train Epoch: 159 [3584/8000 (45%)]\tTotal Loss: 0.138495\n",
      "Reconstruction: 0.135020, Regularization: 0.003475\n",
      "2019-04-10 01:15:03,128 root         INFO     Train Epoch: 159 [4096/8000 (51%)]\tTotal Loss: 0.151406\n",
      "Reconstruction: 0.145922, Regularization: 0.005484\n",
      "2019-04-10 01:15:03,184 root         INFO     Train Epoch: 159 [4608/8000 (58%)]\tTotal Loss: 0.145528\n",
      "Reconstruction: 0.141146, Regularization: 0.004382\n",
      "2019-04-10 01:15:03,241 root         INFO     Train Epoch: 159 [5120/8000 (64%)]\tTotal Loss: 0.163509\n",
      "Reconstruction: 0.157561, Regularization: 0.005948\n",
      "2019-04-10 01:15:03,296 root         INFO     Train Epoch: 159 [5632/8000 (70%)]\tTotal Loss: 0.152500\n",
      "Reconstruction: 0.148573, Regularization: 0.003927\n",
      "2019-04-10 01:15:03,352 root         INFO     Train Epoch: 159 [6144/8000 (77%)]\tTotal Loss: 0.145096\n",
      "Reconstruction: 0.139477, Regularization: 0.005618\n",
      "2019-04-10 01:15:03,409 root         INFO     Train Epoch: 159 [6656/8000 (83%)]\tTotal Loss: 0.153292\n",
      "Reconstruction: 0.145344, Regularization: 0.007948\n",
      "2019-04-10 01:15:03,466 root         INFO     Train Epoch: 159 [7168/8000 (90%)]\tTotal Loss: 0.163152\n",
      "Reconstruction: 0.154016, Regularization: 0.009136\n",
      "2019-04-10 01:15:03,523 root         INFO     Train Epoch: 159 [7680/8000 (96%)]\tTotal Loss: 0.153779\n",
      "Reconstruction: 0.144127, Regularization: 0.009652\n",
      "2019-04-10 01:15:03,573 root         INFO     ====> Epoch: 159 Average loss: 0.1496\n",
      "2019-04-10 01:15:03,597 root         INFO     Train Epoch: 160 [0/8000 (0%)]\tTotal Loss: 0.147865\n",
      "Reconstruction: 0.140890, Regularization: 0.006975\n",
      "2019-04-10 01:15:03,654 root         INFO     Train Epoch: 160 [512/8000 (6%)]\tTotal Loss: 0.143258\n",
      "Reconstruction: 0.137969, Regularization: 0.005289\n",
      "2019-04-10 01:15:03,711 root         INFO     Train Epoch: 160 [1024/8000 (13%)]\tTotal Loss: 0.151868\n",
      "Reconstruction: 0.147260, Regularization: 0.004608\n",
      "2019-04-10 01:15:03,768 root         INFO     Train Epoch: 160 [1536/8000 (19%)]\tTotal Loss: 0.147667\n",
      "Reconstruction: 0.139845, Regularization: 0.007822\n",
      "2019-04-10 01:15:03,824 root         INFO     Train Epoch: 160 [2048/8000 (26%)]\tTotal Loss: 0.140373\n",
      "Reconstruction: 0.135251, Regularization: 0.005121\n",
      "2019-04-10 01:15:03,881 root         INFO     Train Epoch: 160 [2560/8000 (32%)]\tTotal Loss: 0.151768\n",
      "Reconstruction: 0.147327, Regularization: 0.004441\n",
      "2019-04-10 01:15:03,937 root         INFO     Train Epoch: 160 [3072/8000 (38%)]\tTotal Loss: 0.138104\n",
      "Reconstruction: 0.133354, Regularization: 0.004750\n",
      "2019-04-10 01:15:03,994 root         INFO     Train Epoch: 160 [3584/8000 (45%)]\tTotal Loss: 0.155973\n",
      "Reconstruction: 0.149710, Regularization: 0.006263\n",
      "2019-04-10 01:15:04,050 root         INFO     Train Epoch: 160 [4096/8000 (51%)]\tTotal Loss: 0.135459\n",
      "Reconstruction: 0.132366, Regularization: 0.003093\n",
      "2019-04-10 01:15:04,107 root         INFO     Train Epoch: 160 [4608/8000 (58%)]\tTotal Loss: 0.149098\n",
      "Reconstruction: 0.142358, Regularization: 0.006740\n",
      "2019-04-10 01:15:04,164 root         INFO     Train Epoch: 160 [5120/8000 (64%)]\tTotal Loss: 0.151779\n",
      "Reconstruction: 0.145291, Regularization: 0.006487\n",
      "2019-04-10 01:15:04,220 root         INFO     Train Epoch: 160 [5632/8000 (70%)]\tTotal Loss: 0.143253\n",
      "Reconstruction: 0.137925, Regularization: 0.005328\n",
      "2019-04-10 01:15:04,277 root         INFO     Train Epoch: 160 [6144/8000 (77%)]\tTotal Loss: 0.141914\n",
      "Reconstruction: 0.135186, Regularization: 0.006728\n",
      "2019-04-10 01:15:04,333 root         INFO     Train Epoch: 160 [6656/8000 (83%)]\tTotal Loss: 0.165511\n",
      "Reconstruction: 0.158346, Regularization: 0.007165\n",
      "2019-04-10 01:15:04,391 root         INFO     Train Epoch: 160 [7168/8000 (90%)]\tTotal Loss: 0.155902\n",
      "Reconstruction: 0.150286, Regularization: 0.005617\n",
      "2019-04-10 01:15:04,448 root         INFO     Train Epoch: 160 [7680/8000 (96%)]\tTotal Loss: 0.146531\n",
      "Reconstruction: 0.142476, Regularization: 0.004055\n",
      "2019-04-10 01:15:04,498 root         INFO     ====> Epoch: 160 Average loss: 0.1496\n",
      "2019-04-10 01:15:04,522 root         INFO     Train Epoch: 161 [0/8000 (0%)]\tTotal Loss: 0.155565\n",
      "Reconstruction: 0.148279, Regularization: 0.007286\n",
      "2019-04-10 01:15:04,579 root         INFO     Train Epoch: 161 [512/8000 (6%)]\tTotal Loss: 0.143421\n",
      "Reconstruction: 0.138899, Regularization: 0.004522\n",
      "2019-04-10 01:15:04,636 root         INFO     Train Epoch: 161 [1024/8000 (13%)]\tTotal Loss: 0.155686\n",
      "Reconstruction: 0.148951, Regularization: 0.006735\n",
      "2019-04-10 01:15:04,694 root         INFO     Train Epoch: 161 [1536/8000 (19%)]\tTotal Loss: 0.150884\n",
      "Reconstruction: 0.141851, Regularization: 0.009033\n",
      "2019-04-10 01:15:04,752 root         INFO     Train Epoch: 161 [2048/8000 (26%)]\tTotal Loss: 0.145332\n",
      "Reconstruction: 0.139605, Regularization: 0.005727\n",
      "2019-04-10 01:15:04,808 root         INFO     Train Epoch: 161 [2560/8000 (32%)]\tTotal Loss: 0.145667\n",
      "Reconstruction: 0.139169, Regularization: 0.006499\n",
      "2019-04-10 01:15:04,865 root         INFO     Train Epoch: 161 [3072/8000 (38%)]\tTotal Loss: 0.157233\n",
      "Reconstruction: 0.149588, Regularization: 0.007645\n",
      "2019-04-10 01:15:04,926 root         INFO     Train Epoch: 161 [3584/8000 (45%)]\tTotal Loss: 0.163123\n",
      "Reconstruction: 0.159291, Regularization: 0.003832\n",
      "2019-04-10 01:15:04,984 root         INFO     Train Epoch: 161 [4096/8000 (51%)]\tTotal Loss: 0.143086\n",
      "Reconstruction: 0.139946, Regularization: 0.003140\n",
      "2019-04-10 01:15:05,047 root         INFO     Train Epoch: 161 [4608/8000 (58%)]\tTotal Loss: 0.143965\n",
      "Reconstruction: 0.139924, Regularization: 0.004041\n",
      "2019-04-10 01:15:05,132 root         INFO     Train Epoch: 161 [5120/8000 (64%)]\tTotal Loss: 0.144707\n",
      "Reconstruction: 0.140377, Regularization: 0.004330\n",
      "2019-04-10 01:15:05,214 root         INFO     Train Epoch: 161 [5632/8000 (70%)]\tTotal Loss: 0.140820\n",
      "Reconstruction: 0.135005, Regularization: 0.005815\n",
      "2019-04-10 01:15:05,273 root         INFO     Train Epoch: 161 [6144/8000 (77%)]\tTotal Loss: 0.147165\n",
      "Reconstruction: 0.142055, Regularization: 0.005110\n",
      "2019-04-10 01:15:05,332 root         INFO     Train Epoch: 161 [6656/8000 (83%)]\tTotal Loss: 0.151699\n",
      "Reconstruction: 0.145888, Regularization: 0.005810\n",
      "2019-04-10 01:15:05,391 root         INFO     Train Epoch: 161 [7168/8000 (90%)]\tTotal Loss: 0.132703\n",
      "Reconstruction: 0.129034, Regularization: 0.003669\n",
      "2019-04-10 01:15:05,450 root         INFO     Train Epoch: 161 [7680/8000 (96%)]\tTotal Loss: 0.142820\n",
      "Reconstruction: 0.138219, Regularization: 0.004601\n",
      "2019-04-10 01:15:05,506 root         INFO     ====> Epoch: 161 Average loss: 0.1496\n",
      "2019-04-10 01:15:05,530 root         INFO     Train Epoch: 162 [0/8000 (0%)]\tTotal Loss: 0.150202\n",
      "Reconstruction: 0.144530, Regularization: 0.005672\n",
      "2019-04-10 01:15:05,589 root         INFO     Train Epoch: 162 [512/8000 (6%)]\tTotal Loss: 0.144673\n",
      "Reconstruction: 0.137721, Regularization: 0.006951\n",
      "2019-04-10 01:15:05,648 root         INFO     Train Epoch: 162 [1024/8000 (13%)]\tTotal Loss: 0.146163\n",
      "Reconstruction: 0.142488, Regularization: 0.003676\n",
      "2019-04-10 01:15:05,707 root         INFO     Train Epoch: 162 [1536/8000 (19%)]\tTotal Loss: 0.149262\n",
      "Reconstruction: 0.142395, Regularization: 0.006867\n",
      "2019-04-10 01:15:05,766 root         INFO     Train Epoch: 162 [2048/8000 (26%)]\tTotal Loss: 0.142377\n",
      "Reconstruction: 0.138067, Regularization: 0.004309\n",
      "2019-04-10 01:15:05,825 root         INFO     Train Epoch: 162 [2560/8000 (32%)]\tTotal Loss: 0.145179\n",
      "Reconstruction: 0.138540, Regularization: 0.006640\n",
      "2019-04-10 01:15:05,883 root         INFO     Train Epoch: 162 [3072/8000 (38%)]\tTotal Loss: 0.149464\n",
      "Reconstruction: 0.141599, Regularization: 0.007865\n",
      "2019-04-10 01:15:05,942 root         INFO     Train Epoch: 162 [3584/8000 (45%)]\tTotal Loss: 0.133443\n",
      "Reconstruction: 0.130516, Regularization: 0.002927\n",
      "2019-04-10 01:15:06,001 root         INFO     Train Epoch: 162 [4096/8000 (51%)]\tTotal Loss: 0.151947\n",
      "Reconstruction: 0.143870, Regularization: 0.008077\n",
      "2019-04-10 01:15:06,056 root         INFO     Train Epoch: 162 [4608/8000 (58%)]\tTotal Loss: 0.163044\n",
      "Reconstruction: 0.155562, Regularization: 0.007482\n",
      "2019-04-10 01:15:06,111 root         INFO     Train Epoch: 162 [5120/8000 (64%)]\tTotal Loss: 0.138248\n",
      "Reconstruction: 0.132997, Regularization: 0.005252\n",
      "2019-04-10 01:15:06,165 root         INFO     Train Epoch: 162 [5632/8000 (70%)]\tTotal Loss: 0.153055\n",
      "Reconstruction: 0.148498, Regularization: 0.004556\n",
      "2019-04-10 01:15:06,220 root         INFO     Train Epoch: 162 [6144/8000 (77%)]\tTotal Loss: 0.144949\n",
      "Reconstruction: 0.140336, Regularization: 0.004613\n",
      "2019-04-10 01:15:06,275 root         INFO     Train Epoch: 162 [6656/8000 (83%)]\tTotal Loss: 0.152223\n",
      "Reconstruction: 0.145881, Regularization: 0.006342\n",
      "2019-04-10 01:15:06,330 root         INFO     Train Epoch: 162 [7168/8000 (90%)]\tTotal Loss: 0.157087\n",
      "Reconstruction: 0.150206, Regularization: 0.006881\n",
      "2019-04-10 01:15:06,385 root         INFO     Train Epoch: 162 [7680/8000 (96%)]\tTotal Loss: 0.140821\n",
      "Reconstruction: 0.135550, Regularization: 0.005272\n",
      "2019-04-10 01:15:06,435 root         INFO     ====> Epoch: 162 Average loss: 0.1499\n",
      "2019-04-10 01:15:06,459 root         INFO     Train Epoch: 163 [0/8000 (0%)]\tTotal Loss: 0.140192\n",
      "Reconstruction: 0.136676, Regularization: 0.003516\n",
      "2019-04-10 01:15:06,516 root         INFO     Train Epoch: 163 [512/8000 (6%)]\tTotal Loss: 0.154944\n",
      "Reconstruction: 0.148577, Regularization: 0.006367\n",
      "2019-04-10 01:15:06,572 root         INFO     Train Epoch: 163 [1024/8000 (13%)]\tTotal Loss: 0.144424\n",
      "Reconstruction: 0.138647, Regularization: 0.005777\n",
      "2019-04-10 01:15:06,628 root         INFO     Train Epoch: 163 [1536/8000 (19%)]\tTotal Loss: 0.157067\n",
      "Reconstruction: 0.151969, Regularization: 0.005097\n",
      "2019-04-10 01:15:06,685 root         INFO     Train Epoch: 163 [2048/8000 (26%)]\tTotal Loss: 0.137033\n",
      "Reconstruction: 0.133894, Regularization: 0.003139\n",
      "2019-04-10 01:15:06,741 root         INFO     Train Epoch: 163 [2560/8000 (32%)]\tTotal Loss: 0.145125\n",
      "Reconstruction: 0.138823, Regularization: 0.006302\n",
      "2019-04-10 01:15:06,797 root         INFO     Train Epoch: 163 [3072/8000 (38%)]\tTotal Loss: 0.142752\n",
      "Reconstruction: 0.139206, Regularization: 0.003546\n",
      "2019-04-10 01:15:06,853 root         INFO     Train Epoch: 163 [3584/8000 (45%)]\tTotal Loss: 0.150763\n",
      "Reconstruction: 0.146183, Regularization: 0.004580\n",
      "2019-04-10 01:15:06,909 root         INFO     Train Epoch: 163 [4096/8000 (51%)]\tTotal Loss: 0.149784\n",
      "Reconstruction: 0.143290, Regularization: 0.006494\n",
      "2019-04-10 01:15:06,965 root         INFO     Train Epoch: 163 [4608/8000 (58%)]\tTotal Loss: 0.159668\n",
      "Reconstruction: 0.149239, Regularization: 0.010430\n",
      "2019-04-10 01:15:07,022 root         INFO     Train Epoch: 163 [5120/8000 (64%)]\tTotal Loss: 0.148564\n",
      "Reconstruction: 0.143746, Regularization: 0.004818\n",
      "2019-04-10 01:15:07,078 root         INFO     Train Epoch: 163 [5632/8000 (70%)]\tTotal Loss: 0.146661\n",
      "Reconstruction: 0.137535, Regularization: 0.009126\n",
      "2019-04-10 01:15:07,134 root         INFO     Train Epoch: 163 [6144/8000 (77%)]\tTotal Loss: 0.148240\n",
      "Reconstruction: 0.141489, Regularization: 0.006751\n",
      "2019-04-10 01:15:07,190 root         INFO     Train Epoch: 163 [6656/8000 (83%)]\tTotal Loss: 0.146404\n",
      "Reconstruction: 0.142168, Regularization: 0.004235\n",
      "2019-04-10 01:15:07,246 root         INFO     Train Epoch: 163 [7168/8000 (90%)]\tTotal Loss: 0.131380\n",
      "Reconstruction: 0.126308, Regularization: 0.005072\n",
      "2019-04-10 01:15:07,303 root         INFO     Train Epoch: 163 [7680/8000 (96%)]\tTotal Loss: 0.135144\n",
      "Reconstruction: 0.130042, Regularization: 0.005102\n",
      "2019-04-10 01:15:07,354 root         INFO     ====> Epoch: 163 Average loss: 0.1496\n",
      "2019-04-10 01:15:07,377 root         INFO     Train Epoch: 164 [0/8000 (0%)]\tTotal Loss: 0.159906\n",
      "Reconstruction: 0.152221, Regularization: 0.007686\n",
      "2019-04-10 01:15:07,433 root         INFO     Train Epoch: 164 [512/8000 (6%)]\tTotal Loss: 0.140053\n",
      "Reconstruction: 0.133451, Regularization: 0.006601\n",
      "2019-04-10 01:15:07,490 root         INFO     Train Epoch: 164 [1024/8000 (13%)]\tTotal Loss: 0.142004\n",
      "Reconstruction: 0.137778, Regularization: 0.004226\n",
      "2019-04-10 01:15:07,546 root         INFO     Train Epoch: 164 [1536/8000 (19%)]\tTotal Loss: 0.147733\n",
      "Reconstruction: 0.140592, Regularization: 0.007141\n",
      "2019-04-10 01:15:07,602 root         INFO     Train Epoch: 164 [2048/8000 (26%)]\tTotal Loss: 0.142757\n",
      "Reconstruction: 0.138971, Regularization: 0.003786\n",
      "2019-04-10 01:15:07,658 root         INFO     Train Epoch: 164 [2560/8000 (32%)]\tTotal Loss: 0.150329\n",
      "Reconstruction: 0.141316, Regularization: 0.009012\n",
      "2019-04-10 01:15:07,714 root         INFO     Train Epoch: 164 [3072/8000 (38%)]\tTotal Loss: 0.151248\n",
      "Reconstruction: 0.146545, Regularization: 0.004703\n",
      "2019-04-10 01:15:07,770 root         INFO     Train Epoch: 164 [3584/8000 (45%)]\tTotal Loss: 0.147365\n",
      "Reconstruction: 0.141419, Regularization: 0.005946\n",
      "2019-04-10 01:15:07,825 root         INFO     Train Epoch: 164 [4096/8000 (51%)]\tTotal Loss: 0.150643\n",
      "Reconstruction: 0.147306, Regularization: 0.003337\n",
      "2019-04-10 01:15:07,881 root         INFO     Train Epoch: 164 [4608/8000 (58%)]\tTotal Loss: 0.143783\n",
      "Reconstruction: 0.137502, Regularization: 0.006281\n",
      "2019-04-10 01:15:07,937 root         INFO     Train Epoch: 164 [5120/8000 (64%)]\tTotal Loss: 0.137168\n",
      "Reconstruction: 0.133318, Regularization: 0.003850\n",
      "2019-04-10 01:15:07,993 root         INFO     Train Epoch: 164 [5632/8000 (70%)]\tTotal Loss: 0.142720\n",
      "Reconstruction: 0.137783, Regularization: 0.004937\n",
      "2019-04-10 01:15:08,049 root         INFO     Train Epoch: 164 [6144/8000 (77%)]\tTotal Loss: 0.158861\n",
      "Reconstruction: 0.152900, Regularization: 0.005961\n",
      "2019-04-10 01:15:08,106 root         INFO     Train Epoch: 164 [6656/8000 (83%)]\tTotal Loss: 0.154484\n",
      "Reconstruction: 0.148286, Regularization: 0.006198\n",
      "2019-04-10 01:15:08,163 root         INFO     Train Epoch: 164 [7168/8000 (90%)]\tTotal Loss: 0.142829\n",
      "Reconstruction: 0.138611, Regularization: 0.004218\n",
      "2019-04-10 01:15:08,220 root         INFO     Train Epoch: 164 [7680/8000 (96%)]\tTotal Loss: 0.151480\n",
      "Reconstruction: 0.145700, Regularization: 0.005780\n",
      "2019-04-10 01:15:08,271 root         INFO     ====> Epoch: 164 Average loss: 0.1498\n",
      "2019-04-10 01:15:08,294 root         INFO     Train Epoch: 165 [0/8000 (0%)]\tTotal Loss: 0.154453\n",
      "Reconstruction: 0.149604, Regularization: 0.004848\n",
      "2019-04-10 01:15:08,351 root         INFO     Train Epoch: 165 [512/8000 (6%)]\tTotal Loss: 0.141932\n",
      "Reconstruction: 0.138617, Regularization: 0.003315\n",
      "2019-04-10 01:15:08,407 root         INFO     Train Epoch: 165 [1024/8000 (13%)]\tTotal Loss: 0.156961\n",
      "Reconstruction: 0.149175, Regularization: 0.007786\n",
      "2019-04-10 01:15:08,463 root         INFO     Train Epoch: 165 [1536/8000 (19%)]\tTotal Loss: 0.142849\n",
      "Reconstruction: 0.137447, Regularization: 0.005402\n",
      "2019-04-10 01:15:08,519 root         INFO     Train Epoch: 165 [2048/8000 (26%)]\tTotal Loss: 0.147828\n",
      "Reconstruction: 0.141212, Regularization: 0.006616\n",
      "2019-04-10 01:15:08,575 root         INFO     Train Epoch: 165 [2560/8000 (32%)]\tTotal Loss: 0.151017\n",
      "Reconstruction: 0.142835, Regularization: 0.008182\n",
      "2019-04-10 01:15:08,631 root         INFO     Train Epoch: 165 [3072/8000 (38%)]\tTotal Loss: 0.152796\n",
      "Reconstruction: 0.146108, Regularization: 0.006689\n",
      "2019-04-10 01:15:08,686 root         INFO     Train Epoch: 165 [3584/8000 (45%)]\tTotal Loss: 0.141531\n",
      "Reconstruction: 0.138723, Regularization: 0.002808\n",
      "2019-04-10 01:15:08,742 root         INFO     Train Epoch: 165 [4096/8000 (51%)]\tTotal Loss: 0.143356\n",
      "Reconstruction: 0.138205, Regularization: 0.005151\n",
      "2019-04-10 01:15:08,798 root         INFO     Train Epoch: 165 [4608/8000 (58%)]\tTotal Loss: 0.144182\n",
      "Reconstruction: 0.134933, Regularization: 0.009250\n",
      "2019-04-10 01:15:08,853 root         INFO     Train Epoch: 165 [5120/8000 (64%)]\tTotal Loss: 0.165864\n",
      "Reconstruction: 0.160066, Regularization: 0.005798\n",
      "2019-04-10 01:15:08,909 root         INFO     Train Epoch: 165 [5632/8000 (70%)]\tTotal Loss: 0.160086\n",
      "Reconstruction: 0.151243, Regularization: 0.008844\n",
      "2019-04-10 01:15:08,965 root         INFO     Train Epoch: 165 [6144/8000 (77%)]\tTotal Loss: 0.133555\n",
      "Reconstruction: 0.130379, Regularization: 0.003177\n",
      "2019-04-10 01:15:09,021 root         INFO     Train Epoch: 165 [6656/8000 (83%)]\tTotal Loss: 0.144552\n",
      "Reconstruction: 0.139180, Regularization: 0.005371\n",
      "2019-04-10 01:15:09,077 root         INFO     Train Epoch: 165 [7168/8000 (90%)]\tTotal Loss: 0.154700\n",
      "Reconstruction: 0.147583, Regularization: 0.007117\n",
      "2019-04-10 01:15:09,133 root         INFO     Train Epoch: 165 [7680/8000 (96%)]\tTotal Loss: 0.139730\n",
      "Reconstruction: 0.134754, Regularization: 0.004976\n",
      "2019-04-10 01:15:09,182 root         INFO     ====> Epoch: 165 Average loss: 0.1494\n",
      "2019-04-10 01:15:09,206 root         INFO     Train Epoch: 166 [0/8000 (0%)]\tTotal Loss: 0.140012\n",
      "Reconstruction: 0.135858, Regularization: 0.004154\n",
      "2019-04-10 01:15:09,262 root         INFO     Train Epoch: 166 [512/8000 (6%)]\tTotal Loss: 0.141712\n",
      "Reconstruction: 0.136191, Regularization: 0.005521\n",
      "2019-04-10 01:15:09,318 root         INFO     Train Epoch: 166 [1024/8000 (13%)]\tTotal Loss: 0.137579\n",
      "Reconstruction: 0.133815, Regularization: 0.003763\n",
      "2019-04-10 01:15:09,374 root         INFO     Train Epoch: 166 [1536/8000 (19%)]\tTotal Loss: 0.169996\n",
      "Reconstruction: 0.161356, Regularization: 0.008640\n",
      "2019-04-10 01:15:09,430 root         INFO     Train Epoch: 166 [2048/8000 (26%)]\tTotal Loss: 0.157344\n",
      "Reconstruction: 0.151116, Regularization: 0.006228\n",
      "2019-04-10 01:15:09,486 root         INFO     Train Epoch: 166 [2560/8000 (32%)]\tTotal Loss: 0.153030\n",
      "Reconstruction: 0.148693, Regularization: 0.004337\n",
      "2019-04-10 01:15:09,542 root         INFO     Train Epoch: 166 [3072/8000 (38%)]\tTotal Loss: 0.146463\n",
      "Reconstruction: 0.138950, Regularization: 0.007513\n",
      "2019-04-10 01:15:09,599 root         INFO     Train Epoch: 166 [3584/8000 (45%)]\tTotal Loss: 0.164950\n",
      "Reconstruction: 0.157236, Regularization: 0.007714\n",
      "2019-04-10 01:15:09,654 root         INFO     Train Epoch: 166 [4096/8000 (51%)]\tTotal Loss: 0.157503\n",
      "Reconstruction: 0.150459, Regularization: 0.007044\n",
      "2019-04-10 01:15:09,711 root         INFO     Train Epoch: 166 [4608/8000 (58%)]\tTotal Loss: 0.147288\n",
      "Reconstruction: 0.142657, Regularization: 0.004631\n",
      "2019-04-10 01:15:09,767 root         INFO     Train Epoch: 166 [5120/8000 (64%)]\tTotal Loss: 0.152779\n",
      "Reconstruction: 0.146053, Regularization: 0.006727\n",
      "2019-04-10 01:15:09,823 root         INFO     Train Epoch: 166 [5632/8000 (70%)]\tTotal Loss: 0.144692\n",
      "Reconstruction: 0.139316, Regularization: 0.005377\n",
      "2019-04-10 01:15:09,879 root         INFO     Train Epoch: 166 [6144/8000 (77%)]\tTotal Loss: 0.140313\n",
      "Reconstruction: 0.135979, Regularization: 0.004334\n",
      "2019-04-10 01:15:09,935 root         INFO     Train Epoch: 166 [6656/8000 (83%)]\tTotal Loss: 0.143501\n",
      "Reconstruction: 0.137958, Regularization: 0.005543\n",
      "2019-04-10 01:15:09,991 root         INFO     Train Epoch: 166 [7168/8000 (90%)]\tTotal Loss: 0.141498\n",
      "Reconstruction: 0.137454, Regularization: 0.004044\n",
      "2019-04-10 01:15:10,047 root         INFO     Train Epoch: 166 [7680/8000 (96%)]\tTotal Loss: 0.155431\n",
      "Reconstruction: 0.149757, Regularization: 0.005674\n",
      "2019-04-10 01:15:10,097 root         INFO     ====> Epoch: 166 Average loss: 0.1499\n",
      "2019-04-10 01:15:10,121 root         INFO     Train Epoch: 167 [0/8000 (0%)]\tTotal Loss: 0.143843\n",
      "Reconstruction: 0.139443, Regularization: 0.004400\n",
      "2019-04-10 01:15:10,179 root         INFO     Train Epoch: 167 [512/8000 (6%)]\tTotal Loss: 0.156809\n",
      "Reconstruction: 0.149040, Regularization: 0.007768\n",
      "2019-04-10 01:15:10,236 root         INFO     Train Epoch: 167 [1024/8000 (13%)]\tTotal Loss: 0.149850\n",
      "Reconstruction: 0.145277, Regularization: 0.004573\n",
      "2019-04-10 01:15:10,293 root         INFO     Train Epoch: 167 [1536/8000 (19%)]\tTotal Loss: 0.156578\n",
      "Reconstruction: 0.150910, Regularization: 0.005668\n",
      "2019-04-10 01:15:10,351 root         INFO     Train Epoch: 167 [2048/8000 (26%)]\tTotal Loss: 0.156635\n",
      "Reconstruction: 0.148279, Regularization: 0.008356\n",
      "2019-04-10 01:15:10,408 root         INFO     Train Epoch: 167 [2560/8000 (32%)]\tTotal Loss: 0.151920\n",
      "Reconstruction: 0.142631, Regularization: 0.009290\n",
      "2019-04-10 01:15:10,465 root         INFO     Train Epoch: 167 [3072/8000 (38%)]\tTotal Loss: 0.151078\n",
      "Reconstruction: 0.145353, Regularization: 0.005725\n",
      "2019-04-10 01:15:10,522 root         INFO     Train Epoch: 167 [3584/8000 (45%)]\tTotal Loss: 0.145228\n",
      "Reconstruction: 0.138333, Regularization: 0.006895\n",
      "2019-04-10 01:15:10,579 root         INFO     Train Epoch: 167 [4096/8000 (51%)]\tTotal Loss: 0.138088\n",
      "Reconstruction: 0.134232, Regularization: 0.003856\n",
      "2019-04-10 01:15:10,636 root         INFO     Train Epoch: 167 [4608/8000 (58%)]\tTotal Loss: 0.141743\n",
      "Reconstruction: 0.137763, Regularization: 0.003980\n",
      "2019-04-10 01:15:10,693 root         INFO     Train Epoch: 167 [5120/8000 (64%)]\tTotal Loss: 0.149092\n",
      "Reconstruction: 0.143561, Regularization: 0.005531\n",
      "2019-04-10 01:15:10,750 root         INFO     Train Epoch: 167 [5632/8000 (70%)]\tTotal Loss: 0.167158\n",
      "Reconstruction: 0.158332, Regularization: 0.008826\n",
      "2019-04-10 01:15:10,808 root         INFO     Train Epoch: 167 [6144/8000 (77%)]\tTotal Loss: 0.156474\n",
      "Reconstruction: 0.148434, Regularization: 0.008041\n",
      "2019-04-10 01:15:10,865 root         INFO     Train Epoch: 167 [6656/8000 (83%)]\tTotal Loss: 0.160361\n",
      "Reconstruction: 0.154082, Regularization: 0.006279\n",
      "2019-04-10 01:15:10,922 root         INFO     Train Epoch: 167 [7168/8000 (90%)]\tTotal Loss: 0.171416\n",
      "Reconstruction: 0.164131, Regularization: 0.007285\n",
      "2019-04-10 01:15:10,978 root         INFO     Train Epoch: 167 [7680/8000 (96%)]\tTotal Loss: 0.135012\n",
      "Reconstruction: 0.130990, Regularization: 0.004023\n",
      "2019-04-10 01:15:11,029 root         INFO     ====> Epoch: 167 Average loss: 0.1496\n",
      "2019-04-10 01:15:11,052 root         INFO     Train Epoch: 168 [0/8000 (0%)]\tTotal Loss: 0.148031\n",
      "Reconstruction: 0.139913, Regularization: 0.008118\n",
      "2019-04-10 01:15:11,110 root         INFO     Train Epoch: 168 [512/8000 (6%)]\tTotal Loss: 0.159653\n",
      "Reconstruction: 0.151659, Regularization: 0.007993\n",
      "2019-04-10 01:15:11,167 root         INFO     Train Epoch: 168 [1024/8000 (13%)]\tTotal Loss: 0.154373\n",
      "Reconstruction: 0.148607, Regularization: 0.005766\n",
      "2019-04-10 01:15:11,224 root         INFO     Train Epoch: 168 [1536/8000 (19%)]\tTotal Loss: 0.147433\n",
      "Reconstruction: 0.141941, Regularization: 0.005492\n",
      "2019-04-10 01:15:11,282 root         INFO     Train Epoch: 168 [2048/8000 (26%)]\tTotal Loss: 0.136923\n",
      "Reconstruction: 0.133306, Regularization: 0.003616\n",
      "2019-04-10 01:15:11,339 root         INFO     Train Epoch: 168 [2560/8000 (32%)]\tTotal Loss: 0.148807\n",
      "Reconstruction: 0.144090, Regularization: 0.004717\n",
      "2019-04-10 01:15:11,396 root         INFO     Train Epoch: 168 [3072/8000 (38%)]\tTotal Loss: 0.145210\n",
      "Reconstruction: 0.139676, Regularization: 0.005534\n",
      "2019-04-10 01:15:11,453 root         INFO     Train Epoch: 168 [3584/8000 (45%)]\tTotal Loss: 0.140552\n",
      "Reconstruction: 0.135323, Regularization: 0.005230\n",
      "2019-04-10 01:15:11,511 root         INFO     Train Epoch: 168 [4096/8000 (51%)]\tTotal Loss: 0.156353\n",
      "Reconstruction: 0.150071, Regularization: 0.006282\n",
      "2019-04-10 01:15:11,568 root         INFO     Train Epoch: 168 [4608/8000 (58%)]\tTotal Loss: 0.146514\n",
      "Reconstruction: 0.141603, Regularization: 0.004911\n",
      "2019-04-10 01:15:11,624 root         INFO     Train Epoch: 168 [5120/8000 (64%)]\tTotal Loss: 0.141217\n",
      "Reconstruction: 0.137183, Regularization: 0.004034\n",
      "2019-04-10 01:15:11,680 root         INFO     Train Epoch: 168 [5632/8000 (70%)]\tTotal Loss: 0.139654\n",
      "Reconstruction: 0.134425, Regularization: 0.005229\n",
      "2019-04-10 01:15:11,736 root         INFO     Train Epoch: 168 [6144/8000 (77%)]\tTotal Loss: 0.145458\n",
      "Reconstruction: 0.140446, Regularization: 0.005013\n",
      "2019-04-10 01:15:11,793 root         INFO     Train Epoch: 168 [6656/8000 (83%)]\tTotal Loss: 0.138396\n",
      "Reconstruction: 0.132420, Regularization: 0.005976\n",
      "2019-04-10 01:15:11,849 root         INFO     Train Epoch: 168 [7168/8000 (90%)]\tTotal Loss: 0.157198\n",
      "Reconstruction: 0.150761, Regularization: 0.006437\n",
      "2019-04-10 01:15:11,905 root         INFO     Train Epoch: 168 [7680/8000 (96%)]\tTotal Loss: 0.143869\n",
      "Reconstruction: 0.140092, Regularization: 0.003777\n",
      "2019-04-10 01:15:11,955 root         INFO     ====> Epoch: 168 Average loss: 0.1500\n",
      "2019-04-10 01:15:11,979 root         INFO     Train Epoch: 169 [0/8000 (0%)]\tTotal Loss: 0.154361\n",
      "Reconstruction: 0.148270, Regularization: 0.006091\n",
      "2019-04-10 01:15:12,036 root         INFO     Train Epoch: 169 [512/8000 (6%)]\tTotal Loss: 0.151961\n",
      "Reconstruction: 0.145969, Regularization: 0.005992\n",
      "2019-04-10 01:15:12,092 root         INFO     Train Epoch: 169 [1024/8000 (13%)]\tTotal Loss: 0.133418\n",
      "Reconstruction: 0.130342, Regularization: 0.003075\n",
      "2019-04-10 01:15:12,149 root         INFO     Train Epoch: 169 [1536/8000 (19%)]\tTotal Loss: 0.164133\n",
      "Reconstruction: 0.155664, Regularization: 0.008469\n",
      "2019-04-10 01:15:12,206 root         INFO     Train Epoch: 169 [2048/8000 (26%)]\tTotal Loss: 0.138173\n",
      "Reconstruction: 0.133122, Regularization: 0.005051\n",
      "2019-04-10 01:15:12,264 root         INFO     Train Epoch: 169 [2560/8000 (32%)]\tTotal Loss: 0.140464\n",
      "Reconstruction: 0.135969, Regularization: 0.004495\n",
      "2019-04-10 01:15:12,321 root         INFO     Train Epoch: 169 [3072/8000 (38%)]\tTotal Loss: 0.159144\n",
      "Reconstruction: 0.149936, Regularization: 0.009208\n",
      "2019-04-10 01:15:12,378 root         INFO     Train Epoch: 169 [3584/8000 (45%)]\tTotal Loss: 0.131849\n",
      "Reconstruction: 0.128529, Regularization: 0.003320\n",
      "2019-04-10 01:15:12,436 root         INFO     Train Epoch: 169 [4096/8000 (51%)]\tTotal Loss: 0.136274\n",
      "Reconstruction: 0.132043, Regularization: 0.004231\n",
      "2019-04-10 01:15:12,493 root         INFO     Train Epoch: 169 [4608/8000 (58%)]\tTotal Loss: 0.158011\n",
      "Reconstruction: 0.151466, Regularization: 0.006545\n",
      "2019-04-10 01:15:12,551 root         INFO     Train Epoch: 169 [5120/8000 (64%)]\tTotal Loss: 0.152322\n",
      "Reconstruction: 0.146509, Regularization: 0.005814\n",
      "2019-04-10 01:15:12,608 root         INFO     Train Epoch: 169 [5632/8000 (70%)]\tTotal Loss: 0.149260\n",
      "Reconstruction: 0.142655, Regularization: 0.006605\n",
      "2019-04-10 01:15:12,666 root         INFO     Train Epoch: 169 [6144/8000 (77%)]\tTotal Loss: 0.161714\n",
      "Reconstruction: 0.155879, Regularization: 0.005834\n",
      "2019-04-10 01:15:12,724 root         INFO     Train Epoch: 169 [6656/8000 (83%)]\tTotal Loss: 0.158572\n",
      "Reconstruction: 0.153285, Regularization: 0.005287\n",
      "2019-04-10 01:15:12,781 root         INFO     Train Epoch: 169 [7168/8000 (90%)]\tTotal Loss: 0.156182\n",
      "Reconstruction: 0.150828, Regularization: 0.005354\n",
      "2019-04-10 01:15:12,838 root         INFO     Train Epoch: 169 [7680/8000 (96%)]\tTotal Loss: 0.139440\n",
      "Reconstruction: 0.134281, Regularization: 0.005160\n",
      "2019-04-10 01:15:12,888 root         INFO     ====> Epoch: 169 Average loss: 0.1497\n",
      "2019-04-10 01:15:12,912 root         INFO     Train Epoch: 170 [0/8000 (0%)]\tTotal Loss: 0.144601\n",
      "Reconstruction: 0.136372, Regularization: 0.008229\n",
      "2019-04-10 01:15:12,969 root         INFO     Train Epoch: 170 [512/8000 (6%)]\tTotal Loss: 0.148236\n",
      "Reconstruction: 0.142377, Regularization: 0.005859\n",
      "2019-04-10 01:15:13,025 root         INFO     Train Epoch: 170 [1024/8000 (13%)]\tTotal Loss: 0.134030\n",
      "Reconstruction: 0.130718, Regularization: 0.003312\n",
      "2019-04-10 01:15:13,081 root         INFO     Train Epoch: 170 [1536/8000 (19%)]\tTotal Loss: 0.153617\n",
      "Reconstruction: 0.149288, Regularization: 0.004329\n",
      "2019-04-10 01:15:13,137 root         INFO     Train Epoch: 170 [2048/8000 (26%)]\tTotal Loss: 0.154674\n",
      "Reconstruction: 0.146684, Regularization: 0.007990\n",
      "2019-04-10 01:15:13,193 root         INFO     Train Epoch: 170 [2560/8000 (32%)]\tTotal Loss: 0.158796\n",
      "Reconstruction: 0.149293, Regularization: 0.009503\n",
      "2019-04-10 01:15:13,248 root         INFO     Train Epoch: 170 [3072/8000 (38%)]\tTotal Loss: 0.149162\n",
      "Reconstruction: 0.143199, Regularization: 0.005964\n",
      "2019-04-10 01:15:13,303 root         INFO     Train Epoch: 170 [3584/8000 (45%)]\tTotal Loss: 0.141577\n",
      "Reconstruction: 0.137428, Regularization: 0.004148\n",
      "2019-04-10 01:15:13,359 root         INFO     Train Epoch: 170 [4096/8000 (51%)]\tTotal Loss: 0.151096\n",
      "Reconstruction: 0.144156, Regularization: 0.006940\n",
      "2019-04-10 01:15:13,414 root         INFO     Train Epoch: 170 [4608/8000 (58%)]\tTotal Loss: 0.152698\n",
      "Reconstruction: 0.147327, Regularization: 0.005371\n",
      "2019-04-10 01:15:13,470 root         INFO     Train Epoch: 170 [5120/8000 (64%)]\tTotal Loss: 0.147893\n",
      "Reconstruction: 0.143844, Regularization: 0.004049\n",
      "2019-04-10 01:15:13,525 root         INFO     Train Epoch: 170 [5632/8000 (70%)]\tTotal Loss: 0.158897\n",
      "Reconstruction: 0.149910, Regularization: 0.008987\n",
      "2019-04-10 01:15:13,581 root         INFO     Train Epoch: 170 [6144/8000 (77%)]\tTotal Loss: 0.154582\n",
      "Reconstruction: 0.145271, Regularization: 0.009311\n",
      "2019-04-10 01:15:13,636 root         INFO     Train Epoch: 170 [6656/8000 (83%)]\tTotal Loss: 0.146408\n",
      "Reconstruction: 0.141331, Regularization: 0.005077\n",
      "2019-04-10 01:15:13,692 root         INFO     Train Epoch: 170 [7168/8000 (90%)]\tTotal Loss: 0.150748\n",
      "Reconstruction: 0.145884, Regularization: 0.004863\n",
      "2019-04-10 01:15:13,747 root         INFO     Train Epoch: 170 [7680/8000 (96%)]\tTotal Loss: 0.158074\n",
      "Reconstruction: 0.150804, Regularization: 0.007270\n",
      "2019-04-10 01:15:13,797 root         INFO     ====> Epoch: 170 Average loss: 0.1494\n",
      "2019-04-10 01:15:13,820 root         INFO     Train Epoch: 171 [0/8000 (0%)]\tTotal Loss: 0.149571\n",
      "Reconstruction: 0.143217, Regularization: 0.006354\n",
      "2019-04-10 01:15:13,877 root         INFO     Train Epoch: 171 [512/8000 (6%)]\tTotal Loss: 0.147954\n",
      "Reconstruction: 0.141457, Regularization: 0.006497\n",
      "2019-04-10 01:15:13,934 root         INFO     Train Epoch: 171 [1024/8000 (13%)]\tTotal Loss: 0.150125\n",
      "Reconstruction: 0.142020, Regularization: 0.008105\n",
      "2019-04-10 01:15:13,991 root         INFO     Train Epoch: 171 [1536/8000 (19%)]\tTotal Loss: 0.145600\n",
      "Reconstruction: 0.141066, Regularization: 0.004534\n",
      "2019-04-10 01:15:14,048 root         INFO     Train Epoch: 171 [2048/8000 (26%)]\tTotal Loss: 0.147102\n",
      "Reconstruction: 0.141732, Regularization: 0.005370\n",
      "2019-04-10 01:15:14,104 root         INFO     Train Epoch: 171 [2560/8000 (32%)]\tTotal Loss: 0.154758\n",
      "Reconstruction: 0.150066, Regularization: 0.004692\n",
      "2019-04-10 01:15:14,161 root         INFO     Train Epoch: 171 [3072/8000 (38%)]\tTotal Loss: 0.152297\n",
      "Reconstruction: 0.145649, Regularization: 0.006648\n",
      "2019-04-10 01:15:14,218 root         INFO     Train Epoch: 171 [3584/8000 (45%)]\tTotal Loss: 0.152699\n",
      "Reconstruction: 0.146888, Regularization: 0.005811\n",
      "2019-04-10 01:15:14,275 root         INFO     Train Epoch: 171 [4096/8000 (51%)]\tTotal Loss: 0.133430\n",
      "Reconstruction: 0.129176, Regularization: 0.004254\n",
      "2019-04-10 01:15:14,330 root         INFO     Train Epoch: 171 [4608/8000 (58%)]\tTotal Loss: 0.141227\n",
      "Reconstruction: 0.135543, Regularization: 0.005685\n",
      "2019-04-10 01:15:14,386 root         INFO     Train Epoch: 171 [5120/8000 (64%)]\tTotal Loss: 0.150440\n",
      "Reconstruction: 0.142227, Regularization: 0.008213\n",
      "2019-04-10 01:15:14,443 root         INFO     Train Epoch: 171 [5632/8000 (70%)]\tTotal Loss: 0.148280\n",
      "Reconstruction: 0.141072, Regularization: 0.007208\n",
      "2019-04-10 01:15:14,499 root         INFO     Train Epoch: 171 [6144/8000 (77%)]\tTotal Loss: 0.165575\n",
      "Reconstruction: 0.152081, Regularization: 0.013494\n",
      "2019-04-10 01:15:14,554 root         INFO     Train Epoch: 171 [6656/8000 (83%)]\tTotal Loss: 0.152055\n",
      "Reconstruction: 0.147634, Regularization: 0.004420\n",
      "2019-04-10 01:15:14,608 root         INFO     Train Epoch: 171 [7168/8000 (90%)]\tTotal Loss: 0.161930\n",
      "Reconstruction: 0.152329, Regularization: 0.009601\n",
      "2019-04-10 01:15:14,662 root         INFO     Train Epoch: 171 [7680/8000 (96%)]\tTotal Loss: 0.144119\n",
      "Reconstruction: 0.139075, Regularization: 0.005043\n",
      "2019-04-10 01:15:14,711 root         INFO     ====> Epoch: 171 Average loss: 0.1496\n",
      "2019-04-10 01:15:14,736 root         INFO     Train Epoch: 172 [0/8000 (0%)]\tTotal Loss: 0.140010\n",
      "Reconstruction: 0.137415, Regularization: 0.002596\n",
      "2019-04-10 01:15:14,793 root         INFO     Train Epoch: 172 [512/8000 (6%)]\tTotal Loss: 0.169587\n",
      "Reconstruction: 0.159589, Regularization: 0.009998\n",
      "2019-04-10 01:15:14,849 root         INFO     Train Epoch: 172 [1024/8000 (13%)]\tTotal Loss: 0.151257\n",
      "Reconstruction: 0.145703, Regularization: 0.005553\n",
      "2019-04-10 01:15:14,905 root         INFO     Train Epoch: 172 [1536/8000 (19%)]\tTotal Loss: 0.139557\n",
      "Reconstruction: 0.132915, Regularization: 0.006641\n",
      "2019-04-10 01:15:14,961 root         INFO     Train Epoch: 172 [2048/8000 (26%)]\tTotal Loss: 0.149790\n",
      "Reconstruction: 0.141839, Regularization: 0.007951\n",
      "2019-04-10 01:15:15,018 root         INFO     Train Epoch: 172 [2560/8000 (32%)]\tTotal Loss: 0.151413\n",
      "Reconstruction: 0.144751, Regularization: 0.006662\n",
      "2019-04-10 01:15:15,074 root         INFO     Train Epoch: 172 [3072/8000 (38%)]\tTotal Loss: 0.143945\n",
      "Reconstruction: 0.137608, Regularization: 0.006336\n",
      "2019-04-10 01:15:15,130 root         INFO     Train Epoch: 172 [3584/8000 (45%)]\tTotal Loss: 0.144977\n",
      "Reconstruction: 0.139758, Regularization: 0.005219\n",
      "2019-04-10 01:15:15,187 root         INFO     Train Epoch: 172 [4096/8000 (51%)]\tTotal Loss: 0.142686\n",
      "Reconstruction: 0.137106, Regularization: 0.005579\n",
      "2019-04-10 01:15:15,243 root         INFO     Train Epoch: 172 [4608/8000 (58%)]\tTotal Loss: 0.151300\n",
      "Reconstruction: 0.145999, Regularization: 0.005301\n",
      "2019-04-10 01:15:15,299 root         INFO     Train Epoch: 172 [5120/8000 (64%)]\tTotal Loss: 0.161322\n",
      "Reconstruction: 0.153246, Regularization: 0.008076\n",
      "2019-04-10 01:15:15,356 root         INFO     Train Epoch: 172 [5632/8000 (70%)]\tTotal Loss: 0.147483\n",
      "Reconstruction: 0.137370, Regularization: 0.010113\n",
      "2019-04-10 01:15:15,412 root         INFO     Train Epoch: 172 [6144/8000 (77%)]\tTotal Loss: 0.145541\n",
      "Reconstruction: 0.140607, Regularization: 0.004934\n",
      "2019-04-10 01:15:15,469 root         INFO     Train Epoch: 172 [6656/8000 (83%)]\tTotal Loss: 0.154923\n",
      "Reconstruction: 0.149806, Regularization: 0.005117\n",
      "2019-04-10 01:15:15,525 root         INFO     Train Epoch: 172 [7168/8000 (90%)]\tTotal Loss: 0.166174\n",
      "Reconstruction: 0.157425, Regularization: 0.008749\n",
      "2019-04-10 01:15:15,582 root         INFO     Train Epoch: 172 [7680/8000 (96%)]\tTotal Loss: 0.146179\n",
      "Reconstruction: 0.141775, Regularization: 0.004403\n",
      "2019-04-10 01:15:15,632 root         INFO     ====> Epoch: 172 Average loss: 0.1492\n",
      "2019-04-10 01:15:15,655 root         INFO     Train Epoch: 173 [0/8000 (0%)]\tTotal Loss: 0.167180\n",
      "Reconstruction: 0.159519, Regularization: 0.007661\n",
      "2019-04-10 01:15:15,713 root         INFO     Train Epoch: 173 [512/8000 (6%)]\tTotal Loss: 0.145762\n",
      "Reconstruction: 0.140938, Regularization: 0.004824\n",
      "2019-04-10 01:15:15,769 root         INFO     Train Epoch: 173 [1024/8000 (13%)]\tTotal Loss: 0.145928\n",
      "Reconstruction: 0.141556, Regularization: 0.004372\n",
      "2019-04-10 01:15:15,826 root         INFO     Train Epoch: 173 [1536/8000 (19%)]\tTotal Loss: 0.143232\n",
      "Reconstruction: 0.138727, Regularization: 0.004505\n",
      "2019-04-10 01:15:15,883 root         INFO     Train Epoch: 173 [2048/8000 (26%)]\tTotal Loss: 0.137954\n",
      "Reconstruction: 0.131486, Regularization: 0.006468\n",
      "2019-04-10 01:15:15,938 root         INFO     Train Epoch: 173 [2560/8000 (32%)]\tTotal Loss: 0.156184\n",
      "Reconstruction: 0.149040, Regularization: 0.007144\n",
      "2019-04-10 01:15:15,995 root         INFO     Train Epoch: 173 [3072/8000 (38%)]\tTotal Loss: 0.151189\n",
      "Reconstruction: 0.145800, Regularization: 0.005389\n",
      "2019-04-10 01:15:16,052 root         INFO     Train Epoch: 173 [3584/8000 (45%)]\tTotal Loss: 0.146770\n",
      "Reconstruction: 0.141561, Regularization: 0.005209\n",
      "2019-04-10 01:15:16,110 root         INFO     Train Epoch: 173 [4096/8000 (51%)]\tTotal Loss: 0.142780\n",
      "Reconstruction: 0.136888, Regularization: 0.005892\n",
      "2019-04-10 01:15:16,167 root         INFO     Train Epoch: 173 [4608/8000 (58%)]\tTotal Loss: 0.148842\n",
      "Reconstruction: 0.142855, Regularization: 0.005987\n",
      "2019-04-10 01:15:16,224 root         INFO     Train Epoch: 173 [5120/8000 (64%)]\tTotal Loss: 0.165093\n",
      "Reconstruction: 0.157758, Regularization: 0.007335\n",
      "2019-04-10 01:15:16,280 root         INFO     Train Epoch: 173 [5632/8000 (70%)]\tTotal Loss: 0.140498\n",
      "Reconstruction: 0.136772, Regularization: 0.003726\n",
      "2019-04-10 01:15:16,336 root         INFO     Train Epoch: 173 [6144/8000 (77%)]\tTotal Loss: 0.149191\n",
      "Reconstruction: 0.144001, Regularization: 0.005190\n",
      "2019-04-10 01:15:16,391 root         INFO     Train Epoch: 173 [6656/8000 (83%)]\tTotal Loss: 0.137738\n",
      "Reconstruction: 0.132799, Regularization: 0.004939\n",
      "2019-04-10 01:15:16,448 root         INFO     Train Epoch: 173 [7168/8000 (90%)]\tTotal Loss: 0.136369\n",
      "Reconstruction: 0.133168, Regularization: 0.003201\n",
      "2019-04-10 01:15:16,506 root         INFO     Train Epoch: 173 [7680/8000 (96%)]\tTotal Loss: 0.141302\n",
      "Reconstruction: 0.135505, Regularization: 0.005797\n",
      "2019-04-10 01:15:16,557 root         INFO     ====> Epoch: 173 Average loss: 0.1499\n",
      "2019-04-10 01:15:16,580 root         INFO     Train Epoch: 174 [0/8000 (0%)]\tTotal Loss: 0.157132\n",
      "Reconstruction: 0.146119, Regularization: 0.011014\n",
      "2019-04-10 01:15:16,638 root         INFO     Train Epoch: 174 [512/8000 (6%)]\tTotal Loss: 0.148510\n",
      "Reconstruction: 0.141182, Regularization: 0.007328\n",
      "2019-04-10 01:15:16,694 root         INFO     Train Epoch: 174 [1024/8000 (13%)]\tTotal Loss: 0.144351\n",
      "Reconstruction: 0.138008, Regularization: 0.006343\n",
      "2019-04-10 01:15:16,751 root         INFO     Train Epoch: 174 [1536/8000 (19%)]\tTotal Loss: 0.154247\n",
      "Reconstruction: 0.148117, Regularization: 0.006130\n",
      "2019-04-10 01:15:16,807 root         INFO     Train Epoch: 174 [2048/8000 (26%)]\tTotal Loss: 0.147742\n",
      "Reconstruction: 0.141232, Regularization: 0.006510\n",
      "2019-04-10 01:15:16,864 root         INFO     Train Epoch: 174 [2560/8000 (32%)]\tTotal Loss: 0.145163\n",
      "Reconstruction: 0.138747, Regularization: 0.006416\n",
      "2019-04-10 01:15:16,920 root         INFO     Train Epoch: 174 [3072/8000 (38%)]\tTotal Loss: 0.156505\n",
      "Reconstruction: 0.152747, Regularization: 0.003758\n",
      "2019-04-10 01:15:16,977 root         INFO     Train Epoch: 174 [3584/8000 (45%)]\tTotal Loss: 0.149285\n",
      "Reconstruction: 0.143913, Regularization: 0.005372\n",
      "2019-04-10 01:15:17,033 root         INFO     Train Epoch: 174 [4096/8000 (51%)]\tTotal Loss: 0.143743\n",
      "Reconstruction: 0.138375, Regularization: 0.005368\n",
      "2019-04-10 01:15:17,090 root         INFO     Train Epoch: 174 [4608/8000 (58%)]\tTotal Loss: 0.150888\n",
      "Reconstruction: 0.144036, Regularization: 0.006852\n",
      "2019-04-10 01:15:17,146 root         INFO     Train Epoch: 174 [5120/8000 (64%)]\tTotal Loss: 0.136397\n",
      "Reconstruction: 0.131611, Regularization: 0.004785\n",
      "2019-04-10 01:15:17,203 root         INFO     Train Epoch: 174 [5632/8000 (70%)]\tTotal Loss: 0.148770\n",
      "Reconstruction: 0.144462, Regularization: 0.004308\n",
      "2019-04-10 01:15:17,259 root         INFO     Train Epoch: 174 [6144/8000 (77%)]\tTotal Loss: 0.149967\n",
      "Reconstruction: 0.143734, Regularization: 0.006233\n",
      "2019-04-10 01:15:17,316 root         INFO     Train Epoch: 174 [6656/8000 (83%)]\tTotal Loss: 0.156980\n",
      "Reconstruction: 0.149088, Regularization: 0.007893\n",
      "2019-04-10 01:15:17,372 root         INFO     Train Epoch: 174 [7168/8000 (90%)]\tTotal Loss: 0.149275\n",
      "Reconstruction: 0.143574, Regularization: 0.005701\n",
      "2019-04-10 01:15:17,429 root         INFO     Train Epoch: 174 [7680/8000 (96%)]\tTotal Loss: 0.144368\n",
      "Reconstruction: 0.139667, Regularization: 0.004701\n",
      "2019-04-10 01:15:17,479 root         INFO     ====> Epoch: 174 Average loss: 0.1491\n",
      "2019-04-10 01:15:17,503 root         INFO     Train Epoch: 175 [0/8000 (0%)]\tTotal Loss: 0.144171\n",
      "Reconstruction: 0.140333, Regularization: 0.003839\n",
      "2019-04-10 01:15:17,560 root         INFO     Train Epoch: 175 [512/8000 (6%)]\tTotal Loss: 0.148579\n",
      "Reconstruction: 0.140457, Regularization: 0.008122\n",
      "2019-04-10 01:15:17,618 root         INFO     Train Epoch: 175 [1024/8000 (13%)]\tTotal Loss: 0.150596\n",
      "Reconstruction: 0.144576, Regularization: 0.006021\n",
      "2019-04-10 01:15:17,675 root         INFO     Train Epoch: 175 [1536/8000 (19%)]\tTotal Loss: 0.132892\n",
      "Reconstruction: 0.129474, Regularization: 0.003418\n",
      "2019-04-10 01:15:17,732 root         INFO     Train Epoch: 175 [2048/8000 (26%)]\tTotal Loss: 0.140671\n",
      "Reconstruction: 0.134113, Regularization: 0.006558\n",
      "2019-04-10 01:15:17,789 root         INFO     Train Epoch: 175 [2560/8000 (32%)]\tTotal Loss: 0.144837\n",
      "Reconstruction: 0.139976, Regularization: 0.004861\n",
      "2019-04-10 01:15:17,846 root         INFO     Train Epoch: 175 [3072/8000 (38%)]\tTotal Loss: 0.151370\n",
      "Reconstruction: 0.143170, Regularization: 0.008200\n",
      "2019-04-10 01:15:17,903 root         INFO     Train Epoch: 175 [3584/8000 (45%)]\tTotal Loss: 0.149143\n",
      "Reconstruction: 0.142873, Regularization: 0.006269\n",
      "2019-04-10 01:15:17,961 root         INFO     Train Epoch: 175 [4096/8000 (51%)]\tTotal Loss: 0.144484\n",
      "Reconstruction: 0.139035, Regularization: 0.005449\n",
      "2019-04-10 01:15:18,018 root         INFO     Train Epoch: 175 [4608/8000 (58%)]\tTotal Loss: 0.156746\n",
      "Reconstruction: 0.152650, Regularization: 0.004096\n",
      "2019-04-10 01:15:18,075 root         INFO     Train Epoch: 175 [5120/8000 (64%)]\tTotal Loss: 0.157261\n",
      "Reconstruction: 0.150713, Regularization: 0.006548\n",
      "2019-04-10 01:15:18,133 root         INFO     Train Epoch: 175 [5632/8000 (70%)]\tTotal Loss: 0.162008\n",
      "Reconstruction: 0.156797, Regularization: 0.005211\n",
      "2019-04-10 01:15:18,190 root         INFO     Train Epoch: 175 [6144/8000 (77%)]\tTotal Loss: 0.157230\n",
      "Reconstruction: 0.152802, Regularization: 0.004428\n",
      "2019-04-10 01:15:18,247 root         INFO     Train Epoch: 175 [6656/8000 (83%)]\tTotal Loss: 0.148146\n",
      "Reconstruction: 0.143184, Regularization: 0.004962\n",
      "2019-04-10 01:15:18,304 root         INFO     Train Epoch: 175 [7168/8000 (90%)]\tTotal Loss: 0.148568\n",
      "Reconstruction: 0.142406, Regularization: 0.006162\n",
      "2019-04-10 01:15:18,361 root         INFO     Train Epoch: 175 [7680/8000 (96%)]\tTotal Loss: 0.146110\n",
      "Reconstruction: 0.141338, Regularization: 0.004772\n",
      "2019-04-10 01:15:18,412 root         INFO     ====> Epoch: 175 Average loss: 0.1495\n",
      "2019-04-10 01:15:18,435 root         INFO     Train Epoch: 176 [0/8000 (0%)]\tTotal Loss: 0.143469\n",
      "Reconstruction: 0.140303, Regularization: 0.003166\n",
      "2019-04-10 01:15:18,493 root         INFO     Train Epoch: 176 [512/8000 (6%)]\tTotal Loss: 0.148037\n",
      "Reconstruction: 0.144599, Regularization: 0.003438\n",
      "2019-04-10 01:15:18,550 root         INFO     Train Epoch: 176 [1024/8000 (13%)]\tTotal Loss: 0.153803\n",
      "Reconstruction: 0.145273, Regularization: 0.008530\n",
      "2019-04-10 01:15:18,608 root         INFO     Train Epoch: 176 [1536/8000 (19%)]\tTotal Loss: 0.141490\n",
      "Reconstruction: 0.137723, Regularization: 0.003766\n",
      "2019-04-10 01:15:18,664 root         INFO     Train Epoch: 176 [2048/8000 (26%)]\tTotal Loss: 0.145244\n",
      "Reconstruction: 0.137542, Regularization: 0.007702\n",
      "2019-04-10 01:15:18,721 root         INFO     Train Epoch: 176 [2560/8000 (32%)]\tTotal Loss: 0.151374\n",
      "Reconstruction: 0.146693, Regularization: 0.004680\n",
      "2019-04-10 01:15:18,777 root         INFO     Train Epoch: 176 [3072/8000 (38%)]\tTotal Loss: 0.151080\n",
      "Reconstruction: 0.146020, Regularization: 0.005060\n",
      "2019-04-10 01:15:18,833 root         INFO     Train Epoch: 176 [3584/8000 (45%)]\tTotal Loss: 0.147000\n",
      "Reconstruction: 0.142342, Regularization: 0.004658\n",
      "2019-04-10 01:15:18,889 root         INFO     Train Epoch: 176 [4096/8000 (51%)]\tTotal Loss: 0.138089\n",
      "Reconstruction: 0.134620, Regularization: 0.003469\n",
      "2019-04-10 01:15:18,945 root         INFO     Train Epoch: 176 [4608/8000 (58%)]\tTotal Loss: 0.155223\n",
      "Reconstruction: 0.149879, Regularization: 0.005344\n",
      "2019-04-10 01:15:19,002 root         INFO     Train Epoch: 176 [5120/8000 (64%)]\tTotal Loss: 0.152010\n",
      "Reconstruction: 0.143858, Regularization: 0.008152\n",
      "2019-04-10 01:15:19,058 root         INFO     Train Epoch: 176 [5632/8000 (70%)]\tTotal Loss: 0.147311\n",
      "Reconstruction: 0.142771, Regularization: 0.004540\n",
      "2019-04-10 01:15:19,114 root         INFO     Train Epoch: 176 [6144/8000 (77%)]\tTotal Loss: 0.151595\n",
      "Reconstruction: 0.144132, Regularization: 0.007463\n",
      "2019-04-10 01:15:19,170 root         INFO     Train Epoch: 176 [6656/8000 (83%)]\tTotal Loss: 0.142369\n",
      "Reconstruction: 0.138274, Regularization: 0.004095\n",
      "2019-04-10 01:15:19,226 root         INFO     Train Epoch: 176 [7168/8000 (90%)]\tTotal Loss: 0.138622\n",
      "Reconstruction: 0.134573, Regularization: 0.004049\n",
      "2019-04-10 01:15:19,282 root         INFO     Train Epoch: 176 [7680/8000 (96%)]\tTotal Loss: 0.153584\n",
      "Reconstruction: 0.147217, Regularization: 0.006367\n",
      "2019-04-10 01:15:19,332 root         INFO     ====> Epoch: 176 Average loss: 0.1495\n",
      "2019-04-10 01:15:19,356 root         INFO     Train Epoch: 177 [0/8000 (0%)]\tTotal Loss: 0.152632\n",
      "Reconstruction: 0.146855, Regularization: 0.005777\n",
      "2019-04-10 01:15:19,414 root         INFO     Train Epoch: 177 [512/8000 (6%)]\tTotal Loss: 0.159343\n",
      "Reconstruction: 0.148023, Regularization: 0.011321\n",
      "2019-04-10 01:15:19,471 root         INFO     Train Epoch: 177 [1024/8000 (13%)]\tTotal Loss: 0.147288\n",
      "Reconstruction: 0.142350, Regularization: 0.004938\n",
      "2019-04-10 01:15:19,528 root         INFO     Train Epoch: 177 [1536/8000 (19%)]\tTotal Loss: 0.134781\n",
      "Reconstruction: 0.129241, Regularization: 0.005540\n",
      "2019-04-10 01:15:19,584 root         INFO     Train Epoch: 177 [2048/8000 (26%)]\tTotal Loss: 0.144829\n",
      "Reconstruction: 0.138670, Regularization: 0.006160\n",
      "2019-04-10 01:15:19,640 root         INFO     Train Epoch: 177 [2560/8000 (32%)]\tTotal Loss: 0.142889\n",
      "Reconstruction: 0.138440, Regularization: 0.004449\n",
      "2019-04-10 01:15:19,697 root         INFO     Train Epoch: 177 [3072/8000 (38%)]\tTotal Loss: 0.144548\n",
      "Reconstruction: 0.140669, Regularization: 0.003878\n",
      "2019-04-10 01:15:19,755 root         INFO     Train Epoch: 177 [3584/8000 (45%)]\tTotal Loss: 0.136958\n",
      "Reconstruction: 0.132365, Regularization: 0.004593\n",
      "2019-04-10 01:15:19,812 root         INFO     Train Epoch: 177 [4096/8000 (51%)]\tTotal Loss: 0.153582\n",
      "Reconstruction: 0.147277, Regularization: 0.006305\n",
      "2019-04-10 01:15:19,869 root         INFO     Train Epoch: 177 [4608/8000 (58%)]\tTotal Loss: 0.155710\n",
      "Reconstruction: 0.148738, Regularization: 0.006971\n",
      "2019-04-10 01:15:19,926 root         INFO     Train Epoch: 177 [5120/8000 (64%)]\tTotal Loss: 0.150179\n",
      "Reconstruction: 0.146385, Regularization: 0.003794\n",
      "2019-04-10 01:15:19,984 root         INFO     Train Epoch: 177 [5632/8000 (70%)]\tTotal Loss: 0.141344\n",
      "Reconstruction: 0.137262, Regularization: 0.004082\n",
      "2019-04-10 01:15:20,041 root         INFO     Train Epoch: 177 [6144/8000 (77%)]\tTotal Loss: 0.140323\n",
      "Reconstruction: 0.135516, Regularization: 0.004807\n",
      "2019-04-10 01:15:20,099 root         INFO     Train Epoch: 177 [6656/8000 (83%)]\tTotal Loss: 0.138349\n",
      "Reconstruction: 0.134910, Regularization: 0.003439\n",
      "2019-04-10 01:15:20,156 root         INFO     Train Epoch: 177 [7168/8000 (90%)]\tTotal Loss: 0.139455\n",
      "Reconstruction: 0.135688, Regularization: 0.003768\n",
      "2019-04-10 01:15:20,213 root         INFO     Train Epoch: 177 [7680/8000 (96%)]\tTotal Loss: 0.142786\n",
      "Reconstruction: 0.136950, Regularization: 0.005835\n",
      "2019-04-10 01:15:20,264 root         INFO     ====> Epoch: 177 Average loss: 0.1497\n",
      "2019-04-10 01:15:20,287 root         INFO     Train Epoch: 178 [0/8000 (0%)]\tTotal Loss: 0.147550\n",
      "Reconstruction: 0.144234, Regularization: 0.003316\n",
      "2019-04-10 01:15:20,345 root         INFO     Train Epoch: 178 [512/8000 (6%)]\tTotal Loss: 0.146024\n",
      "Reconstruction: 0.139834, Regularization: 0.006190\n",
      "2019-04-10 01:15:20,403 root         INFO     Train Epoch: 178 [1024/8000 (13%)]\tTotal Loss: 0.157002\n",
      "Reconstruction: 0.151801, Regularization: 0.005201\n",
      "2019-04-10 01:15:20,460 root         INFO     Train Epoch: 178 [1536/8000 (19%)]\tTotal Loss: 0.146796\n",
      "Reconstruction: 0.141387, Regularization: 0.005409\n",
      "2019-04-10 01:15:20,517 root         INFO     Train Epoch: 178 [2048/8000 (26%)]\tTotal Loss: 0.155621\n",
      "Reconstruction: 0.148048, Regularization: 0.007572\n",
      "2019-04-10 01:15:20,575 root         INFO     Train Epoch: 178 [2560/8000 (32%)]\tTotal Loss: 0.144192\n",
      "Reconstruction: 0.139251, Regularization: 0.004941\n",
      "2019-04-10 01:15:20,632 root         INFO     Train Epoch: 178 [3072/8000 (38%)]\tTotal Loss: 0.150147\n",
      "Reconstruction: 0.142594, Regularization: 0.007553\n",
      "2019-04-10 01:15:20,690 root         INFO     Train Epoch: 178 [3584/8000 (45%)]\tTotal Loss: 0.153408\n",
      "Reconstruction: 0.145055, Regularization: 0.008352\n",
      "2019-04-10 01:15:20,747 root         INFO     Train Epoch: 178 [4096/8000 (51%)]\tTotal Loss: 0.156922\n",
      "Reconstruction: 0.148914, Regularization: 0.008008\n",
      "2019-04-10 01:15:20,805 root         INFO     Train Epoch: 178 [4608/8000 (58%)]\tTotal Loss: 0.161194\n",
      "Reconstruction: 0.153274, Regularization: 0.007920\n",
      "2019-04-10 01:15:20,862 root         INFO     Train Epoch: 178 [5120/8000 (64%)]\tTotal Loss: 0.142921\n",
      "Reconstruction: 0.136925, Regularization: 0.005996\n",
      "2019-04-10 01:15:20,920 root         INFO     Train Epoch: 178 [5632/8000 (70%)]\tTotal Loss: 0.154976\n",
      "Reconstruction: 0.148805, Regularization: 0.006171\n",
      "2019-04-10 01:15:20,977 root         INFO     Train Epoch: 178 [6144/8000 (77%)]\tTotal Loss: 0.170350\n",
      "Reconstruction: 0.161427, Regularization: 0.008923\n",
      "2019-04-10 01:15:21,035 root         INFO     Train Epoch: 178 [6656/8000 (83%)]\tTotal Loss: 0.153422\n",
      "Reconstruction: 0.147024, Regularization: 0.006399\n",
      "2019-04-10 01:15:21,092 root         INFO     Train Epoch: 178 [7168/8000 (90%)]\tTotal Loss: 0.153263\n",
      "Reconstruction: 0.146063, Regularization: 0.007199\n",
      "2019-04-10 01:15:21,149 root         INFO     Train Epoch: 178 [7680/8000 (96%)]\tTotal Loss: 0.137039\n",
      "Reconstruction: 0.133095, Regularization: 0.003943\n",
      "2019-04-10 01:15:21,199 root         INFO     ====> Epoch: 178 Average loss: 0.1495\n",
      "2019-04-10 01:15:21,222 root         INFO     Train Epoch: 179 [0/8000 (0%)]\tTotal Loss: 0.157634\n",
      "Reconstruction: 0.151700, Regularization: 0.005934\n",
      "2019-04-10 01:15:21,281 root         INFO     Train Epoch: 179 [512/8000 (6%)]\tTotal Loss: 0.160159\n",
      "Reconstruction: 0.150925, Regularization: 0.009234\n",
      "2019-04-10 01:15:21,338 root         INFO     Train Epoch: 179 [1024/8000 (13%)]\tTotal Loss: 0.159776\n",
      "Reconstruction: 0.150788, Regularization: 0.008988\n",
      "2019-04-10 01:15:21,395 root         INFO     Train Epoch: 179 [1536/8000 (19%)]\tTotal Loss: 0.142992\n",
      "Reconstruction: 0.138735, Regularization: 0.004258\n",
      "2019-04-10 01:15:21,452 root         INFO     Train Epoch: 179 [2048/8000 (26%)]\tTotal Loss: 0.135997\n",
      "Reconstruction: 0.128802, Regularization: 0.007195\n",
      "2019-04-10 01:15:21,510 root         INFO     Train Epoch: 179 [2560/8000 (32%)]\tTotal Loss: 0.141568\n",
      "Reconstruction: 0.135492, Regularization: 0.006076\n",
      "2019-04-10 01:15:21,567 root         INFO     Train Epoch: 179 [3072/8000 (38%)]\tTotal Loss: 0.143569\n",
      "Reconstruction: 0.137682, Regularization: 0.005887\n",
      "2019-04-10 01:15:21,624 root         INFO     Train Epoch: 179 [3584/8000 (45%)]\tTotal Loss: 0.139227\n",
      "Reconstruction: 0.135064, Regularization: 0.004162\n",
      "2019-04-10 01:15:21,680 root         INFO     Train Epoch: 179 [4096/8000 (51%)]\tTotal Loss: 0.155979\n",
      "Reconstruction: 0.145944, Regularization: 0.010035\n",
      "2019-04-10 01:15:21,736 root         INFO     Train Epoch: 179 [4608/8000 (58%)]\tTotal Loss: 0.154542\n",
      "Reconstruction: 0.148514, Regularization: 0.006027\n",
      "2019-04-10 01:15:21,793 root         INFO     Train Epoch: 179 [5120/8000 (64%)]\tTotal Loss: 0.137540\n",
      "Reconstruction: 0.133017, Regularization: 0.004523\n",
      "2019-04-10 01:15:21,849 root         INFO     Train Epoch: 179 [5632/8000 (70%)]\tTotal Loss: 0.160047\n",
      "Reconstruction: 0.151205, Regularization: 0.008841\n",
      "2019-04-10 01:15:21,905 root         INFO     Train Epoch: 179 [6144/8000 (77%)]\tTotal Loss: 0.134324\n",
      "Reconstruction: 0.130110, Regularization: 0.004214\n",
      "2019-04-10 01:15:21,962 root         INFO     Train Epoch: 179 [6656/8000 (83%)]\tTotal Loss: 0.136855\n",
      "Reconstruction: 0.133185, Regularization: 0.003670\n",
      "2019-04-10 01:15:22,018 root         INFO     Train Epoch: 179 [7168/8000 (90%)]\tTotal Loss: 0.141684\n",
      "Reconstruction: 0.135852, Regularization: 0.005831\n",
      "2019-04-10 01:15:22,074 root         INFO     Train Epoch: 179 [7680/8000 (96%)]\tTotal Loss: 0.157988\n",
      "Reconstruction: 0.148920, Regularization: 0.009068\n",
      "2019-04-10 01:15:22,124 root         INFO     ====> Epoch: 179 Average loss: 0.1495\n",
      "2019-04-10 01:15:22,148 root         INFO     Train Epoch: 180 [0/8000 (0%)]\tTotal Loss: 0.162502\n",
      "Reconstruction: 0.154908, Regularization: 0.007595\n",
      "2019-04-10 01:15:22,205 root         INFO     Train Epoch: 180 [512/8000 (6%)]\tTotal Loss: 0.147905\n",
      "Reconstruction: 0.141006, Regularization: 0.006900\n",
      "2019-04-10 01:15:22,261 root         INFO     Train Epoch: 180 [1024/8000 (13%)]\tTotal Loss: 0.144751\n",
      "Reconstruction: 0.137984, Regularization: 0.006767\n",
      "2019-04-10 01:15:22,316 root         INFO     Train Epoch: 180 [1536/8000 (19%)]\tTotal Loss: 0.135394\n",
      "Reconstruction: 0.132224, Regularization: 0.003170\n",
      "2019-04-10 01:15:22,370 root         INFO     Train Epoch: 180 [2048/8000 (26%)]\tTotal Loss: 0.141437\n",
      "Reconstruction: 0.137007, Regularization: 0.004430\n",
      "2019-04-10 01:15:22,424 root         INFO     Train Epoch: 180 [2560/8000 (32%)]\tTotal Loss: 0.150247\n",
      "Reconstruction: 0.143003, Regularization: 0.007244\n",
      "2019-04-10 01:15:22,479 root         INFO     Train Epoch: 180 [3072/8000 (38%)]\tTotal Loss: 0.152957\n",
      "Reconstruction: 0.146772, Regularization: 0.006185\n",
      "2019-04-10 01:15:22,534 root         INFO     Train Epoch: 180 [3584/8000 (45%)]\tTotal Loss: 0.135703\n",
      "Reconstruction: 0.131154, Regularization: 0.004549\n",
      "2019-04-10 01:15:22,588 root         INFO     Train Epoch: 180 [4096/8000 (51%)]\tTotal Loss: 0.161837\n",
      "Reconstruction: 0.153823, Regularization: 0.008014\n",
      "2019-04-10 01:15:22,643 root         INFO     Train Epoch: 180 [4608/8000 (58%)]\tTotal Loss: 0.139189\n",
      "Reconstruction: 0.135597, Regularization: 0.003592\n",
      "2019-04-10 01:15:22,697 root         INFO     Train Epoch: 180 [5120/8000 (64%)]\tTotal Loss: 0.150460\n",
      "Reconstruction: 0.143876, Regularization: 0.006585\n",
      "2019-04-10 01:15:22,752 root         INFO     Train Epoch: 180 [5632/8000 (70%)]\tTotal Loss: 0.148102\n",
      "Reconstruction: 0.143194, Regularization: 0.004908\n",
      "2019-04-10 01:15:22,807 root         INFO     Train Epoch: 180 [6144/8000 (77%)]\tTotal Loss: 0.144705\n",
      "Reconstruction: 0.138868, Regularization: 0.005837\n",
      "2019-04-10 01:15:22,862 root         INFO     Train Epoch: 180 [6656/8000 (83%)]\tTotal Loss: 0.150860\n",
      "Reconstruction: 0.147643, Regularization: 0.003217\n",
      "2019-04-10 01:15:22,916 root         INFO     Train Epoch: 180 [7168/8000 (90%)]\tTotal Loss: 0.140717\n",
      "Reconstruction: 0.136550, Regularization: 0.004167\n",
      "2019-04-10 01:15:22,971 root         INFO     Train Epoch: 180 [7680/8000 (96%)]\tTotal Loss: 0.150763\n",
      "Reconstruction: 0.143341, Regularization: 0.007422\n",
      "2019-04-10 01:15:23,020 root         INFO     ====> Epoch: 180 Average loss: 0.1495\n",
      "2019-04-10 01:15:23,044 root         INFO     Train Epoch: 181 [0/8000 (0%)]\tTotal Loss: 0.158972\n",
      "Reconstruction: 0.148319, Regularization: 0.010653\n",
      "2019-04-10 01:15:23,099 root         INFO     Train Epoch: 181 [512/8000 (6%)]\tTotal Loss: 0.147585\n",
      "Reconstruction: 0.139292, Regularization: 0.008293\n",
      "2019-04-10 01:15:23,154 root         INFO     Train Epoch: 181 [1024/8000 (13%)]\tTotal Loss: 0.152713\n",
      "Reconstruction: 0.146844, Regularization: 0.005869\n",
      "2019-04-10 01:15:23,210 root         INFO     Train Epoch: 181 [1536/8000 (19%)]\tTotal Loss: 0.154766\n",
      "Reconstruction: 0.149383, Regularization: 0.005383\n",
      "2019-04-10 01:15:23,265 root         INFO     Train Epoch: 181 [2048/8000 (26%)]\tTotal Loss: 0.155893\n",
      "Reconstruction: 0.148030, Regularization: 0.007863\n",
      "2019-04-10 01:15:23,320 root         INFO     Train Epoch: 181 [2560/8000 (32%)]\tTotal Loss: 0.150590\n",
      "Reconstruction: 0.143816, Regularization: 0.006774\n",
      "2019-04-10 01:15:23,375 root         INFO     Train Epoch: 181 [3072/8000 (38%)]\tTotal Loss: 0.155329\n",
      "Reconstruction: 0.144929, Regularization: 0.010400\n",
      "2019-04-10 01:15:23,430 root         INFO     Train Epoch: 181 [3584/8000 (45%)]\tTotal Loss: 0.145882\n",
      "Reconstruction: 0.139600, Regularization: 0.006283\n",
      "2019-04-10 01:15:23,486 root         INFO     Train Epoch: 181 [4096/8000 (51%)]\tTotal Loss: 0.145404\n",
      "Reconstruction: 0.139826, Regularization: 0.005579\n",
      "2019-04-10 01:15:23,542 root         INFO     Train Epoch: 181 [4608/8000 (58%)]\tTotal Loss: 0.148215\n",
      "Reconstruction: 0.142603, Regularization: 0.005612\n",
      "2019-04-10 01:15:23,597 root         INFO     Train Epoch: 181 [5120/8000 (64%)]\tTotal Loss: 0.136697\n",
      "Reconstruction: 0.130644, Regularization: 0.006054\n",
      "2019-04-10 01:15:23,653 root         INFO     Train Epoch: 181 [5632/8000 (70%)]\tTotal Loss: 0.164982\n",
      "Reconstruction: 0.159708, Regularization: 0.005274\n",
      "2019-04-10 01:15:23,709 root         INFO     Train Epoch: 181 [6144/8000 (77%)]\tTotal Loss: 0.156303\n",
      "Reconstruction: 0.152348, Regularization: 0.003956\n",
      "2019-04-10 01:15:23,766 root         INFO     Train Epoch: 181 [6656/8000 (83%)]\tTotal Loss: 0.145755\n",
      "Reconstruction: 0.139135, Regularization: 0.006620\n",
      "2019-04-10 01:15:23,822 root         INFO     Train Epoch: 181 [7168/8000 (90%)]\tTotal Loss: 0.151796\n",
      "Reconstruction: 0.145704, Regularization: 0.006092\n",
      "2019-04-10 01:15:23,879 root         INFO     Train Epoch: 181 [7680/8000 (96%)]\tTotal Loss: 0.146957\n",
      "Reconstruction: 0.140182, Regularization: 0.006776\n",
      "2019-04-10 01:15:23,929 root         INFO     ====> Epoch: 181 Average loss: 0.1498\n",
      "2019-04-10 01:15:23,953 root         INFO     Train Epoch: 182 [0/8000 (0%)]\tTotal Loss: 0.140312\n",
      "Reconstruction: 0.136100, Regularization: 0.004212\n",
      "2019-04-10 01:15:24,009 root         INFO     Train Epoch: 182 [512/8000 (6%)]\tTotal Loss: 0.148913\n",
      "Reconstruction: 0.141672, Regularization: 0.007240\n",
      "2019-04-10 01:15:24,065 root         INFO     Train Epoch: 182 [1024/8000 (13%)]\tTotal Loss: 0.145958\n",
      "Reconstruction: 0.141334, Regularization: 0.004624\n",
      "2019-04-10 01:15:24,121 root         INFO     Train Epoch: 182 [1536/8000 (19%)]\tTotal Loss: 0.149834\n",
      "Reconstruction: 0.140295, Regularization: 0.009539\n",
      "2019-04-10 01:15:24,176 root         INFO     Train Epoch: 182 [2048/8000 (26%)]\tTotal Loss: 0.146842\n",
      "Reconstruction: 0.142831, Regularization: 0.004010\n",
      "2019-04-10 01:15:24,233 root         INFO     Train Epoch: 182 [2560/8000 (32%)]\tTotal Loss: 0.152071\n",
      "Reconstruction: 0.145104, Regularization: 0.006966\n",
      "2019-04-10 01:15:24,288 root         INFO     Train Epoch: 182 [3072/8000 (38%)]\tTotal Loss: 0.148079\n",
      "Reconstruction: 0.142384, Regularization: 0.005695\n",
      "2019-04-10 01:15:24,344 root         INFO     Train Epoch: 182 [3584/8000 (45%)]\tTotal Loss: 0.146189\n",
      "Reconstruction: 0.140836, Regularization: 0.005353\n",
      "2019-04-10 01:15:24,400 root         INFO     Train Epoch: 182 [4096/8000 (51%)]\tTotal Loss: 0.151910\n",
      "Reconstruction: 0.146485, Regularization: 0.005425\n",
      "2019-04-10 01:15:24,456 root         INFO     Train Epoch: 182 [4608/8000 (58%)]\tTotal Loss: 0.148883\n",
      "Reconstruction: 0.142749, Regularization: 0.006133\n",
      "2019-04-10 01:15:24,511 root         INFO     Train Epoch: 182 [5120/8000 (64%)]\tTotal Loss: 0.139060\n",
      "Reconstruction: 0.135032, Regularization: 0.004028\n",
      "2019-04-10 01:15:24,566 root         INFO     Train Epoch: 182 [5632/8000 (70%)]\tTotal Loss: 0.145914\n",
      "Reconstruction: 0.140085, Regularization: 0.005828\n",
      "2019-04-10 01:15:24,621 root         INFO     Train Epoch: 182 [6144/8000 (77%)]\tTotal Loss: 0.145696\n",
      "Reconstruction: 0.137555, Regularization: 0.008140\n",
      "2019-04-10 01:15:24,676 root         INFO     Train Epoch: 182 [6656/8000 (83%)]\tTotal Loss: 0.153454\n",
      "Reconstruction: 0.145146, Regularization: 0.008308\n",
      "2019-04-10 01:15:24,731 root         INFO     Train Epoch: 182 [7168/8000 (90%)]\tTotal Loss: 0.148222\n",
      "Reconstruction: 0.142753, Regularization: 0.005469\n",
      "2019-04-10 01:15:24,786 root         INFO     Train Epoch: 182 [7680/8000 (96%)]\tTotal Loss: 0.138386\n",
      "Reconstruction: 0.133723, Regularization: 0.004663\n",
      "2019-04-10 01:15:24,835 root         INFO     ====> Epoch: 182 Average loss: 0.1493\n",
      "2019-04-10 01:15:24,859 root         INFO     Train Epoch: 183 [0/8000 (0%)]\tTotal Loss: 0.141121\n",
      "Reconstruction: 0.136191, Regularization: 0.004930\n",
      "2019-04-10 01:15:24,915 root         INFO     Train Epoch: 183 [512/8000 (6%)]\tTotal Loss: 0.149228\n",
      "Reconstruction: 0.143385, Regularization: 0.005843\n",
      "2019-04-10 01:15:24,972 root         INFO     Train Epoch: 183 [1024/8000 (13%)]\tTotal Loss: 0.148330\n",
      "Reconstruction: 0.143290, Regularization: 0.005040\n",
      "2019-04-10 01:15:25,028 root         INFO     Train Epoch: 183 [1536/8000 (19%)]\tTotal Loss: 0.142996\n",
      "Reconstruction: 0.135776, Regularization: 0.007219\n",
      "2019-04-10 01:15:25,084 root         INFO     Train Epoch: 183 [2048/8000 (26%)]\tTotal Loss: 0.134633\n",
      "Reconstruction: 0.130621, Regularization: 0.004012\n",
      "2019-04-10 01:15:25,140 root         INFO     Train Epoch: 183 [2560/8000 (32%)]\tTotal Loss: 0.169886\n",
      "Reconstruction: 0.160421, Regularization: 0.009465\n",
      "2019-04-10 01:15:25,196 root         INFO     Train Epoch: 183 [3072/8000 (38%)]\tTotal Loss: 0.144816\n",
      "Reconstruction: 0.140235, Regularization: 0.004581\n",
      "2019-04-10 01:15:25,253 root         INFO     Train Epoch: 183 [3584/8000 (45%)]\tTotal Loss: 0.145325\n",
      "Reconstruction: 0.138898, Regularization: 0.006426\n",
      "2019-04-10 01:15:25,309 root         INFO     Train Epoch: 183 [4096/8000 (51%)]\tTotal Loss: 0.150686\n",
      "Reconstruction: 0.145745, Regularization: 0.004941\n",
      "2019-04-10 01:15:25,365 root         INFO     Train Epoch: 183 [4608/8000 (58%)]\tTotal Loss: 0.146340\n",
      "Reconstruction: 0.139553, Regularization: 0.006787\n",
      "2019-04-10 01:15:25,422 root         INFO     Train Epoch: 183 [5120/8000 (64%)]\tTotal Loss: 0.137617\n",
      "Reconstruction: 0.132409, Regularization: 0.005208\n",
      "2019-04-10 01:15:25,478 root         INFO     Train Epoch: 183 [5632/8000 (70%)]\tTotal Loss: 0.148391\n",
      "Reconstruction: 0.140965, Regularization: 0.007426\n",
      "2019-04-10 01:15:25,535 root         INFO     Train Epoch: 183 [6144/8000 (77%)]\tTotal Loss: 0.137118\n",
      "Reconstruction: 0.133135, Regularization: 0.003983\n",
      "2019-04-10 01:15:25,591 root         INFO     Train Epoch: 183 [6656/8000 (83%)]\tTotal Loss: 0.149054\n",
      "Reconstruction: 0.143986, Regularization: 0.005068\n",
      "2019-04-10 01:15:25,648 root         INFO     Train Epoch: 183 [7168/8000 (90%)]\tTotal Loss: 0.149199\n",
      "Reconstruction: 0.144232, Regularization: 0.004967\n",
      "2019-04-10 01:15:25,705 root         INFO     Train Epoch: 183 [7680/8000 (96%)]\tTotal Loss: 0.143025\n",
      "Reconstruction: 0.137837, Regularization: 0.005188\n",
      "2019-04-10 01:15:25,755 root         INFO     ====> Epoch: 183 Average loss: 0.1495\n",
      "2019-04-10 01:15:25,778 root         INFO     Train Epoch: 184 [0/8000 (0%)]\tTotal Loss: 0.146520\n",
      "Reconstruction: 0.141138, Regularization: 0.005381\n",
      "2019-04-10 01:15:25,835 root         INFO     Train Epoch: 184 [512/8000 (6%)]\tTotal Loss: 0.145520\n",
      "Reconstruction: 0.139898, Regularization: 0.005622\n",
      "2019-04-10 01:15:25,891 root         INFO     Train Epoch: 184 [1024/8000 (13%)]\tTotal Loss: 0.139916\n",
      "Reconstruction: 0.135137, Regularization: 0.004779\n",
      "2019-04-10 01:15:25,948 root         INFO     Train Epoch: 184 [1536/8000 (19%)]\tTotal Loss: 0.149904\n",
      "Reconstruction: 0.144691, Regularization: 0.005214\n",
      "2019-04-10 01:15:26,004 root         INFO     Train Epoch: 184 [2048/8000 (26%)]\tTotal Loss: 0.148580\n",
      "Reconstruction: 0.142048, Regularization: 0.006532\n",
      "2019-04-10 01:15:26,061 root         INFO     Train Epoch: 184 [2560/8000 (32%)]\tTotal Loss: 0.149661\n",
      "Reconstruction: 0.142246, Regularization: 0.007415\n",
      "2019-04-10 01:15:26,117 root         INFO     Train Epoch: 184 [3072/8000 (38%)]\tTotal Loss: 0.142606\n",
      "Reconstruction: 0.136678, Regularization: 0.005928\n",
      "2019-04-10 01:15:26,174 root         INFO     Train Epoch: 184 [3584/8000 (45%)]\tTotal Loss: 0.144084\n",
      "Reconstruction: 0.139111, Regularization: 0.004973\n",
      "2019-04-10 01:15:26,230 root         INFO     Train Epoch: 184 [4096/8000 (51%)]\tTotal Loss: 0.151387\n",
      "Reconstruction: 0.147168, Regularization: 0.004219\n",
      "2019-04-10 01:15:26,286 root         INFO     Train Epoch: 184 [4608/8000 (58%)]\tTotal Loss: 0.154066\n",
      "Reconstruction: 0.149875, Regularization: 0.004191\n",
      "2019-04-10 01:15:26,343 root         INFO     Train Epoch: 184 [5120/8000 (64%)]\tTotal Loss: 0.152080\n",
      "Reconstruction: 0.144519, Regularization: 0.007561\n",
      "2019-04-10 01:15:26,399 root         INFO     Train Epoch: 184 [5632/8000 (70%)]\tTotal Loss: 0.147065\n",
      "Reconstruction: 0.141480, Regularization: 0.005585\n",
      "2019-04-10 01:15:26,455 root         INFO     Train Epoch: 184 [6144/8000 (77%)]\tTotal Loss: 0.142897\n",
      "Reconstruction: 0.135425, Regularization: 0.007472\n",
      "2019-04-10 01:15:26,511 root         INFO     Train Epoch: 184 [6656/8000 (83%)]\tTotal Loss: 0.137027\n",
      "Reconstruction: 0.133113, Regularization: 0.003914\n",
      "2019-04-10 01:15:26,567 root         INFO     Train Epoch: 184 [7168/8000 (90%)]\tTotal Loss: 0.149769\n",
      "Reconstruction: 0.143878, Regularization: 0.005891\n",
      "2019-04-10 01:15:26,622 root         INFO     Train Epoch: 184 [7680/8000 (96%)]\tTotal Loss: 0.145542\n",
      "Reconstruction: 0.140581, Regularization: 0.004961\n",
      "2019-04-10 01:15:26,672 root         INFO     ====> Epoch: 184 Average loss: 0.1495\n",
      "2019-04-10 01:15:26,695 root         INFO     Train Epoch: 185 [0/8000 (0%)]\tTotal Loss: 0.145427\n",
      "Reconstruction: 0.140869, Regularization: 0.004558\n",
      "2019-04-10 01:15:26,751 root         INFO     Train Epoch: 185 [512/8000 (6%)]\tTotal Loss: 0.144121\n",
      "Reconstruction: 0.139072, Regularization: 0.005049\n",
      "2019-04-10 01:15:26,807 root         INFO     Train Epoch: 185 [1024/8000 (13%)]\tTotal Loss: 0.179158\n",
      "Reconstruction: 0.171484, Regularization: 0.007673\n",
      "2019-04-10 01:15:26,862 root         INFO     Train Epoch: 185 [1536/8000 (19%)]\tTotal Loss: 0.132923\n",
      "Reconstruction: 0.129019, Regularization: 0.003905\n",
      "2019-04-10 01:15:26,917 root         INFO     Train Epoch: 185 [2048/8000 (26%)]\tTotal Loss: 0.161279\n",
      "Reconstruction: 0.151664, Regularization: 0.009616\n",
      "2019-04-10 01:15:26,973 root         INFO     Train Epoch: 185 [2560/8000 (32%)]\tTotal Loss: 0.153989\n",
      "Reconstruction: 0.146135, Regularization: 0.007855\n",
      "2019-04-10 01:15:27,028 root         INFO     Train Epoch: 185 [3072/8000 (38%)]\tTotal Loss: 0.150218\n",
      "Reconstruction: 0.143954, Regularization: 0.006264\n",
      "2019-04-10 01:15:27,083 root         INFO     Train Epoch: 185 [3584/8000 (45%)]\tTotal Loss: 0.154910\n",
      "Reconstruction: 0.147812, Regularization: 0.007098\n",
      "2019-04-10 01:15:27,139 root         INFO     Train Epoch: 185 [4096/8000 (51%)]\tTotal Loss: 0.153948\n",
      "Reconstruction: 0.146390, Regularization: 0.007558\n",
      "2019-04-10 01:15:27,194 root         INFO     Train Epoch: 185 [4608/8000 (58%)]\tTotal Loss: 0.151234\n",
      "Reconstruction: 0.145783, Regularization: 0.005451\n",
      "2019-04-10 01:15:27,249 root         INFO     Train Epoch: 185 [5120/8000 (64%)]\tTotal Loss: 0.164587\n",
      "Reconstruction: 0.156906, Regularization: 0.007681\n",
      "2019-04-10 01:15:27,305 root         INFO     Train Epoch: 185 [5632/8000 (70%)]\tTotal Loss: 0.147593\n",
      "Reconstruction: 0.141641, Regularization: 0.005953\n",
      "2019-04-10 01:15:27,360 root         INFO     Train Epoch: 185 [6144/8000 (77%)]\tTotal Loss: 0.156154\n",
      "Reconstruction: 0.149244, Regularization: 0.006909\n",
      "2019-04-10 01:15:27,416 root         INFO     Train Epoch: 185 [6656/8000 (83%)]\tTotal Loss: 0.148654\n",
      "Reconstruction: 0.141869, Regularization: 0.006785\n",
      "2019-04-10 01:15:27,471 root         INFO     Train Epoch: 185 [7168/8000 (90%)]\tTotal Loss: 0.142316\n",
      "Reconstruction: 0.135204, Regularization: 0.007112\n",
      "2019-04-10 01:15:27,527 root         INFO     Train Epoch: 185 [7680/8000 (96%)]\tTotal Loss: 0.144659\n",
      "Reconstruction: 0.136847, Regularization: 0.007812\n",
      "2019-04-10 01:15:27,576 root         INFO     ====> Epoch: 185 Average loss: 0.1493\n",
      "2019-04-10 01:15:27,599 root         INFO     Train Epoch: 186 [0/8000 (0%)]\tTotal Loss: 0.148466\n",
      "Reconstruction: 0.142861, Regularization: 0.005605\n",
      "2019-04-10 01:15:27,657 root         INFO     Train Epoch: 186 [512/8000 (6%)]\tTotal Loss: 0.139071\n",
      "Reconstruction: 0.135147, Regularization: 0.003923\n",
      "2019-04-10 01:15:27,713 root         INFO     Train Epoch: 186 [1024/8000 (13%)]\tTotal Loss: 0.157377\n",
      "Reconstruction: 0.149849, Regularization: 0.007527\n",
      "2019-04-10 01:15:27,770 root         INFO     Train Epoch: 186 [1536/8000 (19%)]\tTotal Loss: 0.146760\n",
      "Reconstruction: 0.142052, Regularization: 0.004708\n",
      "2019-04-10 01:15:27,826 root         INFO     Train Epoch: 186 [2048/8000 (26%)]\tTotal Loss: 0.148697\n",
      "Reconstruction: 0.142849, Regularization: 0.005848\n",
      "2019-04-10 01:15:27,882 root         INFO     Train Epoch: 186 [2560/8000 (32%)]\tTotal Loss: 0.153667\n",
      "Reconstruction: 0.148372, Regularization: 0.005296\n",
      "2019-04-10 01:15:27,938 root         INFO     Train Epoch: 186 [3072/8000 (38%)]\tTotal Loss: 0.145725\n",
      "Reconstruction: 0.141456, Regularization: 0.004269\n",
      "2019-04-10 01:15:27,995 root         INFO     Train Epoch: 186 [3584/8000 (45%)]\tTotal Loss: 0.151062\n",
      "Reconstruction: 0.146288, Regularization: 0.004774\n",
      "2019-04-10 01:15:28,051 root         INFO     Train Epoch: 186 [4096/8000 (51%)]\tTotal Loss: 0.159002\n",
      "Reconstruction: 0.149643, Regularization: 0.009360\n",
      "2019-04-10 01:15:28,107 root         INFO     Train Epoch: 186 [4608/8000 (58%)]\tTotal Loss: 0.155369\n",
      "Reconstruction: 0.148486, Regularization: 0.006883\n",
      "2019-04-10 01:15:28,164 root         INFO     Train Epoch: 186 [5120/8000 (64%)]\tTotal Loss: 0.147149\n",
      "Reconstruction: 0.141736, Regularization: 0.005413\n",
      "2019-04-10 01:15:28,220 root         INFO     Train Epoch: 186 [5632/8000 (70%)]\tTotal Loss: 0.150959\n",
      "Reconstruction: 0.144734, Regularization: 0.006225\n",
      "2019-04-10 01:15:28,277 root         INFO     Train Epoch: 186 [6144/8000 (77%)]\tTotal Loss: 0.149777\n",
      "Reconstruction: 0.144355, Regularization: 0.005422\n",
      "2019-04-10 01:15:28,333 root         INFO     Train Epoch: 186 [6656/8000 (83%)]\tTotal Loss: 0.151407\n",
      "Reconstruction: 0.143600, Regularization: 0.007807\n",
      "2019-04-10 01:15:28,389 root         INFO     Train Epoch: 186 [7168/8000 (90%)]\tTotal Loss: 0.155617\n",
      "Reconstruction: 0.149944, Regularization: 0.005673\n",
      "2019-04-10 01:15:28,446 root         INFO     Train Epoch: 186 [7680/8000 (96%)]\tTotal Loss: 0.166971\n",
      "Reconstruction: 0.159768, Regularization: 0.007203\n",
      "2019-04-10 01:15:28,496 root         INFO     ====> Epoch: 186 Average loss: 0.1494\n",
      "2019-04-10 01:15:28,519 root         INFO     Train Epoch: 187 [0/8000 (0%)]\tTotal Loss: 0.133509\n",
      "Reconstruction: 0.129390, Regularization: 0.004119\n",
      "2019-04-10 01:15:28,576 root         INFO     Train Epoch: 187 [512/8000 (6%)]\tTotal Loss: 0.150384\n",
      "Reconstruction: 0.146822, Regularization: 0.003562\n",
      "2019-04-10 01:15:28,632 root         INFO     Train Epoch: 187 [1024/8000 (13%)]\tTotal Loss: 0.155305\n",
      "Reconstruction: 0.147013, Regularization: 0.008291\n",
      "2019-04-10 01:15:28,688 root         INFO     Train Epoch: 187 [1536/8000 (19%)]\tTotal Loss: 0.154055\n",
      "Reconstruction: 0.149568, Regularization: 0.004487\n",
      "2019-04-10 01:15:28,745 root         INFO     Train Epoch: 187 [2048/8000 (26%)]\tTotal Loss: 0.147481\n",
      "Reconstruction: 0.141266, Regularization: 0.006215\n",
      "2019-04-10 01:15:28,801 root         INFO     Train Epoch: 187 [2560/8000 (32%)]\tTotal Loss: 0.138382\n",
      "Reconstruction: 0.134641, Regularization: 0.003741\n",
      "2019-04-10 01:15:28,858 root         INFO     Train Epoch: 187 [3072/8000 (38%)]\tTotal Loss: 0.154549\n",
      "Reconstruction: 0.150702, Regularization: 0.003847\n",
      "2019-04-10 01:15:28,915 root         INFO     Train Epoch: 187 [3584/8000 (45%)]\tTotal Loss: 0.153391\n",
      "Reconstruction: 0.147027, Regularization: 0.006363\n",
      "2019-04-10 01:15:28,972 root         INFO     Train Epoch: 187 [4096/8000 (51%)]\tTotal Loss: 0.136231\n",
      "Reconstruction: 0.131559, Regularization: 0.004672\n",
      "2019-04-10 01:15:29,029 root         INFO     Train Epoch: 187 [4608/8000 (58%)]\tTotal Loss: 0.143596\n",
      "Reconstruction: 0.138356, Regularization: 0.005240\n",
      "2019-04-10 01:15:29,086 root         INFO     Train Epoch: 187 [5120/8000 (64%)]\tTotal Loss: 0.144331\n",
      "Reconstruction: 0.139095, Regularization: 0.005236\n",
      "2019-04-10 01:15:29,143 root         INFO     Train Epoch: 187 [5632/8000 (70%)]\tTotal Loss: 0.151392\n",
      "Reconstruction: 0.143566, Regularization: 0.007826\n",
      "2019-04-10 01:15:29,200 root         INFO     Train Epoch: 187 [6144/8000 (77%)]\tTotal Loss: 0.154030\n",
      "Reconstruction: 0.150934, Regularization: 0.003096\n",
      "2019-04-10 01:15:29,257 root         INFO     Train Epoch: 187 [6656/8000 (83%)]\tTotal Loss: 0.165660\n",
      "Reconstruction: 0.153011, Regularization: 0.012649\n",
      "2019-04-10 01:15:29,314 root         INFO     Train Epoch: 187 [7168/8000 (90%)]\tTotal Loss: 0.153014\n",
      "Reconstruction: 0.147380, Regularization: 0.005634\n",
      "2019-04-10 01:15:29,371 root         INFO     Train Epoch: 187 [7680/8000 (96%)]\tTotal Loss: 0.143340\n",
      "Reconstruction: 0.137940, Regularization: 0.005400\n",
      "2019-04-10 01:15:29,421 root         INFO     ====> Epoch: 187 Average loss: 0.1491\n",
      "2019-04-10 01:15:29,445 root         INFO     Train Epoch: 188 [0/8000 (0%)]\tTotal Loss: 0.150879\n",
      "Reconstruction: 0.144679, Regularization: 0.006200\n",
      "2019-04-10 01:15:29,501 root         INFO     Train Epoch: 188 [512/8000 (6%)]\tTotal Loss: 0.156716\n",
      "Reconstruction: 0.147433, Regularization: 0.009283\n",
      "2019-04-10 01:15:29,556 root         INFO     Train Epoch: 188 [1024/8000 (13%)]\tTotal Loss: 0.145408\n",
      "Reconstruction: 0.136773, Regularization: 0.008635\n",
      "2019-04-10 01:15:29,611 root         INFO     Train Epoch: 188 [1536/8000 (19%)]\tTotal Loss: 0.146827\n",
      "Reconstruction: 0.142462, Regularization: 0.004365\n",
      "2019-04-10 01:15:29,667 root         INFO     Train Epoch: 188 [2048/8000 (26%)]\tTotal Loss: 0.149726\n",
      "Reconstruction: 0.144692, Regularization: 0.005034\n",
      "2019-04-10 01:15:29,722 root         INFO     Train Epoch: 188 [2560/8000 (32%)]\tTotal Loss: 0.171708\n",
      "Reconstruction: 0.159423, Regularization: 0.012285\n",
      "2019-04-10 01:15:29,777 root         INFO     Train Epoch: 188 [3072/8000 (38%)]\tTotal Loss: 0.135685\n",
      "Reconstruction: 0.131428, Regularization: 0.004256\n",
      "2019-04-10 01:15:29,832 root         INFO     Train Epoch: 188 [3584/8000 (45%)]\tTotal Loss: 0.143556\n",
      "Reconstruction: 0.137430, Regularization: 0.006126\n",
      "2019-04-10 01:15:29,887 root         INFO     Train Epoch: 188 [4096/8000 (51%)]\tTotal Loss: 0.143217\n",
      "Reconstruction: 0.137966, Regularization: 0.005251\n",
      "2019-04-10 01:15:29,942 root         INFO     Train Epoch: 188 [4608/8000 (58%)]\tTotal Loss: 0.153301\n",
      "Reconstruction: 0.148935, Regularization: 0.004367\n",
      "2019-04-10 01:15:29,997 root         INFO     Train Epoch: 188 [5120/8000 (64%)]\tTotal Loss: 0.137503\n",
      "Reconstruction: 0.133275, Regularization: 0.004228\n",
      "2019-04-10 01:15:30,052 root         INFO     Train Epoch: 188 [5632/8000 (70%)]\tTotal Loss: 0.149198\n",
      "Reconstruction: 0.144478, Regularization: 0.004720\n",
      "2019-04-10 01:15:30,107 root         INFO     Train Epoch: 188 [6144/8000 (77%)]\tTotal Loss: 0.145864\n",
      "Reconstruction: 0.139377, Regularization: 0.006487\n",
      "2019-04-10 01:15:30,162 root         INFO     Train Epoch: 188 [6656/8000 (83%)]\tTotal Loss: 0.144846\n",
      "Reconstruction: 0.140751, Regularization: 0.004094\n",
      "2019-04-10 01:15:30,217 root         INFO     Train Epoch: 188 [7168/8000 (90%)]\tTotal Loss: 0.143717\n",
      "Reconstruction: 0.139982, Regularization: 0.003735\n",
      "2019-04-10 01:15:30,271 root         INFO     Train Epoch: 188 [7680/8000 (96%)]\tTotal Loss: 0.138263\n",
      "Reconstruction: 0.133842, Regularization: 0.004421\n",
      "2019-04-10 01:15:30,320 root         INFO     ====> Epoch: 188 Average loss: 0.1494\n",
      "2019-04-10 01:15:30,344 root         INFO     Train Epoch: 189 [0/8000 (0%)]\tTotal Loss: 0.151745\n",
      "Reconstruction: 0.143904, Regularization: 0.007841\n",
      "2019-04-10 01:15:30,402 root         INFO     Train Epoch: 189 [512/8000 (6%)]\tTotal Loss: 0.162272\n",
      "Reconstruction: 0.156529, Regularization: 0.005742\n",
      "2019-04-10 01:15:30,459 root         INFO     Train Epoch: 189 [1024/8000 (13%)]\tTotal Loss: 0.149538\n",
      "Reconstruction: 0.143433, Regularization: 0.006104\n",
      "2019-04-10 01:15:30,516 root         INFO     Train Epoch: 189 [1536/8000 (19%)]\tTotal Loss: 0.136129\n",
      "Reconstruction: 0.131510, Regularization: 0.004619\n",
      "2019-04-10 01:15:30,572 root         INFO     Train Epoch: 189 [2048/8000 (26%)]\tTotal Loss: 0.148070\n",
      "Reconstruction: 0.143020, Regularization: 0.005050\n",
      "2019-04-10 01:15:30,629 root         INFO     Train Epoch: 189 [2560/8000 (32%)]\tTotal Loss: 0.161838\n",
      "Reconstruction: 0.154476, Regularization: 0.007361\n",
      "2019-04-10 01:15:30,685 root         INFO     Train Epoch: 189 [3072/8000 (38%)]\tTotal Loss: 0.154759\n",
      "Reconstruction: 0.148104, Regularization: 0.006655\n",
      "2019-04-10 01:15:30,741 root         INFO     Train Epoch: 189 [3584/8000 (45%)]\tTotal Loss: 0.154630\n",
      "Reconstruction: 0.149533, Regularization: 0.005097\n",
      "2019-04-10 01:15:30,798 root         INFO     Train Epoch: 189 [4096/8000 (51%)]\tTotal Loss: 0.141201\n",
      "Reconstruction: 0.136087, Regularization: 0.005113\n",
      "2019-04-10 01:15:30,855 root         INFO     Train Epoch: 189 [4608/8000 (58%)]\tTotal Loss: 0.145286\n",
      "Reconstruction: 0.140517, Regularization: 0.004769\n",
      "2019-04-10 01:15:30,911 root         INFO     Train Epoch: 189 [5120/8000 (64%)]\tTotal Loss: 0.140995\n",
      "Reconstruction: 0.136942, Regularization: 0.004053\n",
      "2019-04-10 01:15:30,967 root         INFO     Train Epoch: 189 [5632/8000 (70%)]\tTotal Loss: 0.157800\n",
      "Reconstruction: 0.151242, Regularization: 0.006558\n",
      "2019-04-10 01:15:31,024 root         INFO     Train Epoch: 189 [6144/8000 (77%)]\tTotal Loss: 0.161598\n",
      "Reconstruction: 0.155409, Regularization: 0.006189\n",
      "2019-04-10 01:15:31,080 root         INFO     Train Epoch: 189 [6656/8000 (83%)]\tTotal Loss: 0.136134\n",
      "Reconstruction: 0.132193, Regularization: 0.003941\n",
      "2019-04-10 01:15:31,136 root         INFO     Train Epoch: 189 [7168/8000 (90%)]\tTotal Loss: 0.148224\n",
      "Reconstruction: 0.143756, Regularization: 0.004468\n",
      "2019-04-10 01:15:31,193 root         INFO     Train Epoch: 189 [7680/8000 (96%)]\tTotal Loss: 0.138767\n",
      "Reconstruction: 0.135942, Regularization: 0.002824\n",
      "2019-04-10 01:15:31,243 root         INFO     ====> Epoch: 189 Average loss: 0.1498\n",
      "2019-04-10 01:15:31,268 root         INFO     Train Epoch: 190 [0/8000 (0%)]\tTotal Loss: 0.151809\n",
      "Reconstruction: 0.146491, Regularization: 0.005318\n",
      "2019-04-10 01:15:31,324 root         INFO     Train Epoch: 190 [512/8000 (6%)]\tTotal Loss: 0.145254\n",
      "Reconstruction: 0.140618, Regularization: 0.004636\n",
      "2019-04-10 01:15:31,380 root         INFO     Train Epoch: 190 [1024/8000 (13%)]\tTotal Loss: 0.142927\n",
      "Reconstruction: 0.134951, Regularization: 0.007976\n",
      "2019-04-10 01:15:31,436 root         INFO     Train Epoch: 190 [1536/8000 (19%)]\tTotal Loss: 0.144421\n",
      "Reconstruction: 0.138268, Regularization: 0.006153\n",
      "2019-04-10 01:15:31,491 root         INFO     Train Epoch: 190 [2048/8000 (26%)]\tTotal Loss: 0.149545\n",
      "Reconstruction: 0.143323, Regularization: 0.006222\n",
      "2019-04-10 01:15:31,547 root         INFO     Train Epoch: 190 [2560/8000 (32%)]\tTotal Loss: 0.140409\n",
      "Reconstruction: 0.135429, Regularization: 0.004979\n",
      "2019-04-10 01:15:31,602 root         INFO     Train Epoch: 190 [3072/8000 (38%)]\tTotal Loss: 0.162593\n",
      "Reconstruction: 0.153283, Regularization: 0.009309\n",
      "2019-04-10 01:15:31,658 root         INFO     Train Epoch: 190 [3584/8000 (45%)]\tTotal Loss: 0.146296\n",
      "Reconstruction: 0.140821, Regularization: 0.005475\n",
      "2019-04-10 01:15:31,713 root         INFO     Train Epoch: 190 [4096/8000 (51%)]\tTotal Loss: 0.149345\n",
      "Reconstruction: 0.144918, Regularization: 0.004427\n",
      "2019-04-10 01:15:31,769 root         INFO     Train Epoch: 190 [4608/8000 (58%)]\tTotal Loss: 0.168854\n",
      "Reconstruction: 0.161718, Regularization: 0.007136\n",
      "2019-04-10 01:15:31,825 root         INFO     Train Epoch: 190 [5120/8000 (64%)]\tTotal Loss: 0.168957\n",
      "Reconstruction: 0.159859, Regularization: 0.009098\n",
      "2019-04-10 01:15:31,881 root         INFO     Train Epoch: 190 [5632/8000 (70%)]\tTotal Loss: 0.152492\n",
      "Reconstruction: 0.146365, Regularization: 0.006128\n",
      "2019-04-10 01:15:31,938 root         INFO     Train Epoch: 190 [6144/8000 (77%)]\tTotal Loss: 0.151726\n",
      "Reconstruction: 0.147242, Regularization: 0.004484\n",
      "2019-04-10 01:15:31,994 root         INFO     Train Epoch: 190 [6656/8000 (83%)]\tTotal Loss: 0.151518\n",
      "Reconstruction: 0.145793, Regularization: 0.005725\n",
      "2019-04-10 01:15:32,051 root         INFO     Train Epoch: 190 [7168/8000 (90%)]\tTotal Loss: 0.153132\n",
      "Reconstruction: 0.147307, Regularization: 0.005825\n",
      "2019-04-10 01:15:32,107 root         INFO     Train Epoch: 190 [7680/8000 (96%)]\tTotal Loss: 0.133166\n",
      "Reconstruction: 0.127437, Regularization: 0.005728\n",
      "2019-04-10 01:15:32,156 root         INFO     ====> Epoch: 190 Average loss: 0.1496\n",
      "2019-04-10 01:15:32,180 root         INFO     Train Epoch: 191 [0/8000 (0%)]\tTotal Loss: 0.145623\n",
      "Reconstruction: 0.141203, Regularization: 0.004419\n",
      "2019-04-10 01:15:32,237 root         INFO     Train Epoch: 191 [512/8000 (6%)]\tTotal Loss: 0.139726\n",
      "Reconstruction: 0.134696, Regularization: 0.005030\n",
      "2019-04-10 01:15:32,293 root         INFO     Train Epoch: 191 [1024/8000 (13%)]\tTotal Loss: 0.151988\n",
      "Reconstruction: 0.144497, Regularization: 0.007491\n",
      "2019-04-10 01:15:32,350 root         INFO     Train Epoch: 191 [1536/8000 (19%)]\tTotal Loss: 0.149305\n",
      "Reconstruction: 0.142802, Regularization: 0.006503\n",
      "2019-04-10 01:15:32,407 root         INFO     Train Epoch: 191 [2048/8000 (26%)]\tTotal Loss: 0.143825\n",
      "Reconstruction: 0.135808, Regularization: 0.008017\n",
      "2019-04-10 01:15:32,464 root         INFO     Train Epoch: 191 [2560/8000 (32%)]\tTotal Loss: 0.159399\n",
      "Reconstruction: 0.151899, Regularization: 0.007500\n",
      "2019-04-10 01:15:32,521 root         INFO     Train Epoch: 191 [3072/8000 (38%)]\tTotal Loss: 0.152790\n",
      "Reconstruction: 0.146048, Regularization: 0.006742\n",
      "2019-04-10 01:15:32,577 root         INFO     Train Epoch: 191 [3584/8000 (45%)]\tTotal Loss: 0.148403\n",
      "Reconstruction: 0.143736, Regularization: 0.004667\n",
      "2019-04-10 01:15:32,633 root         INFO     Train Epoch: 191 [4096/8000 (51%)]\tTotal Loss: 0.151439\n",
      "Reconstruction: 0.146656, Regularization: 0.004783\n",
      "2019-04-10 01:15:32,689 root         INFO     Train Epoch: 191 [4608/8000 (58%)]\tTotal Loss: 0.146532\n",
      "Reconstruction: 0.140471, Regularization: 0.006061\n",
      "2019-04-10 01:15:32,744 root         INFO     Train Epoch: 191 [5120/8000 (64%)]\tTotal Loss: 0.154657\n",
      "Reconstruction: 0.148112, Regularization: 0.006544\n",
      "2019-04-10 01:15:32,800 root         INFO     Train Epoch: 191 [5632/8000 (70%)]\tTotal Loss: 0.135323\n",
      "Reconstruction: 0.131163, Regularization: 0.004160\n",
      "2019-04-10 01:15:32,856 root         INFO     Train Epoch: 191 [6144/8000 (77%)]\tTotal Loss: 0.137617\n",
      "Reconstruction: 0.133032, Regularization: 0.004585\n",
      "2019-04-10 01:15:32,913 root         INFO     Train Epoch: 191 [6656/8000 (83%)]\tTotal Loss: 0.156470\n",
      "Reconstruction: 0.142198, Regularization: 0.014273\n",
      "2019-04-10 01:15:32,969 root         INFO     Train Epoch: 191 [7168/8000 (90%)]\tTotal Loss: 0.155872\n",
      "Reconstruction: 0.148895, Regularization: 0.006976\n",
      "2019-04-10 01:15:33,026 root         INFO     Train Epoch: 191 [7680/8000 (96%)]\tTotal Loss: 0.142917\n",
      "Reconstruction: 0.137161, Regularization: 0.005756\n",
      "2019-04-10 01:15:33,076 root         INFO     ====> Epoch: 191 Average loss: 0.1492\n",
      "2019-04-10 01:15:33,100 root         INFO     Train Epoch: 192 [0/8000 (0%)]\tTotal Loss: 0.156802\n",
      "Reconstruction: 0.145657, Regularization: 0.011146\n",
      "2019-04-10 01:15:33,157 root         INFO     Train Epoch: 192 [512/8000 (6%)]\tTotal Loss: 0.161770\n",
      "Reconstruction: 0.154375, Regularization: 0.007395\n",
      "2019-04-10 01:15:33,215 root         INFO     Train Epoch: 192 [1024/8000 (13%)]\tTotal Loss: 0.152511\n",
      "Reconstruction: 0.143770, Regularization: 0.008741\n",
      "2019-04-10 01:15:33,273 root         INFO     Train Epoch: 192 [1536/8000 (19%)]\tTotal Loss: 0.146083\n",
      "Reconstruction: 0.140760, Regularization: 0.005323\n",
      "2019-04-10 01:15:33,331 root         INFO     Train Epoch: 192 [2048/8000 (26%)]\tTotal Loss: 0.142593\n",
      "Reconstruction: 0.139312, Regularization: 0.003281\n",
      "2019-04-10 01:15:33,388 root         INFO     Train Epoch: 192 [2560/8000 (32%)]\tTotal Loss: 0.151778\n",
      "Reconstruction: 0.147582, Regularization: 0.004196\n",
      "2019-04-10 01:15:33,445 root         INFO     Train Epoch: 192 [3072/8000 (38%)]\tTotal Loss: 0.144432\n",
      "Reconstruction: 0.139333, Regularization: 0.005099\n",
      "2019-04-10 01:15:33,501 root         INFO     Train Epoch: 192 [3584/8000 (45%)]\tTotal Loss: 0.169410\n",
      "Reconstruction: 0.160517, Regularization: 0.008894\n",
      "2019-04-10 01:15:33,558 root         INFO     Train Epoch: 192 [4096/8000 (51%)]\tTotal Loss: 0.144481\n",
      "Reconstruction: 0.138340, Regularization: 0.006141\n",
      "2019-04-10 01:15:33,614 root         INFO     Train Epoch: 192 [4608/8000 (58%)]\tTotal Loss: 0.160161\n",
      "Reconstruction: 0.150544, Regularization: 0.009617\n",
      "2019-04-10 01:15:33,671 root         INFO     Train Epoch: 192 [5120/8000 (64%)]\tTotal Loss: 0.141899\n",
      "Reconstruction: 0.136709, Regularization: 0.005190\n",
      "2019-04-10 01:15:33,728 root         INFO     Train Epoch: 192 [5632/8000 (70%)]\tTotal Loss: 0.146240\n",
      "Reconstruction: 0.138949, Regularization: 0.007290\n",
      "2019-04-10 01:15:33,784 root         INFO     Train Epoch: 192 [6144/8000 (77%)]\tTotal Loss: 0.154591\n",
      "Reconstruction: 0.149310, Regularization: 0.005282\n",
      "2019-04-10 01:15:33,841 root         INFO     Train Epoch: 192 [6656/8000 (83%)]\tTotal Loss: 0.140815\n",
      "Reconstruction: 0.136036, Regularization: 0.004779\n",
      "2019-04-10 01:15:33,898 root         INFO     Train Epoch: 192 [7168/8000 (90%)]\tTotal Loss: 0.149305\n",
      "Reconstruction: 0.139286, Regularization: 0.010019\n",
      "2019-04-10 01:15:33,954 root         INFO     Train Epoch: 192 [7680/8000 (96%)]\tTotal Loss: 0.142791\n",
      "Reconstruction: 0.137101, Regularization: 0.005690\n",
      "2019-04-10 01:15:34,005 root         INFO     ====> Epoch: 192 Average loss: 0.1494\n",
      "2019-04-10 01:15:34,028 root         INFO     Train Epoch: 193 [0/8000 (0%)]\tTotal Loss: 0.149910\n",
      "Reconstruction: 0.143814, Regularization: 0.006096\n",
      "2019-04-10 01:15:34,084 root         INFO     Train Epoch: 193 [512/8000 (6%)]\tTotal Loss: 0.161598\n",
      "Reconstruction: 0.152227, Regularization: 0.009371\n",
      "2019-04-10 01:15:34,140 root         INFO     Train Epoch: 193 [1024/8000 (13%)]\tTotal Loss: 0.131245\n",
      "Reconstruction: 0.127835, Regularization: 0.003411\n",
      "2019-04-10 01:15:34,195 root         INFO     Train Epoch: 193 [1536/8000 (19%)]\tTotal Loss: 0.159608\n",
      "Reconstruction: 0.154683, Regularization: 0.004925\n",
      "2019-04-10 01:15:34,250 root         INFO     Train Epoch: 193 [2048/8000 (26%)]\tTotal Loss: 0.141744\n",
      "Reconstruction: 0.138000, Regularization: 0.003745\n",
      "2019-04-10 01:15:34,306 root         INFO     Train Epoch: 193 [2560/8000 (32%)]\tTotal Loss: 0.167924\n",
      "Reconstruction: 0.160042, Regularization: 0.007883\n",
      "2019-04-10 01:15:34,361 root         INFO     Train Epoch: 193 [3072/8000 (38%)]\tTotal Loss: 0.150687\n",
      "Reconstruction: 0.143141, Regularization: 0.007546\n",
      "2019-04-10 01:15:34,416 root         INFO     Train Epoch: 193 [3584/8000 (45%)]\tTotal Loss: 0.144458\n",
      "Reconstruction: 0.140020, Regularization: 0.004438\n",
      "2019-04-10 01:15:34,471 root         INFO     Train Epoch: 193 [4096/8000 (51%)]\tTotal Loss: 0.146819\n",
      "Reconstruction: 0.143043, Regularization: 0.003776\n",
      "2019-04-10 01:15:34,526 root         INFO     Train Epoch: 193 [4608/8000 (58%)]\tTotal Loss: 0.150276\n",
      "Reconstruction: 0.143856, Regularization: 0.006420\n",
      "2019-04-10 01:15:34,582 root         INFO     Train Epoch: 193 [5120/8000 (64%)]\tTotal Loss: 0.167733\n",
      "Reconstruction: 0.162468, Regularization: 0.005265\n",
      "2019-04-10 01:15:34,637 root         INFO     Train Epoch: 193 [5632/8000 (70%)]\tTotal Loss: 0.159048\n",
      "Reconstruction: 0.147720, Regularization: 0.011328\n",
      "2019-04-10 01:15:34,693 root         INFO     Train Epoch: 193 [6144/8000 (77%)]\tTotal Loss: 0.154625\n",
      "Reconstruction: 0.150239, Regularization: 0.004386\n",
      "2019-04-10 01:15:34,749 root         INFO     Train Epoch: 193 [6656/8000 (83%)]\tTotal Loss: 0.151951\n",
      "Reconstruction: 0.145467, Regularization: 0.006483\n",
      "2019-04-10 01:15:34,805 root         INFO     Train Epoch: 193 [7168/8000 (90%)]\tTotal Loss: 0.141399\n",
      "Reconstruction: 0.136193, Regularization: 0.005206\n",
      "2019-04-10 01:15:34,860 root         INFO     Train Epoch: 193 [7680/8000 (96%)]\tTotal Loss: 0.146092\n",
      "Reconstruction: 0.138983, Regularization: 0.007109\n",
      "2019-04-10 01:15:34,910 root         INFO     ====> Epoch: 193 Average loss: 0.1496\n",
      "2019-04-10 01:15:34,933 root         INFO     Train Epoch: 194 [0/8000 (0%)]\tTotal Loss: 0.144083\n",
      "Reconstruction: 0.136445, Regularization: 0.007638\n",
      "2019-04-10 01:15:34,995 root         INFO     Train Epoch: 194 [512/8000 (6%)]\tTotal Loss: 0.140589\n",
      "Reconstruction: 0.135444, Regularization: 0.005144\n",
      "2019-04-10 01:15:35,055 root         INFO     Train Epoch: 194 [1024/8000 (13%)]\tTotal Loss: 0.150104\n",
      "Reconstruction: 0.143746, Regularization: 0.006358\n",
      "2019-04-10 01:15:35,115 root         INFO     Train Epoch: 194 [1536/8000 (19%)]\tTotal Loss: 0.142102\n",
      "Reconstruction: 0.137389, Regularization: 0.004713\n",
      "2019-04-10 01:15:35,175 root         INFO     Train Epoch: 194 [2048/8000 (26%)]\tTotal Loss: 0.140862\n",
      "Reconstruction: 0.137684, Regularization: 0.003178\n",
      "2019-04-10 01:15:35,235 root         INFO     Train Epoch: 194 [2560/8000 (32%)]\tTotal Loss: 0.139585\n",
      "Reconstruction: 0.134987, Regularization: 0.004598\n",
      "2019-04-10 01:15:35,295 root         INFO     Train Epoch: 194 [3072/8000 (38%)]\tTotal Loss: 0.152980\n",
      "Reconstruction: 0.145077, Regularization: 0.007903\n",
      "2019-04-10 01:15:35,355 root         INFO     Train Epoch: 194 [3584/8000 (45%)]\tTotal Loss: 0.152278\n",
      "Reconstruction: 0.145046, Regularization: 0.007232\n",
      "2019-04-10 01:15:35,416 root         INFO     Train Epoch: 194 [4096/8000 (51%)]\tTotal Loss: 0.149664\n",
      "Reconstruction: 0.143865, Regularization: 0.005799\n",
      "2019-04-10 01:15:35,475 root         INFO     Train Epoch: 194 [4608/8000 (58%)]\tTotal Loss: 0.155008\n",
      "Reconstruction: 0.145116, Regularization: 0.009892\n",
      "2019-04-10 01:15:35,533 root         INFO     Train Epoch: 194 [5120/8000 (64%)]\tTotal Loss: 0.155610\n",
      "Reconstruction: 0.147690, Regularization: 0.007920\n",
      "2019-04-10 01:15:35,591 root         INFO     Train Epoch: 194 [5632/8000 (70%)]\tTotal Loss: 0.158706\n",
      "Reconstruction: 0.149096, Regularization: 0.009611\n",
      "2019-04-10 01:15:35,651 root         INFO     Train Epoch: 194 [6144/8000 (77%)]\tTotal Loss: 0.161552\n",
      "Reconstruction: 0.152201, Regularization: 0.009351\n",
      "2019-04-10 01:15:35,710 root         INFO     Train Epoch: 194 [6656/8000 (83%)]\tTotal Loss: 0.149580\n",
      "Reconstruction: 0.144184, Regularization: 0.005397\n",
      "2019-04-10 01:15:35,769 root         INFO     Train Epoch: 194 [7168/8000 (90%)]\tTotal Loss: 0.145590\n",
      "Reconstruction: 0.139128, Regularization: 0.006462\n",
      "2019-04-10 01:15:35,829 root         INFO     Train Epoch: 194 [7680/8000 (96%)]\tTotal Loss: 0.159174\n",
      "Reconstruction: 0.150744, Regularization: 0.008429\n",
      "2019-04-10 01:15:35,880 root         INFO     ====> Epoch: 194 Average loss: 0.1494\n",
      "2019-04-10 01:15:35,903 root         INFO     Train Epoch: 195 [0/8000 (0%)]\tTotal Loss: 0.144859\n",
      "Reconstruction: 0.139603, Regularization: 0.005256\n",
      "2019-04-10 01:15:35,960 root         INFO     Train Epoch: 195 [512/8000 (6%)]\tTotal Loss: 0.139687\n",
      "Reconstruction: 0.136990, Regularization: 0.002698\n",
      "2019-04-10 01:15:36,017 root         INFO     Train Epoch: 195 [1024/8000 (13%)]\tTotal Loss: 0.134453\n",
      "Reconstruction: 0.129159, Regularization: 0.005294\n",
      "2019-04-10 01:15:36,074 root         INFO     Train Epoch: 195 [1536/8000 (19%)]\tTotal Loss: 0.150672\n",
      "Reconstruction: 0.144845, Regularization: 0.005827\n",
      "2019-04-10 01:15:36,131 root         INFO     Train Epoch: 195 [2048/8000 (26%)]\tTotal Loss: 0.141357\n",
      "Reconstruction: 0.137169, Regularization: 0.004187\n",
      "2019-04-10 01:15:36,188 root         INFO     Train Epoch: 195 [2560/8000 (32%)]\tTotal Loss: 0.162426\n",
      "Reconstruction: 0.155799, Regularization: 0.006627\n",
      "2019-04-10 01:15:36,244 root         INFO     Train Epoch: 195 [3072/8000 (38%)]\tTotal Loss: 0.142271\n",
      "Reconstruction: 0.136486, Regularization: 0.005785\n",
      "2019-04-10 01:15:36,301 root         INFO     Train Epoch: 195 [3584/8000 (45%)]\tTotal Loss: 0.151452\n",
      "Reconstruction: 0.145430, Regularization: 0.006021\n",
      "2019-04-10 01:15:36,357 root         INFO     Train Epoch: 195 [4096/8000 (51%)]\tTotal Loss: 0.163100\n",
      "Reconstruction: 0.155254, Regularization: 0.007845\n",
      "2019-04-10 01:15:36,414 root         INFO     Train Epoch: 195 [4608/8000 (58%)]\tTotal Loss: 0.156005\n",
      "Reconstruction: 0.148925, Regularization: 0.007080\n",
      "2019-04-10 01:15:36,470 root         INFO     Train Epoch: 195 [5120/8000 (64%)]\tTotal Loss: 0.151703\n",
      "Reconstruction: 0.144106, Regularization: 0.007597\n",
      "2019-04-10 01:15:36,526 root         INFO     Train Epoch: 195 [5632/8000 (70%)]\tTotal Loss: 0.150192\n",
      "Reconstruction: 0.142063, Regularization: 0.008129\n",
      "2019-04-10 01:15:36,582 root         INFO     Train Epoch: 195 [6144/8000 (77%)]\tTotal Loss: 0.157317\n",
      "Reconstruction: 0.148007, Regularization: 0.009310\n",
      "2019-04-10 01:15:36,638 root         INFO     Train Epoch: 195 [6656/8000 (83%)]\tTotal Loss: 0.142567\n",
      "Reconstruction: 0.136455, Regularization: 0.006112\n",
      "2019-04-10 01:15:36,693 root         INFO     Train Epoch: 195 [7168/8000 (90%)]\tTotal Loss: 0.166867\n",
      "Reconstruction: 0.158959, Regularization: 0.007908\n",
      "2019-04-10 01:15:36,749 root         INFO     Train Epoch: 195 [7680/8000 (96%)]\tTotal Loss: 0.157595\n",
      "Reconstruction: 0.149170, Regularization: 0.008424\n",
      "2019-04-10 01:15:36,799 root         INFO     ====> Epoch: 195 Average loss: 0.1497\n",
      "2019-04-10 01:15:36,822 root         INFO     Train Epoch: 196 [0/8000 (0%)]\tTotal Loss: 0.167379\n",
      "Reconstruction: 0.156768, Regularization: 0.010611\n",
      "2019-04-10 01:15:36,879 root         INFO     Train Epoch: 196 [512/8000 (6%)]\tTotal Loss: 0.149230\n",
      "Reconstruction: 0.141255, Regularization: 0.007974\n",
      "2019-04-10 01:15:36,936 root         INFO     Train Epoch: 196 [1024/8000 (13%)]\tTotal Loss: 0.146781\n",
      "Reconstruction: 0.140602, Regularization: 0.006179\n",
      "2019-04-10 01:15:36,992 root         INFO     Train Epoch: 196 [1536/8000 (19%)]\tTotal Loss: 0.162300\n",
      "Reconstruction: 0.156120, Regularization: 0.006180\n",
      "2019-04-10 01:15:37,049 root         INFO     Train Epoch: 196 [2048/8000 (26%)]\tTotal Loss: 0.158707\n",
      "Reconstruction: 0.151435, Regularization: 0.007272\n",
      "2019-04-10 01:15:37,105 root         INFO     Train Epoch: 196 [2560/8000 (32%)]\tTotal Loss: 0.147407\n",
      "Reconstruction: 0.141922, Regularization: 0.005485\n",
      "2019-04-10 01:15:37,161 root         INFO     Train Epoch: 196 [3072/8000 (38%)]\tTotal Loss: 0.155493\n",
      "Reconstruction: 0.147357, Regularization: 0.008136\n",
      "2019-04-10 01:15:37,216 root         INFO     Train Epoch: 196 [3584/8000 (45%)]\tTotal Loss: 0.129774\n",
      "Reconstruction: 0.125967, Regularization: 0.003807\n",
      "2019-04-10 01:15:37,272 root         INFO     Train Epoch: 196 [4096/8000 (51%)]\tTotal Loss: 0.138182\n",
      "Reconstruction: 0.132843, Regularization: 0.005340\n",
      "2019-04-10 01:15:37,327 root         INFO     Train Epoch: 196 [4608/8000 (58%)]\tTotal Loss: 0.161092\n",
      "Reconstruction: 0.153442, Regularization: 0.007650\n",
      "2019-04-10 01:15:37,383 root         INFO     Train Epoch: 196 [5120/8000 (64%)]\tTotal Loss: 0.127479\n",
      "Reconstruction: 0.123711, Regularization: 0.003769\n",
      "2019-04-10 01:15:37,439 root         INFO     Train Epoch: 196 [5632/8000 (70%)]\tTotal Loss: 0.142336\n",
      "Reconstruction: 0.135242, Regularization: 0.007094\n",
      "2019-04-10 01:15:37,494 root         INFO     Train Epoch: 196 [6144/8000 (77%)]\tTotal Loss: 0.145601\n",
      "Reconstruction: 0.138383, Regularization: 0.007218\n",
      "2019-04-10 01:15:37,549 root         INFO     Train Epoch: 196 [6656/8000 (83%)]\tTotal Loss: 0.163023\n",
      "Reconstruction: 0.155441, Regularization: 0.007582\n",
      "2019-04-10 01:15:37,605 root         INFO     Train Epoch: 196 [7168/8000 (90%)]\tTotal Loss: 0.155957\n",
      "Reconstruction: 0.147002, Regularization: 0.008955\n",
      "2019-04-10 01:15:37,659 root         INFO     Train Epoch: 196 [7680/8000 (96%)]\tTotal Loss: 0.141242\n",
      "Reconstruction: 0.135974, Regularization: 0.005268\n",
      "2019-04-10 01:15:37,708 root         INFO     ====> Epoch: 196 Average loss: 0.1492\n",
      "2019-04-10 01:15:37,732 root         INFO     Train Epoch: 197 [0/8000 (0%)]\tTotal Loss: 0.164060\n",
      "Reconstruction: 0.156935, Regularization: 0.007125\n",
      "2019-04-10 01:15:37,788 root         INFO     Train Epoch: 197 [512/8000 (6%)]\tTotal Loss: 0.159481\n",
      "Reconstruction: 0.155794, Regularization: 0.003687\n",
      "2019-04-10 01:15:37,844 root         INFO     Train Epoch: 197 [1024/8000 (13%)]\tTotal Loss: 0.151328\n",
      "Reconstruction: 0.146498, Regularization: 0.004829\n",
      "2019-04-10 01:15:37,900 root         INFO     Train Epoch: 197 [1536/8000 (19%)]\tTotal Loss: 0.136016\n",
      "Reconstruction: 0.131294, Regularization: 0.004722\n",
      "2019-04-10 01:15:37,957 root         INFO     Train Epoch: 197 [2048/8000 (26%)]\tTotal Loss: 0.161866\n",
      "Reconstruction: 0.153114, Regularization: 0.008752\n",
      "2019-04-10 01:15:38,012 root         INFO     Train Epoch: 197 [2560/8000 (32%)]\tTotal Loss: 0.139135\n",
      "Reconstruction: 0.134284, Regularization: 0.004851\n",
      "2019-04-10 01:15:38,068 root         INFO     Train Epoch: 197 [3072/8000 (38%)]\tTotal Loss: 0.141334\n",
      "Reconstruction: 0.135196, Regularization: 0.006139\n",
      "2019-04-10 01:15:38,123 root         INFO     Train Epoch: 197 [3584/8000 (45%)]\tTotal Loss: 0.146059\n",
      "Reconstruction: 0.140665, Regularization: 0.005394\n",
      "2019-04-10 01:15:38,178 root         INFO     Train Epoch: 197 [4096/8000 (51%)]\tTotal Loss: 0.144446\n",
      "Reconstruction: 0.139700, Regularization: 0.004746\n",
      "2019-04-10 01:15:38,234 root         INFO     Train Epoch: 197 [4608/8000 (58%)]\tTotal Loss: 0.148108\n",
      "Reconstruction: 0.140621, Regularization: 0.007487\n",
      "2019-04-10 01:15:38,289 root         INFO     Train Epoch: 197 [5120/8000 (64%)]\tTotal Loss: 0.145809\n",
      "Reconstruction: 0.140194, Regularization: 0.005615\n",
      "2019-04-10 01:15:38,345 root         INFO     Train Epoch: 197 [5632/8000 (70%)]\tTotal Loss: 0.155289\n",
      "Reconstruction: 0.148758, Regularization: 0.006532\n",
      "2019-04-10 01:15:38,400 root         INFO     Train Epoch: 197 [6144/8000 (77%)]\tTotal Loss: 0.152416\n",
      "Reconstruction: 0.147613, Regularization: 0.004803\n",
      "2019-04-10 01:15:38,456 root         INFO     Train Epoch: 197 [6656/8000 (83%)]\tTotal Loss: 0.152823\n",
      "Reconstruction: 0.146829, Regularization: 0.005993\n",
      "2019-04-10 01:15:38,511 root         INFO     Train Epoch: 197 [7168/8000 (90%)]\tTotal Loss: 0.141386\n",
      "Reconstruction: 0.134256, Regularization: 0.007130\n",
      "2019-04-10 01:15:38,566 root         INFO     Train Epoch: 197 [7680/8000 (96%)]\tTotal Loss: 0.155449\n",
      "Reconstruction: 0.144541, Regularization: 0.010908\n",
      "2019-04-10 01:15:38,616 root         INFO     ====> Epoch: 197 Average loss: 0.1498\n",
      "2019-04-10 01:15:38,639 root         INFO     Train Epoch: 198 [0/8000 (0%)]\tTotal Loss: 0.151539\n",
      "Reconstruction: 0.144517, Regularization: 0.007022\n",
      "2019-04-10 01:15:38,696 root         INFO     Train Epoch: 198 [512/8000 (6%)]\tTotal Loss: 0.159784\n",
      "Reconstruction: 0.155265, Regularization: 0.004520\n",
      "2019-04-10 01:15:38,752 root         INFO     Train Epoch: 198 [1024/8000 (13%)]\tTotal Loss: 0.156763\n",
      "Reconstruction: 0.152533, Regularization: 0.004231\n",
      "2019-04-10 01:15:38,809 root         INFO     Train Epoch: 198 [1536/8000 (19%)]\tTotal Loss: 0.148276\n",
      "Reconstruction: 0.143687, Regularization: 0.004590\n",
      "2019-04-10 01:15:38,865 root         INFO     Train Epoch: 198 [2048/8000 (26%)]\tTotal Loss: 0.157998\n",
      "Reconstruction: 0.147359, Regularization: 0.010639\n",
      "2019-04-10 01:15:38,921 root         INFO     Train Epoch: 198 [2560/8000 (32%)]\tTotal Loss: 0.146403\n",
      "Reconstruction: 0.139994, Regularization: 0.006409\n",
      "2019-04-10 01:15:38,978 root         INFO     Train Epoch: 198 [3072/8000 (38%)]\tTotal Loss: 0.138737\n",
      "Reconstruction: 0.132690, Regularization: 0.006047\n",
      "2019-04-10 01:15:39,033 root         INFO     Train Epoch: 198 [3584/8000 (45%)]\tTotal Loss: 0.151386\n",
      "Reconstruction: 0.143096, Regularization: 0.008290\n",
      "2019-04-10 01:15:39,088 root         INFO     Train Epoch: 198 [4096/8000 (51%)]\tTotal Loss: 0.137471\n",
      "Reconstruction: 0.130591, Regularization: 0.006880\n",
      "2019-04-10 01:15:39,143 root         INFO     Train Epoch: 198 [4608/8000 (58%)]\tTotal Loss: 0.144361\n",
      "Reconstruction: 0.137896, Regularization: 0.006465\n",
      "2019-04-10 01:15:39,197 root         INFO     Train Epoch: 198 [5120/8000 (64%)]\tTotal Loss: 0.142603\n",
      "Reconstruction: 0.136514, Regularization: 0.006088\n",
      "2019-04-10 01:15:39,252 root         INFO     Train Epoch: 198 [5632/8000 (70%)]\tTotal Loss: 0.156804\n",
      "Reconstruction: 0.146120, Regularization: 0.010684\n",
      "2019-04-10 01:15:39,308 root         INFO     Train Epoch: 198 [6144/8000 (77%)]\tTotal Loss: 0.136717\n",
      "Reconstruction: 0.133102, Regularization: 0.003614\n",
      "2019-04-10 01:15:39,364 root         INFO     Train Epoch: 198 [6656/8000 (83%)]\tTotal Loss: 0.155899\n",
      "Reconstruction: 0.147978, Regularization: 0.007921\n",
      "2019-04-10 01:15:39,420 root         INFO     Train Epoch: 198 [7168/8000 (90%)]\tTotal Loss: 0.152358\n",
      "Reconstruction: 0.148212, Regularization: 0.004146\n",
      "2019-04-10 01:15:39,475 root         INFO     Train Epoch: 198 [7680/8000 (96%)]\tTotal Loss: 0.137717\n",
      "Reconstruction: 0.133332, Regularization: 0.004385\n",
      "2019-04-10 01:15:39,526 root         INFO     ====> Epoch: 198 Average loss: 0.1496\n",
      "2019-04-10 01:15:39,549 root         INFO     Train Epoch: 199 [0/8000 (0%)]\tTotal Loss: 0.150058\n",
      "Reconstruction: 0.142437, Regularization: 0.007621\n",
      "2019-04-10 01:15:39,606 root         INFO     Train Epoch: 199 [512/8000 (6%)]\tTotal Loss: 0.135204\n",
      "Reconstruction: 0.129729, Regularization: 0.005475\n",
      "2019-04-10 01:15:39,662 root         INFO     Train Epoch: 199 [1024/8000 (13%)]\tTotal Loss: 0.147956\n",
      "Reconstruction: 0.141381, Regularization: 0.006575\n",
      "2019-04-10 01:15:39,718 root         INFO     Train Epoch: 199 [1536/8000 (19%)]\tTotal Loss: 0.144575\n",
      "Reconstruction: 0.138717, Regularization: 0.005859\n",
      "2019-04-10 01:15:39,774 root         INFO     Train Epoch: 199 [2048/8000 (26%)]\tTotal Loss: 0.152862\n",
      "Reconstruction: 0.144829, Regularization: 0.008033\n",
      "2019-04-10 01:15:39,831 root         INFO     Train Epoch: 199 [2560/8000 (32%)]\tTotal Loss: 0.144081\n",
      "Reconstruction: 0.138959, Regularization: 0.005122\n",
      "2019-04-10 01:15:39,887 root         INFO     Train Epoch: 199 [3072/8000 (38%)]\tTotal Loss: 0.147534\n",
      "Reconstruction: 0.143505, Regularization: 0.004029\n",
      "2019-04-10 01:15:39,943 root         INFO     Train Epoch: 199 [3584/8000 (45%)]\tTotal Loss: 0.148170\n",
      "Reconstruction: 0.142547, Regularization: 0.005623\n",
      "2019-04-10 01:15:39,999 root         INFO     Train Epoch: 199 [4096/8000 (51%)]\tTotal Loss: 0.148660\n",
      "Reconstruction: 0.139445, Regularization: 0.009215\n",
      "2019-04-10 01:15:40,055 root         INFO     Train Epoch: 199 [4608/8000 (58%)]\tTotal Loss: 0.137653\n",
      "Reconstruction: 0.133593, Regularization: 0.004060\n",
      "2019-04-10 01:15:40,112 root         INFO     Train Epoch: 199 [5120/8000 (64%)]\tTotal Loss: 0.149973\n",
      "Reconstruction: 0.144942, Regularization: 0.005030\n",
      "2019-04-10 01:15:40,168 root         INFO     Train Epoch: 199 [5632/8000 (70%)]\tTotal Loss: 0.139749\n",
      "Reconstruction: 0.135371, Regularization: 0.004377\n",
      "2019-04-10 01:15:40,224 root         INFO     Train Epoch: 199 [6144/8000 (77%)]\tTotal Loss: 0.149144\n",
      "Reconstruction: 0.142793, Regularization: 0.006351\n",
      "2019-04-10 01:15:40,280 root         INFO     Train Epoch: 199 [6656/8000 (83%)]\tTotal Loss: 0.148892\n",
      "Reconstruction: 0.140249, Regularization: 0.008643\n",
      "2019-04-10 01:15:40,336 root         INFO     Train Epoch: 199 [7168/8000 (90%)]\tTotal Loss: 0.147862\n",
      "Reconstruction: 0.143070, Regularization: 0.004792\n",
      "2019-04-10 01:15:40,392 root         INFO     Train Epoch: 199 [7680/8000 (96%)]\tTotal Loss: 0.163229\n",
      "Reconstruction: 0.158528, Regularization: 0.004701\n",
      "2019-04-10 01:15:40,442 root         INFO     ====> Epoch: 199 Average loss: 0.1491\n",
      "2019-04-10 01:15:40,452 luigi-interface INFO     [pid 6006] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=6006) done      TrainVAE()\n",
      "2019-04-10 01:15:40,452 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 01:15:40,453 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   DONE\n",
      "2019-04-10 01:15:40,453 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:15:40,453 luigi-interface DEBUG    Pending tasks: 2\n",
      "2019-04-10 01:15:40,453 luigi-interface INFO     [pid 6006] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=6006) running   TrainVEM()\n",
      "2019-04-10 01:15:40,454 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-10 01:15:40,454 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-10 01:15:40,455 root         INFO     Values of VEM's decoder parameters before training:\n",
      "2019-04-10 01:15:40,455 root         INFO     layers.0.weight\n",
      "2019-04-10 01:15:40,455 root         INFO     tensor([[-0.0579],\n",
      "        [-0.7300]], device='cuda:0')\n",
      "2019-04-10 01:15:40,456 root         INFO     layers.0.bias\n",
      "2019-04-10 01:15:40,456 root         INFO     tensor([-0.4374, -0.8691], device='cuda:0')\n",
      "2019-04-10 01:15:40,457 root         INFO     layers.1.weight\n",
      "2019-04-10 01:15:40,457 root         INFO     tensor([[ 0.5430, -0.5742],\n",
      "        [ 0.0914,  0.3519]], device='cuda:0')\n",
      "2019-04-10 01:15:40,458 root         INFO     layers.1.bias\n",
      "2019-04-10 01:15:40,459 root         INFO     tensor([0.0299, 0.0259], device='cuda:0')\n",
      "2019-04-10 01:15:40,459 root         INFO     layers.2.weight\n",
      "2019-04-10 01:15:40,460 root         INFO     tensor([[-0.3639,  0.4931],\n",
      "        [-0.2239,  0.5638]], device='cuda:0')\n",
      "2019-04-10 01:15:40,461 root         INFO     layers.2.bias\n",
      "2019-04-10 01:15:40,461 root         INFO     tensor([-0.3158, -0.1928], device='cuda:0')\n",
      "2019-04-10 01:15:40,501 luigi-interface ERROR    [pid 6006] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=6006) failed    TrainVEM()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/luigi/worker.py\", line 199, in run\n",
      "    new_deps = self._run_get_new_deps()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/luigi/worker.py\", line 139, in _run_get_new_deps\n",
      "    task_gen = self.task.run()\n",
      "  File \"toypipeline.py\", line 541, in run\n",
      "    epoch, train_loader, modules, optimizers)\n",
      "  File \"toypipeline.py\", line 367, in train_vem\n",
      "    batch_data, batch_recon, batch_logvarx)\n",
      "  File \"/home/nina/code/vaetree/toylosses.py\", line 39, in reconstruction_loss\n",
      "    raise ValueError('aux has a inf')\n",
      "ValueError: aux has a inf\n",
      "2019-04-10 01:15:40,502 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 01:15:40,504 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   FAILED\n",
      "2019-04-10 01:15:40,504 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:15:40,505 luigi-interface DEBUG    Done\n",
      "2019-04-10 01:15:40,505 luigi-interface DEBUG    There are no more tasks to run at this time\n",
      "2019-04-10 01:15:40,505 luigi-interface DEBUG    There are 2 pending tasks possibly being run by other workers\n",
      "2019-04-10 01:15:40,505 luigi-interface DEBUG    There are 2 pending tasks unique to this worker\n",
      "2019-04-10 01:15:40,505 luigi-interface DEBUG    There are 2 pending tasks last scheduled by this worker\n",
      "2019-04-10 01:15:40,505 luigi-interface INFO     Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=6006) was stopped. Shutting down Keep-Alive thread\n",
      "2019-04-10 01:15:40,506 luigi-interface INFO     \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 MakeDataSet()\n",
      "* 1 ran successfully:\n",
      "    - 1 TrainVAE()\n",
      "* 1 failed:\n",
      "    - 1 TrainVEM()\n",
      "* 1 were left pending, among these:\n",
      "    * 1 had failed dependencies:\n",
      "        - 1 RunAll()\n",
      "\n",
      "This progress looks :( because there were failed tasks\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "\n",
      "\n",
      "-- Log file: logs2019-04-10 01:12:19.372168.txt\n",
      "\n",
      "2019-04-10 01:12:19,372 root         INFO     start\n",
      "2019-04-10 01:12:19,387 luigi        INFO     logging configured by default settings\n",
      "2019-04-10 01:12:19,391 luigi-interface DEBUG    Checking if RunAll() is complete\n",
      "2019-04-10 01:12:19,392 luigi-interface DEBUG    Checking if TrainVAE() is complete\n",
      "2019-04-10 01:12:19,392 luigi-interface DEBUG    Checking if TrainVEM() is complete\n",
      "2019-04-10 01:12:19,392 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   PENDING\n",
      "2019-04-10 01:12:19,393 luigi-interface DEBUG    Checking if MakeDataSet() is complete\n",
      "2019-04-10 01:12:19,393 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   PENDING\n",
      "2019-04-10 01:12:19,393 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   PENDING\n",
      "2019-04-10 01:12:19,394 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   PENDING\n",
      "2019-04-10 01:12:19,394 luigi-interface INFO     Done scheduling tasks\n",
      "2019-04-10 01:12:19,394 luigi-interface INFO     Running Worker with 1 processes\n",
      "2019-04-10 01:12:19,394 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:12:19,394 luigi-interface DEBUG    Pending tasks: 4\n",
      "2019-04-10 01:12:19,394 luigi-interface INFO     [pid 5941] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=5941) running   MakeDataSet()\n",
      "2019-04-10 01:12:19,395 root         INFO     Configuration:\n",
      "2019-04-10 01:12:19,395 root         INFO     DATA_DIM = 2\n",
      "2019-04-10 01:12:19,395 root         INFO     LATENT_DIM = 1\n",
      "2019-04-10 01:12:19,395 root         INFO     N_DECODER_LAYERS = 2\n",
      "2019-04-10 01:12:19,395 root         INFO     NONLINEARITY=True\n",
      "2019-04-10 01:12:19,395 root         INFO     WITH_BIASX=True\n",
      "2019-04-10 01:12:19,395 root         INFO     WITH_LOGVARX=True\n",
      "2019-04-10 01:12:19,395 root         INFO     WITH_BIASZ=True\n",
      "2019-04-10 01:12:19,395 root         INFO     WITH_LOGVARZ=True\n",
      "2019-04-10 01:12:19,395 root         INFO     N_SAMPLES=10000\n",
      "2019-04-10 01:12:19,395 root         INFO     W_TRUE:\n",
      "2019-04-10 01:12:19,395 root         INFO     {0: [[0.6], [-0.7]], 1: [[10.0, 0.0], [0.0, -5.0]], 2: [[0.0, 0.0], [0.0, 0.0]]}\n",
      "2019-04-10 01:12:19,395 root         INFO     B_TRUE:\n",
      "2019-04-10 01:12:19,395 root         INFO     {0: [0.0, -0.1], 1: [2.0, 0.0], 2: [0.0, 0.0]}\n",
      "2019-04-10 01:12:23,219 root         INFO     Values of true 'decoder' parameters:\n",
      "2019-04-10 01:12:23,219 root         INFO     layers.0.weight\n",
      "2019-04-10 01:12:23,219 root         INFO     tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0')\n",
      "2019-04-10 01:12:23,239 root         INFO     layers.0.bias\n",
      "2019-04-10 01:12:23,239 root         INFO     tensor([ 0.0000, -0.1000], device='cuda:0')\n",
      "2019-04-10 01:12:23,240 root         INFO     layers.1.weight\n",
      "2019-04-10 01:12:23,240 root         INFO     tensor([[10.,  0.],\n",
      "        [ 0., -5.]], device='cuda:0')\n",
      "2019-04-10 01:12:23,241 root         INFO     layers.1.bias\n",
      "2019-04-10 01:12:23,241 root         INFO     tensor([2., 0.], device='cuda:0')\n",
      "2019-04-10 01:12:23,242 root         INFO     layers.2.weight\n",
      "2019-04-10 01:12:23,242 root         INFO     tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "2019-04-10 01:12:23,243 root         INFO     layers.2.bias\n",
      "2019-04-10 01:12:23,243 root         INFO     tensor([0., 0.], device='cuda:0')\n",
      "2019-04-10 01:12:23,348 luigi-interface INFO     [pid 5941] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=5941) done      MakeDataSet()\n",
      "2019-04-10 01:12:23,348 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 01:12:23,349 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   DONE\n",
      "2019-04-10 01:12:23,349 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:12:23,349 luigi-interface DEBUG    Pending tasks: 3\n",
      "2019-04-10 01:12:23,349 luigi-interface INFO     [pid 5941] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=5941) running   TrainVEM()\n",
      "2019-04-10 01:12:23,351 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-10 01:12:23,351 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-10 01:12:23,353 root         INFO     Values of VEM's decoder parameters before training:\n",
      "2019-04-10 01:12:23,353 root         INFO     layers.0.weight\n",
      "2019-04-10 01:12:23,353 root         INFO     tensor([[-0.8110],\n",
      "        [ 0.7163]], device='cuda:0')\n",
      "2019-04-10 01:12:23,354 root         INFO     layers.0.bias\n",
      "2019-04-10 01:12:23,354 root         INFO     tensor([-0.7052,  0.2492], device='cuda:0')\n",
      "2019-04-10 01:12:23,355 root         INFO     layers.1.weight\n",
      "2019-04-10 01:12:23,355 root         INFO     tensor([[-0.3539, -0.5961],\n",
      "        [-0.0450,  0.2346]], device='cuda:0')\n",
      "2019-04-10 01:12:23,356 root         INFO     layers.1.bias\n",
      "2019-04-10 01:12:23,356 root         INFO     tensor([-0.6488,  0.0511], device='cuda:0')\n",
      "2019-04-10 01:12:23,357 root         INFO     layers.2.weight\n",
      "2019-04-10 01:12:23,357 root         INFO     tensor([[-0.2659, -0.1007],\n",
      "        [-0.3153, -0.0881]], device='cuda:0')\n",
      "2019-04-10 01:12:23,359 root         INFO     layers.2.bias\n",
      "2019-04-10 01:12:23,359 root         INFO     tensor([ 0.6361, -0.6798], device='cuda:0')\n",
      "2019-04-10 01:12:23,422 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 1716454814917391810560.000000\n",
      "Reconstruction: 1716454814917391810560.000000, Regularization: 309.845184, Discriminator: 0.042861; Generator: 0.020580,\n",
      "D(x): 0.526, D(G(z)): 0.518\n",
      "2019-04-10 01:12:23,522 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042481; Generator: 0.020728,\n",
      "D(x): 0.530, D(G(z)): 0.515\n",
      "2019-04-10 01:12:23,622 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042226; Generator: 0.020838,\n",
      "D(x): 0.532, D(G(z)): 0.513\n",
      "2019-04-10 01:12:23,721 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041777; Generator: 0.020964,\n",
      "D(x): 0.538, D(G(z)): 0.511\n",
      "2019-04-10 01:12:23,820 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041387; Generator: 0.021111,\n",
      "D(x): 0.542, D(G(z)): 0.509\n",
      "2019-04-10 01:12:23,919 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041441; Generator: 0.021225,\n",
      "D(x): 0.539, D(G(z)): 0.507\n",
      "2019-04-10 01:12:24,019 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040877; Generator: 0.021353,\n",
      "D(x): 0.547, D(G(z)): 0.505\n",
      "2019-04-10 01:12:24,120 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041181; Generator: 0.021491,\n",
      "D(x): 0.539, D(G(z)): 0.503\n",
      "2019-04-10 01:12:24,221 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041133; Generator: 0.021586,\n",
      "D(x): 0.538, D(G(z)): 0.501\n",
      "2019-04-10 01:12:24,320 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039879; Generator: 0.021810,\n",
      "D(x): 0.557, D(G(z)): 0.498\n",
      "2019-04-10 01:12:24,420 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039708; Generator: 0.021904,\n",
      "D(x): 0.558, D(G(z)): 0.496\n",
      "2019-04-10 01:12:24,519 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040030; Generator: 0.022028,\n",
      "D(x): 0.550, D(G(z)): 0.494\n",
      "2019-04-10 01:12:24,618 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039201; Generator: 0.022237,\n",
      "D(x): 0.563, D(G(z)): 0.491\n",
      "2019-04-10 01:12:24,717 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038496; Generator: 0.022371,\n",
      "D(x): 0.572, D(G(z)): 0.489\n",
      "2019-04-10 01:12:24,819 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038826; Generator: 0.022580,\n",
      "D(x): 0.563, D(G(z)): 0.486\n",
      "2019-04-10 01:12:24,923 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038619; Generator: 0.022663,\n",
      "D(x): 0.565, D(G(z)): 0.484\n",
      "2019-04-10 01:12:25,000 root         INFO     ====> Epoch: 0 Average loss: nan\n",
      "2019-04-10 01:12:25,026 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038176; Generator: 0.022831,\n",
      "D(x): 0.571, D(G(z)): 0.482\n",
      "2019-04-10 01:12:25,126 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037890; Generator: 0.023040,\n",
      "D(x): 0.572, D(G(z)): 0.478\n",
      "2019-04-10 01:12:25,225 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038332; Generator: 0.023171,\n",
      "D(x): 0.562, D(G(z)): 0.476\n",
      "2019-04-10 01:12:25,325 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037716; Generator: 0.023221,\n",
      "D(x): 0.574, D(G(z)): 0.476\n",
      "2019-04-10 01:12:25,424 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035347; Generator: 0.023496,\n",
      "D(x): 0.615, D(G(z)): 0.472\n",
      "2019-04-10 01:12:25,524 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037649; Generator: 0.023546,\n",
      "D(x): 0.571, D(G(z)): 0.471\n",
      "2019-04-10 01:12:25,622 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035982; Generator: 0.023920,\n",
      "D(x): 0.595, D(G(z)): 0.465\n",
      "2019-04-10 01:12:25,720 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036417; Generator: 0.024219,\n",
      "D(x): 0.582, D(G(z)): 0.461\n",
      "2019-04-10 01:12:25,820 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035776; Generator: 0.024339,\n",
      "D(x): 0.592, D(G(z)): 0.459\n",
      "2019-04-10 01:12:25,921 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034794; Generator: 0.024417,\n",
      "D(x): 0.611, D(G(z)): 0.458\n",
      "2019-04-10 01:12:26,022 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035221; Generator: 0.024619,\n",
      "D(x): 0.600, D(G(z)): 0.455\n",
      "2019-04-10 01:12:26,122 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033312; Generator: 0.024864,\n",
      "D(x): 0.637, D(G(z)): 0.451\n",
      "2019-04-10 01:12:26,220 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035252; Generator: 0.024950,\n",
      "D(x): 0.593, D(G(z)): 0.450\n",
      "2019-04-10 01:12:26,320 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034287; Generator: 0.025204,\n",
      "D(x): 0.610, D(G(z)): 0.446\n",
      "2019-04-10 01:12:26,419 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032510; Generator: 0.025499,\n",
      "D(x): 0.639, D(G(z)): 0.442\n",
      "2019-04-10 01:12:26,518 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033531; Generator: 0.025745,\n",
      "D(x): 0.614, D(G(z)): 0.439\n",
      "2019-04-10 01:12:26,592 root         INFO     ====> Epoch: 1 Average loss: nan\n",
      "2019-04-10 01:12:26,618 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033605; Generator: 0.025726,\n",
      "D(x): 0.615, D(G(z)): 0.439\n",
      "2019-04-10 01:12:26,720 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032770; Generator: 0.025892,\n",
      "D(x): 0.627, D(G(z)): 0.437\n",
      "2019-04-10 01:12:26,821 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031788; Generator: 0.026281,\n",
      "D(x): 0.644, D(G(z)): 0.431\n",
      "2019-04-10 01:12:26,923 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033823; Generator: 0.026330,\n",
      "D(x): 0.602, D(G(z)): 0.431\n",
      "2019-04-10 01:12:27,024 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031690; Generator: 0.026786,\n",
      "D(x): 0.644, D(G(z)): 0.425\n",
      "2019-04-10 01:12:27,126 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031270; Generator: 0.026916,\n",
      "D(x): 0.645, D(G(z)): 0.423\n",
      "2019-04-10 01:12:27,227 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031324; Generator: 0.027279,\n",
      "D(x): 0.641, D(G(z)): 0.418\n",
      "2019-04-10 01:12:27,329 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.029384; Generator: 0.027159,\n",
      "D(x): 0.686, D(G(z)): 0.419\n",
      "2019-04-10 01:12:27,430 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030891; Generator: 0.027769,\n",
      "D(x): 0.641, D(G(z)): 0.411\n",
      "2019-04-10 01:12:27,532 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030531; Generator: 0.028030,\n",
      "D(x): 0.643, D(G(z)): 0.408\n",
      "2019-04-10 01:12:27,633 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030774; Generator: 0.027823,\n",
      "D(x): 0.645, D(G(z)): 0.411\n",
      "2019-04-10 01:12:27,735 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028297; Generator: 0.028262,\n",
      "D(x): 0.689, D(G(z)): 0.405\n",
      "2019-04-10 01:12:27,836 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031130; Generator: 0.028574,\n",
      "D(x): 0.630, D(G(z)): 0.401\n",
      "2019-04-10 01:12:27,938 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.029487; Generator: 0.028751,\n",
      "D(x): 0.664, D(G(z)): 0.399\n",
      "2019-04-10 01:12:28,039 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028288; Generator: 0.028884,\n",
      "D(x): 0.685, D(G(z)): 0.397\n",
      "2019-04-10 01:12:28,138 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027775; Generator: 0.029264,\n",
      "D(x): 0.691, D(G(z)): 0.392\n",
      "2019-04-10 01:12:28,212 root         INFO     ====> Epoch: 2 Average loss: nan\n",
      "2019-04-10 01:12:28,238 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.029014; Generator: 0.029343,\n",
      "D(x): 0.659, D(G(z)): 0.391\n",
      "2019-04-10 01:12:28,336 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.029087; Generator: 0.029866,\n",
      "D(x): 0.650, D(G(z)): 0.385\n",
      "2019-04-10 01:12:28,435 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026617; Generator: 0.030024,\n",
      "D(x): 0.706, D(G(z)): 0.383\n",
      "2019-04-10 01:12:28,532 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026004; Generator: 0.030259,\n",
      "D(x): 0.713, D(G(z)): 0.380\n",
      "2019-04-10 01:12:28,630 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027081; Generator: 0.030029,\n",
      "D(x): 0.697, D(G(z)): 0.383\n",
      "2019-04-10 01:12:28,728 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026670; Generator: 0.030777,\n",
      "D(x): 0.698, D(G(z)): 0.374\n",
      "2019-04-10 01:12:28,826 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028178; Generator: 0.030988,\n",
      "D(x): 0.663, D(G(z)): 0.372\n",
      "2019-04-10 01:12:28,924 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027085; Generator: 0.030910,\n",
      "D(x): 0.680, D(G(z)): 0.373\n",
      "2019-04-10 01:12:29,022 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.025436; Generator: 0.031410,\n",
      "D(x): 0.719, D(G(z)): 0.367\n",
      "2019-04-10 01:12:29,120 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027015; Generator: 0.032192,\n",
      "D(x): 0.675, D(G(z)): 0.357\n",
      "2019-04-10 01:12:29,218 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.022762; Generator: 0.031407,\n",
      "D(x): 0.776, D(G(z)): 0.367\n",
      "2019-04-10 01:12:29,317 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027174; Generator: 0.031883,\n",
      "D(x): 0.672, D(G(z)): 0.361\n",
      "2019-04-10 01:12:29,419 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.024613; Generator: 0.032724,\n",
      "D(x): 0.719, D(G(z)): 0.351\n",
      "2019-04-10 01:12:29,521 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026319; Generator: 0.032437,\n",
      "D(x): 0.686, D(G(z)): 0.355\n",
      "2019-04-10 01:12:29,624 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.024680; Generator: 0.032777,\n",
      "D(x): 0.717, D(G(z)): 0.351\n",
      "2019-04-10 01:12:29,726 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.024197; Generator: 0.033015,\n",
      "D(x): 0.727, D(G(z)): 0.349\n",
      "2019-04-10 01:12:29,800 root         INFO     ====> Epoch: 3 Average loss: nan\n",
      "2019-04-10 01:12:29,827 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.020447; Generator: 0.033471,\n",
      "D(x): 0.808, D(G(z)): 0.343\n",
      "2019-04-10 01:12:29,895 luigi-interface INFO     Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=5941) was stopped. Shutting down Keep-Alive thread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = 'logs'\n",
    "logs = []\n",
    "for filename in os.listdir(OUTPUT):\n",
    "    if re.search(pattern, filename):\n",
    "        logs.append(filename)\n",
    "\n",
    "print('Found %d log files.' % len(logs))\n",
    "        \n",
    "for filename in logs:\n",
    "    path = os.path.join(OUTPUT, filename)\n",
    "    print('\\n-- Log file: %s\\n' % filename)\n",
    "    with open(path, 'r') as f:\n",
    "        message = f.read()\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
