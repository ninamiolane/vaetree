{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import importlib\n",
    "import toylosses\n",
    "importlib.reload(toylosses)\n",
    "import toynn\n",
    "importlib.reload(toynn)\n",
    "import toyvis\n",
    "importlib.reload(toyvis)\n",
    "\n",
    "import torch\n",
    "sns.set()\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "OUTPUT = '/scratch/users/nmiolane/toyoutput'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on experiment's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[10.,  0.],\n",
      "        [ 0., -5.]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([2., 0.], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0., 0.], device='cuda:0') \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-2.6265309218690756, 25.59907905318763, -18.40980352803763, 4.358925864351171)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_DIM = 2\n",
    "LATENT_DIM = 1\n",
    "N_DECODER_LAYERS = 2\n",
    "NONLINEARITY = True\n",
    "N_SAMPLES = 10000\n",
    "WITH_BIASX = True\n",
    "WITH_LOGVARX = True\n",
    "\n",
    "W_TRUE = {}\n",
    "B_TRUE = {}\n",
    "\n",
    "W_TRUE[0] = [[0.6], [-0.7]]\n",
    "B_TRUE[0] = [0., -0.1]\n",
    "\n",
    "# For the reconstruction\n",
    "W_TRUE[1] = [[10., 0.], [0., -5.]]\n",
    "B_TRUE[1] = [2., 0.]\n",
    "\n",
    "# For the logvarx\n",
    "W_TRUE[2] = [[0., 0.], [0., 0.]]\n",
    "B_TRUE[2] = [0., 0.]\n",
    "\n",
    "if WITH_LOGVARX:\n",
    "    assert len(W_TRUE) == N_DECODER_LAYERS + 1, len(W_TRUE)\n",
    "else:\n",
    "    assert len(W_TRUE) == N_DECODER_LAYERS\n",
    "\n",
    "WITH_BIASZ = True\n",
    "WITH_LOGVARZ = True\n",
    "\n",
    "decoder_true = toynn.make_decoder_true(\n",
    "    w_true=W_TRUE, b_true=B_TRUE, latent_dim=LATENT_DIM, \n",
    "    data_dim=DATA_DIM, n_layers=N_DECODER_LAYERS,\n",
    "    nonlinearity=NONLINEARITY, with_biasx=WITH_BIASX, with_logvarx=WITH_LOGVARX)\n",
    "\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, N_SAMPLES)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = toyvis.plot_data(generated_true_x, color='green', label='from decoder true', ax=ax)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect generation of synthetic data from decoder_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[10.,  0.],\n",
      "        [ 0., -5.]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([2., 0.], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0., 0.], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(nina): Add a comparison to a FA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results from standard VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- True values of parameters\n",
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[10.,  0.],\n",
      "        [ 0., -5.]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([2., 0.], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0., 0.], device='cuda:0') \n",
      "\n",
      "\n",
      "-- Learnt values of parameters\n",
      "layers.0.weight tensor([[ 0.6173],\n",
      "        [-0.6216]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 1.3559, -1.6938], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[ 2.2002, -0.3396],\n",
      "        [-0.4842,  0.5791]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([ 1.3150, -0.4553], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[ 1.1083,  0.1278],\n",
      "        [ 0.5776, -0.0298]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0.5741, 0.6079], device='cuda:0') \n",
      "\n",
      "Last losses:\n",
      "[0.1497068026661873, 0.14915091699361802, 0.1498452787399292, 0.14956466609239577, 0.14914862048625946]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "decoder_path = glob.glob(f'{OUTPUT}/train_vae/models/decoder.pth')[0]\n",
    "decoder = torch.load(decoder_path, map_location=DEVICE)\n",
    "\n",
    "print('-- True values of parameters')\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "print('\\n-- Learnt values of parameters')\n",
    "for name, param in decoder.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "    \n",
    "losses_vae_path = glob.glob(f'{OUTPUT}/train_vae/train_losses.pkl')[0]\n",
    "train_losses_all_epochs = pickle.load(open(losses_vae_path, 'rb'))\n",
    "\n",
    "train_losses_total = [loss['total'] for loss in train_losses_all_epochs]\n",
    "n_epochs = len(train_losses_total)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(range(n_epochs), train_losses_total)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(range(90, n_epochs), train_losses_total[90:])\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(range(160, n_epochs), train_losses_total[160:])\n",
    "\n",
    "print('Last losses:')\n",
    "print(train_losses_total[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.039029157504746, 23.082172768232716, -16.66602428389796, 6.311862572128717)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, n_samples)\n",
    "generated_x = toynn.generate_from_decoder(decoder, n_samples)\n",
    "\n",
    "# For 1D\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(16, 4))\n",
    "axis_side = 20\n",
    "\n",
    "ax = axes[0]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[1]\n",
    "toyvis.plot_data(generated_x, color='red', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[2]\n",
    "toyvis.plot_data(generated_x, color='red', ax=ax)\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "\n",
    "\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results from VEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-d2e48b3d7e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_true_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecoder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{OUTPUT}/train_vem/models/decoder.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "decoder_path = glob.glob(f'{OUTPUT}/train_vem/models/decoder.pth')[0]\n",
    "decoder = torch.load(decoder_path, map_location=DEVICE)\n",
    "\n",
    "print('-- True values of parameters')\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "print('\\n-- Learnt values of parameters')\n",
    "for name, param in decoder.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "    \n",
    "losses_vae_path = glob.glob(f'{OUTPUT}/train_vem/train_losses.pkl')[0]\n",
    "train_losses_all_epochs = pickle.load(open(losses_vae_path, 'rb'))\n",
    "\n",
    "plt.figure()\n",
    "train_losses_total = [loss['total'] for loss in train_losses_all_epochs]\n",
    "n_epochs = len(train_losses_total)\n",
    "plt.plot(range(n_epochs), train_losses_total)\n",
    "print('Last losses:')\n",
    "print(train_losses_total[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-9.171311360398654,\n",
       " 31.247013523571848,\n",
       " -11.891899123307798,\n",
       " 7.033674354587097)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, n_samples)\n",
    "generated_x = toynn.generate_from_decoder(decoder, n_samples)\n",
    "\n",
    "# For 1D\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(16, 4))\n",
    "axis_side = 20\n",
    "\n",
    "ax = axes[0]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[1]\n",
    "toyvis.plot_data(generated_x, color='orange', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[2]\n",
    "toyvis.plot_data(generated_x, color='orange', ax=ax)\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print pipeline logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 log files.\n",
      "\n",
      "-- Log file: logs2019-04-10 00:55:18.169271.txt\n",
      "\n",
      "2019-04-10 00:55:18,169 root         INFO     start\n",
      "2019-04-10 00:55:18,184 luigi        INFO     logging configured by default settings\n",
      "2019-04-10 00:55:18,188 luigi-interface DEBUG    Checking if RunAll() is complete\n",
      "2019-04-10 00:55:18,188 luigi-interface DEBUG    Checking if TrainVAE() is complete\n",
      "2019-04-10 00:55:18,189 luigi-interface DEBUG    Checking if TrainVEM() is complete\n",
      "2019-04-10 00:55:18,189 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   PENDING\n",
      "2019-04-10 00:55:18,190 luigi-interface DEBUG    Checking if MakeDataSet() is complete\n",
      "2019-04-10 00:55:18,190 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   PENDING\n",
      "2019-04-10 00:55:18,191 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   PENDING\n",
      "2019-04-10 00:55:18,191 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   PENDING\n",
      "2019-04-10 00:55:18,191 luigi-interface INFO     Done scheduling tasks\n",
      "2019-04-10 00:55:18,191 luigi-interface INFO     Running Worker with 1 processes\n",
      "2019-04-10 00:55:18,192 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 00:55:18,192 luigi-interface DEBUG    Pending tasks: 4\n",
      "2019-04-10 00:55:18,192 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) running   MakeDataSet()\n",
      "2019-04-10 00:55:18,193 root         INFO     Configuration:\n",
      "2019-04-10 00:55:18,193 root         INFO     DATA_DIM = 2\n",
      "2019-04-10 00:55:18,193 root         INFO     LATENT_DIM = 1\n",
      "2019-04-10 00:55:18,193 root         INFO     N_DECODER_LAYERS = 5\n",
      "2019-04-10 00:55:18,193 root         INFO     NONLINEARITY=False\n",
      "2019-04-10 00:55:18,193 root         INFO     WITH_BIASX=True\n",
      "2019-04-10 00:55:18,193 root         INFO     WITH_LOGVARX=True\n",
      "2019-04-10 00:55:18,193 root         INFO     WITH_BIASZ=True\n",
      "2019-04-10 00:55:18,193 root         INFO     WITH_LOGVARZ=True\n",
      "2019-04-10 00:55:18,193 root         INFO     N_SAMPLES=10000\n",
      "2019-04-10 00:55:18,193 root         INFO     W_TRUE:\n",
      "2019-04-10 00:55:18,193 root         INFO     {0: [[0.6], [-0.7]], 1: [[0.1, -0.1], [-0.1, 0.1]], 2: [[0.1, -0.1], [-0.1, 0.1]], 3: [[0.1, -0.1], [-0.1, 0.1]], 4: [[200.0, 0.0], [0.0, -80.0]], 5: [[0.0, 0.0], [0.0, 0.0]]}\n",
      "2019-04-10 00:55:18,194 root         INFO     B_TRUE:\n",
      "2019-04-10 00:55:18,194 root         INFO     {0: [0.0, -0.1], 1: [0.1, 0.0], 2: [0.1, 0.0], 3: [0.1, 0.0], 4: [2.0, 2.4], 5: [0.0, 0.0]}\n",
      "2019-04-10 00:55:22,009 root         INFO     Values of true 'decoder' parameters:\n",
      "2019-04-10 00:55:22,009 root         INFO     layers.0.weight\n",
      "2019-04-10 00:55:22,009 root         INFO     tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0')\n",
      "2019-04-10 00:55:22,030 root         INFO     layers.0.bias\n",
      "2019-04-10 00:55:22,030 root         INFO     tensor([ 0.0000, -0.1000], device='cuda:0')\n",
      "2019-04-10 00:55:22,031 root         INFO     layers.1.weight\n",
      "2019-04-10 00:55:22,031 root         INFO     tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0')\n",
      "2019-04-10 00:55:22,032 root         INFO     layers.1.bias\n",
      "2019-04-10 00:55:22,032 root         INFO     tensor([0.1000, 0.0000], device='cuda:0')\n",
      "2019-04-10 00:55:22,033 root         INFO     layers.2.weight\n",
      "2019-04-10 00:55:22,033 root         INFO     tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0')\n",
      "2019-04-10 00:55:22,034 root         INFO     layers.2.bias\n",
      "2019-04-10 00:55:22,034 root         INFO     tensor([0.1000, 0.0000], device='cuda:0')\n",
      "2019-04-10 00:55:22,035 root         INFO     layers.3.weight\n",
      "2019-04-10 00:55:22,035 root         INFO     tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0')\n",
      "2019-04-10 00:55:22,037 root         INFO     layers.3.bias\n",
      "2019-04-10 00:55:22,037 root         INFO     tensor([0.1000, 0.0000], device='cuda:0')\n",
      "2019-04-10 00:55:22,038 root         INFO     layers.4.weight\n",
      "2019-04-10 00:55:22,038 root         INFO     tensor([[200.,   0.],\n",
      "        [  0., -80.]], device='cuda:0')\n",
      "2019-04-10 00:55:22,039 root         INFO     layers.4.bias\n",
      "2019-04-10 00:55:22,039 root         INFO     tensor([2.0000, 2.4000], device='cuda:0')\n",
      "2019-04-10 00:55:22,040 root         INFO     layers.5.weight\n",
      "2019-04-10 00:55:22,040 root         INFO     tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "2019-04-10 00:55:22,040 root         INFO     layers.5.bias\n",
      "2019-04-10 00:55:22,041 root         INFO     tensor([0., 0.], device='cuda:0')\n",
      "2019-04-10 00:55:22,149 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) done      MakeDataSet()\n",
      "2019-04-10 00:55:22,150 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 00:55:22,150 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   DONE\n",
      "2019-04-10 00:55:22,150 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 00:55:22,150 luigi-interface DEBUG    Pending tasks: 3\n",
      "2019-04-10 00:55:22,151 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) running   TrainVEM()\n",
      "2019-04-10 00:55:22,152 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-10 00:55:22,153 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-10 00:55:22,154 root         INFO     Values of VEM's decoder parameters before training:\n",
      "2019-04-10 00:55:22,154 root         INFO     layers.0.weight\n",
      "2019-04-10 00:55:22,154 root         INFO     tensor([[-0.0526],\n",
      "        [-0.2078]], device='cuda:0')\n",
      "2019-04-10 00:55:22,155 root         INFO     layers.0.bias\n",
      "2019-04-10 00:55:22,155 root         INFO     tensor([ 0.3363, -0.4224], device='cuda:0')\n",
      "2019-04-10 00:55:22,156 root         INFO     layers.1.weight\n",
      "2019-04-10 00:55:22,156 root         INFO     tensor([[ 0.2904, -0.4362],\n",
      "        [-0.2126,  0.5412]], device='cuda:0')\n",
      "2019-04-10 00:55:22,157 root         INFO     layers.1.bias\n",
      "2019-04-10 00:55:22,157 root         INFO     tensor([ 0.0693, -0.5593], device='cuda:0')\n",
      "2019-04-10 00:55:22,158 root         INFO     layers.2.weight\n",
      "2019-04-10 00:55:22,159 root         INFO     tensor([[ 0.4248, -0.4699],\n",
      "        [-0.0456, -0.0020]], device='cuda:0')\n",
      "2019-04-10 00:55:22,160 root         INFO     layers.2.bias\n",
      "2019-04-10 00:55:22,160 root         INFO     tensor([-0.3116,  0.3165], device='cuda:0')\n",
      "2019-04-10 00:55:22,161 root         INFO     layers.3.weight\n",
      "2019-04-10 00:55:22,161 root         INFO     tensor([[-0.0699,  0.3861],\n",
      "        [ 0.3593, -0.5530]], device='cuda:0')\n",
      "2019-04-10 00:55:22,162 root         INFO     layers.3.bias\n",
      "2019-04-10 00:55:22,162 root         INFO     tensor([0.5281, 0.0176], device='cuda:0')\n",
      "2019-04-10 00:55:22,163 root         INFO     layers.4.weight\n",
      "2019-04-10 00:55:22,163 root         INFO     tensor([[ 0.5583,  0.4401],\n",
      "        [-0.2079, -0.1400]], device='cuda:0')\n",
      "2019-04-10 00:55:22,164 root         INFO     layers.4.bias\n",
      "2019-04-10 00:55:22,164 root         INFO     tensor([-0.2586, -0.3348], device='cuda:0')\n",
      "2019-04-10 00:55:22,165 root         INFO     layers.5.weight\n",
      "2019-04-10 00:55:22,165 root         INFO     tensor([[0.5477, 0.6593],\n",
      "        [0.3130, 0.6352]], device='cuda:0')\n",
      "2019-04-10 00:55:22,166 root         INFO     layers.5.bias\n",
      "2019-04-10 00:55:22,166 root         INFO     tensor([0.2400, 0.1101], device='cuda:0')\n",
      "2019-04-10 00:55:22,240 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 18.402164\n",
      "Reconstruction: 16.903975, Regularization: 0.414003, Discriminator: 1.079039; Generator: 0.005147,\n",
      "D(x): 0.000, D(G(z)): 0.848\n",
      "2019-04-10 00:55:22,362 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: 14.873638\n",
      "Reconstruction: 13.554897, Regularization: 0.352149, Discriminator: 0.961879; Generator: 0.004713,\n",
      "D(x): 0.000, D(G(z)): 0.860\n",
      "2019-04-10 00:55:22,472 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: 14.322220\n",
      "Reconstruction: 13.067829, Regularization: 0.335233, Discriminator: 0.914039; Generator: 0.005119,\n",
      "D(x): 0.000, D(G(z)): 0.849\n",
      "2019-04-10 00:55:22,581 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: 16.720507\n",
      "Reconstruction: 15.390631, Regularization: 0.362313, Discriminator: 0.962442; Generator: 0.005121,\n",
      "D(x): 0.000, D(G(z)): 0.849\n",
      "2019-04-10 00:55:22,690 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: 15.075704\n",
      "Reconstruction: 13.817432, Regularization: 0.334537, Discriminator: 0.918702; Generator: 0.005031,\n",
      "D(x): 0.000, D(G(z)): 0.852\n",
      "2019-04-10 00:55:22,799 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: 17.618729\n",
      "Reconstruction: 16.275896, Regularization: 0.364545, Discriminator: 0.973155; Generator: 0.005133,\n",
      "D(x): 0.000, D(G(z)): 0.849\n",
      "2019-04-10 00:55:22,908 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: 15.180911\n",
      "Reconstruction: 13.984457, Regularization: 0.320031, Discriminator: 0.871012; Generator: 0.005411,\n",
      "D(x): 0.000, D(G(z)): 0.841\n",
      "2019-04-10 00:55:23,017 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: 15.312880\n",
      "Reconstruction: 14.124370, Regularization: 0.315860, Discriminator: 0.867180; Generator: 0.005470,\n",
      "D(x): 0.000, D(G(z)): 0.840\n",
      "2019-04-10 00:55:23,126 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: 15.771794\n",
      "Reconstruction: 14.603186, Regularization: 0.313317, Discriminator: 0.849833; Generator: 0.005458,\n",
      "D(x): 0.000, D(G(z)): 0.840\n",
      "2019-04-10 00:55:23,235 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: 18.249220\n",
      "Reconstruction: 16.995396, Regularization: 0.340847, Discriminator: 0.907654; Generator: 0.005322,\n",
      "D(x): 0.002, D(G(z)): 0.844\n",
      "2019-04-10 00:55:23,344 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: 16.657240\n",
      "Reconstruction: 15.479665, Regularization: 0.315959, Discriminator: 0.856272; Generator: 0.005344,\n",
      "D(x): 0.000, D(G(z)): 0.843\n",
      "2019-04-10 00:55:23,453 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: 17.197477\n",
      "Reconstruction: 16.003288, Regularization: 0.318439, Discriminator: 0.870241; Generator: 0.005508,\n",
      "D(x): 0.000, D(G(z)): 0.839\n",
      "2019-04-10 00:55:23,563 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: 16.528212\n",
      "Reconstruction: 15.398626, Regularization: 0.301138, Discriminator: 0.823014; Generator: 0.005434,\n",
      "D(x): 0.000, D(G(z)): 0.841\n",
      "2019-04-10 00:55:23,672 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: 17.035700\n",
      "Reconstruction: 15.887972, Regularization: 0.304038, Discriminator: 0.838222; Generator: 0.005467,\n",
      "D(x): 0.000, D(G(z)): 0.840\n",
      "2019-04-10 00:55:23,782 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: 16.341021\n",
      "Reconstruction: 15.264828, Regularization: 0.287378, Discriminator: 0.783134; Generator: 0.005682,\n",
      "D(x): 0.000, D(G(z)): 0.834\n",
      "2019-04-10 00:55:23,893 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: 14.913841\n",
      "Reconstruction: 13.900007, Regularization: 0.265170, Discriminator: 0.742872; Generator: 0.005793,\n",
      "D(x): 0.000, D(G(z)): 0.831\n",
      "2019-04-10 00:55:23,973 root         INFO     ====> Epoch: 0 Average loss: 16.4691\n",
      "2019-04-10 00:55:24,000 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: 17.639872\n",
      "Reconstruction: 16.554617, Regularization: 0.289846, Discriminator: 0.789695; Generator: 0.005714,\n",
      "D(x): 0.000, D(G(z)): 0.833\n",
      "2019-04-10 00:55:24,113 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: 20.301348\n",
      "Reconstruction: 19.146629, Regularization: 0.311848, Discriminator: 0.837000; Generator: 0.005871,\n",
      "D(x): 0.000, D(G(z)): 0.829\n",
      "2019-04-10 00:55:24,225 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: 19.879028\n",
      "Reconstruction: 18.768600, Regularization: 0.299008, Discriminator: 0.805544; Generator: 0.005877,\n",
      "D(x): 0.000, D(G(z)): 0.829\n",
      "2019-04-10 00:55:24,338 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: 18.128071\n",
      "Reconstruction: 17.067663, Regularization: 0.279884, Discriminator: 0.774356; Generator: 0.006168,\n",
      "D(x): 0.000, D(G(z)): 0.821\n",
      "2019-04-10 00:55:24,450 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: 17.082636\n",
      "Reconstruction: 16.096815, Regularization: 0.259742, Discriminator: 0.719990; Generator: 0.006089,\n",
      "D(x): 0.000, D(G(z)): 0.823\n",
      "2019-04-10 00:55:24,562 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: 17.913795\n",
      "Reconstruction: 16.923403, Regularization: 0.261063, Discriminator: 0.723272; Generator: 0.006059,\n",
      "D(x): 0.000, D(G(z)): 0.824\n",
      "2019-04-10 00:55:24,674 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: 16.868818\n",
      "Reconstruction: 15.937393, Regularization: 0.244577, Discriminator: 0.680843; Generator: 0.006005,\n",
      "D(x): 0.000, D(G(z)): 0.825\n",
      "2019-04-10 00:55:24,785 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: 16.923950\n",
      "Reconstruction: 16.016457, Regularization: 0.239657, Discriminator: 0.661746; Generator: 0.006091,\n",
      "D(x): 0.000, D(G(z)): 0.823\n",
      "2019-04-10 00:55:24,896 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: 16.208532\n",
      "Reconstruction: 15.340147, Regularization: 0.228529, Discriminator: 0.633631; Generator: 0.006225,\n",
      "D(x): 0.001, D(G(z)): 0.820\n",
      "2019-04-10 00:55:25,008 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: 16.457336\n",
      "Reconstruction: 15.589820, Regularization: 0.227056, Discriminator: 0.633916; Generator: 0.006544,\n",
      "D(x): 0.001, D(G(z)): 0.811\n",
      "2019-04-10 00:55:25,120 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: 19.993484\n",
      "Reconstruction: 19.058935, Regularization: 0.248178, Discriminator: 0.680345; Generator: 0.006026,\n",
      "D(x): 0.000, D(G(z)): 0.825\n",
      "2019-04-10 00:55:25,231 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: 18.401117\n",
      "Reconstruction: 17.512749, Regularization: 0.231914, Discriminator: 0.650129; Generator: 0.006324,\n",
      "D(x): 0.000, D(G(z)): 0.817\n",
      "2019-04-10 00:55:25,342 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: 20.485704\n",
      "Reconstruction: 19.565567, Regularization: 0.242499, Discriminator: 0.671170; Generator: 0.006468,\n",
      "D(x): 0.000, D(G(z)): 0.813\n",
      "2019-04-10 00:55:25,454 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: 20.541687\n",
      "Reconstruction: 19.643541, Regularization: 0.235851, Discriminator: 0.655947; Generator: 0.006346,\n",
      "D(x): 0.000, D(G(z)): 0.817\n",
      "2019-04-10 00:55:25,566 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: 19.182793\n",
      "Reconstruction: 18.340939, Regularization: 0.221490, Discriminator: 0.613664; Generator: 0.006701,\n",
      "D(x): 0.000, D(G(z)): 0.807\n",
      "2019-04-10 00:55:25,679 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: 19.324099\n",
      "Reconstruction: 18.488651, Regularization: 0.217248, Discriminator: 0.611605; Generator: 0.006593,\n",
      "D(x): 0.000, D(G(z)): 0.810\n",
      "2019-04-10 00:55:25,762 root         INFO     ====> Epoch: 1 Average loss: 18.2893\n",
      "2019-04-10 00:55:25,788 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: 19.666765\n",
      "Reconstruction: 18.830078, Regularization: 0.215563, Discriminator: 0.614536; Generator: 0.006590,\n",
      "D(x): 0.000, D(G(z)): 0.810\n",
      "2019-04-10 00:55:25,901 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: 17.600933\n",
      "Reconstruction: 16.834925, Regularization: 0.197688, Discriminator: 0.561360; Generator: 0.006961,\n",
      "D(x): 0.000, D(G(z)): 0.800\n",
      "2019-04-10 00:55:26,011 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: 16.227280\n",
      "Reconstruction: 15.510183, Regularization: 0.184316, Discriminator: 0.525962; Generator: 0.006818,\n",
      "D(x): 0.000, D(G(z)): 0.804\n",
      "2019-04-10 00:55:26,121 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: 20.461420\n",
      "Reconstruction: 19.666676, Regularization: 0.203760, Discriminator: 0.584000; Generator: 0.006985,\n",
      "D(x): 0.000, D(G(z)): 0.800\n",
      "2019-04-10 00:55:26,232 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: 15.970567\n",
      "Reconstruction: 15.287148, Regularization: 0.175844, Discriminator: 0.500783; Generator: 0.006793,\n",
      "D(x): 0.000, D(G(z)): 0.805\n",
      "2019-04-10 00:55:26,342 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: 20.671066\n",
      "Reconstruction: 19.900249, Regularization: 0.198085, Discriminator: 0.566167; Generator: 0.006566,\n",
      "D(x): 0.000, D(G(z)): 0.811\n",
      "2019-04-10 00:55:26,452 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: 21.421160\n",
      "Reconstruction: 20.643324, Regularization: 0.197864, Discriminator: 0.573274; Generator: 0.006698,\n",
      "D(x): 0.000, D(G(z)): 0.807\n",
      "2019-04-10 00:55:26,562 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: 23.362743\n",
      "Reconstruction: 22.578394, Regularization: 0.200240, Discriminator: 0.577024; Generator: 0.007084,\n",
      "D(x): 0.000, D(G(z)): 0.797\n",
      "2019-04-10 00:55:26,671 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: 21.486658\n",
      "Reconstruction: 20.747307, Regularization: 0.187409, Discriminator: 0.545029; Generator: 0.006914,\n",
      "D(x): 0.000, D(G(z)): 0.802\n",
      "2019-04-10 00:55:26,782 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: 20.764389\n",
      "Reconstruction: 20.048058, Regularization: 0.181016, Discriminator: 0.528348; Generator: 0.006967,\n",
      "D(x): 0.000, D(G(z)): 0.800\n",
      "2019-04-10 00:55:26,893 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: 22.074812\n",
      "Reconstruction: 21.351057, Regularization: 0.182322, Discriminator: 0.534557; Generator: 0.006876,\n",
      "D(x): 0.000, D(G(z)): 0.803\n",
      "2019-04-10 00:55:27,003 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: 18.152281\n",
      "Reconstruction: 17.531900, Regularization: 0.157831, Discriminator: 0.455327; Generator: 0.007221,\n",
      "D(x): 0.001, D(G(z)): 0.794\n",
      "2019-04-10 00:55:27,113 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: 21.454725\n",
      "Reconstruction: 20.771627, Regularization: 0.172127, Discriminator: 0.504011; Generator: 0.006960,\n",
      "D(x): 0.000, D(G(z)): 0.800\n",
      "2019-04-10 00:55:27,224 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: 20.241915\n",
      "Reconstruction: 19.603378, Regularization: 0.161022, Discriminator: 0.470343; Generator: 0.007172,\n",
      "D(x): 0.001, D(G(z)): 0.795\n",
      "2019-04-10 00:55:27,334 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: 19.571575\n",
      "Reconstruction: 18.947136, Regularization: 0.156171, Discriminator: 0.461214; Generator: 0.007052,\n",
      "D(x): 0.000, D(G(z)): 0.798\n",
      "2019-04-10 00:55:27,445 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: 23.448576\n",
      "Reconstruction: 22.777281, Regularization: 0.168455, Discriminator: 0.495824; Generator: 0.007016,\n",
      "D(x): 0.000, D(G(z)): 0.799\n",
      "2019-04-10 00:55:27,525 root         INFO     ====> Epoch: 2 Average loss: 20.2118\n",
      "2019-04-10 00:55:27,552 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: 20.826836\n",
      "Reconstruction: 20.204277, Regularization: 0.154135, Discriminator: 0.461111; Generator: 0.007314,\n",
      "D(x): 0.000, D(G(z)): 0.791\n",
      "2019-04-10 00:55:27,664 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: 19.253181\n",
      "Reconstruction: 18.672741, Regularization: 0.145518, Discriminator: 0.427591; Generator: 0.007331,\n",
      "D(x): 0.000, D(G(z)): 0.791\n",
      "2019-04-10 00:55:27,774 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: 19.042868\n",
      "Reconstruction: 18.474703, Regularization: 0.141386, Discriminator: 0.419476; Generator: 0.007304,\n",
      "D(x): 0.008, D(G(z)): 0.792\n",
      "2019-04-10 00:55:27,884 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: 18.215254\n",
      "Reconstruction: 17.670681, Regularization: 0.138322, Discriminator: 0.398836; Generator: 0.007415,\n",
      "D(x): 0.000, D(G(z)): 0.789\n",
      "2019-04-10 00:55:27,996 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: 23.586847\n",
      "Reconstruction: 22.967268, Regularization: 0.152801, Discriminator: 0.459236; Generator: 0.007543,\n",
      "D(x): 0.001, D(G(z)): 0.786\n",
      "2019-04-10 00:55:28,107 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: 20.313669\n",
      "Reconstruction: 19.749735, Regularization: 0.140239, Discriminator: 0.416282; Generator: 0.007412,\n",
      "D(x): 0.000, D(G(z)): 0.789\n",
      "2019-04-10 00:55:28,218 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: 19.546268\n",
      "Reconstruction: 19.008869, Regularization: 0.133317, Discriminator: 0.396720; Generator: 0.007362,\n",
      "D(x): 0.001, D(G(z)): 0.790\n",
      "2019-04-10 00:55:28,329 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: 22.356133\n",
      "Reconstruction: 21.787560, Regularization: 0.139630, Discriminator: 0.421375; Generator: 0.007569,\n",
      "D(x): 0.000, D(G(z)): 0.785\n",
      "2019-04-10 00:55:28,441 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: 23.565372\n",
      "Reconstruction: 22.995033, Regularization: 0.140479, Discriminator: 0.422374; Generator: 0.007485,\n",
      "D(x): 0.002, D(G(z)): 0.787\n",
      "2019-04-10 00:55:28,552 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: 23.336950\n",
      "Reconstruction: 22.770576, Regularization: 0.139180, Discriminator: 0.419628; Generator: 0.007565,\n",
      "D(x): 0.000, D(G(z)): 0.785\n",
      "2019-04-10 00:55:28,662 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: 23.048109\n",
      "Reconstruction: 22.498493, Regularization: 0.137229, Discriminator: 0.404893; Generator: 0.007494,\n",
      "D(x): 0.000, D(G(z)): 0.787\n",
      "2019-04-10 00:55:28,773 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: 20.734251\n",
      "Reconstruction: 20.228111, Regularization: 0.126951, Discriminator: 0.371700; Generator: 0.007489,\n",
      "D(x): 0.000, D(G(z)): 0.787\n",
      "2019-04-10 00:55:28,884 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: 20.344296\n",
      "Reconstruction: 19.847801, Regularization: 0.122274, Discriminator: 0.366531; Generator: 0.007689,\n",
      "D(x): 0.005, D(G(z)): 0.782\n",
      "2019-04-10 00:55:28,995 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: 21.194555\n",
      "Reconstruction: 20.698305, Regularization: 0.123100, Discriminator: 0.365347; Generator: 0.007802,\n",
      "D(x): 0.001, D(G(z)): 0.779\n",
      "2019-04-10 00:55:29,106 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: 23.656429\n",
      "Reconstruction: 23.137756, Regularization: 0.129318, Discriminator: 0.381499; Generator: 0.007856,\n",
      "D(x): 0.004, D(G(z)): 0.778\n",
      "2019-04-10 00:55:29,218 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: 24.695330\n",
      "Reconstruction: 24.166061, Regularization: 0.131321, Discriminator: 0.390177; Generator: 0.007771,\n",
      "D(x): 0.000, D(G(z)): 0.780\n",
      "2019-04-10 00:55:29,299 root         INFO     ====> Epoch: 3 Average loss: 21.9532\n",
      "2019-04-10 00:55:29,326 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: 26.408606\n",
      "Reconstruction: 25.868145, Regularization: 0.132459, Discriminator: 0.400052; Generator: 0.007949,\n",
      "D(x): 0.000, D(G(z)): 0.775\n",
      "2019-04-10 00:55:29,439 root         INFO     Train Epoch: 4 [512/8000 (6%)]\tTotal Loss: 23.186396\n",
      "Reconstruction: 22.686842, Regularization: 0.123101, Discriminator: 0.368639; Generator: 0.007814,\n",
      "D(x): 0.000, D(G(z)): 0.779\n",
      "2019-04-10 00:55:29,550 root         INFO     Train Epoch: 4 [1024/8000 (13%)]\tTotal Loss: 20.869234\n",
      "Reconstruction: 20.413330, Regularization: 0.114221, Discriminator: 0.333727; Generator: 0.007956,\n",
      "D(x): 0.003, D(G(z)): 0.775\n",
      "2019-04-10 00:55:29,662 root         INFO     Train Epoch: 4 [1536/8000 (19%)]\tTotal Loss: 24.539593\n",
      "Reconstruction: 24.044159, Regularization: 0.124559, Discriminator: 0.362966; Generator: 0.007908,\n",
      "D(x): 0.001, D(G(z)): 0.777\n",
      "2019-04-10 00:55:29,773 root         INFO     Train Epoch: 4 [2048/8000 (26%)]\tTotal Loss: 23.537672\n",
      "Reconstruction: 23.059324, Regularization: 0.119026, Discriminator: 0.351239; Generator: 0.008083,\n",
      "D(x): 0.005, D(G(z)): 0.772\n",
      "2019-04-10 00:55:29,883 root         INFO     Train Epoch: 4 [2560/8000 (32%)]\tTotal Loss: 24.460625\n",
      "Reconstruction: 23.980999, Regularization: 0.121391, Discriminator: 0.350273; Generator: 0.007961,\n",
      "D(x): 0.002, D(G(z)): 0.775\n",
      "2019-04-10 00:55:29,994 root         INFO     Train Epoch: 4 [3072/8000 (38%)]\tTotal Loss: 24.611168\n",
      "Reconstruction: 24.137440, Regularization: 0.123315, Discriminator: 0.342407; Generator: 0.008007,\n",
      "D(x): 0.002, D(G(z)): 0.774\n",
      "2019-04-10 00:55:30,104 root         INFO     Train Epoch: 4 [3584/8000 (45%)]\tTotal Loss: 23.377642\n",
      "Reconstruction: 22.918163, Regularization: 0.118335, Discriminator: 0.332918; Generator: 0.008226,\n",
      "D(x): 0.000, D(G(z)): 0.769\n",
      "2019-04-10 00:55:30,214 root         INFO     Train Epoch: 4 [4096/8000 (51%)]\tTotal Loss: 23.664799\n",
      "Reconstruction: 23.215080, Regularization: 0.117287, Discriminator: 0.324293; Generator: 0.008138,\n",
      "D(x): 0.004, D(G(z)): 0.771\n",
      "2019-04-10 00:55:30,324 root         INFO     Train Epoch: 4 [4608/8000 (58%)]\tTotal Loss: 23.403728\n",
      "Reconstruction: 22.959450, Regularization: 0.117230, Discriminator: 0.318686; Generator: 0.008363,\n",
      "D(x): 0.001, D(G(z)): 0.765\n",
      "2019-04-10 00:55:30,434 root         INFO     Train Epoch: 4 [5120/8000 (64%)]\tTotal Loss: 23.648233\n",
      "Reconstruction: 23.208935, Regularization: 0.115476, Discriminator: 0.315648; Generator: 0.008173,\n",
      "D(x): 0.006, D(G(z)): 0.770\n",
      "2019-04-10 00:55:30,544 root         INFO     Train Epoch: 4 [5632/8000 (70%)]\tTotal Loss: 25.745132\n",
      "Reconstruction: 25.287483, Regularization: 0.121867, Discriminator: 0.327269; Generator: 0.008514,\n",
      "D(x): 0.002, D(G(z)): 0.762\n",
      "2019-04-10 00:55:30,654 root         INFO     Train Epoch: 4 [6144/8000 (77%)]\tTotal Loss: 25.421745\n",
      "Reconstruction: 24.969738, Regularization: 0.121370, Discriminator: 0.322234; Generator: 0.008402,\n",
      "D(x): 0.001, D(G(z)): 0.764\n",
      "2019-04-10 00:55:30,763 root         INFO     Train Epoch: 4 [6656/8000 (83%)]\tTotal Loss: 21.673523\n",
      "Reconstruction: 21.266388, Regularization: 0.111916, Discriminator: 0.286688; Generator: 0.008531,\n",
      "D(x): 0.001, D(G(z)): 0.761\n",
      "2019-04-10 00:55:30,872 root         INFO     Train Epoch: 4 [7168/8000 (90%)]\tTotal Loss: 22.196751\n",
      "Reconstruction: 21.792242, Regularization: 0.111531, Discriminator: 0.284507; Generator: 0.008472,\n",
      "D(x): 0.004, D(G(z)): 0.763\n",
      "2019-04-10 00:55:30,981 root         INFO     Train Epoch: 4 [7680/8000 (96%)]\tTotal Loss: 22.107733\n",
      "Reconstruction: 21.707729, Regularization: 0.111048, Discriminator: 0.280258; Generator: 0.008698,\n",
      "D(x): 0.003, D(G(z)): 0.757\n",
      "2019-04-10 00:55:31,061 root         INFO     ====> Epoch: 4 Average loss: 23.2168\n",
      "2019-04-10 00:55:31,089 root         INFO     Train Epoch: 5 [0/8000 (0%)]\tTotal Loss: 21.692648\n",
      "Reconstruction: 21.299965, Regularization: 0.109589, Discriminator: 0.274581; Generator: 0.008513,\n",
      "D(x): 0.003, D(G(z)): 0.762\n",
      "2019-04-10 00:55:31,200 root         INFO     Train Epoch: 5 [512/8000 (6%)]\tTotal Loss: 25.027571\n",
      "Reconstruction: 24.604523, Regularization: 0.121426, Discriminator: 0.292930; Generator: 0.008693,\n",
      "D(x): 0.009, D(G(z)): 0.757\n",
      "2019-04-10 00:55:31,311 root         INFO     Train Epoch: 5 [1024/8000 (13%)]\tTotal Loss: 23.602139\n",
      "Reconstruction: 23.198418, Regularization: 0.117066, Discriminator: 0.277925; Generator: 0.008730,\n",
      "D(x): 0.012, D(G(z)): 0.756\n",
      "2019-04-10 00:55:31,421 root         INFO     Train Epoch: 5 [1536/8000 (19%)]\tTotal Loss: 22.558289\n",
      "Reconstruction: 22.169945, Regularization: 0.115133, Discriminator: 0.264381; Generator: 0.008831,\n",
      "D(x): 0.009, D(G(z)): 0.754\n",
      "2019-04-10 00:55:31,532 root         INFO     Train Epoch: 5 [2048/8000 (26%)]\tTotal Loss: 25.580513\n",
      "Reconstruction: 25.163763, Regularization: 0.122791, Discriminator: 0.285227; Generator: 0.008731,\n",
      "D(x): 0.006, D(G(z)): 0.756\n",
      "2019-04-10 00:55:31,642 root         INFO     Train Epoch: 5 [2560/8000 (32%)]\tTotal Loss: 25.960676\n",
      "Reconstruction: 25.541510, Regularization: 0.127396, Discriminator: 0.282867; Generator: 0.008904,\n",
      "D(x): 0.004, D(G(z)): 0.752\n",
      "2019-04-10 00:55:31,753 root         INFO     Train Epoch: 5 [3072/8000 (38%)]\tTotal Loss: 23.218081\n",
      "Reconstruction: 22.830248, Regularization: 0.118619, Discriminator: 0.260068; Generator: 0.009146,\n",
      "D(x): 0.004, D(G(z)): 0.746\n",
      "2019-04-10 00:55:31,864 root         INFO     Train Epoch: 5 [3584/8000 (45%)]\tTotal Loss: 23.589108\n",
      "Reconstruction: 23.199785, Regularization: 0.119328, Discriminator: 0.261059; Generator: 0.008934,\n",
      "D(x): 0.006, D(G(z)): 0.751\n",
      "2019-04-10 00:55:31,975 root         INFO     Train Epoch: 5 [4096/8000 (51%)]\tTotal Loss: 24.473763\n",
      "Reconstruction: 24.077492, Regularization: 0.124785, Discriminator: 0.262441; Generator: 0.009045,\n",
      "D(x): 0.010, D(G(z)): 0.749\n",
      "2019-04-10 00:55:32,085 root         INFO     Train Epoch: 5 [4608/8000 (58%)]\tTotal Loss: 22.984442\n",
      "Reconstruction: 22.607922, Regularization: 0.121735, Discriminator: 0.245709; Generator: 0.009077,\n",
      "D(x): 0.005, D(G(z)): 0.748\n",
      "2019-04-10 00:55:32,195 root         INFO     Train Epoch: 5 [5120/8000 (64%)]\tTotal Loss: 24.563196\n",
      "Reconstruction: 24.171314, Regularization: 0.129697, Discriminator: 0.253110; Generator: 0.009074,\n",
      "D(x): 0.004, D(G(z)): 0.748\n",
      "2019-04-10 00:55:32,306 root         INFO     Train Epoch: 5 [5632/8000 (70%)]\tTotal Loss: 24.815165\n",
      "Reconstruction: 24.422480, Regularization: 0.130061, Discriminator: 0.253357; Generator: 0.009267,\n",
      "D(x): 0.006, D(G(z)): 0.743\n",
      "2019-04-10 00:55:32,416 root         INFO     Train Epoch: 5 [6144/8000 (77%)]\tTotal Loss: 23.921402\n",
      "Reconstruction: 23.541611, Regularization: 0.130451, Discriminator: 0.239958; Generator: 0.009383,\n",
      "D(x): 0.007, D(G(z)): 0.741\n",
      "2019-04-10 00:55:32,526 root         INFO     Train Epoch: 5 [6656/8000 (83%)]\tTotal Loss: 22.353798\n",
      "Reconstruction: 21.991718, Regularization: 0.122627, Discriminator: 0.230011; Generator: 0.009442,\n",
      "D(x): 0.008, D(G(z)): 0.739\n",
      "2019-04-10 00:55:32,636 root         INFO     Train Epoch: 5 [7168/8000 (90%)]\tTotal Loss: 23.733021\n",
      "Reconstruction: 23.358885, Regularization: 0.133730, Discriminator: 0.231114; Generator: 0.009293,\n",
      "D(x): 0.010, D(G(z)): 0.743\n",
      "2019-04-10 00:55:32,746 root         INFO     Train Epoch: 5 [7680/8000 (96%)]\tTotal Loss: 23.270369\n",
      "Reconstruction: 22.901421, Regularization: 0.131184, Discriminator: 0.228108; Generator: 0.009656,\n",
      "D(x): 0.007, D(G(z)): 0.734\n",
      "2019-04-10 00:55:32,827 root         INFO     ====> Epoch: 5 Average loss: 23.7927\n",
      "2019-04-10 00:55:32,854 root         INFO     Train Epoch: 6 [0/8000 (0%)]\tTotal Loss: 23.604631\n",
      "Reconstruction: 23.233604, Regularization: 0.136097, Discriminator: 0.225440; Generator: 0.009490,\n",
      "D(x): 0.008, D(G(z)): 0.738\n",
      "2019-04-10 00:55:32,967 root         INFO     Train Epoch: 6 [512/8000 (6%)]\tTotal Loss: 23.074839\n",
      "Reconstruction: 22.711658, Regularization: 0.135761, Discriminator: 0.217900; Generator: 0.009520,\n",
      "D(x): 0.013, D(G(z)): 0.738\n",
      "2019-04-10 00:55:33,079 root         INFO     Train Epoch: 6 [1024/8000 (13%)]\tTotal Loss: 22.040806\n",
      "Reconstruction: 21.690687, Regularization: 0.132926, Discriminator: 0.207548; Generator: 0.009644,\n",
      "D(x): 0.015, D(G(z)): 0.735\n",
      "2019-04-10 00:55:33,191 root         INFO     Train Epoch: 6 [1536/8000 (19%)]\tTotal Loss: 21.866390\n",
      "Reconstruction: 21.519154, Regularization: 0.135385, Discriminator: 0.202191; Generator: 0.009661,\n",
      "D(x): 0.032, D(G(z)): 0.734\n",
      "2019-04-10 00:55:33,303 root         INFO     Train Epoch: 6 [2048/8000 (26%)]\tTotal Loss: 22.353344\n",
      "Reconstruction: 22.000315, Regularization: 0.139987, Discriminator: 0.203171; Generator: 0.009873,\n",
      "D(x): 0.023, D(G(z)): 0.729\n",
      "2019-04-10 00:55:33,415 root         INFO     Train Epoch: 6 [2560/8000 (32%)]\tTotal Loss: 23.759817\n",
      "Reconstruction: 23.388306, Regularization: 0.152513, Discriminator: 0.209143; Generator: 0.009855,\n",
      "D(x): 0.014, D(G(z)): 0.730\n",
      "2019-04-10 00:55:33,527 root         INFO     Train Epoch: 6 [3072/8000 (38%)]\tTotal Loss: 23.665167\n",
      "Reconstruction: 23.293175, Regularization: 0.152480, Discriminator: 0.209691; Generator: 0.009822,\n",
      "D(x): 0.010, D(G(z)): 0.730\n",
      "2019-04-10 00:55:33,639 root         INFO     Train Epoch: 6 [3584/8000 (45%)]\tTotal Loss: 23.171837\n",
      "Reconstruction: 22.807077, Regularization: 0.153647, Discriminator: 0.201144; Generator: 0.009970,\n",
      "D(x): 0.017, D(G(z)): 0.727\n",
      "2019-04-10 00:55:33,751 root         INFO     Train Epoch: 6 [4096/8000 (51%)]\tTotal Loss: 27.179281\n",
      "Reconstruction: 26.757893, Regularization: 0.187260, Discriminator: 0.224140; Generator: 0.009988,\n",
      "D(x): 0.006, D(G(z)): 0.727\n",
      "2019-04-10 00:55:33,863 root         INFO     Train Epoch: 6 [4608/8000 (58%)]\tTotal Loss: 23.128210\n",
      "Reconstruction: 22.762222, Regularization: 0.158795, Discriminator: 0.197079; Generator: 0.010114,\n",
      "D(x): 0.012, D(G(z)): 0.724\n",
      "2019-04-10 00:55:33,975 root         INFO     Train Epoch: 6 [5120/8000 (64%)]\tTotal Loss: 22.670650\n",
      "Reconstruction: 22.310740, Regularization: 0.159141, Discriminator: 0.190604; Generator: 0.010166,\n",
      "D(x): 0.019, D(G(z)): 0.722\n",
      "2019-04-10 00:55:34,086 root         INFO     Train Epoch: 6 [5632/8000 (70%)]\tTotal Loss: 21.727146\n",
      "Reconstruction: 21.377869, Regularization: 0.155037, Discriminator: 0.183900; Generator: 0.010340,\n",
      "D(x): 0.021, D(G(z)): 0.718\n",
      "2019-04-10 00:55:34,199 root         INFO     Train Epoch: 6 [6144/8000 (77%)]\tTotal Loss: 23.032276\n",
      "Reconstruction: 22.663158, Regularization: 0.169768, Discriminator: 0.188930; Generator: 0.010421,\n",
      "D(x): 0.017, D(G(z)): 0.716\n",
      "2019-04-10 00:55:34,311 root         INFO     Train Epoch: 6 [6656/8000 (83%)]\tTotal Loss: 23.803865\n",
      "Reconstruction: 23.424408, Regularization: 0.181016, Discriminator: 0.187966; Generator: 0.010476,\n",
      "D(x): 0.015, D(G(z)): 0.715\n",
      "2019-04-10 00:55:34,422 root         INFO     Train Epoch: 6 [7168/8000 (90%)]\tTotal Loss: 22.709124\n",
      "Reconstruction: 22.343407, Regularization: 0.175824, Discriminator: 0.179456; Generator: 0.010438,\n",
      "D(x): 0.024, D(G(z)): 0.716\n",
      "2019-04-10 00:55:34,532 root         INFO     Train Epoch: 6 [7680/8000 (96%)]\tTotal Loss: 23.541119\n",
      "Reconstruction: 23.160252, Regularization: 0.188568, Discriminator: 0.181563; Generator: 0.010736,\n",
      "D(x): 0.019, D(G(z)): 0.709\n",
      "2019-04-10 00:55:34,614 root         INFO     ====> Epoch: 6 Average loss: 23.4182\n",
      "2019-04-10 00:55:34,641 root         INFO     Train Epoch: 7 [0/8000 (0%)]\tTotal Loss: 20.459616\n",
      "Reconstruction: 20.124775, Regularization: 0.159728, Discriminator: 0.164393; Generator: 0.010718,\n",
      "D(x): 0.030, D(G(z)): 0.710\n",
      "2019-04-10 00:55:34,754 root         INFO     Train Epoch: 7 [512/8000 (6%)]\tTotal Loss: 22.656368\n",
      "Reconstruction: 22.282288, Regularization: 0.188617, Discriminator: 0.174714; Generator: 0.010748,\n",
      "D(x): 0.029, D(G(z)): 0.709\n",
      "2019-04-10 00:55:34,866 root         INFO     Train Epoch: 7 [1024/8000 (13%)]\tTotal Loss: 21.903486\n",
      "Reconstruction: 21.539013, Regularization: 0.187582, Discriminator: 0.166127; Generator: 0.010764,\n",
      "D(x): 0.040, D(G(z)): 0.709\n",
      "2019-04-10 00:55:34,976 root         INFO     Train Epoch: 7 [1536/8000 (19%)]\tTotal Loss: 21.777584\n",
      "Reconstruction: 21.417599, Regularization: 0.187021, Discriminator: 0.162076; Generator: 0.010888,\n",
      "D(x): 0.034, D(G(z)): 0.706\n",
      "2019-04-10 00:55:35,086 root         INFO     Train Epoch: 7 [2048/8000 (26%)]\tTotal Loss: 21.197115\n",
      "Reconstruction: 20.843800, Regularization: 0.184926, Discriminator: 0.157454; Generator: 0.010937,\n",
      "D(x): 0.037, D(G(z)): 0.705\n",
      "2019-04-10 00:55:35,197 root         INFO     Train Epoch: 7 [2560/8000 (32%)]\tTotal Loss: 23.420673\n",
      "Reconstruction: 23.021069, Regularization: 0.222006, Discriminator: 0.166612; Generator: 0.010987,\n",
      "D(x): 0.044, D(G(z)): 0.704\n",
      "2019-04-10 00:55:35,307 root         INFO     Train Epoch: 7 [3072/8000 (38%)]\tTotal Loss: 23.570452\n",
      "Reconstruction: 23.167618, Regularization: 0.226456, Discriminator: 0.165233; Generator: 0.011145,\n",
      "D(x): 0.031, D(G(z)): 0.700\n",
      "2019-04-10 00:55:35,418 root         INFO     Train Epoch: 7 [3584/8000 (45%)]\tTotal Loss: 21.153460\n",
      "Reconstruction: 20.789219, Regularization: 0.200220, Discriminator: 0.152952; Generator: 0.011068,\n",
      "D(x): 0.054, D(G(z)): 0.702\n",
      "2019-04-10 00:55:35,529 root         INFO     Train Epoch: 7 [4096/8000 (51%)]\tTotal Loss: 22.674778\n",
      "Reconstruction: 22.282646, Regularization: 0.223787, Discriminator: 0.157154; Generator: 0.011191,\n",
      "D(x): 0.034, D(G(z)): 0.699\n",
      "2019-04-10 00:55:35,639 root         INFO     Train Epoch: 7 [4608/8000 (58%)]\tTotal Loss: 22.493206\n",
      "Reconstruction: 22.099627, Regularization: 0.226103, Discriminator: 0.156235; Generator: 0.011241,\n",
      "D(x): 0.034, D(G(z)): 0.698\n",
      "2019-04-10 00:55:35,748 root         INFO     Train Epoch: 7 [5120/8000 (64%)]\tTotal Loss: 22.249912\n",
      "Reconstruction: 21.856445, Regularization: 0.231255, Discriminator: 0.150782; Generator: 0.011431,\n",
      "D(x): 0.045, D(G(z)): 0.694\n",
      "2019-04-10 00:55:35,856 root         INFO     Train Epoch: 7 [5632/8000 (70%)]\tTotal Loss: 21.009497\n",
      "Reconstruction: 20.638367, Regularization: 0.216568, Discriminator: 0.143176; Generator: 0.011386,\n",
      "D(x): 0.056, D(G(z)): 0.695\n",
      "2019-04-10 00:55:35,966 root         INFO     Train Epoch: 7 [6144/8000 (77%)]\tTotal Loss: 23.469894\n",
      "Reconstruction: 23.041958, Regularization: 0.262237, Discriminator: 0.154154; Generator: 0.011544,\n",
      "D(x): 0.033, D(G(z)): 0.691\n",
      "2019-04-10 00:55:36,076 root         INFO     Train Epoch: 7 [6656/8000 (83%)]\tTotal Loss: 21.376419\n",
      "Reconstruction: 20.990974, Regularization: 0.233125, Discriminator: 0.140673; Generator: 0.011647,\n",
      "D(x): 0.048, D(G(z)): 0.689\n",
      "2019-04-10 00:55:36,187 root         INFO     Train Epoch: 7 [7168/8000 (90%)]\tTotal Loss: 22.301722\n",
      "Reconstruction: 21.880890, Regularization: 0.264066, Discriminator: 0.145053; Generator: 0.011713,\n",
      "D(x): 0.048, D(G(z)): 0.688\n",
      "2019-04-10 00:55:36,297 root         INFO     Train Epoch: 7 [7680/8000 (96%)]\tTotal Loss: 21.613543\n",
      "Reconstruction: 21.208258, Regularization: 0.253892, Discriminator: 0.139750; Generator: 0.011642,\n",
      "D(x): 0.055, D(G(z)): 0.689\n",
      "2019-04-10 00:55:36,377 root         INFO     ====> Epoch: 7 Average loss: 22.3043\n",
      "2019-04-10 00:55:36,405 root         INFO     Train Epoch: 8 [0/8000 (0%)]\tTotal Loss: 19.947306\n",
      "Reconstruction: 19.574520, Regularization: 0.230385, Discriminator: 0.130540; Generator: 0.011861,\n",
      "D(x): 0.067, D(G(z)): 0.684\n",
      "2019-04-10 00:55:36,518 root         INFO     Train Epoch: 8 [512/8000 (6%)]\tTotal Loss: 22.300444\n",
      "Reconstruction: 21.860062, Regularization: 0.287500, Discriminator: 0.141024; Generator: 0.011858,\n",
      "D(x): 0.053, D(G(z)): 0.684\n",
      "2019-04-10 00:55:36,630 root         INFO     Train Epoch: 8 [1024/8000 (13%)]\tTotal Loss: 20.340219\n",
      "Reconstruction: 19.952969, Regularization: 0.246048, Discriminator: 0.129188; Generator: 0.012014,\n",
      "D(x): 0.063, D(G(z)): 0.681\n",
      "2019-04-10 00:55:36,742 root         INFO     Train Epoch: 8 [1536/8000 (19%)]\tTotal Loss: 20.065100\n",
      "Reconstruction: 19.675381, Regularization: 0.251193, Discriminator: 0.126478; Generator: 0.012046,\n",
      "D(x): 0.078, D(G(z)): 0.680\n",
      "2019-04-10 00:55:36,854 root         INFO     Train Epoch: 8 [2048/8000 (26%)]\tTotal Loss: 21.401358\n",
      "Reconstruction: 20.966557, Regularization: 0.291294, Discriminator: 0.131315; Generator: 0.012192,\n",
      "D(x): 0.069, D(G(z)): 0.677\n",
      "2019-04-10 00:55:36,967 root         INFO     Train Epoch: 8 [2560/8000 (32%)]\tTotal Loss: 20.370420\n",
      "Reconstruction: 19.960800, Regularization: 0.273042, Discriminator: 0.124403; Generator: 0.012175,\n",
      "D(x): 0.081, D(G(z)): 0.677\n",
      "2019-04-10 00:55:37,079 root         INFO     Train Epoch: 8 [3072/8000 (38%)]\tTotal Loss: 20.336308\n",
      "Reconstruction: 19.920807, Regularization: 0.279618, Discriminator: 0.123499; Generator: 0.012384,\n",
      "D(x): 0.074, D(G(z)): 0.673\n",
      "2019-04-10 00:55:37,191 root         INFO     Train Epoch: 8 [3584/8000 (45%)]\tTotal Loss: 22.029179\n",
      "Reconstruction: 21.549793, Regularization: 0.336517, Discriminator: 0.130423; Generator: 0.012446,\n",
      "D(x): 0.063, D(G(z)): 0.672\n",
      "2019-04-10 00:55:37,300 root         INFO     Train Epoch: 8 [4096/8000 (51%)]\tTotal Loss: 20.186022\n",
      "Reconstruction: 19.760834, Regularization: 0.293367, Discriminator: 0.119434; Generator: 0.012386,\n",
      "D(x): 0.088, D(G(z)): 0.673\n",
      "2019-04-10 00:55:37,410 root         INFO     Train Epoch: 8 [4608/8000 (58%)]\tTotal Loss: 22.019810\n",
      "Reconstruction: 21.509607, Regularization: 0.369014, Discriminator: 0.128646; Generator: 0.012542,\n",
      "D(x): 0.068, D(G(z)): 0.669\n",
      "2019-04-10 00:55:37,519 root         INFO     Train Epoch: 8 [5120/8000 (64%)]\tTotal Loss: 20.311163\n",
      "Reconstruction: 19.871424, Regularization: 0.309641, Discriminator: 0.117432; Generator: 0.012666,\n",
      "D(x): 0.084, D(G(z)): 0.667\n",
      "2019-04-10 00:55:37,629 root         INFO     Train Epoch: 8 [5632/8000 (70%)]\tTotal Loss: 19.090693\n",
      "Reconstruction: 18.683958, Regularization: 0.283792, Discriminator: 0.110210; Generator: 0.012731,\n",
      "D(x): 0.108, D(G(z)): 0.665\n",
      "2019-04-10 00:55:37,737 root         INFO     Train Epoch: 8 [6144/8000 (77%)]\tTotal Loss: 19.994785\n",
      "Reconstruction: 19.552666, Regularization: 0.316574, Discriminator: 0.112784; Generator: 0.012762,\n",
      "D(x): 0.096, D(G(z)): 0.665\n",
      "2019-04-10 00:55:37,846 root         INFO     Train Epoch: 8 [6656/8000 (83%)]\tTotal Loss: 20.505571\n",
      "Reconstruction: 20.036352, Regularization: 0.341911, Discriminator: 0.114397; Generator: 0.012911,\n",
      "D(x): 0.087, D(G(z)): 0.662\n",
      "2019-04-10 00:55:37,954 root         INFO     Train Epoch: 8 [7168/8000 (90%)]\tTotal Loss: 18.790684\n",
      "Reconstruction: 18.366148, Regularization: 0.304848, Discriminator: 0.106738; Generator: 0.012950,\n",
      "D(x): 0.119, D(G(z)): 0.661\n",
      "2019-04-10 00:55:38,061 root         INFO     Train Epoch: 8 [7680/8000 (96%)]\tTotal Loss: 20.356939\n",
      "Reconstruction: 19.871098, Regularization: 0.361347, Discriminator: 0.111435; Generator: 0.013059,\n",
      "D(x): 0.096, D(G(z)): 0.658\n",
      "2019-04-10 00:55:38,140 root         INFO     ====> Epoch: 8 Average loss: 20.5873\n",
      "2019-04-10 00:55:38,168 root         INFO     Train Epoch: 9 [0/8000 (0%)]\tTotal Loss: 20.190731\n",
      "Reconstruction: 19.697004, Regularization: 0.370432, Discriminator: 0.110206; Generator: 0.013089,\n",
      "D(x): 0.103, D(G(z)): 0.658\n",
      "2019-04-10 00:55:38,277 root         INFO     Train Epoch: 9 [512/8000 (6%)]\tTotal Loss: 19.195910\n",
      "Reconstruction: 18.731236, Regularization: 0.346013, Discriminator: 0.105406; Generator: 0.013256,\n",
      "D(x): 0.114, D(G(z)): 0.654\n",
      "2019-04-10 00:55:38,385 root         INFO     Train Epoch: 9 [1024/8000 (13%)]\tTotal Loss: 19.073383\n",
      "Reconstruction: 18.601664, Regularization: 0.355327, Discriminator: 0.103144; Generator: 0.013249,\n",
      "D(x): 0.133, D(G(z)): 0.654\n",
      "2019-04-10 00:55:38,494 root         INFO     Train Epoch: 9 [1536/8000 (19%)]\tTotal Loss: 19.679415\n",
      "Reconstruction: 19.189554, Regularization: 0.372704, Discriminator: 0.103724; Generator: 0.013433,\n",
      "D(x): 0.116, D(G(z)): 0.651\n",
      "2019-04-10 00:55:38,603 root         INFO     Train Epoch: 9 [2048/8000 (26%)]\tTotal Loss: 20.408773\n",
      "Reconstruction: 19.853281, Regularization: 0.434775, Discriminator: 0.107206; Generator: 0.013511,\n",
      "D(x): 0.106, D(G(z)): 0.649\n",
      "2019-04-10 00:55:38,712 root         INFO     Train Epoch: 9 [2560/8000 (32%)]\tTotal Loss: 19.570587\n",
      "Reconstruction: 19.046043, Regularization: 0.407917, Discriminator: 0.103064; Generator: 0.013563,\n",
      "D(x): 0.119, D(G(z)): 0.648\n",
      "2019-04-10 00:55:38,820 root         INFO     Train Epoch: 9 [3072/8000 (38%)]\tTotal Loss: 18.654509\n",
      "Reconstruction: 18.145653, Regularization: 0.396966, Discriminator: 0.098172; Generator: 0.013717,\n",
      "D(x): 0.147, D(G(z)): 0.645\n",
      "2019-04-10 00:55:38,929 root         INFO     Train Epoch: 9 [3584/8000 (45%)]\tTotal Loss: 18.562891\n",
      "Reconstruction: 18.046618, Regularization: 0.404970, Discriminator: 0.097571; Generator: 0.013733,\n",
      "D(x): 0.144, D(G(z)): 0.644\n",
      "2019-04-10 00:55:39,038 root         INFO     Train Epoch: 9 [4096/8000 (51%)]\tTotal Loss: 18.082001\n",
      "Reconstruction: 17.604824, Regularization: 0.370106, Discriminator: 0.093277; Generator: 0.013793,\n",
      "D(x): 0.156, D(G(z)): 0.643\n",
      "2019-04-10 00:55:39,146 root         INFO     Train Epoch: 9 [4608/8000 (58%)]\tTotal Loss: 19.274828\n",
      "Reconstruction: 18.665960, Regularization: 0.494930, Discriminator: 0.100008; Generator: 0.013929,\n",
      "D(x): 0.138, D(G(z)): 0.640\n",
      "2019-04-10 00:55:39,255 root         INFO     Train Epoch: 9 [5120/8000 (64%)]\tTotal Loss: 18.327991\n",
      "Reconstruction: 17.792715, Regularization: 0.427770, Discriminator: 0.093348; Generator: 0.014157,\n",
      "D(x): 0.155, D(G(z)): 0.636\n",
      "2019-04-10 00:55:39,364 root         INFO     Train Epoch: 9 [5632/8000 (70%)]\tTotal Loss: 18.693520\n",
      "Reconstruction: 18.133858, Regularization: 0.452373, Discriminator: 0.093148; Generator: 0.014141,\n",
      "D(x): 0.153, D(G(z)): 0.636\n",
      "2019-04-10 00:55:39,472 root         INFO     Train Epoch: 9 [6144/8000 (77%)]\tTotal Loss: 17.714844\n",
      "Reconstruction: 17.190754, Regularization: 0.420305, Discriminator: 0.089618; Generator: 0.014166,\n",
      "D(x): 0.170, D(G(z)): 0.636\n",
      "2019-04-10 00:55:39,581 root         INFO     Train Epoch: 9 [6656/8000 (83%)]\tTotal Loss: 17.425301\n",
      "Reconstruction: 16.921757, Regularization: 0.402813, Discriminator: 0.086496; Generator: 0.014235,\n",
      "D(x): 0.186, D(G(z)): 0.634\n",
      "2019-04-10 00:55:39,690 root         INFO     Train Epoch: 9 [7168/8000 (90%)]\tTotal Loss: 18.294792\n",
      "Reconstruction: 17.710579, Regularization: 0.480298, Discriminator: 0.089578; Generator: 0.014339,\n",
      "D(x): 0.168, D(G(z)): 0.632\n",
      "2019-04-10 00:55:39,799 root         INFO     Train Epoch: 9 [7680/8000 (96%)]\tTotal Loss: 17.982918\n",
      "Reconstruction: 17.402081, Regularization: 0.478299, Discriminator: 0.088013; Generator: 0.014526,\n",
      "D(x): 0.173, D(G(z)): 0.628\n",
      "2019-04-10 00:55:39,878 root         INFO     ====> Epoch: 9 Average loss: 18.5956\n",
      "2019-04-10 00:55:39,905 root         INFO     Train Epoch: 10 [0/8000 (0%)]\tTotal Loss: 17.103462\n",
      "Reconstruction: 16.578636, Regularization: 0.426349, Discriminator: 0.083963; Generator: 0.014514,\n",
      "D(x): 0.196, D(G(z)): 0.629\n",
      "2019-04-10 00:55:40,016 root         INFO     Train Epoch: 10 [512/8000 (6%)]\tTotal Loss: 16.958780\n",
      "Reconstruction: 16.417355, Regularization: 0.443275, Discriminator: 0.083582; Generator: 0.014568,\n",
      "D(x): 0.197, D(G(z)): 0.627\n",
      "2019-04-10 00:55:40,125 root         INFO     Train Epoch: 10 [1024/8000 (13%)]\tTotal Loss: 17.710512\n",
      "Reconstruction: 17.108719, Regularization: 0.502214, Discriminator: 0.084803; Generator: 0.014776,\n",
      "D(x): 0.187, D(G(z)): 0.623\n",
      "2019-04-10 00:55:40,234 root         INFO     Train Epoch: 10 [1536/8000 (19%)]\tTotal Loss: 17.554874\n",
      "Reconstruction: 16.917290, Regularization: 0.537953, Discriminator: 0.084804; Generator: 0.014829,\n",
      "D(x): 0.188, D(G(z)): 0.622\n",
      "2019-04-10 00:55:40,343 root         INFO     Train Epoch: 10 [2048/8000 (26%)]\tTotal Loss: 17.079975\n",
      "Reconstruction: 16.492231, Regularization: 0.491200, Discriminator: 0.081582; Generator: 0.014963,\n",
      "D(x): 0.200, D(G(z)): 0.620\n",
      "2019-04-10 00:55:40,452 root         INFO     Train Epoch: 10 [2560/8000 (32%)]\tTotal Loss: 17.292824\n",
      "Reconstruction: 16.705160, Regularization: 0.492614, Discriminator: 0.080069; Generator: 0.014981,\n",
      "D(x): 0.208, D(G(z)): 0.619\n",
      "2019-04-10 00:55:40,562 root         INFO     Train Epoch: 10 [3072/8000 (38%)]\tTotal Loss: 16.399250\n",
      "Reconstruction: 15.831583, Regularization: 0.474601, Discriminator: 0.077965; Generator: 0.015102,\n",
      "D(x): 0.224, D(G(z)): 0.617\n",
      "2019-04-10 00:55:40,670 root         INFO     Train Epoch: 10 [3584/8000 (45%)]\tTotal Loss: 16.449055\n",
      "Reconstruction: 15.839688, Regularization: 0.516122, Discriminator: 0.078053; Generator: 0.015191,\n",
      "D(x): 0.224, D(G(z)): 0.615\n",
      "2019-04-10 00:55:40,779 root         INFO     Train Epoch: 10 [4096/8000 (51%)]\tTotal Loss: 16.244555\n",
      "Reconstruction: 15.666451, Regularization: 0.487162, Discriminator: 0.075628; Generator: 0.015314,\n",
      "D(x): 0.237, D(G(z)): 0.613\n",
      "2019-04-10 00:55:40,889 root         INFO     Train Epoch: 10 [4608/8000 (58%)]\tTotal Loss: 16.621895\n",
      "Reconstruction: 15.980053, Regularization: 0.549686, Discriminator: 0.076860; Generator: 0.015296,\n",
      "D(x): 0.228, D(G(z)): 0.613\n",
      "2019-04-10 00:55:40,998 root         INFO     Train Epoch: 10 [5120/8000 (64%)]\tTotal Loss: 16.469568\n",
      "Reconstruction: 15.794535, Regularization: 0.583068, Discriminator: 0.076443; Generator: 0.015522,\n",
      "D(x): 0.229, D(G(z)): 0.609\n",
      "2019-04-10 00:55:41,107 root         INFO     Train Epoch: 10 [5632/8000 (70%)]\tTotal Loss: 15.569262\n",
      "Reconstruction: 14.999222, Regularization: 0.482496, Discriminator: 0.071974; Generator: 0.015569,\n",
      "D(x): 0.262, D(G(z)): 0.608\n",
      "2019-04-10 00:55:41,216 root         INFO     Train Epoch: 10 [6144/8000 (77%)]\tTotal Loss: 15.409785\n",
      "Reconstruction: 14.826946, Regularization: 0.496142, Discriminator: 0.071027; Generator: 0.015671,\n",
      "D(x): 0.270, D(G(z)): 0.606\n",
      "2019-04-10 00:55:41,325 root         INFO     Train Epoch: 10 [6656/8000 (83%)]\tTotal Loss: 15.648114\n",
      "Reconstruction: 14.937134, Regularization: 0.622039, Discriminator: 0.073134; Generator: 0.015808,\n",
      "D(x): 0.256, D(G(z)): 0.603\n",
      "2019-04-10 00:55:41,434 root         INFO     Train Epoch: 10 [7168/8000 (90%)]\tTotal Loss: 16.029694\n",
      "Reconstruction: 15.327967, Regularization: 0.613482, Discriminator: 0.072332; Generator: 0.015913,\n",
      "D(x): 0.253, D(G(z)): 0.601\n",
      "2019-04-10 00:55:41,543 root         INFO     Train Epoch: 10 [7680/8000 (96%)]\tTotal Loss: 15.428186\n",
      "Reconstruction: 14.824322, Regularization: 0.519091, Discriminator: 0.068841; Generator: 0.015932,\n",
      "D(x): 0.281, D(G(z)): 0.601\n",
      "2019-04-10 00:55:41,623 root         INFO     ====> Epoch: 10 Average loss: 16.4145\n",
      "2019-04-10 00:55:41,650 root         INFO     Train Epoch: 11 [0/8000 (0%)]\tTotal Loss: 14.927077\n",
      "Reconstruction: 14.348022, Regularization: 0.495611, Discriminator: 0.067330; Generator: 0.016114,\n",
      "D(x): 0.293, D(G(z)): 0.597\n",
      "2019-04-10 00:55:41,759 root         INFO     Train Epoch: 11 [512/8000 (6%)]\tTotal Loss: 15.403107\n",
      "Reconstruction: 14.741829, Regularization: 0.576720, Discriminator: 0.068366; Generator: 0.016191,\n",
      "D(x): 0.283, D(G(z)): 0.596\n",
      "2019-04-10 00:55:41,868 root         INFO     Train Epoch: 11 [1024/8000 (13%)]\tTotal Loss: 15.656789\n",
      "Reconstruction: 14.910965, Regularization: 0.660292, Discriminator: 0.069292; Generator: 0.016240,\n",
      "D(x): 0.273, D(G(z)): 0.595\n",
      "2019-04-10 00:55:41,976 root         INFO     Train Epoch: 11 [1536/8000 (19%)]\tTotal Loss: 14.926915\n",
      "Reconstruction: 14.209921, Regularization: 0.633289, Discriminator: 0.067369; Generator: 0.016336,\n",
      "D(x): 0.292, D(G(z)): 0.593\n",
      "2019-04-10 00:55:42,084 root         INFO     Train Epoch: 11 [2048/8000 (26%)]\tTotal Loss: 14.634690\n",
      "Reconstruction: 13.982622, Regularization: 0.570329, Discriminator: 0.065274; Generator: 0.016465,\n",
      "D(x): 0.307, D(G(z)): 0.590\n",
      "2019-04-10 00:55:42,193 root         INFO     Train Epoch: 11 [2560/8000 (32%)]\tTotal Loss: 14.851189\n",
      "Reconstruction: 14.208054, Regularization: 0.562266, Discriminator: 0.064300; Generator: 0.016570,\n",
      "D(x): 0.314, D(G(z)): 0.588\n",
      "2019-04-10 00:55:42,301 root         INFO     Train Epoch: 11 [3072/8000 (38%)]\tTotal Loss: 15.181608\n",
      "Reconstruction: 14.401316, Regularization: 0.697899, Discriminator: 0.065714; Generator: 0.016680,\n",
      "D(x): 0.299, D(G(z)): 0.586\n",
      "2019-04-10 00:55:42,409 root         INFO     Train Epoch: 11 [3584/8000 (45%)]\tTotal Loss: 14.376367\n",
      "Reconstruction: 13.689514, Regularization: 0.606961, Discriminator: 0.063080; Generator: 0.016811,\n",
      "D(x): 0.324, D(G(z)): 0.584\n",
      "2019-04-10 00:55:42,517 root         INFO     Train Epoch: 11 [4096/8000 (51%)]\tTotal Loss: 14.213656\n",
      "Reconstruction: 13.483274, Regularization: 0.650596, Discriminator: 0.062922; Generator: 0.016864,\n",
      "D(x): 0.325, D(G(z)): 0.583\n",
      "2019-04-10 00:55:42,626 root         INFO     Train Epoch: 11 [4608/8000 (58%)]\tTotal Loss: 13.823803\n",
      "Reconstruction: 13.196796, Regularization: 0.549385, Discriminator: 0.060633; Generator: 0.016988,\n",
      "D(x): 0.346, D(G(z)): 0.581\n",
      "2019-04-10 00:55:42,734 root         INFO     Train Epoch: 11 [5120/8000 (64%)]\tTotal Loss: 14.095413\n",
      "Reconstruction: 13.397338, Regularization: 0.620069, Discriminator: 0.060910; Generator: 0.017096,\n",
      "D(x): 0.341, D(G(z)): 0.579\n",
      "2019-04-10 00:55:42,842 root         INFO     Train Epoch: 11 [5632/8000 (70%)]\tTotal Loss: 14.010506\n",
      "Reconstruction: 13.319530, Regularization: 0.613740, Discriminator: 0.060041; Generator: 0.017195,\n",
      "D(x): 0.348, D(G(z)): 0.577\n",
      "2019-04-10 00:55:42,951 root         INFO     Train Epoch: 11 [6144/8000 (77%)]\tTotal Loss: 13.735520\n",
      "Reconstruction: 13.062904, Regularization: 0.596352, Discriminator: 0.058929; Generator: 0.017334,\n",
      "D(x): 0.359, D(G(z)): 0.574\n",
      "2019-04-10 00:55:43,060 root         INFO     Train Epoch: 11 [6656/8000 (83%)]\tTotal Loss: 13.959442\n",
      "Reconstruction: 13.195082, Regularization: 0.687528, Discriminator: 0.059396; Generator: 0.017437,\n",
      "D(x): 0.351, D(G(z)): 0.572\n",
      "2019-04-10 00:55:43,168 root         INFO     Train Epoch: 11 [7168/8000 (90%)]\tTotal Loss: 13.634026\n",
      "Reconstruction: 12.956780, Regularization: 0.602055, Discriminator: 0.057631; Generator: 0.017558,\n",
      "D(x): 0.370, D(G(z)): 0.570\n",
      "2019-04-10 00:55:43,277 root         INFO     Train Epoch: 11 [7680/8000 (96%)]\tTotal Loss: 13.552413\n",
      "Reconstruction: 12.840930, Regularization: 0.636503, Discriminator: 0.057322; Generator: 0.017658,\n",
      "D(x): 0.371, D(G(z)): 0.568\n",
      "2019-04-10 00:55:43,358 root         INFO     ====> Epoch: 11 Average loss: 14.4387\n",
      "2019-04-10 00:55:43,384 root         INFO     Train Epoch: 12 [0/8000 (0%)]\tTotal Loss: 13.674131\n",
      "Reconstruction: 12.875954, Regularization: 0.722886, Discriminator: 0.057559; Generator: 0.017732,\n",
      "D(x): 0.368, D(G(z)): 0.567\n",
      "2019-04-10 00:55:43,492 root         INFO     Train Epoch: 12 [512/8000 (6%)]\tTotal Loss: 13.670603\n",
      "Reconstruction: 12.895039, Regularization: 0.700922, Discriminator: 0.056793; Generator: 0.017849,\n",
      "D(x): 0.375, D(G(z)): 0.565\n",
      "2019-04-10 00:55:43,598 root         INFO     Train Epoch: 12 [1024/8000 (13%)]\tTotal Loss: 13.400497\n",
      "Reconstruction: 12.619572, Regularization: 0.706944, Discriminator: 0.056008; Generator: 0.017973,\n",
      "D(x): 0.382, D(G(z)): 0.563\n",
      "2019-04-10 00:55:43,704 root         INFO     Train Epoch: 12 [1536/8000 (19%)]\tTotal Loss: 13.292945\n",
      "Reconstruction: 12.531626, Regularization: 0.687967, Discriminator: 0.055273; Generator: 0.018080,\n",
      "D(x): 0.389, D(G(z)): 0.561\n",
      "2019-04-10 00:55:43,810 root         INFO     Train Epoch: 12 [2048/8000 (26%)]\tTotal Loss: 13.439750\n",
      "Reconstruction: 12.583115, Regularization: 0.783228, Discriminator: 0.055214; Generator: 0.018193,\n",
      "D(x): 0.388, D(G(z)): 0.559\n",
      "2019-04-10 00:55:43,917 root         INFO     Train Epoch: 12 [2560/8000 (32%)]\tTotal Loss: 13.006163\n",
      "Reconstruction: 12.224369, Regularization: 0.709564, Discriminator: 0.053917; Generator: 0.018312,\n",
      "D(x): 0.403, D(G(z)): 0.557\n",
      "2019-04-10 00:55:44,025 root         INFO     Train Epoch: 12 [3072/8000 (38%)]\tTotal Loss: 13.379152\n",
      "Reconstruction: 12.400064, Regularization: 0.906154, Discriminator: 0.054510; Generator: 0.018424,\n",
      "D(x): 0.393, D(G(z)): 0.555\n",
      "2019-04-10 00:55:44,133 root         INFO     Train Epoch: 12 [3584/8000 (45%)]\tTotal Loss: 13.392931\n",
      "Reconstruction: 12.495148, Regularization: 0.825713, Discriminator: 0.053526; Generator: 0.018544,\n",
      "D(x): 0.403, D(G(z)): 0.552\n",
      "2019-04-10 00:55:44,238 root         INFO     Train Epoch: 12 [4096/8000 (51%)]\tTotal Loss: 13.124681\n",
      "Reconstruction: 12.239673, Regularization: 0.813367, Discriminator: 0.052973; Generator: 0.018667,\n",
      "D(x): 0.409, D(G(z)): 0.550\n",
      "2019-04-10 00:55:44,343 root         INFO     Train Epoch: 12 [4608/8000 (58%)]\tTotal Loss: 12.903163\n",
      "Reconstruction: 11.985380, Regularization: 0.846789, Discriminator: 0.052218; Generator: 0.018775,\n",
      "D(x): 0.417, D(G(z)): 0.548\n",
      "2019-04-10 00:55:44,449 root         INFO     Train Epoch: 12 [5120/8000 (64%)]\tTotal Loss: 12.542150\n",
      "Reconstruction: 11.729564, Regularization: 0.742529, Discriminator: 0.051156; Generator: 0.018902,\n",
      "D(x): 0.429, D(G(z)): 0.546\n",
      "2019-04-10 00:55:44,555 root         INFO     Train Epoch: 12 [5632/8000 (70%)]\tTotal Loss: 12.931684\n",
      "Reconstruction: 12.031490, Regularization: 0.830220, Discriminator: 0.050940; Generator: 0.019036,\n",
      "D(x): 0.430, D(G(z)): 0.544\n",
      "2019-04-10 00:55:44,661 root         INFO     Train Epoch: 12 [6144/8000 (77%)]\tTotal Loss: 12.247618\n",
      "Reconstruction: 11.405599, Regularization: 0.772837, Discriminator: 0.050063; Generator: 0.019118,\n",
      "D(x): 0.441, D(G(z)): 0.542\n",
      "2019-04-10 00:55:44,767 root         INFO     Train Epoch: 12 [6656/8000 (83%)]\tTotal Loss: 12.427217\n",
      "Reconstruction: 11.534889, Regularization: 0.823269, Discriminator: 0.049808; Generator: 0.019251,\n",
      "D(x): 0.442, D(G(z)): 0.540\n",
      "2019-04-10 00:55:44,873 root         INFO     Train Epoch: 12 [7168/8000 (90%)]\tTotal Loss: 12.161916\n",
      "Reconstruction: 11.324258, Regularization: 0.769165, Discriminator: 0.049107; Generator: 0.019386,\n",
      "D(x): 0.450, D(G(z)): 0.538\n",
      "2019-04-10 00:55:44,979 root         INFO     Train Epoch: 12 [7680/8000 (96%)]\tTotal Loss: 12.149148\n",
      "Reconstruction: 11.333366, Regularization: 0.747642, Discriminator: 0.048638; Generator: 0.019501,\n",
      "D(x): 0.454, D(G(z)): 0.536\n",
      "2019-04-10 00:55:45,057 root         INFO     ====> Epoch: 12 Average loss: 12.7978\n",
      "2019-04-10 00:55:45,084 root         INFO     Train Epoch: 13 [0/8000 (0%)]\tTotal Loss: 12.315528\n",
      "Reconstruction: 11.417385, Regularization: 0.830153, Discriminator: 0.048440; Generator: 0.019550,\n",
      "D(x): 0.457, D(G(z)): 0.535\n",
      "2019-04-10 00:55:45,194 root         INFO     Train Epoch: 13 [512/8000 (6%)]\tTotal Loss: 12.013599\n",
      "Reconstruction: 11.160071, Regularization: 0.786005, Discriminator: 0.047860; Generator: 0.019663,\n",
      "D(x): 0.463, D(G(z)): 0.533\n",
      "2019-04-10 00:55:45,303 root         INFO     Train Epoch: 13 [1024/8000 (13%)]\tTotal Loss: 11.860842\n",
      "Reconstruction: 11.081861, Regularization: 0.711944, Discriminator: 0.047229; Generator: 0.019809,\n",
      "D(x): 0.470, D(G(z)): 0.531\n",
      "2019-04-10 00:55:45,412 root         INFO     Train Epoch: 13 [1536/8000 (19%)]\tTotal Loss: 11.693398\n",
      "Reconstruction: 10.821837, Regularization: 0.805065, Discriminator: 0.046573; Generator: 0.019923,\n",
      "D(x): 0.478, D(G(z)): 0.529\n",
      "2019-04-10 00:55:45,521 root         INFO     Train Epoch: 13 [2048/8000 (26%)]\tTotal Loss: 11.932131\n",
      "Reconstruction: 11.024460, Regularization: 0.841444, Discriminator: 0.046140; Generator: 0.020086,\n",
      "D(x): 0.482, D(G(z)): 0.526\n",
      "2019-04-10 00:55:45,630 root         INFO     Train Epoch: 13 [2560/8000 (32%)]\tTotal Loss: 11.713670\n",
      "Reconstruction: 10.830822, Regularization: 0.816647, Discriminator: 0.046006; Generator: 0.020195,\n",
      "D(x): 0.482, D(G(z)): 0.524\n",
      "2019-04-10 00:55:45,738 root         INFO     Train Epoch: 13 [3072/8000 (38%)]\tTotal Loss: 11.759605\n",
      "Reconstruction: 10.607616, Regularization: 1.086252, Discriminator: 0.045432; Generator: 0.020305,\n",
      "D(x): 0.489, D(G(z)): 0.522\n",
      "2019-04-10 00:55:45,847 root         INFO     Train Epoch: 13 [3584/8000 (45%)]\tTotal Loss: 11.707765\n",
      "Reconstruction: 10.782348, Regularization: 0.860047, Discriminator: 0.044942; Generator: 0.020427,\n",
      "D(x): 0.495, D(G(z)): 0.520\n",
      "2019-04-10 00:55:45,955 root         INFO     Train Epoch: 13 [4096/8000 (51%)]\tTotal Loss: 11.585486\n",
      "Reconstruction: 10.618916, Regularization: 0.901549, Discriminator: 0.044497; Generator: 0.020525,\n",
      "D(x): 0.500, D(G(z)): 0.519\n",
      "2019-04-10 00:55:46,064 root         INFO     Train Epoch: 13 [4608/8000 (58%)]\tTotal Loss: 11.596655\n",
      "Reconstruction: 10.628178, Regularization: 0.903821, Discriminator: 0.044006; Generator: 0.020650,\n",
      "D(x): 0.506, D(G(z)): 0.516\n",
      "2019-04-10 00:55:46,172 root         INFO     Train Epoch: 13 [5120/8000 (64%)]\tTotal Loss: 11.677989\n",
      "Reconstruction: 10.605286, Regularization: 1.008366, Discriminator: 0.043567; Generator: 0.020770,\n",
      "D(x): 0.511, D(G(z)): 0.514\n",
      "2019-04-10 00:55:46,280 root         INFO     Train Epoch: 13 [5632/8000 (70%)]\tTotal Loss: 11.302940\n",
      "Reconstruction: 10.474097, Regularization: 0.764370, Discriminator: 0.043560; Generator: 0.020913,\n",
      "D(x): 0.509, D(G(z)): 0.512\n",
      "2019-04-10 00:55:46,389 root         INFO     Train Epoch: 13 [6144/8000 (77%)]\tTotal Loss: 11.488511\n",
      "Reconstruction: 10.402251, Regularization: 1.022575, Discriminator: 0.042696; Generator: 0.020988,\n",
      "D(x): 0.522, D(G(z)): 0.511\n",
      "2019-04-10 00:55:46,497 root         INFO     Train Epoch: 13 [6656/8000 (83%)]\tTotal Loss: 11.172525\n",
      "Reconstruction: 10.304673, Regularization: 0.803957, Discriminator: 0.042781; Generator: 0.021114,\n",
      "D(x): 0.518, D(G(z)): 0.509\n",
      "2019-04-10 00:55:46,605 root         INFO     Train Epoch: 13 [7168/8000 (90%)]\tTotal Loss: 11.141961\n",
      "Reconstruction: 10.171885, Regularization: 0.906777, Discriminator: 0.042053; Generator: 0.021248,\n",
      "D(x): 0.528, D(G(z)): 0.507\n",
      "2019-04-10 00:55:46,714 root         INFO     Train Epoch: 13 [7680/8000 (96%)]\tTotal Loss: 11.150840\n",
      "Reconstruction: 10.269711, Regularization: 0.817631, Discriminator: 0.042122; Generator: 0.021377,\n",
      "D(x): 0.525, D(G(z)): 0.505\n",
      "2019-04-10 00:55:46,793 root         INFO     ====> Epoch: 13 Average loss: 11.6083\n",
      "2019-04-10 00:55:46,821 root         INFO     Train Epoch: 14 [0/8000 (0%)]\tTotal Loss: 10.819774\n",
      "Reconstruction: 9.985849, Regularization: 0.770867, Discriminator: 0.041668; Generator: 0.021389,\n",
      "D(x): 0.532, D(G(z)): 0.504\n",
      "2019-04-10 00:55:46,930 root         INFO     Train Epoch: 14 [512/8000 (6%)]\tTotal Loss: 11.146255\n",
      "Reconstruction: 10.193553, Regularization: 0.889903, Discriminator: 0.041269; Generator: 0.021531,\n",
      "D(x): 0.537, D(G(z)): 0.502\n",
      "2019-04-10 00:55:47,039 root         INFO     Train Epoch: 14 [1024/8000 (13%)]\tTotal Loss: 10.935161\n",
      "Reconstruction: 10.034725, Regularization: 0.838016, Discriminator: 0.040703; Generator: 0.021716,\n",
      "D(x): 0.543, D(G(z)): 0.499\n",
      "2019-04-10 00:55:47,149 root         INFO     Train Epoch: 14 [1536/8000 (19%)]\tTotal Loss: 10.978194\n",
      "Reconstruction: 10.033878, Regularization: 0.882217, Discriminator: 0.040304; Generator: 0.021795,\n",
      "D(x): 0.549, D(G(z)): 0.498\n",
      "2019-04-10 00:55:47,257 root         INFO     Train Epoch: 14 [2048/8000 (26%)]\tTotal Loss: 11.222855\n",
      "Reconstruction: 10.090746, Regularization: 1.070836, Discriminator: 0.039302; Generator: 0.021972,\n",
      "D(x): 0.564, D(G(z)): 0.495\n",
      "2019-04-10 00:55:47,366 root         INFO     Train Epoch: 14 [2560/8000 (32%)]\tTotal Loss: 11.267599\n",
      "Reconstruction: 10.160416, Regularization: 1.045852, Discriminator: 0.039300; Generator: 0.022032,\n",
      "D(x): 0.563, D(G(z)): 0.494\n",
      "2019-04-10 00:55:47,475 root         INFO     Train Epoch: 14 [3072/8000 (38%)]\tTotal Loss: 10.687463\n",
      "Reconstruction: 9.627571, Regularization: 0.998495, Discriminator: 0.039109; Generator: 0.022288,\n",
      "D(x): 0.562, D(G(z)): 0.490\n",
      "2019-04-10 00:55:47,584 root         INFO     Train Epoch: 14 [3584/8000 (45%)]\tTotal Loss: 11.033722\n",
      "Reconstruction: 10.047341, Regularization: 0.924999, Discriminator: 0.039064; Generator: 0.022318,\n",
      "D(x): 0.562, D(G(z)): 0.490\n",
      "2019-04-10 00:55:47,692 root         INFO     Train Epoch: 14 [4096/8000 (51%)]\tTotal Loss: 10.679327\n",
      "Reconstruction: 9.661858, Regularization: 0.956277, Discriminator: 0.038744; Generator: 0.022448,\n",
      "D(x): 0.566, D(G(z)): 0.488\n",
      "2019-04-10 00:55:47,798 root         INFO     Train Epoch: 14 [4608/8000 (58%)]\tTotal Loss: 10.788875\n",
      "Reconstruction: 9.714176, Regularization: 1.014430, Discriminator: 0.037788; Generator: 0.022481,\n",
      "D(x): 0.583, D(G(z)): 0.487\n",
      "2019-04-10 00:55:47,904 root         INFO     Train Epoch: 14 [5120/8000 (64%)]\tTotal Loss: 11.289041\n",
      "Reconstruction: 10.120483, Regularization: 1.108557, Discriminator: 0.037374; Generator: 0.022626,\n",
      "D(x): 0.588, D(G(z)): 0.485\n",
      "2019-04-10 00:55:48,009 root         INFO     Train Epoch: 14 [5632/8000 (70%)]\tTotal Loss: 10.743349\n",
      "Reconstruction: 9.717529, Regularization: 0.965397, Discriminator: 0.037639; Generator: 0.022783,\n",
      "D(x): 0.581, D(G(z)): 0.482\n",
      "2019-04-10 00:55:48,115 root         INFO     Train Epoch: 14 [6144/8000 (77%)]\tTotal Loss: 10.955278\n",
      "Reconstruction: 9.989551, Regularization: 0.905360, Discriminator: 0.037510; Generator: 0.022858,\n",
      "D(x): 0.581, D(G(z)): 0.481\n",
      "2019-04-10 00:55:48,222 root         INFO     Train Epoch: 14 [6656/8000 (83%)]\tTotal Loss: 10.834062\n",
      "Reconstruction: 9.812922, Regularization: 0.961372, Discriminator: 0.036747; Generator: 0.023020,\n",
      "D(x): 0.593, D(G(z)): 0.479\n",
      "2019-04-10 00:55:48,328 root         INFO     Train Epoch: 14 [7168/8000 (90%)]\tTotal Loss: 11.270953\n",
      "Reconstruction: 10.115683, Regularization: 1.096121, Discriminator: 0.036037; Generator: 0.023113,\n",
      "D(x): 0.605, D(G(z)): 0.477\n",
      "2019-04-10 00:55:48,434 root         INFO     Train Epoch: 14 [7680/8000 (96%)]\tTotal Loss: 11.485295\n",
      "Reconstruction: 10.166622, Regularization: 1.260384, Discriminator: 0.035139; Generator: 0.023151,\n",
      "D(x): 0.622, D(G(z)): 0.477\n",
      "2019-04-10 00:55:48,513 root         INFO     ====> Epoch: 14 Average loss: 10.9688\n",
      "2019-04-10 00:55:48,540 root         INFO     Train Epoch: 15 [0/8000 (0%)]\tTotal Loss: 11.306636\n",
      "Reconstruction: 10.182686, Regularization: 1.064357, Discriminator: 0.036329; Generator: 0.023264,\n",
      "D(x): 0.597, D(G(z)): 0.475\n",
      "2019-04-10 00:55:48,648 root         INFO     Train Epoch: 15 [512/8000 (6%)]\tTotal Loss: 10.365807\n",
      "Reconstruction: 9.376842, Regularization: 0.929132, Discriminator: 0.036432; Generator: 0.023399,\n",
      "D(x): 0.594, D(G(z)): 0.473\n",
      "2019-04-10 00:55:48,753 root         INFO     Train Epoch: 15 [1024/8000 (13%)]\tTotal Loss: 10.782516\n",
      "Reconstruction: 9.669476, Regularization: 1.054205, Discriminator: 0.035317; Generator: 0.023518,\n",
      "D(x): 0.613, D(G(z)): 0.471\n",
      "2019-04-10 00:55:48,859 root         INFO     Train Epoch: 15 [1536/8000 (19%)]\tTotal Loss: 10.417523\n",
      "Reconstruction: 9.403872, Regularization: 0.954439, Discriminator: 0.035530; Generator: 0.023682,\n",
      "D(x): 0.605, D(G(z)): 0.469\n",
      "2019-04-10 00:55:48,966 root         INFO     Train Epoch: 15 [2048/8000 (26%)]\tTotal Loss: 10.307969\n",
      "Reconstruction: 9.322668, Regularization: 0.926199, Discriminator: 0.035390; Generator: 0.023712,\n",
      "D(x): 0.608, D(G(z)): 0.468\n",
      "2019-04-10 00:55:49,073 root         INFO     Train Epoch: 15 [2560/8000 (32%)]\tTotal Loss: 11.046239\n",
      "Reconstruction: 9.875545, Regularization: 1.112383, Discriminator: 0.034426; Generator: 0.023886,\n",
      "D(x): 0.624, D(G(z)): 0.466\n",
      "2019-04-10 00:55:49,179 root         INFO     Train Epoch: 15 [3072/8000 (38%)]\tTotal Loss: 10.558803\n",
      "Reconstruction: 9.569269, Regularization: 0.931066, Discriminator: 0.034485; Generator: 0.023982,\n",
      "D(x): 0.620, D(G(z)): 0.464\n",
      "2019-04-10 00:55:49,286 root         INFO     Train Epoch: 15 [3584/8000 (45%)]\tTotal Loss: 10.557793\n",
      "Reconstruction: 9.547530, Regularization: 0.951601, Discriminator: 0.034629; Generator: 0.024032,\n",
      "D(x): 0.618, D(G(z)): 0.464\n",
      "2019-04-10 00:55:49,392 root         INFO     Train Epoch: 15 [4096/8000 (51%)]\tTotal Loss: 11.194419\n",
      "Reconstruction: 9.926546, Regularization: 1.210485, Discriminator: 0.033205; Generator: 0.024183,\n",
      "D(x): 0.643, D(G(z)): 0.461\n",
      "2019-04-10 00:55:49,499 root         INFO     Train Epoch: 15 [4608/8000 (58%)]\tTotal Loss: 10.143989\n",
      "Reconstruction: 9.136719, Regularization: 0.948549, Discriminator: 0.034374; Generator: 0.024346,\n",
      "D(x): 0.618, D(G(z)): 0.459\n",
      "2019-04-10 00:55:49,606 root         INFO     Train Epoch: 15 [5120/8000 (64%)]\tTotal Loss: 10.582614\n",
      "Reconstruction: 9.489325, Regularization: 1.035691, Discriminator: 0.033234; Generator: 0.024364,\n",
      "D(x): 0.640, D(G(z)): 0.459\n",
      "2019-04-10 00:55:49,713 root         INFO     Train Epoch: 15 [5632/8000 (70%)]\tTotal Loss: 10.702805\n",
      "Reconstruction: 9.527033, Regularization: 1.118691, Discriminator: 0.032524; Generator: 0.024557,\n",
      "D(x): 0.652, D(G(z)): 0.456\n",
      "2019-04-10 00:55:49,821 root         INFO     Train Epoch: 15 [6144/8000 (77%)]\tTotal Loss: 10.026256\n",
      "Reconstruction: 9.054340, Regularization: 0.913079, Discriminator: 0.034303; Generator: 0.024534,\n",
      "D(x): 0.618, D(G(z)): 0.456\n",
      "2019-04-10 00:55:49,929 root         INFO     Train Epoch: 15 [6656/8000 (83%)]\tTotal Loss: 11.093073\n",
      "Reconstruction: 9.866834, Regularization: 1.169484, Discriminator: 0.032091; Generator: 0.024664,\n",
      "D(x): 0.659, D(G(z)): 0.454\n",
      "2019-04-10 00:55:50,039 root         INFO     Train Epoch: 15 [7168/8000 (90%)]\tTotal Loss: 10.623861\n",
      "Reconstruction: 9.465403, Regularization: 1.101110, Discriminator: 0.032638; Generator: 0.024710,\n",
      "D(x): 0.648, D(G(z)): 0.454\n",
      "2019-04-10 00:55:50,147 root         INFO     Train Epoch: 15 [7680/8000 (96%)]\tTotal Loss: 11.014719\n",
      "Reconstruction: 9.692224, Regularization: 1.266177, Discriminator: 0.031389; Generator: 0.024930,\n",
      "D(x): 0.670, D(G(z)): 0.450\n",
      "2019-04-10 00:55:50,227 root         INFO     ====> Epoch: 15 Average loss: 10.7410\n",
      "2019-04-10 00:55:50,254 root         INFO     Train Epoch: 16 [0/8000 (0%)]\tTotal Loss: 10.541103\n",
      "Reconstruction: 9.359297, Regularization: 1.125007, Discriminator: 0.031910; Generator: 0.024889,\n",
      "D(x): 0.659, D(G(z)): 0.451\n",
      "2019-04-10 00:55:50,364 root         INFO     Train Epoch: 16 [512/8000 (6%)]\tTotal Loss: 10.271174\n",
      "Reconstruction: 9.219515, Regularization: 0.993814, Discriminator: 0.032636; Generator: 0.025210,\n",
      "D(x): 0.640, D(G(z)): 0.446\n",
      "2019-04-10 00:55:50,473 root         INFO     Train Epoch: 16 [1024/8000 (13%)]\tTotal Loss: 10.911324\n",
      "Reconstruction: 9.619874, Regularization: 1.235550, Discriminator: 0.030781; Generator: 0.025118,\n",
      "D(x): 0.678, D(G(z)): 0.448\n",
      "2019-04-10 00:55:50,583 root         INFO     Train Epoch: 16 [1536/8000 (19%)]\tTotal Loss: 9.667801\n",
      "Reconstruction: 8.669312, Regularization: 0.941252, Discriminator: 0.031835; Generator: 0.025402,\n",
      "D(x): 0.653, D(G(z)): 0.444\n",
      "2019-04-10 00:55:50,692 root         INFO     Train Epoch: 16 [2048/8000 (26%)]\tTotal Loss: 10.498780\n",
      "Reconstruction: 9.239525, Regularization: 1.203449, Discriminator: 0.030547; Generator: 0.025260,\n",
      "D(x): 0.683, D(G(z)): 0.446\n",
      "2019-04-10 00:55:50,800 root         INFO     Train Epoch: 16 [2560/8000 (32%)]\tTotal Loss: 10.476863\n",
      "Reconstruction: 9.242815, Regularization: 1.177605, Discriminator: 0.031117; Generator: 0.025326,\n",
      "D(x): 0.671, D(G(z)): 0.445\n",
      "2019-04-10 00:55:50,909 root         INFO     Train Epoch: 16 [3072/8000 (38%)]\tTotal Loss: 10.710165\n",
      "Reconstruction: 9.401812, Regularization: 1.252243, Discriminator: 0.030596; Generator: 0.025515,\n",
      "D(x): 0.679, D(G(z)): 0.442\n",
      "2019-04-10 00:55:51,018 root         INFO     Train Epoch: 16 [3584/8000 (45%)]\tTotal Loss: 10.257706\n",
      "Reconstruction: 9.063648, Regularization: 1.137768, Discriminator: 0.030678; Generator: 0.025612,\n",
      "D(x): 0.675, D(G(z)): 0.441\n",
      "2019-04-10 00:55:51,127 root         INFO     Train Epoch: 16 [4096/8000 (51%)]\tTotal Loss: 9.183309\n",
      "Reconstruction: 8.232264, Regularization: 0.893527, Discriminator: 0.031969; Generator: 0.025549,\n",
      "D(x): 0.649, D(G(z)): 0.442\n",
      "2019-04-10 00:55:51,235 root         INFO     Train Epoch: 16 [4608/8000 (58%)]\tTotal Loss: 9.405194\n",
      "Reconstruction: 8.413948, Regularization: 0.934044, Discriminator: 0.031403; Generator: 0.025798,\n",
      "D(x): 0.656, D(G(z)): 0.438\n",
      "2019-04-10 00:55:51,342 root         INFO     Train Epoch: 16 [5120/8000 (64%)]\tTotal Loss: 10.358417\n",
      "Reconstruction: 9.201780, Regularization: 1.100418, Discriminator: 0.030361; Generator: 0.025857,\n",
      "D(x): 0.676, D(G(z)): 0.437\n",
      "2019-04-10 00:55:51,449 root         INFO     Train Epoch: 16 [5632/8000 (70%)]\tTotal Loss: 9.856994\n",
      "Reconstruction: 8.731364, Regularization: 1.069160, Discriminator: 0.030466; Generator: 0.026004,\n",
      "D(x): 0.674, D(G(z)): 0.435\n",
      "2019-04-10 00:55:51,555 root         INFO     Train Epoch: 16 [6144/8000 (77%)]\tTotal Loss: 10.816317\n",
      "Reconstruction: 9.468097, Regularization: 1.293379, Discriminator: 0.028770; Generator: 0.026071,\n",
      "D(x): 0.707, D(G(z)): 0.434\n",
      "2019-04-10 00:55:51,662 root         INFO     Train Epoch: 16 [6656/8000 (83%)]\tTotal Loss: 10.319368\n",
      "Reconstruction: 9.089057, Regularization: 1.175182, Discriminator: 0.028895; Generator: 0.026235,\n",
      "D(x): 0.702, D(G(z)): 0.432\n",
      "2019-04-10 00:55:51,769 root         INFO     Train Epoch: 16 [7168/8000 (90%)]\tTotal Loss: 9.732991\n",
      "Reconstruction: 8.656542, Regularization: 1.020511, Discriminator: 0.029705; Generator: 0.026233,\n",
      "D(x): 0.684, D(G(z)): 0.432\n",
      "2019-04-10 00:55:51,875 root         INFO     Train Epoch: 16 [7680/8000 (96%)]\tTotal Loss: 10.417162\n",
      "Reconstruction: 9.110995, Regularization: 1.251275, Discriminator: 0.028590; Generator: 0.026301,\n",
      "D(x): 0.709, D(G(z)): 0.431\n",
      "2019-04-10 00:55:51,953 root         INFO     ====> Epoch: 16 Average loss: 10.3778\n",
      "2019-04-10 00:55:51,980 root         INFO     Train Epoch: 17 [0/8000 (0%)]\tTotal Loss: 9.838724\n",
      "Reconstruction: 8.674088, Regularization: 1.109340, Discriminator: 0.028936; Generator: 0.026359,\n",
      "D(x): 0.701, D(G(z)): 0.430\n",
      "2019-04-10 00:55:52,089 root         INFO     Train Epoch: 17 [512/8000 (6%)]\tTotal Loss: 10.615492\n",
      "Reconstruction: 9.256413, Regularization: 1.304732, Discriminator: 0.027887; Generator: 0.026459,\n",
      "D(x): 0.721, D(G(z)): 0.429\n",
      "2019-04-10 00:55:52,198 root         INFO     Train Epoch: 17 [1024/8000 (13%)]\tTotal Loss: 10.099142\n",
      "Reconstruction: 8.751236, Regularization: 1.292715, Discriminator: 0.028557; Generator: 0.026634,\n",
      "D(x): 0.711, D(G(z)): 0.426\n",
      "2019-04-10 00:55:52,308 root         INFO     Train Epoch: 17 [1536/8000 (19%)]\tTotal Loss: 10.055817\n",
      "Reconstruction: 8.842695, Regularization: 1.158182, Discriminator: 0.028325; Generator: 0.026614,\n",
      "D(x): 0.710, D(G(z)): 0.427\n",
      "2019-04-10 00:55:52,417 root         INFO     Train Epoch: 17 [2048/8000 (26%)]\tTotal Loss: 10.858622\n",
      "Reconstruction: 9.514187, Regularization: 1.290641, Discriminator: 0.027114; Generator: 0.026680,\n",
      "D(x): 0.735, D(G(z)): 0.426\n",
      "2019-04-10 00:55:52,526 root         INFO     Train Epoch: 17 [2560/8000 (32%)]\tTotal Loss: 10.623804\n",
      "Reconstruction: 9.305122, Regularization: 1.264412, Discriminator: 0.027568; Generator: 0.026701,\n",
      "D(x): 0.727, D(G(z)): 0.426\n",
      "2019-04-10 00:55:52,635 root         INFO     Train Epoch: 17 [3072/8000 (38%)]\tTotal Loss: 10.122412\n",
      "Reconstruction: 8.877348, Regularization: 1.190442, Discriminator: 0.027870; Generator: 0.026753,\n",
      "D(x): 0.719, D(G(z)): 0.425\n",
      "2019-04-10 00:55:52,744 root         INFO     Train Epoch: 17 [3584/8000 (45%)]\tTotal Loss: 10.654292\n",
      "Reconstruction: 9.337208, Regularization: 1.263554, Discriminator: 0.026602; Generator: 0.026928,\n",
      "D(x): 0.742, D(G(z)): 0.422\n",
      "2019-04-10 00:55:52,853 root         INFO     Train Epoch: 17 [4096/8000 (51%)]\tTotal Loss: 9.644846\n",
      "Reconstruction: 8.516862, Regularization: 1.072852, Discriminator: 0.028167; Generator: 0.026965,\n",
      "D(x): 0.708, D(G(z)): 0.422\n",
      "2019-04-10 00:55:52,962 root         INFO     Train Epoch: 17 [4608/8000 (58%)]\tTotal Loss: 10.586317\n",
      "Reconstruction: 9.279225, Regularization: 1.253531, Discriminator: 0.026440; Generator: 0.027121,\n",
      "D(x): 0.743, D(G(z)): 0.420\n",
      "2019-04-10 00:55:53,071 root         INFO     Train Epoch: 17 [5120/8000 (64%)]\tTotal Loss: 11.253935\n",
      "Reconstruction: 9.771444, Regularization: 1.429328, Discriminator: 0.026020; Generator: 0.027143,\n",
      "D(x): 0.756, D(G(z)): 0.420\n",
      "2019-04-10 00:55:53,181 root         INFO     Train Epoch: 17 [5632/8000 (70%)]\tTotal Loss: 9.806041\n",
      "Reconstruction: 8.660788, Regularization: 1.090335, Discriminator: 0.027720; Generator: 0.027198,\n",
      "D(x): 0.715, D(G(z)): 0.419\n",
      "2019-04-10 00:55:53,289 root         INFO     Train Epoch: 17 [6144/8000 (77%)]\tTotal Loss: 10.283448\n",
      "Reconstruction: 9.077834, Regularization: 1.151415, Discriminator: 0.026954; Generator: 0.027244,\n",
      "D(x): 0.730, D(G(z)): 0.418\n",
      "2019-04-10 00:55:53,399 root         INFO     Train Epoch: 17 [6656/8000 (83%)]\tTotal Loss: 11.694304\n",
      "Reconstruction: 10.182734, Regularization: 1.459382, Discriminator: 0.024808; Generator: 0.027379,\n",
      "D(x): 0.778, D(G(z)): 0.416\n",
      "2019-04-10 00:55:53,508 root         INFO     Train Epoch: 17 [7168/8000 (90%)]\tTotal Loss: 10.282840\n",
      "Reconstruction: 9.114609, Regularization: 1.113820, Discriminator: 0.026980; Generator: 0.027430,\n",
      "D(x): 0.727, D(G(z)): 0.416\n",
      "2019-04-10 00:55:53,617 root         INFO     Train Epoch: 17 [7680/8000 (96%)]\tTotal Loss: 11.542135\n",
      "Reconstruction: 10.097668, Regularization: 1.391273, Discriminator: 0.025784; Generator: 0.027410,\n",
      "D(x): 0.755, D(G(z)): 0.416\n",
      "2019-04-10 00:55:53,697 root         INFO     ====> Epoch: 17 Average loss: 10.4807\n",
      "2019-04-10 00:55:53,725 root         INFO     Train Epoch: 18 [0/8000 (0%)]\tTotal Loss: 10.604265\n",
      "Reconstruction: 9.381599, Regularization: 1.169130, Discriminator: 0.026042; Generator: 0.027494,\n",
      "D(x): 0.746, D(G(z)): 0.415\n",
      "2019-04-10 00:55:53,835 root         INFO     Train Epoch: 18 [512/8000 (6%)]\tTotal Loss: 11.056128\n",
      "Reconstruction: 9.689533, Regularization: 1.312663, Discriminator: 0.026407; Generator: 0.027524,\n",
      "D(x): 0.743, D(G(z)): 0.414\n",
      "2019-04-10 00:55:53,946 root         INFO     Train Epoch: 18 [1024/8000 (13%)]\tTotal Loss: 10.437351\n",
      "Reconstruction: 9.173109, Regularization: 1.210870, Discriminator: 0.025697; Generator: 0.027674,\n",
      "D(x): 0.754, D(G(z)): 0.412\n",
      "2019-04-10 00:55:54,056 root         INFO     Train Epoch: 18 [1536/8000 (19%)]\tTotal Loss: 10.732108\n",
      "Reconstruction: 9.445237, Regularization: 1.233120, Discriminator: 0.026030; Generator: 0.027721,\n",
      "D(x): 0.748, D(G(z)): 0.412\n",
      "2019-04-10 00:55:54,166 root         INFO     Train Epoch: 18 [2048/8000 (26%)]\tTotal Loss: 10.422514\n",
      "Reconstruction: 9.209751, Regularization: 1.159260, Discriminator: 0.025666; Generator: 0.027837,\n",
      "D(x): 0.751, D(G(z)): 0.410\n",
      "2019-04-10 00:55:54,277 root         INFO     Train Epoch: 18 [2560/8000 (32%)]\tTotal Loss: 11.674793\n",
      "Reconstruction: 10.250784, Regularization: 1.371883, Discriminator: 0.024331; Generator: 0.027795,\n",
      "D(x): 0.784, D(G(z)): 0.411\n",
      "2019-04-10 00:55:54,388 root         INFO     Train Epoch: 18 [3072/8000 (38%)]\tTotal Loss: 11.185751\n",
      "Reconstruction: 9.800468, Regularization: 1.332329, Discriminator: 0.024994; Generator: 0.027960,\n",
      "D(x): 0.768, D(G(z)): 0.409\n",
      "2019-04-10 00:55:54,498 root         INFO     Train Epoch: 18 [3584/8000 (45%)]\tTotal Loss: 9.824634\n",
      "Reconstruction: 8.731030, Regularization: 1.039218, Discriminator: 0.026414; Generator: 0.027972,\n",
      "D(x): 0.732, D(G(z)): 0.409\n",
      "2019-04-10 00:55:54,609 root         INFO     Train Epoch: 18 [4096/8000 (51%)]\tTotal Loss: 10.352077\n",
      "Reconstruction: 9.156301, Regularization: 1.141562, Discriminator: 0.026162; Generator: 0.028052,\n",
      "D(x): 0.738, D(G(z)): 0.408\n",
      "2019-04-10 00:55:54,719 root         INFO     Train Epoch: 18 [4608/8000 (58%)]\tTotal Loss: 11.314969\n",
      "Reconstruction: 9.954151, Regularization: 1.308298, Discriminator: 0.024426; Generator: 0.028094,\n",
      "D(x): 0.778, D(G(z)): 0.407\n",
      "2019-04-10 00:55:54,829 root         INFO     Train Epoch: 18 [5120/8000 (64%)]\tTotal Loss: 10.744425\n",
      "Reconstruction: 9.503914, Regularization: 1.187225, Discriminator: 0.025110; Generator: 0.028176,\n",
      "D(x): 0.759, D(G(z)): 0.406\n",
      "2019-04-10 00:55:54,940 root         INFO     Train Epoch: 18 [5632/8000 (70%)]\tTotal Loss: 10.672112\n",
      "Reconstruction: 9.444732, Regularization: 1.174261, Discriminator: 0.024946; Generator: 0.028175,\n",
      "D(x): 0.764, D(G(z)): 0.406\n",
      "2019-04-10 00:55:55,050 root         INFO     Train Epoch: 18 [6144/8000 (77%)]\tTotal Loss: 10.289575\n",
      "Reconstruction: 9.103279, Regularization: 1.133524, Discriminator: 0.024540; Generator: 0.028232,\n",
      "D(x): 0.770, D(G(z)): 0.405\n",
      "2019-04-10 00:55:55,160 root         INFO     Train Epoch: 18 [6656/8000 (83%)]\tTotal Loss: 11.693369\n",
      "Reconstruction: 10.236857, Regularization: 1.404180, Discriminator: 0.024007; Generator: 0.028325,\n",
      "D(x): 0.787, D(G(z)): 0.404\n",
      "2019-04-10 00:55:55,270 root         INFO     Train Epoch: 18 [7168/8000 (90%)]\tTotal Loss: 10.512078\n",
      "Reconstruction: 9.266031, Regularization: 1.192520, Discriminator: 0.025157; Generator: 0.028371,\n",
      "D(x): 0.759, D(G(z)): 0.403\n",
      "2019-04-10 00:55:55,381 root         INFO     Train Epoch: 18 [7680/8000 (96%)]\tTotal Loss: 10.789911\n",
      "Reconstruction: 9.516027, Regularization: 1.221619, Discriminator: 0.023870; Generator: 0.028394,\n",
      "D(x): 0.786, D(G(z)): 0.403\n",
      "2019-04-10 00:55:55,461 root         INFO     ====> Epoch: 18 Average loss: 11.0432\n",
      "2019-04-10 00:55:55,488 root         INFO     Train Epoch: 19 [0/8000 (0%)]\tTotal Loss: 12.180853\n",
      "Reconstruction: 10.687173, Regularization: 1.442746, Discriminator: 0.022501; Generator: 0.028432,\n",
      "D(x): 0.819, D(G(z)): 0.403\n",
      "2019-04-10 00:55:55,596 root         INFO     Train Epoch: 19 [512/8000 (6%)]\tTotal Loss: 10.875748\n",
      "Reconstruction: 9.600191, Regularization: 1.223340, Discriminator: 0.023721; Generator: 0.028496,\n",
      "D(x): 0.787, D(G(z)): 0.402\n",
      "2019-04-10 00:55:55,705 root         INFO     Train Epoch: 19 [1024/8000 (13%)]\tTotal Loss: 12.166670\n",
      "Reconstruction: 10.722393, Regularization: 1.392959, Discriminator: 0.022787; Generator: 0.028532,\n",
      "D(x): 0.813, D(G(z)): 0.401\n",
      "2019-04-10 00:55:55,812 root         INFO     Train Epoch: 19 [1536/8000 (19%)]\tTotal Loss: 10.618987\n",
      "Reconstruction: 9.342374, Regularization: 1.222925, Discriminator: 0.025108; Generator: 0.028580,\n",
      "D(x): 0.756, D(G(z)): 0.401\n",
      "2019-04-10 00:55:55,920 root         INFO     Train Epoch: 19 [2048/8000 (26%)]\tTotal Loss: 10.741363\n",
      "Reconstruction: 9.495914, Regularization: 1.192575, Discriminator: 0.024230; Generator: 0.028642,\n",
      "D(x): 0.777, D(G(z)): 0.400\n",
      "2019-04-10 00:55:56,028 root         INFO     Train Epoch: 19 [2560/8000 (32%)]\tTotal Loss: 11.858562\n",
      "Reconstruction: 10.486462, Regularization: 1.319223, Discriminator: 0.024198; Generator: 0.028679,\n",
      "D(x): 0.782, D(G(z)): 0.399\n",
      "2019-04-10 00:55:56,137 root         INFO     Train Epoch: 19 [3072/8000 (38%)]\tTotal Loss: 10.605149\n",
      "Reconstruction: 9.393120, Regularization: 1.158920, Discriminator: 0.024381; Generator: 0.028729,\n",
      "D(x): 0.774, D(G(z)): 0.399\n",
      "2019-04-10 00:55:56,245 root         INFO     Train Epoch: 19 [3584/8000 (45%)]\tTotal Loss: 12.581161\n",
      "Reconstruction: 11.125726, Regularization: 1.404664, Discriminator: 0.021996; Generator: 0.028775,\n",
      "D(x): 0.825, D(G(z)): 0.398\n",
      "2019-04-10 00:55:56,352 root         INFO     Train Epoch: 19 [4096/8000 (51%)]\tTotal Loss: 12.146233\n",
      "Reconstruction: 10.717422, Regularization: 1.376789, Discriminator: 0.023198; Generator: 0.028823,\n",
      "D(x): 0.799, D(G(z)): 0.398\n",
      "2019-04-10 00:55:56,460 root         INFO     Train Epoch: 19 [4608/8000 (58%)]\tTotal Loss: 10.067615\n",
      "Reconstruction: 8.913520, Regularization: 1.101698, Discriminator: 0.023549; Generator: 0.028848,\n",
      "D(x): 0.788, D(G(z)): 0.397\n",
      "2019-04-10 00:55:56,568 root         INFO     Train Epoch: 19 [5120/8000 (64%)]\tTotal Loss: 11.441824\n",
      "Reconstruction: 10.121704, Regularization: 1.268789, Discriminator: 0.022433; Generator: 0.028898,\n",
      "D(x): 0.814, D(G(z)): 0.397\n",
      "2019-04-10 00:55:56,676 root         INFO     Train Epoch: 19 [5632/8000 (70%)]\tTotal Loss: 10.921810\n",
      "Reconstruction: 9.676899, Regularization: 1.192393, Discriminator: 0.023576; Generator: 0.028942,\n",
      "D(x): 0.786, D(G(z)): 0.396\n",
      "2019-04-10 00:55:56,781 root         INFO     Train Epoch: 19 [6144/8000 (77%)]\tTotal Loss: 13.165399\n",
      "Reconstruction: 11.609643, Regularization: 1.505993, Discriminator: 0.020779; Generator: 0.028984,\n",
      "D(x): 0.854, D(G(z)): 0.396\n",
      "2019-04-10 00:55:56,888 root         INFO     Train Epoch: 19 [6656/8000 (83%)]\tTotal Loss: 10.148805\n",
      "Reconstruction: 8.960351, Regularization: 1.136065, Discriminator: 0.023358; Generator: 0.029031,\n",
      "D(x): 0.790, D(G(z)): 0.395\n",
      "2019-04-10 00:55:56,994 root         INFO     Train Epoch: 19 [7168/8000 (90%)]\tTotal Loss: 9.365902\n",
      "Reconstruction: 8.277653, Regularization: 1.035373, Discriminator: 0.023812; Generator: 0.029064,\n",
      "D(x): 0.783, D(G(z)): 0.395\n",
      "2019-04-10 00:55:57,099 root         INFO     Train Epoch: 19 [7680/8000 (96%)]\tTotal Loss: 11.477691\n",
      "Reconstruction: 10.148589, Regularization: 1.277507, Discriminator: 0.022491; Generator: 0.029104,\n",
      "D(x): 0.812, D(G(z)): 0.394\n",
      "2019-04-10 00:55:57,177 root         INFO     ====> Epoch: 19 Average loss: 11.3414\n",
      "2019-04-10 00:55:57,204 root         INFO     Train Epoch: 20 [0/8000 (0%)]\tTotal Loss: 11.560186\n",
      "Reconstruction: 10.211643, Regularization: 1.297801, Discriminator: 0.021601; Generator: 0.029142,\n",
      "D(x): 0.831, D(G(z)): 0.394\n",
      "2019-04-10 00:55:57,313 root         INFO     Train Epoch: 20 [512/8000 (6%)]\tTotal Loss: 10.668427\n",
      "Reconstruction: 9.428722, Regularization: 1.188254, Discriminator: 0.022268; Generator: 0.029181,\n",
      "D(x): 0.815, D(G(z)): 0.393\n",
      "2019-04-10 00:55:57,421 root         INFO     Train Epoch: 20 [1024/8000 (13%)]\tTotal Loss: 10.259554\n",
      "Reconstruction: 9.074637, Regularization: 1.133271, Discriminator: 0.022430; Generator: 0.029215,\n",
      "D(x): 0.807, D(G(z)): 0.393\n",
      "2019-04-10 00:55:57,530 root         INFO     Train Epoch: 20 [1536/8000 (19%)]\tTotal Loss: 10.648081\n",
      "Reconstruction: 9.402507, Regularization: 1.193481, Discriminator: 0.022859; Generator: 0.029233,\n",
      "D(x): 0.800, D(G(z)): 0.392\n",
      "2019-04-10 00:55:57,638 root         INFO     Train Epoch: 20 [2048/8000 (26%)]\tTotal Loss: 10.131065\n",
      "Reconstruction: 8.932712, Regularization: 1.146682, Discriminator: 0.022396; Generator: 0.029276,\n",
      "D(x): 0.810, D(G(z)): 0.392\n",
      "2019-04-10 00:55:57,747 root         INFO     Train Epoch: 20 [2560/8000 (32%)]\tTotal Loss: 11.548193\n",
      "Reconstruction: 10.212652, Regularization: 1.284080, Discriminator: 0.022118; Generator: 0.029343,\n",
      "D(x): 0.819, D(G(z)): 0.391\n",
      "2019-04-10 00:55:57,856 root         INFO     Train Epoch: 20 [3072/8000 (38%)]\tTotal Loss: 10.487329\n",
      "Reconstruction: 9.276896, Regularization: 1.159206, Discriminator: 0.021859; Generator: 0.029367,\n",
      "D(x): 0.822, D(G(z)): 0.391\n",
      "2019-04-10 00:55:57,964 root         INFO     Train Epoch: 20 [3584/8000 (45%)]\tTotal Loss: 11.057918\n",
      "Reconstruction: 9.778081, Regularization: 1.228888, Discriminator: 0.021572; Generator: 0.029377,\n",
      "D(x): 0.830, D(G(z)): 0.391\n",
      "2019-04-10 00:55:58,072 root         INFO     Train Epoch: 20 [4096/8000 (51%)]\tTotal Loss: 11.483554\n",
      "Reconstruction: 10.152086, Regularization: 1.280641, Discriminator: 0.021444; Generator: 0.029382,\n",
      "D(x): 0.833, D(G(z)): 0.391\n",
      "2019-04-10 00:55:58,179 root         INFO     Train Epoch: 20 [4608/8000 (58%)]\tTotal Loss: 11.340602\n",
      "Reconstruction: 10.019173, Regularization: 1.271196, Discriminator: 0.020794; Generator: 0.029439,\n",
      "D(x): 0.847, D(G(z)): 0.390\n",
      "2019-04-10 00:55:58,287 root         INFO     Train Epoch: 20 [5120/8000 (64%)]\tTotal Loss: 10.698549\n",
      "Reconstruction: 9.444971, Regularization: 1.202736, Discriminator: 0.021445; Generator: 0.029398,\n",
      "D(x): 0.833, D(G(z)): 0.390\n",
      "2019-04-10 00:55:58,395 root         INFO     Train Epoch: 20 [5632/8000 (70%)]\tTotal Loss: 10.110891\n",
      "Reconstruction: 8.944741, Regularization: 1.114919, Discriminator: 0.021701; Generator: 0.029531,\n",
      "D(x): 0.826, D(G(z)): 0.389\n",
      "2019-04-10 00:55:58,503 root         INFO     Train Epoch: 20 [6144/8000 (77%)]\tTotal Loss: 11.772483\n",
      "Reconstruction: 10.427454, Regularization: 1.292706, Discriminator: 0.022754; Generator: 0.029568,\n",
      "D(x): 0.813, D(G(z)): 0.388\n",
      "2019-04-10 00:55:58,611 root         INFO     Train Epoch: 20 [6656/8000 (83%)]\tTotal Loss: 9.465110\n",
      "Reconstruction: 8.363209, Regularization: 1.050507, Discriminator: 0.021789; Generator: 0.029604,\n",
      "D(x): 0.818, D(G(z)): 0.388\n",
      "2019-04-10 00:55:58,719 root         INFO     Train Epoch: 20 [7168/8000 (90%)]\tTotal Loss: 10.164701\n",
      "Reconstruction: 8.987859, Regularization: 1.125588, Discriminator: 0.021609; Generator: 0.029645,\n",
      "D(x): 0.824, D(G(z)): 0.387\n",
      "2019-04-10 00:55:58,828 root         INFO     Train Epoch: 20 [7680/8000 (96%)]\tTotal Loss: 12.210551\n",
      "Reconstruction: 10.800715, Regularization: 1.360438, Discriminator: 0.019770; Generator: 0.029627,\n",
      "D(x): 0.871, D(G(z)): 0.387\n",
      "2019-04-10 00:55:58,908 root         INFO     ====> Epoch: 20 Average loss: 10.9650\n",
      "2019-04-10 00:55:58,935 root         INFO     Train Epoch: 21 [0/8000 (0%)]\tTotal Loss: 11.039198\n",
      "Reconstruction: 9.762689, Regularization: 1.225846, Discriminator: 0.021017; Generator: 0.029646,\n",
      "D(x): 0.842, D(G(z)): 0.387\n",
      "2019-04-10 00:55:59,045 root         INFO     Train Epoch: 21 [512/8000 (6%)]\tTotal Loss: 10.628945\n",
      "Reconstruction: 9.373828, Regularization: 1.204840, Discriminator: 0.020647; Generator: 0.029630,\n",
      "D(x): 0.852, D(G(z)): 0.387\n",
      "2019-04-10 00:55:59,156 root         INFO     Train Epoch: 21 [1024/8000 (13%)]\tTotal Loss: 9.651212\n",
      "Reconstruction: 8.539145, Regularization: 1.061258, Discriminator: 0.021076; Generator: 0.029734,\n",
      "D(x): 0.835, D(G(z)): 0.386\n",
      "2019-04-10 00:55:59,264 root         INFO     Train Epoch: 21 [1536/8000 (19%)]\tTotal Loss: 10.087270\n",
      "Reconstruction: 8.949573, Regularization: 1.086609, Discriminator: 0.021382; Generator: 0.029706,\n",
      "D(x): 0.830, D(G(z)): 0.387\n",
      "2019-04-10 00:55:59,371 root         INFO     Train Epoch: 21 [2048/8000 (26%)]\tTotal Loss: 11.002236\n",
      "Reconstruction: 9.741433, Regularization: 1.211283, Discriminator: 0.019761; Generator: 0.029758,\n",
      "D(x): 0.870, D(G(z)): 0.386\n",
      "2019-04-10 00:55:59,480 root         INFO     Train Epoch: 21 [2560/8000 (32%)]\tTotal Loss: 9.368333\n",
      "Reconstruction: 8.324582, Regularization: 0.991669, Discriminator: 0.022258; Generator: 0.029825,\n",
      "D(x): 0.808, D(G(z)): 0.385\n",
      "2019-04-10 00:55:59,588 root         INFO     Train Epoch: 21 [3072/8000 (38%)]\tTotal Loss: 10.760287\n",
      "Reconstruction: 9.554729, Regularization: 1.156044, Discriminator: 0.019744; Generator: 0.029770,\n",
      "D(x): 0.868, D(G(z)): 0.386\n",
      "2019-04-10 00:55:59,696 root         INFO     Train Epoch: 21 [3584/8000 (45%)]\tTotal Loss: 11.729835\n",
      "Reconstruction: 10.425013, Regularization: 1.255257, Discriminator: 0.019764; Generator: 0.029801,\n",
      "D(x): 0.869, D(G(z)): 0.385\n",
      "2019-04-10 00:55:59,804 root         INFO     Train Epoch: 21 [4096/8000 (51%)]\tTotal Loss: 10.276360\n",
      "Reconstruction: 9.117088, Regularization: 1.109019, Discriminator: 0.020480; Generator: 0.029773,\n",
      "D(x): 0.852, D(G(z)): 0.386\n",
      "2019-04-10 00:55:59,913 root         INFO     Train Epoch: 21 [4608/8000 (58%)]\tTotal Loss: 10.816037\n",
      "Reconstruction: 9.603963, Regularization: 1.162593, Discriminator: 0.019590; Generator: 0.029892,\n",
      "D(x): 0.872, D(G(z)): 0.384\n",
      "2019-04-10 00:56:00,021 root         INFO     Train Epoch: 21 [5120/8000 (64%)]\tTotal Loss: 10.787313\n",
      "Reconstruction: 9.588947, Regularization: 1.148630, Discriminator: 0.019860; Generator: 0.029876,\n",
      "D(x): 0.865, D(G(z)): 0.384\n",
      "2019-04-10 00:56:00,129 root         INFO     Train Epoch: 21 [5632/8000 (70%)]\tTotal Loss: 11.187887\n",
      "Reconstruction: 9.936769, Regularization: 1.201810, Discriminator: 0.019438; Generator: 0.029869,\n",
      "D(x): 0.876, D(G(z)): 0.385\n",
      "2019-04-10 00:56:00,237 root         INFO     Train Epoch: 21 [6144/8000 (77%)]\tTotal Loss: 12.789725\n",
      "Reconstruction: 11.370346, Regularization: 1.370795, Discriminator: 0.018686; Generator: 0.029898,\n",
      "D(x): 0.901, D(G(z)): 0.384\n",
      "2019-04-10 00:56:00,344 root         INFO     Train Epoch: 21 [6656/8000 (83%)]\tTotal Loss: 11.166059\n",
      "Reconstruction: 9.917593, Regularization: 1.199227, Discriminator: 0.019219; Generator: 0.030021,\n",
      "D(x): 0.879, D(G(z)): 0.383\n",
      "2019-04-10 00:56:00,450 root         INFO     Train Epoch: 21 [7168/8000 (90%)]\tTotal Loss: 9.062257\n",
      "Reconstruction: 8.006584, Regularization: 1.005921, Discriminator: 0.019751; Generator: 0.030001,\n",
      "D(x): 0.864, D(G(z)): 0.383\n",
      "2019-04-10 00:56:00,558 root         INFO     Train Epoch: 21 [7680/8000 (96%)]\tTotal Loss: 8.743512\n",
      "Reconstruction: 7.734617, Regularization: 0.958280, Discriminator: 0.020733; Generator: 0.029882,\n",
      "D(x): 0.844, D(G(z)): 0.384\n",
      "2019-04-10 00:56:00,636 root         INFO     ====> Epoch: 21 Average loss: 10.4526\n",
      "2019-04-10 00:56:00,663 root         INFO     Train Epoch: 22 [0/8000 (0%)]\tTotal Loss: 9.294789\n",
      "Reconstruction: 8.227951, Regularization: 1.016912, Discriminator: 0.019978; Generator: 0.029947,\n",
      "D(x): 0.864, D(G(z)): 0.384\n",
      "2019-04-10 00:56:00,773 root         INFO     Train Epoch: 22 [512/8000 (6%)]\tTotal Loss: 10.681782\n",
      "Reconstruction: 9.512769, Regularization: 1.119455, Discriminator: 0.019515; Generator: 0.030043,\n",
      "D(x): 0.874, D(G(z)): 0.382\n",
      "2019-04-10 00:56:00,881 root         INFO     Train Epoch: 22 [1024/8000 (13%)]\tTotal Loss: 8.792122\n",
      "Reconstruction: 7.788338, Regularization: 0.953473, Discriminator: 0.020191; Generator: 0.030120,\n",
      "D(x): 0.855, D(G(z)): 0.381\n",
      "2019-04-10 00:56:00,990 root         INFO     Train Epoch: 22 [1536/8000 (19%)]\tTotal Loss: 8.926098\n",
      "Reconstruction: 7.897985, Regularization: 0.978708, Discriminator: 0.019355; Generator: 0.030050,\n",
      "D(x): 0.875, D(G(z)): 0.382\n",
      "2019-04-10 00:56:01,098 root         INFO     Train Epoch: 22 [2048/8000 (26%)]\tTotal Loss: 8.425866\n",
      "Reconstruction: 7.459742, Regularization: 0.915525, Discriminator: 0.020559; Generator: 0.030040,\n",
      "D(x): 0.847, D(G(z)): 0.382\n",
      "2019-04-10 00:56:01,207 root         INFO     Train Epoch: 22 [2560/8000 (32%)]\tTotal Loss: 11.547269\n",
      "Reconstruction: 10.315489, Regularization: 1.182628, Discriminator: 0.019191; Generator: 0.029961,\n",
      "D(x): 0.884, D(G(z)): 0.383\n",
      "2019-04-10 00:56:01,316 root         INFO     Train Epoch: 22 [3072/8000 (38%)]\tTotal Loss: 11.315036\n",
      "Reconstruction: 10.087801, Regularization: 1.178407, Discriminator: 0.018732; Generator: 0.030095,\n",
      "D(x): 0.895, D(G(z)): 0.382\n",
      "2019-04-10 00:56:01,425 root         INFO     Train Epoch: 22 [3584/8000 (45%)]\tTotal Loss: 9.600751\n",
      "Reconstruction: 8.531606, Regularization: 1.019999, Discriminator: 0.019093; Generator: 0.030053,\n",
      "D(x): 0.884, D(G(z)): 0.382\n",
      "2019-04-10 00:56:01,533 root         INFO     Train Epoch: 22 [4096/8000 (51%)]\tTotal Loss: 9.650208\n",
      "Reconstruction: 8.570459, Regularization: 1.030469, Discriminator: 0.019095; Generator: 0.030184,\n",
      "D(x): 0.881, D(G(z)): 0.381\n",
      "2019-04-10 00:56:01,642 root         INFO     Train Epoch: 22 [4608/8000 (58%)]\tTotal Loss: 9.995842\n",
      "Reconstruction: 8.900813, Regularization: 1.046139, Discriminator: 0.018904; Generator: 0.029986,\n",
      "D(x): 0.890, D(G(z)): 0.383\n",
      "2019-04-10 00:56:01,751 root         INFO     Train Epoch: 22 [5120/8000 (64%)]\tTotal Loss: 8.185465\n",
      "Reconstruction: 7.260102, Regularization: 0.874358, Discriminator: 0.020963; Generator: 0.030042,\n",
      "D(x): 0.839, D(G(z)): 0.382\n",
      "2019-04-10 00:56:01,859 root         INFO     Train Epoch: 22 [5632/8000 (70%)]\tTotal Loss: 8.823509\n",
      "Reconstruction: 7.820905, Regularization: 0.953164, Discriminator: 0.019337; Generator: 0.030104,\n",
      "D(x): 0.876, D(G(z)): 0.382\n",
      "2019-04-10 00:56:01,968 root         INFO     Train Epoch: 22 [6144/8000 (77%)]\tTotal Loss: 8.200886\n",
      "Reconstruction: 7.251376, Regularization: 0.900087, Discriminator: 0.019337; Generator: 0.030086,\n",
      "D(x): 0.879, D(G(z)): 0.382\n",
      "2019-04-10 00:56:02,076 root         INFO     Train Epoch: 22 [6656/8000 (83%)]\tTotal Loss: 9.009712\n",
      "Reconstruction: 7.951147, Regularization: 1.010172, Discriminator: 0.018374; Generator: 0.030019,\n",
      "D(x): 0.904, D(G(z)): 0.383\n",
      "2019-04-10 00:56:02,185 root         INFO     Train Epoch: 22 [7168/8000 (90%)]\tTotal Loss: 7.887390\n",
      "Reconstruction: 6.967196, Regularization: 0.870208, Discriminator: 0.019831; Generator: 0.030154,\n",
      "D(x): 0.866, D(G(z)): 0.381\n",
      "2019-04-10 00:56:02,293 root         INFO     Train Epoch: 22 [7680/8000 (96%)]\tTotal Loss: 8.580185\n",
      "Reconstruction: 7.604731, Regularization: 0.926141, Discriminator: 0.019242; Generator: 0.030071,\n",
      "D(x): 0.879, D(G(z)): 0.382\n",
      "2019-04-10 00:56:02,374 root         INFO     ====> Epoch: 22 Average loss: 9.3724\n",
      "2019-04-10 00:56:02,401 root         INFO     Train Epoch: 23 [0/8000 (0%)]\tTotal Loss: 9.239624\n",
      "Reconstruction: 8.213153, Regularization: 0.977303, Discriminator: 0.019111; Generator: 0.030057,\n",
      "D(x): 0.886, D(G(z)): 0.382\n",
      "2019-04-10 00:56:02,511 root         INFO     Train Epoch: 23 [512/8000 (6%)]\tTotal Loss: 8.193470\n",
      "Reconstruction: 7.256813, Regularization: 0.887128, Discriminator: 0.019338; Generator: 0.030191,\n",
      "D(x): 0.875, D(G(z)): 0.381\n",
      "2019-04-10 00:56:02,622 root         INFO     Train Epoch: 23 [1024/8000 (13%)]\tTotal Loss: 9.227649\n",
      "Reconstruction: 8.205413, Regularization: 0.972253, Discriminator: 0.019931; Generator: 0.030052,\n",
      "D(x): 0.869, D(G(z)): 0.382\n",
      "2019-04-10 00:56:02,733 root         INFO     Train Epoch: 23 [1536/8000 (19%)]\tTotal Loss: 8.673560\n",
      "Reconstruction: 7.693672, Regularization: 0.931359, Discriminator: 0.018480; Generator: 0.030049,\n",
      "D(x): 0.900, D(G(z)): 0.382\n",
      "2019-04-10 00:56:02,844 root         INFO     Train Epoch: 23 [2048/8000 (26%)]\tTotal Loss: 7.517411\n",
      "Reconstruction: 6.626324, Regularization: 0.842075, Discriminator: 0.018819; Generator: 0.030194,\n",
      "D(x): 0.887, D(G(z)): 0.381\n",
      "2019-04-10 00:56:02,954 root         INFO     Train Epoch: 23 [2560/8000 (32%)]\tTotal Loss: 8.599797\n",
      "Reconstruction: 7.614170, Regularization: 0.936993, Discriminator: 0.018382; Generator: 0.030251,\n",
      "D(x): 0.900, D(G(z)): 0.380\n",
      "2019-04-10 00:56:03,065 root         INFO     Train Epoch: 23 [3072/8000 (38%)]\tTotal Loss: 8.104529\n",
      "Reconstruction: 7.166130, Regularization: 0.889298, Discriminator: 0.018895; Generator: 0.030206,\n",
      "D(x): 0.888, D(G(z)): 0.380\n",
      "2019-04-10 00:56:03,175 root         INFO     Train Epoch: 23 [3584/8000 (45%)]\tTotal Loss: 7.717506\n",
      "Reconstruction: 6.803133, Regularization: 0.865202, Discriminator: 0.019018; Generator: 0.030153,\n",
      "D(x): 0.888, D(G(z)): 0.381\n",
      "2019-04-10 00:56:03,286 root         INFO     Train Epoch: 23 [4096/8000 (51%)]\tTotal Loss: 7.479242\n",
      "Reconstruction: 6.586501, Regularization: 0.844319, Discriminator: 0.018208; Generator: 0.030215,\n",
      "D(x): 0.904, D(G(z)): 0.380\n",
      "2019-04-10 00:56:03,396 root         INFO     Train Epoch: 23 [4608/8000 (58%)]\tTotal Loss: 7.298406\n",
      "Reconstruction: 6.434642, Regularization: 0.815073, Discriminator: 0.018497; Generator: 0.030195,\n",
      "D(x): 0.897, D(G(z)): 0.381\n",
      "2019-04-10 00:56:03,507 root         INFO     Train Epoch: 23 [5120/8000 (64%)]\tTotal Loss: 7.327638\n",
      "Reconstruction: 6.482356, Regularization: 0.795345, Discriminator: 0.019625; Generator: 0.030312,\n",
      "D(x): 0.872, D(G(z)): 0.379\n",
      "2019-04-10 00:56:03,618 root         INFO     Train Epoch: 23 [5632/8000 (70%)]\tTotal Loss: 7.680209\n",
      "Reconstruction: 6.796609, Regularization: 0.834411, Discriminator: 0.018955; Generator: 0.030235,\n",
      "D(x): 0.887, D(G(z)): 0.380\n",
      "2019-04-10 00:56:03,727 root         INFO     Train Epoch: 23 [6144/8000 (77%)]\tTotal Loss: 7.432920\n",
      "Reconstruction: 6.569922, Regularization: 0.813895, Discriminator: 0.018790; Generator: 0.030312,\n",
      "D(x): 0.891, D(G(z)): 0.379\n",
      "2019-04-10 00:56:03,836 root         INFO     Train Epoch: 23 [6656/8000 (83%)]\tTotal Loss: 8.737126\n",
      "Reconstruction: 7.752552, Regularization: 0.936593, Discriminator: 0.017456; Generator: 0.030525,\n",
      "D(x): 0.920, D(G(z)): 0.377\n",
      "2019-04-10 00:56:03,945 root         INFO     Train Epoch: 23 [7168/8000 (90%)]\tTotal Loss: 7.617158\n",
      "Reconstruction: 6.742805, Regularization: 0.825963, Discriminator: 0.017958; Generator: 0.030433,\n",
      "D(x): 0.907, D(G(z)): 0.378\n",
      "2019-04-10 00:56:04,052 root         INFO     Train Epoch: 23 [7680/8000 (96%)]\tTotal Loss: 7.440086\n",
      "Reconstruction: 6.559703, Regularization: 0.832446, Discriminator: 0.017309; Generator: 0.030628,\n",
      "D(x): 0.922, D(G(z)): 0.375\n",
      "2019-04-10 00:56:04,132 root         INFO     ====> Epoch: 23 Average loss: 7.7767\n",
      "2019-04-10 00:56:04,159 root         INFO     Train Epoch: 24 [0/8000 (0%)]\tTotal Loss: 6.329896\n",
      "Reconstruction: 5.581537, Regularization: 0.698633, Discriminator: 0.019411; Generator: 0.030314,\n",
      "D(x): 0.873, D(G(z)): 0.379\n",
      "2019-04-10 00:56:04,271 root         INFO     Train Epoch: 24 [512/8000 (6%)]\tTotal Loss: 7.647749\n",
      "Reconstruction: 6.784037, Regularization: 0.815062, Discriminator: 0.018397; Generator: 0.030253,\n",
      "D(x): 0.902, D(G(z)): 0.380\n",
      "2019-04-10 00:56:04,382 root         INFO     Train Epoch: 24 [1024/8000 (13%)]\tTotal Loss: 7.414011\n",
      "Reconstruction: 6.562187, Regularization: 0.803343, Discriminator: 0.017935; Generator: 0.030546,\n",
      "D(x): 0.907, D(G(z)): 0.376\n",
      "2019-04-10 00:56:04,492 root         INFO     Train Epoch: 24 [1536/8000 (19%)]\tTotal Loss: 7.359939\n",
      "Reconstruction: 6.591247, Regularization: 0.718648, Discriminator: 0.019492; Generator: 0.030552,\n",
      "D(x): 0.870, D(G(z)): 0.376\n",
      "2019-04-10 00:56:04,602 root         INFO     Train Epoch: 24 [2048/8000 (26%)]\tTotal Loss: 7.461722\n",
      "Reconstruction: 6.601018, Regularization: 0.812727, Discriminator: 0.017325; Generator: 0.030652,\n",
      "D(x): 0.925, D(G(z)): 0.375\n",
      "2019-04-10 00:56:04,711 root         INFO     Train Epoch: 24 [2560/8000 (32%)]\tTotal Loss: 8.100662\n",
      "Reconstruction: 7.201382, Regularization: 0.850973, Discriminator: 0.017717; Generator: 0.030591,\n",
      "D(x): 0.914, D(G(z)): 0.376\n",
      "2019-04-10 00:56:04,820 root         INFO     Train Epoch: 24 [3072/8000 (38%)]\tTotal Loss: 6.524470\n",
      "Reconstruction: 5.747097, Regularization: 0.728567, Discriminator: 0.018128; Generator: 0.030679,\n",
      "D(x): 0.901, D(G(z)): 0.375\n",
      "2019-04-10 00:56:04,929 root         INFO     Train Epoch: 24 [3584/8000 (45%)]\tTotal Loss: 6.664235\n",
      "Reconstruction: 5.889407, Regularization: 0.725509, Discriminator: 0.018521; Generator: 0.030797,\n",
      "D(x): 0.890, D(G(z)): 0.373\n",
      "2019-04-10 00:56:05,039 root         INFO     Train Epoch: 24 [4096/8000 (51%)]\tTotal Loss: 7.548241\n",
      "Reconstruction: 6.701179, Regularization: 0.798959, Discriminator: 0.017409; Generator: 0.030695,\n",
      "D(x): 0.920, D(G(z)): 0.375\n",
      "2019-04-10 00:56:05,148 root         INFO     Train Epoch: 24 [4608/8000 (58%)]\tTotal Loss: 5.658801\n",
      "Reconstruction: 4.942408, Regularization: 0.667978, Discriminator: 0.017523; Generator: 0.030892,\n",
      "D(x): 0.911, D(G(z)): 0.372\n",
      "2019-04-10 00:56:05,258 root         INFO     Train Epoch: 24 [5120/8000 (64%)]\tTotal Loss: 5.204479\n",
      "Reconstruction: 4.521449, Regularization: 0.634709, Discriminator: 0.017350; Generator: 0.030971,\n",
      "D(x): 0.915, D(G(z)): 0.371\n",
      "2019-04-10 00:56:05,367 root         INFO     Train Epoch: 24 [5632/8000 (70%)]\tTotal Loss: 4.777027\n",
      "Reconstruction: 4.152865, Regularization: 0.573863, Discriminator: 0.019017; Generator: 0.031281,\n",
      "D(x): 0.873, D(G(z)): 0.368\n",
      "2019-04-10 00:56:05,476 root         INFO     Train Epoch: 24 [6144/8000 (77%)]\tTotal Loss: 6.091756\n",
      "Reconstruction: 5.331097, Regularization: 0.712370, Discriminator: 0.016989; Generator: 0.031300,\n",
      "D(x): 0.921, D(G(z)): 0.367\n",
      "2019-04-10 00:56:05,586 root         INFO     Train Epoch: 24 [6656/8000 (83%)]\tTotal Loss: 5.862623\n",
      "Reconstruction: 5.135287, Regularization: 0.678807, Discriminator: 0.017046; Generator: 0.031482,\n",
      "D(x): 0.916, D(G(z)): 0.365\n",
      "2019-04-10 00:56:05,694 root         INFO     Train Epoch: 24 [7168/8000 (90%)]\tTotal Loss: 5.668803\n",
      "Reconstruction: 4.950433, Regularization: 0.669635, Discriminator: 0.017514; Generator: 0.031221,\n",
      "D(x): 0.909, D(G(z)): 0.368\n",
      "2019-04-10 00:56:05,803 root         INFO     Train Epoch: 24 [7680/8000 (96%)]\tTotal Loss: 5.288312\n",
      "Reconstruction: 4.593190, Regularization: 0.646582, Discriminator: 0.017116; Generator: 0.031423,\n",
      "D(x): 0.914, D(G(z)): 0.366\n",
      "2019-04-10 00:56:05,882 root         INFO     ====> Epoch: 24 Average loss: 6.2961\n",
      "2019-04-10 00:56:05,910 root         INFO     Train Epoch: 25 [0/8000 (0%)]\tTotal Loss: 5.749629\n",
      "Reconstruction: 4.987687, Regularization: 0.714140, Discriminator: 0.016043; Generator: 0.031759,\n",
      "D(x): 0.939, D(G(z)): 0.362\n",
      "2019-04-10 00:56:06,022 root         INFO     Train Epoch: 25 [512/8000 (6%)]\tTotal Loss: 5.321002\n",
      "Reconstruction: 4.605362, Regularization: 0.666937, Discriminator: 0.016735; Generator: 0.031967,\n",
      "D(x): 0.917, D(G(z)): 0.360\n",
      "2019-04-10 00:56:06,132 root         INFO     Train Epoch: 25 [1024/8000 (13%)]\tTotal Loss: 4.690787\n",
      "Reconstruction: 4.047657, Regularization: 0.594453, Discriminator: 0.016728; Generator: 0.031949,\n",
      "D(x): 0.916, D(G(z)): 0.360\n",
      "2019-04-10 00:56:06,242 root         INFO     Train Epoch: 25 [1536/8000 (19%)]\tTotal Loss: 5.242977\n",
      "Reconstruction: 4.541203, Regularization: 0.653389, Discriminator: 0.016510; Generator: 0.031875,\n",
      "D(x): 0.925, D(G(z)): 0.361\n",
      "2019-04-10 00:56:06,352 root         INFO     Train Epoch: 25 [2048/8000 (26%)]\tTotal Loss: 5.292949\n",
      "Reconstruction: 4.583989, Regularization: 0.660638, Discriminator: 0.016277; Generator: 0.032045,\n",
      "D(x): 0.928, D(G(z)): 0.359\n",
      "2019-04-10 00:56:06,462 root         INFO     Train Epoch: 25 [2560/8000 (32%)]\tTotal Loss: 4.478037\n",
      "Reconstruction: 3.852045, Regularization: 0.576860, Discriminator: 0.016954; Generator: 0.032179,\n",
      "D(x): 0.908, D(G(z)): 0.357\n",
      "2019-04-10 00:56:06,572 root         INFO     Train Epoch: 25 [3072/8000 (38%)]\tTotal Loss: 4.869043\n",
      "Reconstruction: 4.212008, Regularization: 0.608109, Discriminator: 0.016359; Generator: 0.032566,\n",
      "D(x): 0.918, D(G(z)): 0.353\n",
      "2019-04-10 00:56:06,683 root         INFO     Train Epoch: 25 [3584/8000 (45%)]\tTotal Loss: 4.819259\n",
      "Reconstruction: 4.154475, Regularization: 0.615644, Discriminator: 0.016621; Generator: 0.032519,\n",
      "D(x): 0.916, D(G(z)): 0.353\n",
      "2019-04-10 00:56:06,793 root         INFO     Train Epoch: 25 [4096/8000 (51%)]\tTotal Loss: 4.874907\n",
      "Reconstruction: 4.219647, Regularization: 0.606243, Discriminator: 0.016164; Generator: 0.032853,\n",
      "D(x): 0.919, D(G(z)): 0.350\n",
      "2019-04-10 00:56:06,903 root         INFO     Train Epoch: 25 [4608/8000 (58%)]\tTotal Loss: 4.030258\n",
      "Reconstruction: 3.455048, Regularization: 0.524900, Discriminator: 0.017067; Generator: 0.033242,\n",
      "D(x): 0.895, D(G(z)): 0.345\n",
      "2019-04-10 00:56:07,013 root         INFO     Train Epoch: 25 [5120/8000 (64%)]\tTotal Loss: 3.491222\n",
      "Reconstruction: 2.952852, Regularization: 0.488240, Discriminator: 0.016932; Generator: 0.033199,\n",
      "D(x): 0.895, D(G(z)): 0.346\n",
      "2019-04-10 00:56:07,123 root         INFO     Train Epoch: 25 [5632/8000 (70%)]\tTotal Loss: 4.173939\n",
      "Reconstruction: 3.551652, Regularization: 0.573330, Discriminator: 0.015720; Generator: 0.033237,\n",
      "D(x): 0.926, D(G(z)): 0.345\n",
      "2019-04-10 00:56:07,233 root         INFO     Train Epoch: 25 [6144/8000 (77%)]\tTotal Loss: 5.260338\n",
      "Reconstruction: 4.523664, Regularization: 0.688256, Discriminator: 0.014984; Generator: 0.033434,\n",
      "D(x): 0.945, D(G(z)): 0.343\n",
      "2019-04-10 00:56:07,343 root         INFO     Train Epoch: 25 [6656/8000 (83%)]\tTotal Loss: 3.592643\n",
      "Reconstruction: 3.041076, Regularization: 0.501549, Discriminator: 0.016264; Generator: 0.033755,\n",
      "D(x): 0.903, D(G(z)): 0.340\n",
      "2019-04-10 00:56:07,453 root         INFO     Train Epoch: 25 [7168/8000 (90%)]\tTotal Loss: 3.956803\n",
      "Reconstruction: 3.378132, Regularization: 0.528839, Discriminator: 0.016002; Generator: 0.033830,\n",
      "D(x): 0.910, D(G(z)): 0.339\n",
      "2019-04-10 00:56:07,564 root         INFO     Train Epoch: 25 [7680/8000 (96%)]\tTotal Loss: 5.039092\n",
      "Reconstruction: 4.342199, Regularization: 0.647735, Discriminator: 0.015006; Generator: 0.034151,\n",
      "D(x): 0.933, D(G(z)): 0.335\n",
      "2019-04-10 00:56:07,644 root         INFO     ====> Epoch: 25 Average loss: 4.6085\n",
      "2019-04-10 00:56:07,671 root         INFO     Train Epoch: 26 [0/8000 (0%)]\tTotal Loss: 4.210301\n",
      "Reconstruction: 3.592725, Regularization: 0.568323, Discriminator: 0.014787; Generator: 0.034466,\n",
      "D(x): 0.934, D(G(z)): 0.332\n",
      "2019-04-10 00:56:07,780 root         INFO     Train Epoch: 26 [512/8000 (6%)]\tTotal Loss: 4.291739\n",
      "Reconstruction: 3.665479, Regularization: 0.576767, Discriminator: 0.014690; Generator: 0.034803,\n",
      "D(x): 0.932, D(G(z)): 0.328\n",
      "2019-04-10 00:56:07,890 root         INFO     Train Epoch: 26 [1024/8000 (13%)]\tTotal Loss: 3.865022\n",
      "Reconstruction: 3.283908, Regularization: 0.531460, Discriminator: 0.014780; Generator: 0.034875,\n",
      "D(x): 0.929, D(G(z)): 0.328\n",
      "2019-04-10 00:56:08,000 root         INFO     Train Epoch: 26 [1536/8000 (19%)]\tTotal Loss: 3.844629\n",
      "Reconstruction: 3.260774, Regularization: 0.534323, Discriminator: 0.014452; Generator: 0.035079,\n",
      "D(x): 0.935, D(G(z)): 0.326\n",
      "2019-04-10 00:56:08,111 root         INFO     Train Epoch: 26 [2048/8000 (26%)]\tTotal Loss: 3.899887\n",
      "Reconstruction: 3.318486, Regularization: 0.531342, Discriminator: 0.014647; Generator: 0.035412,\n",
      "D(x): 0.926, D(G(z)): 0.322\n",
      "2019-04-10 00:56:08,223 root         INFO     Train Epoch: 26 [2560/8000 (32%)]\tTotal Loss: 4.558446\n",
      "Reconstruction: 3.904676, Regularization: 0.604399, Discriminator: 0.013413; Generator: 0.035958,\n",
      "D(x): 0.953, D(G(z)): 0.316\n",
      "2019-04-10 00:56:08,335 root         INFO     Train Epoch: 26 [3072/8000 (38%)]\tTotal Loss: 3.816552\n",
      "Reconstruction: 3.239073, Regularization: 0.527223, Discriminator: 0.014221; Generator: 0.036036,\n",
      "D(x): 0.930, D(G(z)): 0.316\n",
      "2019-04-10 00:56:08,447 root         INFO     Train Epoch: 26 [3584/8000 (45%)]\tTotal Loss: 3.295818\n",
      "Reconstruction: 2.782086, Regularization: 0.462647, Discriminator: 0.014468; Generator: 0.036617,\n",
      "D(x): 0.916, D(G(z)): 0.310\n",
      "2019-04-10 00:56:08,559 root         INFO     Train Epoch: 26 [4096/8000 (51%)]\tTotal Loss: 3.548419\n",
      "Reconstruction: 2.993540, Regularization: 0.504143, Discriminator: 0.013885; Generator: 0.036851,\n",
      "D(x): 0.929, D(G(z)): 0.308\n",
      "2019-04-10 00:56:08,671 root         INFO     Train Epoch: 26 [4608/8000 (58%)]\tTotal Loss: 2.509184\n",
      "Reconstruction: 2.078663, Regularization: 0.378649, Discriminator: 0.014713; Generator: 0.037159,\n",
      "D(x): 0.901, D(G(z)): 0.305\n",
      "2019-04-10 00:56:08,781 root         INFO     Train Epoch: 26 [5120/8000 (64%)]\tTotal Loss: 3.780048\n",
      "Reconstruction: 3.208114, Regularization: 0.520347, Discriminator: 0.014399; Generator: 0.037189,\n",
      "D(x): 0.915, D(G(z)): 0.304\n",
      "2019-04-10 00:56:08,892 root         INFO     Train Epoch: 26 [5632/8000 (70%)]\tTotal Loss: 3.437830\n",
      "Reconstruction: 2.893259, Regularization: 0.493774, Discriminator: 0.012914; Generator: 0.037883,\n",
      "D(x): 0.943, D(G(z)): 0.298\n",
      "2019-04-10 00:56:09,003 root         INFO     Train Epoch: 26 [6144/8000 (77%)]\tTotal Loss: 2.644349\n",
      "Reconstruction: 2.195053, Regularization: 0.397487, Discriminator: 0.013671; Generator: 0.038138,\n",
      "D(x): 0.918, D(G(z)): 0.295\n",
      "2019-04-10 00:56:09,113 root         INFO     Train Epoch: 26 [6656/8000 (83%)]\tTotal Loss: 3.033315\n",
      "Reconstruction: 2.536933, Regularization: 0.445266, Discriminator: 0.012899; Generator: 0.038216,\n",
      "D(x): 0.939, D(G(z)): 0.294\n",
      "2019-04-10 00:56:09,224 root         INFO     Train Epoch: 26 [7168/8000 (90%)]\tTotal Loss: 2.920959\n",
      "Reconstruction: 2.438893, Regularization: 0.430392, Discriminator: 0.012803; Generator: 0.038871,\n",
      "D(x): 0.934, D(G(z)): 0.288\n",
      "2019-04-10 00:56:09,334 root         INFO     Train Epoch: 26 [7680/8000 (96%)]\tTotal Loss: 3.264723\n",
      "Reconstruction: 2.750608, Regularization: 0.461697, Discriminator: 0.013244; Generator: 0.039174,\n",
      "D(x): 0.921, D(G(z)): 0.286\n",
      "2019-04-10 00:56:09,416 root         INFO     ====> Epoch: 26 Average loss: 3.6187\n",
      "2019-04-10 00:56:09,443 root         INFO     Train Epoch: 27 [0/8000 (0%)]\tTotal Loss: 3.100604\n",
      "Reconstruction: 2.592962, Regularization: 0.455862, Discriminator: 0.012222; Generator: 0.039558,\n",
      "D(x): 0.943, D(G(z)): 0.282\n",
      "2019-04-10 00:56:09,554 root         INFO     Train Epoch: 27 [512/8000 (6%)]\tTotal Loss: 3.322973\n",
      "Reconstruction: 2.782205, Regularization: 0.489147, Discriminator: 0.011817; Generator: 0.039805,\n",
      "D(x): 0.952, D(G(z)): 0.280\n",
      "2019-04-10 00:56:09,663 root         INFO     Train Epoch: 27 [1024/8000 (13%)]\tTotal Loss: 2.843936\n",
      "Reconstruction: 2.380531, Regularization: 0.410168, Discriminator: 0.012990; Generator: 0.040247,\n",
      "D(x): 0.915, D(G(z)): 0.276\n",
      "2019-04-10 00:56:09,772 root         INFO     Train Epoch: 27 [1536/8000 (19%)]\tTotal Loss: 2.805399\n",
      "Reconstruction: 2.327420, Regularization: 0.425227, Discriminator: 0.012084; Generator: 0.040668,\n",
      "D(x): 0.935, D(G(z)): 0.272\n",
      "2019-04-10 00:56:09,881 root         INFO     Train Epoch: 27 [2048/8000 (26%)]\tTotal Loss: 3.105614\n",
      "Reconstruction: 2.597878, Regularization: 0.454936, Discriminator: 0.011799; Generator: 0.041001,\n",
      "D(x): 0.940, D(G(z)): 0.269\n",
      "2019-04-10 00:56:09,990 root         INFO     Train Epoch: 27 [2560/8000 (32%)]\tTotal Loss: 2.843093\n",
      "Reconstruction: 2.378098, Regularization: 0.411405, Discriminator: 0.012048; Generator: 0.041542,\n",
      "D(x): 0.928, D(G(z)): 0.265\n",
      "2019-04-10 00:56:10,099 root         INFO     Train Epoch: 27 [3072/8000 (38%)]\tTotal Loss: 2.398000\n",
      "Reconstruction: 1.988355, Regularization: 0.355223, Discriminator: 0.012500; Generator: 0.041921,\n",
      "D(x): 0.913, D(G(z)): 0.261\n",
      "2019-04-10 00:56:10,208 root         INFO     Train Epoch: 27 [3584/8000 (45%)]\tTotal Loss: 2.817948\n",
      "Reconstruction: 2.355793, Regularization: 0.408252, Discriminator: 0.011499; Generator: 0.042404,\n",
      "D(x): 0.935, D(G(z)): 0.257\n",
      "2019-04-10 00:56:10,318 root         INFO     Train Epoch: 27 [4096/8000 (51%)]\tTotal Loss: 2.587473\n",
      "Reconstruction: 2.153310, Regularization: 0.380107, Discriminator: 0.011158; Generator: 0.042899,\n",
      "D(x): 0.939, D(G(z)): 0.253\n",
      "2019-04-10 00:56:10,427 root         INFO     Train Epoch: 27 [4608/8000 (58%)]\tTotal Loss: 2.715865\n",
      "Reconstruction: 2.264473, Regularization: 0.397159, Discriminator: 0.010960; Generator: 0.043273,\n",
      "D(x): 0.941, D(G(z)): 0.250\n",
      "2019-04-10 00:56:10,536 root         INFO     Train Epoch: 27 [5120/8000 (64%)]\tTotal Loss: 3.102308\n",
      "Reconstruction: 2.585335, Regularization: 0.463079, Discriminator: 0.010147; Generator: 0.043748,\n",
      "D(x): 0.960, D(G(z)): 0.247\n",
      "2019-04-10 00:56:10,646 root         INFO     Train Epoch: 27 [5632/8000 (70%)]\tTotal Loss: 2.538739\n",
      "Reconstruction: 2.113915, Regularization: 0.369738, Discriminator: 0.010923; Generator: 0.044164,\n",
      "D(x): 0.934, D(G(z)): 0.243\n",
      "2019-04-10 00:56:10,755 root         INFO     Train Epoch: 27 [6144/8000 (77%)]\tTotal Loss: 2.264752\n",
      "Reconstruction: 1.870754, Regularization: 0.338756, Discriminator: 0.010518; Generator: 0.044725,\n",
      "D(x): 0.940, D(G(z)): 0.239\n",
      "2019-04-10 00:56:10,864 root         INFO     Train Epoch: 27 [6656/8000 (83%)]\tTotal Loss: 2.574342\n",
      "Reconstruction: 2.171240, Regularization: 0.346918, Discriminator: 0.011021; Generator: 0.045163,\n",
      "D(x): 0.923, D(G(z)): 0.236\n",
      "2019-04-10 00:56:10,973 root         INFO     Train Epoch: 27 [7168/8000 (90%)]\tTotal Loss: 2.834406\n",
      "Reconstruction: 2.394146, Regularization: 0.383662, Discriminator: 0.010924; Generator: 0.045673,\n",
      "D(x): 0.924, D(G(z)): 0.232\n",
      "2019-04-10 00:56:11,082 root         INFO     Train Epoch: 27 [7680/8000 (96%)]\tTotal Loss: 2.625340\n",
      "Reconstruction: 2.184280, Regularization: 0.385007, Discriminator: 0.009912; Generator: 0.046141,\n",
      "D(x): 0.945, D(G(z)): 0.228\n",
      "2019-04-10 00:56:11,162 root         INFO     ====> Epoch: 27 Average loss: 2.9178\n",
      "2019-04-10 00:56:11,190 root         INFO     Train Epoch: 28 [0/8000 (0%)]\tTotal Loss: 2.502206\n",
      "Reconstruction: 2.081628, Regularization: 0.364165, Discriminator: 0.009953; Generator: 0.046460,\n",
      "D(x): 0.942, D(G(z)): 0.226\n",
      "2019-04-10 00:56:11,303 root         INFO     Train Epoch: 28 [512/8000 (6%)]\tTotal Loss: 2.352833\n",
      "Reconstruction: 1.950633, Regularization: 0.345673, Discriminator: 0.009589; Generator: 0.046938,\n",
      "D(x): 0.947, D(G(z)): 0.223\n",
      "2019-04-10 00:56:11,411 root         INFO     Train Epoch: 28 [1024/8000 (13%)]\tTotal Loss: 2.740700\n",
      "Reconstruction: 2.289567, Regularization: 0.394336, Discriminator: 0.009397; Generator: 0.047400,\n",
      "D(x): 0.949, D(G(z)): 0.219\n",
      "2019-04-10 00:56:11,521 root         INFO     Train Epoch: 28 [1536/8000 (19%)]\tTotal Loss: 2.425025\n",
      "Reconstruction: 2.023672, Regularization: 0.344252, Discriminator: 0.009276; Generator: 0.047825,\n",
      "D(x): 0.949, D(G(z)): 0.216\n",
      "2019-04-10 00:56:11,629 root         INFO     Train Epoch: 28 [2048/8000 (26%)]\tTotal Loss: 2.686773\n",
      "Reconstruction: 2.246764, Regularization: 0.382695, Discriminator: 0.008948; Generator: 0.048366,\n",
      "D(x): 0.956, D(G(z)): 0.213\n",
      "2019-04-10 00:56:11,739 root         INFO     Train Epoch: 28 [2560/8000 (32%)]\tTotal Loss: 2.832167\n",
      "Reconstruction: 2.402711, Regularization: 0.371372, Discriminator: 0.009291; Generator: 0.048792,\n",
      "D(x): 0.942, D(G(z)): 0.210\n",
      "2019-04-10 00:56:11,848 root         INFO     Train Epoch: 28 [3072/8000 (38%)]\tTotal Loss: 2.415521\n",
      "Reconstruction: 2.019676, Regularization: 0.337789, Discriminator: 0.008787; Generator: 0.049269,\n",
      "D(x): 0.952, D(G(z)): 0.207\n",
      "2019-04-10 00:56:11,957 root         INFO     Train Epoch: 28 [3584/8000 (45%)]\tTotal Loss: 2.649925\n",
      "Reconstruction: 2.227485, Regularization: 0.364282, Discriminator: 0.008415; Generator: 0.049743,\n",
      "D(x): 0.960, D(G(z)): 0.204\n",
      "2019-04-10 00:56:12,066 root         INFO     Train Epoch: 28 [4096/8000 (51%)]\tTotal Loss: 2.880988\n",
      "Reconstruction: 2.434187, Regularization: 0.388528, Discriminator: 0.008083; Generator: 0.050190,\n",
      "D(x): 0.966, D(G(z)): 0.201\n",
      "2019-04-10 00:56:12,174 root         INFO     Train Epoch: 28 [4608/8000 (58%)]\tTotal Loss: 2.580286\n",
      "Reconstruction: 2.165709, Regularization: 0.355685, Discriminator: 0.008245; Generator: 0.050647,\n",
      "D(x): 0.958, D(G(z)): 0.198\n",
      "2019-04-10 00:56:12,285 root         INFO     Train Epoch: 28 [5120/8000 (64%)]\tTotal Loss: 2.395153\n",
      "Reconstruction: 1.999841, Regularization: 0.336076, Discriminator: 0.008142; Generator: 0.051094,\n",
      "D(x): 0.958, D(G(z)): 0.195\n",
      "2019-04-10 00:56:12,397 root         INFO     Train Epoch: 28 [5632/8000 (70%)]\tTotal Loss: 2.561085\n",
      "Reconstruction: 2.156969, Regularization: 0.344627, Discriminator: 0.007960; Generator: 0.051529,\n",
      "D(x): 0.961, D(G(z)): 0.192\n",
      "2019-04-10 00:56:12,509 root         INFO     Train Epoch: 28 [6144/8000 (77%)]\tTotal Loss: 2.684472\n",
      "Reconstruction: 2.269280, Regularization: 0.355335, Discriminator: 0.007895; Generator: 0.051963,\n",
      "D(x): 0.959, D(G(z)): 0.190\n",
      "2019-04-10 00:56:12,622 root         INFO     Train Epoch: 28 [6656/8000 (83%)]\tTotal Loss: 2.313429\n",
      "Reconstruction: 1.935473, Regularization: 0.317852, Discriminator: 0.007700; Generator: 0.052404,\n",
      "D(x): 0.962, D(G(z)): 0.187\n",
      "2019-04-10 00:56:12,734 root         INFO     Train Epoch: 28 [7168/8000 (90%)]\tTotal Loss: 2.144293\n",
      "Reconstruction: 1.787193, Regularization: 0.296183, Discriminator: 0.008110; Generator: 0.052808,\n",
      "D(x): 0.949, D(G(z)): 0.185\n",
      "2019-04-10 00:56:12,845 root         INFO     Train Epoch: 28 [7680/8000 (96%)]\tTotal Loss: 2.046083\n",
      "Reconstruction: 1.709019, Regularization: 0.275913, Discriminator: 0.007938; Generator: 0.053213,\n",
      "D(x): 0.949, D(G(z)): 0.182\n",
      "2019-04-10 00:56:12,926 root         INFO     ====> Epoch: 28 Average loss: 2.4997\n",
      "2019-04-10 00:56:12,953 root         INFO     Train Epoch: 29 [0/8000 (0%)]\tTotal Loss: 2.465785\n",
      "Reconstruction: 2.080096, Regularization: 0.324823, Discriminator: 0.007341; Generator: 0.053526,\n",
      "D(x): 0.965, D(G(z)): 0.180\n",
      "2019-04-10 00:56:13,065 root         INFO     Train Epoch: 29 [512/8000 (6%)]\tTotal Loss: 2.337616\n",
      "Reconstruction: 1.966472, Regularization: 0.309748, Discriminator: 0.007472; Generator: 0.053925,\n",
      "D(x): 0.959, D(G(z)): 0.178\n",
      "2019-04-10 00:56:13,177 root         INFO     Train Epoch: 29 [1024/8000 (13%)]\tTotal Loss: 2.191988\n",
      "Reconstruction: 1.842873, Regularization: 0.287372, Discriminator: 0.007457; Generator: 0.054285,\n",
      "D(x): 0.957, D(G(z)): 0.176\n",
      "2019-04-10 00:56:13,290 root         INFO     Train Epoch: 29 [1536/8000 (19%)]\tTotal Loss: 2.644645\n",
      "Reconstruction: 2.258922, Regularization: 0.323968, Discriminator: 0.007079; Generator: 0.054677,\n",
      "D(x): 0.966, D(G(z)): 0.174\n",
      "2019-04-10 00:56:13,403 root         INFO     Train Epoch: 29 [2048/8000 (26%)]\tTotal Loss: 2.447041\n",
      "Reconstruction: 2.070489, Regularization: 0.314521, Discriminator: 0.006928; Generator: 0.055103,\n",
      "D(x): 0.967, D(G(z)): 0.171\n",
      "2019-04-10 00:56:13,515 root         INFO     Train Epoch: 29 [2560/8000 (32%)]\tTotal Loss: 2.186018\n",
      "Reconstruction: 1.842193, Regularization: 0.281313, Discriminator: 0.007081; Generator: 0.055430,\n",
      "D(x): 0.961, D(G(z)): 0.170\n",
      "2019-04-10 00:56:13,625 root         INFO     Train Epoch: 29 [3072/8000 (38%)]\tTotal Loss: 2.054440\n",
      "Reconstruction: 1.725621, Regularization: 0.266035, Discriminator: 0.006974; Generator: 0.055810,\n",
      "D(x): 0.961, D(G(z)): 0.168\n",
      "2019-04-10 00:56:13,736 root         INFO     Train Epoch: 29 [3584/8000 (45%)]\tTotal Loss: 2.637277\n",
      "Reconstruction: 2.262500, Regularization: 0.311687, Discriminator: 0.006876; Generator: 0.056214,\n",
      "D(x): 0.963, D(G(z)): 0.165\n",
      "2019-04-10 00:56:13,847 root         INFO     Train Epoch: 29 [4096/8000 (51%)]\tTotal Loss: 1.813727\n",
      "Reconstruction: 1.517821, Regularization: 0.232403, Discriminator: 0.006973; Generator: 0.056530,\n",
      "D(x): 0.957, D(G(z)): 0.164\n",
      "2019-04-10 00:56:13,958 root         INFO     Train Epoch: 29 [4608/8000 (58%)]\tTotal Loss: 2.448982\n",
      "Reconstruction: 2.080447, Regularization: 0.305119, Discriminator: 0.006600; Generator: 0.056816,\n",
      "D(x): 0.967, D(G(z)): 0.162\n",
      "2019-04-10 00:56:14,070 root         INFO     Train Epoch: 29 [5120/8000 (64%)]\tTotal Loss: 2.308293\n",
      "Reconstruction: 1.964091, Regularization: 0.280428, Discriminator: 0.006540; Generator: 0.057234,\n",
      "D(x): 0.966, D(G(z)): 0.160\n",
      "2019-04-10 00:56:14,182 root         INFO     Train Epoch: 29 [5632/8000 (70%)]\tTotal Loss: 2.055128\n",
      "Reconstruction: 1.736296, Regularization: 0.254958, Discriminator: 0.006470; Generator: 0.057403,\n",
      "D(x): 0.967, D(G(z)): 0.159\n",
      "2019-04-10 00:56:14,294 root         INFO     Train Epoch: 29 [6144/8000 (77%)]\tTotal Loss: 2.007918\n",
      "Reconstruction: 1.696853, Regularization: 0.246901, Discriminator: 0.006334; Generator: 0.057830,\n",
      "D(x): 0.969, D(G(z)): 0.157\n",
      "2019-04-10 00:56:14,404 root         INFO     Train Epoch: 29 [6656/8000 (83%)]\tTotal Loss: 2.168753\n",
      "Reconstruction: 1.840358, Regularization: 0.263852, Discriminator: 0.006406; Generator: 0.058137,\n",
      "D(x): 0.966, D(G(z)): 0.156\n",
      "2019-04-10 00:56:14,516 root         INFO     Train Epoch: 29 [7168/8000 (90%)]\tTotal Loss: 2.273653\n",
      "Reconstruction: 1.952025, Regularization: 0.257006, Discriminator: 0.006215; Generator: 0.058407,\n",
      "D(x): 0.969, D(G(z)): 0.154\n",
      "2019-04-10 00:56:14,627 root         INFO     Train Epoch: 29 [7680/8000 (96%)]\tTotal Loss: 1.754753\n",
      "Reconstruction: 1.475954, Regularization: 0.213781, Discriminator: 0.006311; Generator: 0.058708,\n",
      "D(x): 0.965, D(G(z)): 0.153\n",
      "2019-04-10 00:56:14,708 root         INFO     ====> Epoch: 29 Average loss: 2.2968\n",
      "2019-04-10 00:56:14,736 root         INFO     Train Epoch: 30 [0/8000 (0%)]\tTotal Loss: 1.835581\n",
      "Reconstruction: 1.551393, Regularization: 0.218960, Discriminator: 0.006273; Generator: 0.058955,\n",
      "D(x): 0.965, D(G(z)): 0.152\n",
      "2019-04-10 00:56:14,849 root         INFO     Train Epoch: 30 [512/8000 (6%)]\tTotal Loss: 2.728952\n",
      "Reconstruction: 2.357100, Regularization: 0.306778, Discriminator: 0.005860; Generator: 0.059214,\n",
      "D(x): 0.976, D(G(z)): 0.150\n",
      "2019-04-10 00:56:14,961 root         INFO     Train Epoch: 30 [1024/8000 (13%)]\tTotal Loss: 2.547894\n",
      "Reconstruction: 2.198905, Regularization: 0.283754, Discriminator: 0.005896; Generator: 0.059339,\n",
      "D(x): 0.974, D(G(z)): 0.150\n",
      "2019-04-10 00:56:15,072 root         INFO     Train Epoch: 30 [1536/8000 (19%)]\tTotal Loss: 1.950237\n",
      "Reconstruction: 1.663899, Regularization: 0.220244, Discriminator: 0.006385; Generator: 0.059709,\n",
      "D(x): 0.958, D(G(z)): 0.148\n",
      "2019-04-10 00:56:15,185 root         INFO     Train Epoch: 30 [2048/8000 (26%)]\tTotal Loss: 2.670004\n",
      "Reconstruction: 2.323059, Regularization: 0.281283, Discriminator: 0.005773; Generator: 0.059890,\n",
      "D(x): 0.975, D(G(z)): 0.147\n",
      "2019-04-10 00:56:15,297 root         INFO     Train Epoch: 30 [2560/8000 (32%)]\tTotal Loss: 2.151678\n",
      "Reconstruction: 1.857363, Regularization: 0.228201, Discriminator: 0.005965; Generator: 0.060148,\n",
      "D(x): 0.968, D(G(z)): 0.146\n",
      "2019-04-10 00:56:15,409 root         INFO     Train Epoch: 30 [3072/8000 (38%)]\tTotal Loss: 2.496409\n",
      "Reconstruction: 2.171176, Regularization: 0.259242, Discriminator: 0.005666; Generator: 0.060325,\n",
      "D(x): 0.976, D(G(z)): 0.145\n",
      "2019-04-10 00:56:15,521 root         INFO     Train Epoch: 30 [3584/8000 (45%)]\tTotal Loss: 1.926851\n",
      "Reconstruction: 1.661097, Regularization: 0.199255, Discriminator: 0.005880; Generator: 0.060619,\n",
      "D(x): 0.968, D(G(z)): 0.144\n",
      "2019-04-10 00:56:15,632 root         INFO     Train Epoch: 30 [4096/8000 (51%)]\tTotal Loss: 2.224339\n",
      "Reconstruction: 1.933734, Regularization: 0.223985, Discriminator: 0.005746; Generator: 0.060874,\n",
      "D(x): 0.971, D(G(z)): 0.143\n",
      "2019-04-10 00:56:15,741 root         INFO     Train Epoch: 30 [4608/8000 (58%)]\tTotal Loss: 1.971005\n",
      "Reconstruction: 1.703097, Regularization: 0.201157, Discriminator: 0.005656; Generator: 0.061094,\n",
      "D(x): 0.972, D(G(z)): 0.142\n",
      "2019-04-10 00:56:15,851 root         INFO     Train Epoch: 30 [5120/8000 (64%)]\tTotal Loss: 2.242278\n",
      "Reconstruction: 1.958838, Regularization: 0.216673, Discriminator: 0.005652; Generator: 0.061114,\n",
      "D(x): 0.972, D(G(z)): 0.141\n",
      "2019-04-10 00:56:15,961 root         INFO     Train Epoch: 30 [5632/8000 (70%)]\tTotal Loss: 2.432997\n",
      "Reconstruction: 2.136236, Regularization: 0.229764, Discriminator: 0.005454; Generator: 0.061543,\n",
      "D(x): 0.976, D(G(z)): 0.140\n",
      "2019-04-10 00:56:16,070 root         INFO     Train Epoch: 30 [6144/8000 (77%)]\tTotal Loss: 2.711105\n",
      "Reconstruction: 2.393744, Regularization: 0.250292, Discriminator: 0.005443; Generator: 0.061626,\n",
      "D(x): 0.976, D(G(z)): 0.139\n",
      "2019-04-10 00:56:16,180 root         INFO     Train Epoch: 30 [6656/8000 (83%)]\tTotal Loss: 2.500954\n",
      "Reconstruction: 2.203903, Regularization: 0.229827, Discriminator: 0.005424; Generator: 0.061801,\n",
      "D(x): 0.976, D(G(z)): 0.138\n",
      "2019-04-10 00:56:16,289 root         INFO     Train Epoch: 30 [7168/8000 (90%)]\tTotal Loss: 2.420740\n",
      "Reconstruction: 2.137830, Regularization: 0.215329, Discriminator: 0.005572; Generator: 0.062009,\n",
      "D(x): 0.971, D(G(z)): 0.137\n",
      "2019-04-10 00:56:16,399 root         INFO     Train Epoch: 30 [7680/8000 (96%)]\tTotal Loss: 2.143611\n",
      "Reconstruction: 1.875862, Regularization: 0.200229, Discriminator: 0.005326; Generator: 0.062194,\n",
      "D(x): 0.977, D(G(z)): 0.137\n",
      "2019-04-10 00:56:16,480 root         INFO     ====> Epoch: 30 Average loss: 2.2611\n",
      "2019-04-10 00:56:16,507 root         INFO     Train Epoch: 31 [0/8000 (0%)]\tTotal Loss: 2.392722\n",
      "Reconstruction: 2.114627, Regularization: 0.210602, Discriminator: 0.005249; Generator: 0.062243,\n",
      "D(x): 0.979, D(G(z)): 0.136\n",
      "2019-04-10 00:56:16,619 root         INFO     Train Epoch: 31 [512/8000 (6%)]\tTotal Loss: 2.540454\n",
      "Reconstruction: 2.257755, Regularization: 0.214975, Discriminator: 0.005280; Generator: 0.062443,\n",
      "D(x): 0.977, D(G(z)): 0.136\n",
      "2019-04-10 00:56:16,730 root         INFO     Train Epoch: 31 [1024/8000 (13%)]\tTotal Loss: 2.125549\n",
      "Reconstruction: 1.875996, Regularization: 0.181746, Discriminator: 0.005323; Generator: 0.062484,\n",
      "D(x): 0.976, D(G(z)): 0.135\n",
      "2019-04-10 00:56:16,839 root         INFO     Train Epoch: 31 [1536/8000 (19%)]\tTotal Loss: 2.186754\n",
      "Reconstruction: 1.933317, Regularization: 0.185426, Discriminator: 0.005305; Generator: 0.062706,\n",
      "D(x): 0.975, D(G(z)): 0.134\n",
      "2019-04-10 00:56:16,947 root         INFO     Train Epoch: 31 [2048/8000 (26%)]\tTotal Loss: 2.820635\n",
      "Reconstruction: 2.524563, Regularization: 0.228122, Discriminator: 0.005117; Generator: 0.062833,\n",
      "D(x): 0.981, D(G(z)): 0.134\n",
      "2019-04-10 00:56:17,055 root         INFO     Train Epoch: 31 [2560/8000 (32%)]\tTotal Loss: 2.591163\n",
      "Reconstruction: 2.318839, Regularization: 0.204157, Discriminator: 0.005106; Generator: 0.063062,\n",
      "D(x): 0.980, D(G(z)): 0.133\n",
      "2019-04-10 00:56:17,164 root         INFO     Train Epoch: 31 [3072/8000 (38%)]\tTotal Loss: 2.201041\n",
      "Reconstruction: 1.957210, Regularization: 0.175453, Discriminator: 0.005197; Generator: 0.063182,\n",
      "D(x): 0.977, D(G(z)): 0.132\n",
      "2019-04-10 00:56:17,272 root         INFO     Train Epoch: 31 [3584/8000 (45%)]\tTotal Loss: 2.193452\n",
      "Reconstruction: 1.958787, Regularization: 0.166079, Discriminator: 0.005325; Generator: 0.063261,\n",
      "D(x): 0.972, D(G(z)): 0.132\n",
      "2019-04-10 00:56:17,379 root         INFO     Train Epoch: 31 [4096/8000 (51%)]\tTotal Loss: 2.904840\n",
      "Reconstruction: 2.625858, Regularization: 0.210674, Discriminator: 0.004962; Generator: 0.063345,\n",
      "D(x): 0.983, D(G(z)): 0.132\n",
      "2019-04-10 00:56:17,488 root         INFO     Train Epoch: 31 [4608/8000 (58%)]\tTotal Loss: 2.131787\n",
      "Reconstruction: 1.906307, Regularization: 0.156531, Discriminator: 0.005456; Generator: 0.063494,\n",
      "D(x): 0.967, D(G(z)): 0.131\n",
      "2019-04-10 00:56:17,596 root         INFO     Train Epoch: 31 [5120/8000 (64%)]\tTotal Loss: 2.530393\n",
      "Reconstruction: 2.283373, Regularization: 0.178592, Discriminator: 0.004922; Generator: 0.063505,\n",
      "D(x): 0.983, D(G(z)): 0.131\n",
      "2019-04-10 00:56:17,704 root         INFO     Train Epoch: 31 [5632/8000 (70%)]\tTotal Loss: 2.701067\n",
      "Reconstruction: 2.446305, Regularization: 0.186151, Discriminator: 0.004855; Generator: 0.063756,\n",
      "D(x): 0.984, D(G(z)): 0.130\n",
      "2019-04-10 00:56:17,813 root         INFO     Train Epoch: 31 [6144/8000 (77%)]\tTotal Loss: 2.455461\n",
      "Reconstruction: 2.219447, Regularization: 0.167330, Discriminator: 0.004828; Generator: 0.063856,\n",
      "D(x): 0.984, D(G(z)): 0.130\n",
      "2019-04-10 00:56:17,921 root         INFO     Train Epoch: 31 [6656/8000 (83%)]\tTotal Loss: 2.444478\n",
      "Reconstruction: 2.215961, Regularization: 0.159483, Discriminator: 0.005007; Generator: 0.064028,\n",
      "D(x): 0.978, D(G(z)): 0.129\n",
      "2019-04-10 00:56:18,029 root         INFO     Train Epoch: 31 [7168/8000 (90%)]\tTotal Loss: 2.998775\n",
      "Reconstruction: 2.745844, Regularization: 0.184022, Discriminator: 0.004770; Generator: 0.064139,\n",
      "D(x): 0.985, D(G(z)): 0.128\n",
      "2019-04-10 00:56:18,138 root         INFO     Train Epoch: 31 [7680/8000 (96%)]\tTotal Loss: 2.488254\n",
      "Reconstruction: 2.262953, Regularization: 0.156151, Discriminator: 0.004944; Generator: 0.064205,\n",
      "D(x): 0.979, D(G(z)): 0.128\n",
      "2019-04-10 00:56:18,217 root         INFO     ====> Epoch: 31 Average loss: 2.4346\n",
      "2019-04-10 00:56:18,245 root         INFO     Train Epoch: 32 [0/8000 (0%)]\tTotal Loss: 2.792152\n",
      "Reconstruction: 2.551364, Regularization: 0.171894, Discriminator: 0.004698; Generator: 0.064196,\n",
      "D(x): 0.987, D(G(z)): 0.128\n",
      "2019-04-10 00:56:18,357 root         INFO     Train Epoch: 32 [512/8000 (6%)]\tTotal Loss: 2.648772\n",
      "Reconstruction: 2.419719, Regularization: 0.160007, Discriminator: 0.004717; Generator: 0.064329,\n",
      "D(x): 0.986, D(G(z)): 0.128\n",
      "2019-04-10 00:56:18,468 root         INFO     Train Epoch: 32 [1024/8000 (13%)]\tTotal Loss: 2.767724\n",
      "Reconstruction: 2.534859, Regularization: 0.163698, Discriminator: 0.004688; Generator: 0.064478,\n",
      "D(x): 0.986, D(G(z)): 0.127\n",
      "2019-04-10 00:56:18,580 root         INFO     Train Epoch: 32 [1536/8000 (19%)]\tTotal Loss: 2.453698\n",
      "Reconstruction: 2.248670, Regularization: 0.135462, Discriminator: 0.004994; Generator: 0.064571,\n",
      "D(x): 0.976, D(G(z)): 0.127\n",
      "2019-04-10 00:56:18,692 root         INFO     Train Epoch: 32 [2048/8000 (26%)]\tTotal Loss: 2.838884\n",
      "Reconstruction: 2.611587, Regularization: 0.157946, Discriminator: 0.004752; Generator: 0.064600,\n",
      "D(x): 0.984, D(G(z)): 0.127\n",
      "2019-04-10 00:56:18,803 root         INFO     Train Epoch: 32 [2560/8000 (32%)]\tTotal Loss: 2.899166\n",
      "Reconstruction: 2.674941, Regularization: 0.154837, Discriminator: 0.004674; Generator: 0.064715,\n",
      "D(x): 0.985, D(G(z)): 0.126\n",
      "2019-04-10 00:56:18,914 root         INFO     Train Epoch: 32 [3072/8000 (38%)]\tTotal Loss: 2.549798\n",
      "Reconstruction: 2.344517, Regularization: 0.135753, Discriminator: 0.004703; Generator: 0.064825,\n",
      "D(x): 0.984, D(G(z)): 0.126\n",
      "2019-04-10 00:56:19,026 root         INFO     Train Epoch: 32 [3584/8000 (45%)]\tTotal Loss: 2.576360\n",
      "Reconstruction: 2.375168, Regularization: 0.131607, Discriminator: 0.004723; Generator: 0.064861,\n",
      "D(x): 0.983, D(G(z)): 0.125\n",
      "2019-04-10 00:56:19,137 root         INFO     Train Epoch: 32 [4096/8000 (51%)]\tTotal Loss: 2.798285\n",
      "Reconstruction: 2.590129, Regularization: 0.138489, Discriminator: 0.004619; Generator: 0.065048,\n",
      "D(x): 0.986, D(G(z)): 0.125\n",
      "2019-04-10 00:56:19,248 root         INFO     Train Epoch: 32 [4608/8000 (58%)]\tTotal Loss: 2.530038\n",
      "Reconstruction: 2.334209, Regularization: 0.126107, Discriminator: 0.004668; Generator: 0.065053,\n",
      "D(x): 0.984, D(G(z)): 0.125\n",
      "2019-04-10 00:56:19,359 root         INFO     Train Epoch: 32 [5120/8000 (64%)]\tTotal Loss: 2.953952\n",
      "Reconstruction: 2.742320, Regularization: 0.141838, Discriminator: 0.004658; Generator: 0.065137,\n",
      "D(x): 0.984, D(G(z)): 0.124\n",
      "2019-04-10 00:56:19,471 root         INFO     Train Epoch: 32 [5632/8000 (70%)]\tTotal Loss: 2.961066\n",
      "Reconstruction: 2.757078, Regularization: 0.134154, Discriminator: 0.004623; Generator: 0.065211,\n",
      "D(x): 0.985, D(G(z)): 0.124\n",
      "2019-04-10 00:56:19,583 root         INFO     Train Epoch: 32 [6144/8000 (77%)]\tTotal Loss: 2.678488\n",
      "Reconstruction: 2.489455, Regularization: 0.119070, Discriminator: 0.004680; Generator: 0.065284,\n",
      "D(x): 0.983, D(G(z)): 0.124\n",
      "2019-04-10 00:56:19,694 root         INFO     Train Epoch: 32 [6656/8000 (83%)]\tTotal Loss: 2.838346\n",
      "Reconstruction: 2.644054, Regularization: 0.124418, Discriminator: 0.004524; Generator: 0.065351,\n",
      "D(x): 0.987, D(G(z)): 0.124\n",
      "2019-04-10 00:56:19,805 root         INFO     Train Epoch: 32 [7168/8000 (90%)]\tTotal Loss: 2.824184\n",
      "Reconstruction: 2.634804, Regularization: 0.119309, Discriminator: 0.004611; Generator: 0.065460,\n",
      "D(x): 0.984, D(G(z)): 0.123\n",
      "2019-04-10 00:56:19,915 root         INFO     Train Epoch: 32 [7680/8000 (96%)]\tTotal Loss: 2.795926\n",
      "Reconstruction: 2.611173, Regularization: 0.114413, Discriminator: 0.004835; Generator: 0.065505,\n",
      "D(x): 0.977, D(G(z)): 0.123\n",
      "2019-04-10 00:56:19,996 root         INFO     ====> Epoch: 32 Average loss: 2.7330\n",
      "2019-04-10 00:56:20,023 root         INFO     Train Epoch: 33 [0/8000 (0%)]\tTotal Loss: 3.205499\n",
      "Reconstruction: 3.002140, Regularization: 0.133313, Discriminator: 0.004504; Generator: 0.065542,\n",
      "D(x): 0.987, D(G(z)): 0.123\n",
      "2019-04-10 00:56:20,136 root         INFO     Train Epoch: 33 [512/8000 (6%)]\tTotal Loss: 2.960148\n",
      "Reconstruction: 2.771486, Regularization: 0.118531, Discriminator: 0.004522; Generator: 0.065610,\n",
      "D(x): 0.986, D(G(z)): 0.123\n",
      "2019-04-10 00:56:20,246 root         INFO     Train Epoch: 33 [1024/8000 (13%)]\tTotal Loss: 2.811936\n",
      "Reconstruction: 2.630412, Regularization: 0.111392, Discriminator: 0.004451; Generator: 0.065682,\n",
      "D(x): 0.988, D(G(z)): 0.122\n",
      "2019-04-10 00:56:20,355 root         INFO     Train Epoch: 33 [1536/8000 (19%)]\tTotal Loss: 3.228702\n",
      "Reconstruction: 3.036225, Regularization: 0.122217, Discriminator: 0.004517; Generator: 0.065744,\n",
      "D(x): 0.986, D(G(z)): 0.122\n",
      "2019-04-10 00:56:20,464 root         INFO     Train Epoch: 33 [2048/8000 (26%)]\tTotal Loss: 2.821381\n",
      "Reconstruction: 2.645597, Regularization: 0.105443, Discriminator: 0.004535; Generator: 0.065805,\n",
      "D(x): 0.985, D(G(z)): 0.122\n",
      "2019-04-10 00:56:20,574 root         INFO     Train Epoch: 33 [2560/8000 (32%)]\tTotal Loss: 2.897346\n",
      "Reconstruction: 2.722065, Regularization: 0.104894, Discriminator: 0.004526; Generator: 0.065861,\n",
      "D(x): 0.985, D(G(z)): 0.122\n",
      "2019-04-10 00:56:20,683 root         INFO     Train Epoch: 33 [3072/8000 (38%)]\tTotal Loss: 3.203012\n",
      "Reconstruction: 3.020495, Regularization: 0.112174, Discriminator: 0.004419; Generator: 0.065924,\n",
      "D(x): 0.988, D(G(z)): 0.121\n",
      "2019-04-10 00:56:20,792 root         INFO     Train Epoch: 33 [3584/8000 (45%)]\tTotal Loss: 2.678667\n",
      "Reconstruction: 2.515478, Regularization: 0.092665, Discriminator: 0.004502; Generator: 0.066021,\n",
      "D(x): 0.985, D(G(z)): 0.121\n",
      "2019-04-10 00:56:20,902 root         INFO     Train Epoch: 33 [4096/8000 (51%)]\tTotal Loss: 3.107474\n",
      "Reconstruction: 2.933114, Regularization: 0.103924, Discriminator: 0.004388; Generator: 0.066049,\n",
      "D(x): 0.988, D(G(z)): 0.121\n",
      "2019-04-10 00:56:21,012 root         INFO     Train Epoch: 33 [4608/8000 (58%)]\tTotal Loss: 3.017105\n",
      "Reconstruction: 2.848242, Regularization: 0.098328, Discriminator: 0.004411; Generator: 0.066124,\n",
      "D(x): 0.987, D(G(z)): 0.121\n",
      "2019-04-10 00:56:21,121 root         INFO     Train Epoch: 33 [5120/8000 (64%)]\tTotal Loss: 2.911181\n",
      "Reconstruction: 2.748352, Regularization: 0.092262, Discriminator: 0.004357; Generator: 0.066211,\n",
      "D(x): 0.989, D(G(z)): 0.120\n",
      "2019-04-10 00:56:21,231 root         INFO     Train Epoch: 33 [5632/8000 (70%)]\tTotal Loss: 3.068334\n",
      "Reconstruction: 2.903758, Regularization: 0.093918, Discriminator: 0.004382; Generator: 0.066277,\n",
      "D(x): 0.988, D(G(z)): 0.120\n",
      "2019-04-10 00:56:21,340 root         INFO     Train Epoch: 33 [6144/8000 (77%)]\tTotal Loss: 2.957916\n",
      "Reconstruction: 2.798871, Regularization: 0.088327, Discriminator: 0.004419; Generator: 0.066298,\n",
      "D(x): 0.986, D(G(z)): 0.120\n",
      "2019-04-10 00:56:21,450 root         INFO     Train Epoch: 33 [6656/8000 (83%)]\tTotal Loss: 3.322559\n",
      "Reconstruction: 3.154899, Regularization: 0.097039, Discriminator: 0.004262; Generator: 0.066360,\n",
      "D(x): 0.991, D(G(z)): 0.120\n",
      "2019-04-10 00:56:21,560 root         INFO     Train Epoch: 33 [7168/8000 (90%)]\tTotal Loss: 3.499171\n",
      "Reconstruction: 3.328794, Regularization: 0.099593, Discriminator: 0.004293; Generator: 0.066490,\n",
      "D(x): 0.990, D(G(z)): 0.119\n",
      "2019-04-10 00:56:21,672 root         INFO     Train Epoch: 33 [7680/8000 (96%)]\tTotal Loss: 3.235152\n",
      "Reconstruction: 3.075706, Regularization: 0.088426, Discriminator: 0.004511; Generator: 0.066509,\n",
      "D(x): 0.983, D(G(z)): 0.119\n",
      "2019-04-10 00:56:21,752 root         INFO     ====> Epoch: 33 Average loss: 3.0534\n",
      "2019-04-10 00:56:21,780 root         INFO     Train Epoch: 34 [0/8000 (0%)]\tTotal Loss: 3.294729\n",
      "Reconstruction: 3.135675, Regularization: 0.088116, Discriminator: 0.004389; Generator: 0.066549,\n",
      "D(x): 0.986, D(G(z)): 0.119\n",
      "2019-04-10 00:56:21,892 root         INFO     Train Epoch: 34 [512/8000 (6%)]\tTotal Loss: 3.011081\n",
      "Reconstruction: 2.861572, Regularization: 0.078596, Discriminator: 0.004281; Generator: 0.066632,\n",
      "D(x): 0.989, D(G(z)): 0.119\n",
      "2019-04-10 00:56:22,003 root         INFO     Train Epoch: 34 [1024/8000 (13%)]\tTotal Loss: 3.378971\n",
      "Reconstruction: 3.223147, Regularization: 0.084711, Discriminator: 0.004489; Generator: 0.066623,\n",
      "D(x): 0.983, D(G(z)): 0.119\n",
      "2019-04-10 00:56:22,115 root         INFO     Train Epoch: 34 [1536/8000 (19%)]\tTotal Loss: 3.738968\n",
      "Reconstruction: 3.575540, Regularization: 0.092548, Discriminator: 0.004241; Generator: 0.066638,\n",
      "D(x): 0.991, D(G(z)): 0.119\n",
      "2019-04-10 00:56:22,226 root         INFO     Train Epoch: 34 [2048/8000 (26%)]\tTotal Loss: 3.614149\n",
      "Reconstruction: 3.456186, Regularization: 0.087069, Discriminator: 0.004203; Generator: 0.066691,\n",
      "D(x): 0.992, D(G(z)): 0.118\n",
      "2019-04-10 00:56:22,338 root         INFO     Train Epoch: 34 [2560/8000 (32%)]\tTotal Loss: 3.434153\n",
      "Reconstruction: 3.282618, Regularization: 0.080460, Discriminator: 0.004243; Generator: 0.066832,\n",
      "D(x): 0.990, D(G(z)): 0.118\n",
      "2019-04-10 00:56:22,449 root         INFO     Train Epoch: 34 [3072/8000 (38%)]\tTotal Loss: 3.513176\n",
      "Reconstruction: 3.360419, Regularization: 0.081407, Discriminator: 0.004262; Generator: 0.067088,\n",
      "D(x): 0.988, D(G(z)): 0.117\n",
      "2019-04-10 00:56:22,561 root         INFO     Train Epoch: 34 [3584/8000 (45%)]\tTotal Loss: 3.529074\n",
      "Reconstruction: 3.379756, Regularization: 0.078093, Discriminator: 0.004206; Generator: 0.067019,\n",
      "D(x): 0.990, D(G(z)): 0.117\n",
      "2019-04-10 00:56:22,674 root         INFO     Train Epoch: 34 [4096/8000 (51%)]\tTotal Loss: 3.494310\n",
      "Reconstruction: 3.347564, Regularization: 0.075333, Discriminator: 0.004187; Generator: 0.067226,\n",
      "D(x): 0.990, D(G(z)): 0.116\n",
      "2019-04-10 00:56:22,786 root         INFO     Train Epoch: 34 [4608/8000 (58%)]\tTotal Loss: 3.545555\n",
      "Reconstruction: 3.399385, Regularization: 0.074876, Discriminator: 0.004251; Generator: 0.067043,\n",
      "D(x): 0.989, D(G(z)): 0.117\n",
      "2019-04-10 00:56:22,899 root         INFO     Train Epoch: 34 [5120/8000 (64%)]\tTotal Loss: 3.698925\n",
      "Reconstruction: 3.549374, Regularization: 0.077937, Discriminator: 0.004092; Generator: 0.067523,\n",
      "D(x): 0.992, D(G(z)): 0.115\n",
      "2019-04-10 00:56:23,013 root         INFO     Train Epoch: 34 [5632/8000 (70%)]\tTotal Loss: 3.906603\n",
      "Reconstruction: 3.756829, Regularization: 0.078239, Discriminator: 0.004000; Generator: 0.067536,\n",
      "D(x): 0.994, D(G(z)): 0.115\n",
      "2019-04-10 00:56:23,125 root         INFO     Train Epoch: 34 [6144/8000 (77%)]\tTotal Loss: 3.521121\n",
      "Reconstruction: 3.381539, Regularization: 0.068056, Discriminator: 0.004128; Generator: 0.067398,\n",
      "D(x): 0.991, D(G(z)): 0.116\n",
      "2019-04-10 00:56:23,238 root         INFO     Train Epoch: 34 [6656/8000 (83%)]\tTotal Loss: 3.158023\n",
      "Reconstruction: 3.026502, Regularization: 0.059652, Discriminator: 0.004323; Generator: 0.067546,\n",
      "D(x): 0.984, D(G(z)): 0.115\n",
      "2019-04-10 00:56:23,351 root         INFO     Train Epoch: 34 [7168/8000 (90%)]\tTotal Loss: 3.673711\n",
      "Reconstruction: 3.533919, Regularization: 0.068090, Discriminator: 0.004110; Generator: 0.067591,\n",
      "D(x): 0.991, D(G(z)): 0.115\n",
      "2019-04-10 00:56:23,464 root         INFO     Train Epoch: 34 [7680/8000 (96%)]\tTotal Loss: 3.682148\n",
      "Reconstruction: 3.543849, Regularization: 0.066374, Discriminator: 0.004131; Generator: 0.067794,\n",
      "D(x): 0.989, D(G(z)): 0.114\n",
      "2019-04-10 00:56:23,546 root         INFO     ====> Epoch: 34 Average loss: 3.4861\n",
      "2019-04-10 00:56:23,573 root         INFO     Train Epoch: 35 [0/8000 (0%)]\tTotal Loss: 3.599991\n",
      "Reconstruction: 3.464339, Regularization: 0.063654, Discriminator: 0.004073; Generator: 0.067925,\n",
      "D(x): 0.991, D(G(z)): 0.114\n",
      "2019-04-10 00:56:23,684 root         INFO     Train Epoch: 35 [512/8000 (6%)]\tTotal Loss: 3.843556\n",
      "Reconstruction: 3.705126, Regularization: 0.066403, Discriminator: 0.004041; Generator: 0.067987,\n",
      "D(x): 0.991, D(G(z)): 0.114\n",
      "2019-04-10 00:56:23,795 root         INFO     Train Epoch: 35 [1024/8000 (13%)]\tTotal Loss: 3.719793\n",
      "Reconstruction: 3.586107, Regularization: 0.061430, Discriminator: 0.004162; Generator: 0.068094,\n",
      "D(x): 0.987, D(G(z)): 0.113\n",
      "2019-04-10 00:56:23,906 root         INFO     Train Epoch: 35 [1536/8000 (19%)]\tTotal Loss: 3.689172\n",
      "Reconstruction: 3.557006, Regularization: 0.059882, Discriminator: 0.004021; Generator: 0.068262,\n",
      "D(x): 0.991, D(G(z)): 0.113\n",
      "2019-04-10 00:56:24,017 root         INFO     Train Epoch: 35 [2048/8000 (26%)]\tTotal Loss: 3.565825\n",
      "Reconstruction: 3.438222, Regularization: 0.055309, Discriminator: 0.004008; Generator: 0.068285,\n",
      "D(x): 0.991, D(G(z)): 0.113\n",
      "2019-04-10 00:56:24,128 root         INFO     Train Epoch: 35 [2560/8000 (32%)]\tTotal Loss: 3.800227\n",
      "Reconstruction: 3.669602, Regularization: 0.058289, Discriminator: 0.003920; Generator: 0.068416,\n",
      "D(x): 0.993, D(G(z)): 0.112\n",
      "2019-04-10 00:56:24,239 root         INFO     Train Epoch: 35 [3072/8000 (38%)]\tTotal Loss: 3.738332\n",
      "Reconstruction: 3.610219, Regularization: 0.055598, Discriminator: 0.003966; Generator: 0.068549,\n",
      "D(x): 0.991, D(G(z)): 0.112\n",
      "2019-04-10 00:56:24,348 root         INFO     Train Epoch: 35 [3584/8000 (45%)]\tTotal Loss: 3.975499\n",
      "Reconstruction: 3.843486, Regularization: 0.059085, Discriminator: 0.004008; Generator: 0.068920,\n",
      "D(x): 0.989, D(G(z)): 0.110\n",
      "2019-04-10 00:56:24,459 root         INFO     Train Epoch: 35 [4096/8000 (51%)]\tTotal Loss: 4.226278\n",
      "Reconstruction: 4.092432, Regularization: 0.060896, Discriminator: 0.003906; Generator: 0.069043,\n",
      "D(x): 0.991, D(G(z)): 0.110\n",
      "2019-04-10 00:56:24,569 root         INFO     Train Epoch: 35 [4608/8000 (58%)]\tTotal Loss: 3.966596\n",
      "Reconstruction: 3.838678, Regularization: 0.055253, Discriminator: 0.004091; Generator: 0.068574,\n",
      "D(x): 0.988, D(G(z)): 0.111\n",
      "2019-04-10 00:56:24,680 root         INFO     Train Epoch: 35 [5120/8000 (64%)]\tTotal Loss: 4.091503\n",
      "Reconstruction: 3.961632, Regularization: 0.056730, Discriminator: 0.003938; Generator: 0.069204,\n",
      "D(x): 0.990, D(G(z)): 0.109\n",
      "2019-04-10 00:56:24,792 root         INFO     Train Epoch: 35 [5632/8000 (70%)]\tTotal Loss: 4.436427\n",
      "Reconstruction: 4.302605, Regularization: 0.060757, Discriminator: 0.003788; Generator: 0.069276,\n",
      "D(x): 0.994, D(G(z)): 0.109\n",
      "2019-04-10 00:56:24,902 root         INFO     Train Epoch: 35 [6144/8000 (77%)]\tTotal Loss: 4.007117\n",
      "Reconstruction: 3.881991, Regularization: 0.051371, Discriminator: 0.003812; Generator: 0.069943,\n",
      "D(x): 0.991, D(G(z)): 0.107\n",
      "2019-04-10 00:56:25,012 root         INFO     Train Epoch: 35 [6656/8000 (83%)]\tTotal Loss: 3.994648\n",
      "Reconstruction: 3.870392, Regularization: 0.050888, Discriminator: 0.003775; Generator: 0.069593,\n",
      "D(x): 0.993, D(G(z)): 0.108\n",
      "2019-04-10 00:56:25,123 root         INFO     Train Epoch: 35 [7168/8000 (90%)]\tTotal Loss: 4.463316\n",
      "Reconstruction: 4.334076, Regularization: 0.055424, Discriminator: 0.004029; Generator: 0.069788,\n",
      "D(x): 0.986, D(G(z)): 0.107\n",
      "2019-04-10 00:56:25,234 root         INFO     Train Epoch: 35 [7680/8000 (96%)]\tTotal Loss: 4.276431\n",
      "Reconstruction: 4.150446, Regularization: 0.051995, Discriminator: 0.003709; Generator: 0.070281,\n",
      "D(x): 0.993, D(G(z)): 0.106\n",
      "2019-04-10 00:56:25,314 root         INFO     ====> Epoch: 35 Average loss: 3.9751\n",
      "2019-04-10 00:56:25,341 root         INFO     Train Epoch: 36 [0/8000 (0%)]\tTotal Loss: 4.911242\n",
      "Reconstruction: 4.778729, Regularization: 0.058595, Discriminator: 0.003646; Generator: 0.070272,\n",
      "D(x): 0.995, D(G(z)): 0.106\n",
      "2019-04-10 00:56:25,454 root         INFO     Train Epoch: 36 [512/8000 (6%)]\tTotal Loss: 4.017025\n",
      "Reconstruction: 3.897411, Regularization: 0.045503, Discriminator: 0.003714; Generator: 0.070397,\n",
      "D(x): 0.992, D(G(z)): 0.105\n",
      "2019-04-10 00:56:25,567 root         INFO     Train Epoch: 36 [1024/8000 (13%)]\tTotal Loss: 4.334722\n",
      "Reconstruction: 4.211667, Regularization: 0.048819, Discriminator: 0.003604; Generator: 0.070632,\n",
      "D(x): 0.995, D(G(z)): 0.104\n",
      "2019-04-10 00:56:25,680 root         INFO     Train Epoch: 36 [1536/8000 (19%)]\tTotal Loss: 4.106432\n",
      "Reconstruction: 3.986166, Regularization: 0.045639, Discriminator: 0.003706; Generator: 0.070921,\n",
      "D(x): 0.991, D(G(z)): 0.103\n",
      "2019-04-10 00:56:25,792 root         INFO     Train Epoch: 36 [2048/8000 (26%)]\tTotal Loss: 4.320260\n",
      "Reconstruction: 4.198354, Regularization: 0.047096, Discriminator: 0.003771; Generator: 0.071038,\n",
      "D(x): 0.988, D(G(z)): 0.103\n",
      "2019-04-10 00:56:25,905 root         INFO     Train Epoch: 36 [2560/8000 (32%)]\tTotal Loss: 5.155642\n",
      "Reconstruction: 5.023866, Regularization: 0.056832, Discriminator: 0.003485; Generator: 0.071459,\n",
      "D(x): 0.996, D(G(z)): 0.102\n",
      "2019-04-10 00:56:26,017 root         INFO     Train Epoch: 36 [3072/8000 (38%)]\tTotal Loss: 4.217706\n",
      "Reconstruction: 4.098378, Regularization: 0.044064, Discriminator: 0.003507; Generator: 0.071758,\n",
      "D(x): 0.994, D(G(z)): 0.101\n",
      "2019-04-10 00:56:26,129 root         INFO     Train Epoch: 36 [3584/8000 (45%)]\tTotal Loss: 4.259050\n",
      "Reconstruction: 4.140065, Regularization: 0.043190, Discriminator: 0.003873; Generator: 0.071922,\n",
      "D(x): 0.983, D(G(z)): 0.100\n",
      "2019-04-10 00:56:26,241 root         INFO     Train Epoch: 36 [4096/8000 (51%)]\tTotal Loss: 4.439920\n",
      "Reconstruction: 4.320960, Regularization: 0.043181, Discriminator: 0.003455; Generator: 0.072324,\n",
      "D(x): 0.994, D(G(z)): 0.099\n",
      "2019-04-10 00:56:26,354 root         INFO     Train Epoch: 36 [4608/8000 (58%)]\tTotal Loss: 3.980131\n",
      "Reconstruction: 3.865700, Regularization: 0.038336, Discriminator: 0.003481; Generator: 0.072614,\n",
      "D(x): 0.992, D(G(z)): 0.098\n",
      "2019-04-10 00:56:26,466 root         INFO     Train Epoch: 36 [5120/8000 (64%)]\tTotal Loss: 4.437674\n",
      "Reconstruction: 4.319245, Regularization: 0.042358, Discriminator: 0.003396; Generator: 0.072675,\n",
      "D(x): 0.994, D(G(z)): 0.098\n",
      "2019-04-10 00:56:26,578 root         INFO     Train Epoch: 36 [5632/8000 (70%)]\tTotal Loss: 4.401498\n",
      "Reconstruction: 4.283877, Regularization: 0.041241, Discriminator: 0.003432; Generator: 0.072947,\n",
      "D(x): 0.992, D(G(z)): 0.097\n",
      "2019-04-10 00:56:26,691 root         INFO     Train Epoch: 36 [6144/8000 (77%)]\tTotal Loss: 4.681550\n",
      "Reconstruction: 4.561536, Regularization: 0.043293, Discriminator: 0.003279; Generator: 0.073442,\n",
      "D(x): 0.995, D(G(z)): 0.095\n",
      "2019-04-10 00:56:26,804 root         INFO     Train Epoch: 36 [6656/8000 (83%)]\tTotal Loss: 5.370732\n",
      "Reconstruction: 5.245928, Regularization: 0.048162, Discriminator: 0.003235; Generator: 0.073407,\n",
      "D(x): 0.997, D(G(z)): 0.096\n",
      "2019-04-10 00:56:26,916 root         INFO     Train Epoch: 36 [7168/8000 (90%)]\tTotal Loss: 4.762434\n",
      "Reconstruction: 4.643367, Regularization: 0.042376, Discriminator: 0.003324; Generator: 0.073367,\n",
      "D(x): 0.994, D(G(z)): 0.096\n",
      "2019-04-10 00:56:27,029 root         INFO     Train Epoch: 36 [7680/8000 (96%)]\tTotal Loss: 4.649828\n",
      "Reconstruction: 4.532249, Regularization: 0.040131, Discriminator: 0.003290; Generator: 0.074158,\n",
      "D(x): 0.993, D(G(z)): 0.093\n",
      "2019-04-10 00:56:27,110 root         INFO     ====> Epoch: 36 Average loss: 4.4395\n",
      "2019-04-10 00:56:27,138 root         INFO     Train Epoch: 37 [0/8000 (0%)]\tTotal Loss: 3.720942\n",
      "Reconstruction: 3.611744, Regularization: 0.031410, Discriminator: 0.003344; Generator: 0.074444,\n",
      "D(x): 0.990, D(G(z)): 0.092\n",
      "2019-04-10 00:56:27,251 root         INFO     Train Epoch: 37 [512/8000 (6%)]\tTotal Loss: 4.451229\n",
      "Reconstruction: 4.336581, Regularization: 0.036922, Discriminator: 0.003204; Generator: 0.074521,\n",
      "D(x): 0.994, D(G(z)): 0.092\n",
      "2019-04-10 00:56:27,363 root         INFO     Train Epoch: 37 [1024/8000 (13%)]\tTotal Loss: 4.242425\n",
      "Reconstruction: 4.129397, Regularization: 0.034930, Discriminator: 0.003086; Generator: 0.075013,\n",
      "D(x): 0.996, D(G(z)): 0.091\n",
      "2019-04-10 00:56:27,475 root         INFO     Train Epoch: 37 [1536/8000 (19%)]\tTotal Loss: 4.827755\n",
      "Reconstruction: 4.709893, Regularization: 0.039252, Discriminator: 0.003124; Generator: 0.075486,\n",
      "D(x): 0.994, D(G(z)): 0.089\n",
      "2019-04-10 00:56:27,587 root         INFO     Train Epoch: 37 [2048/8000 (26%)]\tTotal Loss: 4.956317\n",
      "Reconstruction: 4.837963, Regularization: 0.039424, Discriminator: 0.003136; Generator: 0.075794,\n",
      "D(x): 0.993, D(G(z)): 0.089\n",
      "2019-04-10 00:56:27,700 root         INFO     Train Epoch: 37 [2560/8000 (32%)]\tTotal Loss: 4.025681\n",
      "Reconstruction: 3.914060, Regularization: 0.032597, Discriminator: 0.003049; Generator: 0.075976,\n",
      "D(x): 0.995, D(G(z)): 0.088\n",
      "2019-04-10 00:56:27,812 root         INFO     Train Epoch: 37 [3072/8000 (38%)]\tTotal Loss: 4.616326\n",
      "Reconstruction: 4.501181, Regularization: 0.035511, Discriminator: 0.003032; Generator: 0.076603,\n",
      "D(x): 0.993, D(G(z)): 0.086\n",
      "2019-04-10 00:56:27,924 root         INFO     Train Epoch: 37 [3584/8000 (45%)]\tTotal Loss: 4.586791\n",
      "Reconstruction: 4.472778, Regularization: 0.034380, Discriminator: 0.002993; Generator: 0.076640,\n",
      "D(x): 0.994, D(G(z)): 0.086\n",
      "2019-04-10 00:56:28,035 root         INFO     Train Epoch: 37 [4096/8000 (51%)]\tTotal Loss: 4.396820\n",
      "Reconstruction: 4.283899, Regularization: 0.032948, Discriminator: 0.002905; Generator: 0.077068,\n",
      "D(x): 0.996, D(G(z)): 0.085\n",
      "2019-04-10 00:56:28,145 root         INFO     Train Epoch: 37 [4608/8000 (58%)]\tTotal Loss: 4.159216\n",
      "Reconstruction: 4.048575, Regularization: 0.030741, Discriminator: 0.002943; Generator: 0.076956,\n",
      "D(x): 0.995, D(G(z)): 0.085\n",
      "2019-04-10 00:56:28,254 root         INFO     Train Epoch: 37 [5120/8000 (64%)]\tTotal Loss: 5.111768\n",
      "Reconstruction: 4.992937, Regularization: 0.038056, Discriminator: 0.002884; Generator: 0.077891,\n",
      "D(x): 0.994, D(G(z)): 0.083\n",
      "2019-04-10 00:56:28,365 root         INFO     Train Epoch: 37 [5632/8000 (70%)]\tTotal Loss: 4.302647\n",
      "Reconstruction: 4.190930, Regularization: 0.030932, Discriminator: 0.002904; Generator: 0.077879,\n",
      "D(x): 0.994, D(G(z)): 0.083\n",
      "2019-04-10 00:56:28,476 root         INFO     Train Epoch: 37 [6144/8000 (77%)]\tTotal Loss: 5.093184\n",
      "Reconstruction: 4.974904, Regularization: 0.037098, Discriminator: 0.002833; Generator: 0.078349,\n",
      "D(x): 0.995, D(G(z)): 0.082\n",
      "2019-04-10 00:56:28,586 root         INFO     Train Epoch: 37 [6656/8000 (83%)]\tTotal Loss: 4.093338\n",
      "Reconstruction: 3.981705, Regularization: 0.029793, Discriminator: 0.002839; Generator: 0.079001,\n",
      "D(x): 0.993, D(G(z)): 0.080\n",
      "2019-04-10 00:56:28,696 root         INFO     Train Epoch: 37 [7168/8000 (90%)]\tTotal Loss: 4.137822\n",
      "Reconstruction: 4.028064, Regularization: 0.027996, Discriminator: 0.002856; Generator: 0.078906,\n",
      "D(x): 0.992, D(G(z)): 0.080\n",
      "2019-04-10 00:56:28,807 root         INFO     Train Epoch: 37 [7680/8000 (96%)]\tTotal Loss: 3.929260\n",
      "Reconstruction: 3.819889, Regularization: 0.027269, Discriminator: 0.002673; Generator: 0.079430,\n",
      "D(x): 0.997, D(G(z)): 0.079\n",
      "2019-04-10 00:56:28,888 root         INFO     ====> Epoch: 37 Average loss: 4.5517\n",
      "2019-04-10 00:56:28,916 root         INFO     Train Epoch: 38 [0/8000 (0%)]\tTotal Loss: 4.966405\n",
      "Reconstruction: 4.848826, Regularization: 0.034974, Discriminator: 0.002619; Generator: 0.079986,\n",
      "D(x): 0.997, D(G(z)): 0.077\n",
      "2019-04-10 00:56:29,027 root         INFO     Train Epoch: 38 [512/8000 (6%)]\tTotal Loss: 4.492909\n",
      "Reconstruction: 4.380859, Regularization: 0.029306, Discriminator: 0.002671; Generator: 0.080073,\n",
      "D(x): 0.995, D(G(z)): 0.077\n",
      "2019-04-10 00:56:29,142 root         INFO     Train Epoch: 38 [1024/8000 (13%)]\tTotal Loss: 4.664603\n",
      "Reconstruction: 4.550421, Regularization: 0.030797, Discriminator: 0.002528; Generator: 0.080857,\n",
      "D(x): 0.997, D(G(z)): 0.075\n",
      "2019-04-10 00:56:29,254 root         INFO     Train Epoch: 38 [1536/8000 (19%)]\tTotal Loss: 4.536564\n",
      "Reconstruction: 4.423392, Regularization: 0.029637, Discriminator: 0.002536; Generator: 0.081000,\n",
      "D(x): 0.997, D(G(z)): 0.075\n",
      "2019-04-10 00:56:29,366 root         INFO     Train Epoch: 38 [2048/8000 (26%)]\tTotal Loss: 4.215173\n",
      "Reconstruction: 4.104082, Regularization: 0.027082, Discriminator: 0.002590; Generator: 0.081420,\n",
      "D(x): 0.994, D(G(z)): 0.074\n",
      "2019-04-10 00:56:29,477 root         INFO     Train Epoch: 38 [2560/8000 (32%)]\tTotal Loss: 4.034816\n",
      "Reconstruction: 3.924881, Regularization: 0.025736, Discriminator: 0.002453; Generator: 0.081746,\n",
      "D(x): 0.997, D(G(z)): 0.073\n",
      "2019-04-10 00:56:29,588 root         INFO     Train Epoch: 38 [3072/8000 (38%)]\tTotal Loss: 4.545074\n",
      "Reconstruction: 4.431458, Regularization: 0.028786, Discriminator: 0.002690; Generator: 0.082139,\n",
      "D(x): 0.990, D(G(z)): 0.072\n",
      "2019-04-10 00:56:29,699 root         INFO     Train Epoch: 38 [3584/8000 (45%)]\tTotal Loss: 3.973087\n",
      "Reconstruction: 3.862837, Regularization: 0.024706, Discriminator: 0.002506; Generator: 0.083038,\n",
      "D(x): 0.993, D(G(z)): 0.070\n",
      "2019-04-10 00:56:29,809 root         INFO     Train Epoch: 38 [4096/8000 (51%)]\tTotal Loss: 3.801920\n",
      "Reconstruction: 3.693624, Regularization: 0.023146, Discriminator: 0.002502; Generator: 0.082649,\n",
      "D(x): 0.994, D(G(z)): 0.071\n",
      "2019-04-10 00:56:29,921 root         INFO     Train Epoch: 38 [4608/8000 (58%)]\tTotal Loss: 3.756813\n",
      "Reconstruction: 3.647027, Regularization: 0.024210, Discriminator: 0.002390; Generator: 0.083186,\n",
      "D(x): 0.996, D(G(z)): 0.070\n",
      "2019-04-10 00:56:30,033 root         INFO     Train Epoch: 38 [5120/8000 (64%)]\tTotal Loss: 3.846335\n",
      "Reconstruction: 3.736960, Regularization: 0.023568, Discriminator: 0.002361; Generator: 0.083446,\n",
      "D(x): 0.996, D(G(z)): 0.069\n",
      "2019-04-10 00:56:30,145 root         INFO     Train Epoch: 38 [5632/8000 (70%)]\tTotal Loss: 3.665213\n",
      "Reconstruction: 3.556579, Regularization: 0.022699, Discriminator: 0.002360; Generator: 0.083576,\n",
      "D(x): 0.996, D(G(z)): 0.069\n",
      "2019-04-10 00:56:30,257 root         INFO     Train Epoch: 38 [6144/8000 (77%)]\tTotal Loss: 3.936462\n",
      "Reconstruction: 3.826458, Regularization: 0.023280, Discriminator: 0.002575; Generator: 0.084150,\n",
      "D(x): 0.989, D(G(z)): 0.068\n",
      "2019-04-10 00:56:30,368 root         INFO     Train Epoch: 38 [6656/8000 (83%)]\tTotal Loss: 3.743938\n",
      "Reconstruction: 3.634661, Regularization: 0.022762, Discriminator: 0.002357; Generator: 0.084158,\n",
      "D(x): 0.995, D(G(z)): 0.068\n",
      "2019-04-10 00:56:30,478 root         INFO     Train Epoch: 38 [7168/8000 (90%)]\tTotal Loss: 3.559790\n",
      "Reconstruction: 3.450478, Regularization: 0.021993, Discriminator: 0.002319; Generator: 0.085000,\n",
      "D(x): 0.994, D(G(z)): 0.066\n",
      "2019-04-10 00:56:30,587 root         INFO     Train Epoch: 38 [7680/8000 (96%)]\tTotal Loss: 3.448253\n",
      "Reconstruction: 3.340861, Regularization: 0.019974, Discriminator: 0.002313; Generator: 0.085105,\n",
      "D(x): 0.994, D(G(z)): 0.066\n",
      "2019-04-10 00:56:30,667 root         INFO     ====> Epoch: 38 Average loss: 4.0522\n",
      "2019-04-10 00:56:30,694 root         INFO     Train Epoch: 39 [0/8000 (0%)]\tTotal Loss: 4.215939\n",
      "Reconstruction: 4.104291, Regularization: 0.023868, Discriminator: 0.002190; Generator: 0.085591,\n",
      "D(x): 0.997, D(G(z)): 0.065\n",
      "2019-04-10 00:56:30,804 root         INFO     Train Epoch: 39 [512/8000 (6%)]\tTotal Loss: 3.415266\n",
      "Reconstruction: 3.307868, Regularization: 0.019175, Discriminator: 0.002294; Generator: 0.085929,\n",
      "D(x): 0.993, D(G(z)): 0.064\n",
      "2019-04-10 00:56:30,913 root         INFO     Train Epoch: 39 [1024/8000 (13%)]\tTotal Loss: 3.611300\n",
      "Reconstruction: 3.502709, Regularization: 0.020375, Discriminator: 0.002178; Generator: 0.086038,\n",
      "D(x): 0.996, D(G(z)): 0.064\n",
      "2019-04-10 00:56:31,021 root         INFO     Train Epoch: 39 [1536/8000 (19%)]\tTotal Loss: 3.465332\n",
      "Reconstruction: 3.358378, Regularization: 0.018860, Discriminator: 0.002245; Generator: 0.085848,\n",
      "D(x): 0.995, D(G(z)): 0.064\n",
      "2019-04-10 00:56:31,130 root         INFO     Train Epoch: 39 [2048/8000 (26%)]\tTotal Loss: 3.668875\n",
      "Reconstruction: 3.561421, Regularization: 0.018917, Discriminator: 0.002111; Generator: 0.086426,\n",
      "D(x): 0.998, D(G(z)): 0.063\n",
      "2019-04-10 00:56:31,238 root         INFO     Train Epoch: 39 [2560/8000 (32%)]\tTotal Loss: 3.032233\n",
      "Reconstruction: 2.927607, Regularization: 0.015465, Discriminator: 0.002065; Generator: 0.087095,\n",
      "D(x): 0.998, D(G(z)): 0.062\n",
      "2019-04-10 00:56:31,347 root         INFO     Train Epoch: 39 [3072/8000 (38%)]\tTotal Loss: 2.820203\n",
      "Reconstruction: 2.716316, Regularization: 0.014635, Discriminator: 0.002196; Generator: 0.087056,\n",
      "D(x): 0.994, D(G(z)): 0.062\n",
      "2019-04-10 00:56:31,456 root         INFO     Train Epoch: 39 [3584/8000 (45%)]\tTotal Loss: 3.134867\n",
      "Reconstruction: 3.030359, Regularization: 0.015431, Discriminator: 0.002116; Generator: 0.086961,\n",
      "D(x): 0.996, D(G(z)): 0.062\n",
      "2019-04-10 00:56:31,565 root         INFO     Train Epoch: 39 [4096/8000 (51%)]\tTotal Loss: 3.175772\n",
      "Reconstruction: 3.070987, Regularization: 0.015448, Discriminator: 0.002170; Generator: 0.087168,\n",
      "D(x): 0.994, D(G(z)): 0.061\n",
      "2019-04-10 00:56:31,673 root         INFO     Train Epoch: 39 [4608/8000 (58%)]\tTotal Loss: 2.911788\n",
      "Reconstruction: 2.808071, Regularization: 0.014444, Discriminator: 0.002121; Generator: 0.087152,\n",
      "D(x): 0.996, D(G(z)): 0.062\n",
      "2019-04-10 00:56:31,782 root         INFO     Train Epoch: 39 [5120/8000 (64%)]\tTotal Loss: 2.719664\n",
      "Reconstruction: 2.617498, Regularization: 0.012380, Discriminator: 0.002038; Generator: 0.087748,\n",
      "D(x): 0.997, D(G(z)): 0.060\n",
      "2019-04-10 00:56:31,890 root         INFO     Train Epoch: 39 [5632/8000 (70%)]\tTotal Loss: 3.241239\n",
      "Reconstruction: 3.137402, Regularization: 0.014063, Discriminator: 0.001984; Generator: 0.087790,\n",
      "D(x): 0.999, D(G(z)): 0.060\n",
      "2019-04-10 00:56:31,999 root         INFO     Train Epoch: 39 [6144/8000 (77%)]\tTotal Loss: 2.473568\n",
      "Reconstruction: 2.371145, Regularization: 0.012003, Discriminator: 0.002094; Generator: 0.088327,\n",
      "D(x): 0.994, D(G(z)): 0.059\n",
      "2019-04-10 00:56:32,108 root         INFO     Train Epoch: 39 [6656/8000 (83%)]\tTotal Loss: 2.647160\n",
      "Reconstruction: 2.545838, Regularization: 0.010614, Discriminator: 0.002176; Generator: 0.088532,\n",
      "D(x): 0.992, D(G(z)): 0.059\n",
      "2019-04-10 00:56:32,216 root         INFO     Train Epoch: 39 [7168/8000 (90%)]\tTotal Loss: 2.696903\n",
      "Reconstruction: 2.594892, Regularization: 0.010994, Discriminator: 0.001984; Generator: 0.089033,\n",
      "D(x): 0.996, D(G(z)): 0.058\n",
      "2019-04-10 00:56:32,324 root         INFO     Train Epoch: 39 [7680/8000 (96%)]\tTotal Loss: 2.680940\n",
      "Reconstruction: 2.579534, Regularization: 0.010485, Discriminator: 0.001974; Generator: 0.088946,\n",
      "D(x): 0.997, D(G(z)): 0.058\n",
      "2019-04-10 00:56:32,403 root         INFO     ====> Epoch: 39 Average loss: 3.0386\n",
      "2019-04-10 00:56:32,430 root         INFO     Train Epoch: 40 [0/8000 (0%)]\tTotal Loss: 2.370187\n",
      "Reconstruction: 2.269997, Regularization: 0.009755, Discriminator: 0.002037; Generator: 0.088397,\n",
      "D(x): 0.996, D(G(z)): 0.059\n",
      "2019-04-10 00:56:32,542 root         INFO     Train Epoch: 40 [512/8000 (6%)]\tTotal Loss: 2.522660\n",
      "Reconstruction: 2.422775, Regularization: 0.008788, Discriminator: 0.001938; Generator: 0.089159,\n",
      "D(x): 0.997, D(G(z)): 0.058\n",
      "2019-04-10 00:56:32,653 root         INFO     Train Epoch: 40 [1024/8000 (13%)]\tTotal Loss: 2.139882\n",
      "Reconstruction: 2.038686, Regularization: 0.009430, Discriminator: 0.002038; Generator: 0.089727,\n",
      "D(x): 0.993, D(G(z)): 0.057\n",
      "2019-04-10 00:56:32,763 root         INFO     Train Epoch: 40 [1536/8000 (19%)]\tTotal Loss: 2.278811\n",
      "Reconstruction: 2.178306, Regularization: 0.009477, Discriminator: 0.001954; Generator: 0.089074,\n",
      "D(x): 0.997, D(G(z)): 0.058\n",
      "2019-04-10 00:56:32,874 root         INFO     Train Epoch: 40 [2048/8000 (26%)]\tTotal Loss: 2.302252\n",
      "Reconstruction: 2.201928, Regularization: 0.009141, Discriminator: 0.001985; Generator: 0.089198,\n",
      "D(x): 0.996, D(G(z)): 0.058\n",
      "2019-04-10 00:56:32,985 root         INFO     Train Epoch: 40 [2560/8000 (32%)]\tTotal Loss: 1.993562\n",
      "Reconstruction: 1.894963, Regularization: 0.006823, Discriminator: 0.002111; Generator: 0.089665,\n",
      "D(x): 0.991, D(G(z)): 0.057\n",
      "2019-04-10 00:56:33,096 root         INFO     Train Epoch: 40 [3072/8000 (38%)]\tTotal Loss: 1.956291\n",
      "Reconstruction: 1.856420, Regularization: 0.007938, Discriminator: 0.002090; Generator: 0.089843,\n",
      "D(x): 0.992, D(G(z)): 0.056\n",
      "2019-04-10 00:56:33,206 root         INFO     Train Epoch: 40 [3584/8000 (45%)]\tTotal Loss: 1.893299\n",
      "Reconstruction: 1.795655, Regularization: 0.005736, Discriminator: 0.001891; Generator: 0.090017,\n",
      "D(x): 0.997, D(G(z)): 0.056\n",
      "2019-04-10 00:56:33,317 root         INFO     Train Epoch: 40 [4096/8000 (51%)]\tTotal Loss: 1.841226\n",
      "Reconstruction: 1.742573, Regularization: 0.006943, Discriminator: 0.002223; Generator: 0.089487,\n",
      "D(x): 0.989, D(G(z)): 0.057\n",
      "2019-04-10 00:56:33,428 root         INFO     Train Epoch: 40 [4608/8000 (58%)]\tTotal Loss: 2.027914\n",
      "Reconstruction: 1.930354, Regularization: 0.005699, Discriminator: 0.002165; Generator: 0.089695,\n",
      "D(x): 0.990, D(G(z)): 0.057\n",
      "2019-04-10 00:56:33,539 root         INFO     Train Epoch: 40 [5120/8000 (64%)]\tTotal Loss: 1.663903\n",
      "Reconstruction: 1.565820, Regularization: 0.006302, Discriminator: 0.002221; Generator: 0.089560,\n",
      "D(x): 0.988, D(G(z)): 0.057\n",
      "2019-04-10 00:56:33,650 root         INFO     Train Epoch: 40 [5632/8000 (70%)]\tTotal Loss: 1.731923\n",
      "Reconstruction: 1.635070, Regularization: 0.005037, Discriminator: 0.001913; Generator: 0.089904,\n",
      "D(x): 0.997, D(G(z)): 0.056\n",
      "2019-04-10 00:56:33,760 root         INFO     Train Epoch: 40 [6144/8000 (77%)]\tTotal Loss: 1.667457\n",
      "Reconstruction: 1.571029, Regularization: 0.004926, Discriminator: 0.002265; Generator: 0.089238,\n",
      "D(x): 0.989, D(G(z)): 0.058\n",
      "2019-04-10 00:56:33,872 root         INFO     Train Epoch: 40 [6656/8000 (83%)]\tTotal Loss: 1.581872\n",
      "Reconstruction: 1.484704, Regularization: 0.004708, Discriminator: 0.002041; Generator: 0.090420,\n",
      "D(x): 0.992, D(G(z)): 0.056\n",
      "2019-04-10 00:56:33,982 root         INFO     Train Epoch: 40 [7168/8000 (90%)]\tTotal Loss: 1.780905\n",
      "Reconstruction: 1.683519, Regularization: 0.005369, Discriminator: 0.001814; Generator: 0.090202,\n",
      "D(x): 0.999, D(G(z)): 0.056\n",
      "2019-04-10 00:56:34,092 root         INFO     Train Epoch: 40 [7680/8000 (96%)]\tTotal Loss: 1.544785\n",
      "Reconstruction: 1.447608, Regularization: 0.004646, Discriminator: 0.001842; Generator: 0.090690,\n",
      "D(x): 0.998, D(G(z)): 0.055\n",
      "2019-04-10 00:56:34,172 root         INFO     ====> Epoch: 40 Average loss: 1.9336\n",
      "2019-04-10 00:56:34,199 root         INFO     Train Epoch: 41 [0/8000 (0%)]\tTotal Loss: 1.439436\n",
      "Reconstruction: 1.341534, Regularization: 0.005136, Discriminator: 0.002144; Generator: 0.090622,\n",
      "D(x): 0.989, D(G(z)): 0.055\n",
      "2019-04-10 00:56:34,311 root         INFO     Train Epoch: 41 [512/8000 (6%)]\tTotal Loss: 1.380589\n",
      "Reconstruction: 1.285092, Regularization: 0.003703, Discriminator: 0.002086; Generator: 0.089707,\n",
      "D(x): 0.992, D(G(z)): 0.057\n",
      "2019-04-10 00:56:34,422 root         INFO     Train Epoch: 41 [1024/8000 (13%)]\tTotal Loss: 1.317771\n",
      "Reconstruction: 1.221622, Regularization: 0.004213, Discriminator: 0.002021; Generator: 0.089915,\n",
      "D(x): 0.994, D(G(z)): 0.056\n",
      "2019-04-10 00:56:34,532 root         INFO     Train Epoch: 41 [1536/8000 (19%)]\tTotal Loss: 1.362123\n",
      "Reconstruction: 1.266135, Regularization: 0.004073, Discriminator: 0.001963; Generator: 0.089951,\n",
      "D(x): 0.995, D(G(z)): 0.056\n",
      "2019-04-10 00:56:34,643 root         INFO     Train Epoch: 41 [2048/8000 (26%)]\tTotal Loss: 1.244714\n",
      "Reconstruction: 1.147502, Regularization: 0.004405, Discriminator: 0.002140; Generator: 0.090667,\n",
      "D(x): 0.989, D(G(z)): 0.055\n",
      "2019-04-10 00:56:34,753 root         INFO     Train Epoch: 41 [2560/8000 (32%)]\tTotal Loss: 1.212071\n",
      "Reconstruction: 1.116663, Regularization: 0.003449, Discriminator: 0.001960; Generator: 0.089998,\n",
      "D(x): 0.995, D(G(z)): 0.056\n",
      "2019-04-10 00:56:34,862 root         INFO     Train Epoch: 41 [3072/8000 (38%)]\tTotal Loss: 1.083897\n",
      "Reconstruction: 0.987018, Regularization: 0.004722, Discriminator: 0.002061; Generator: 0.090096,\n",
      "D(x): 0.992, D(G(z)): 0.056\n",
      "2019-04-10 00:56:34,971 root         INFO     Train Epoch: 41 [3584/8000 (45%)]\tTotal Loss: 1.065092\n",
      "Reconstruction: 0.966432, Regularization: 0.005979, Discriminator: 0.002638; Generator: 0.090043,\n",
      "D(x): 0.978, D(G(z)): 0.056\n",
      "2019-04-10 00:56:35,079 root         INFO     Train Epoch: 41 [4096/8000 (51%)]\tTotal Loss: 1.169409\n",
      "Reconstruction: 1.073340, Regularization: 0.004385, Discriminator: 0.002107; Generator: 0.089577,\n",
      "D(x): 0.992, D(G(z)): 0.057\n",
      "2019-04-10 00:56:35,188 root         INFO     Train Epoch: 41 [4608/8000 (58%)]\tTotal Loss: 1.083231\n",
      "Reconstruction: 0.985921, Regularization: 0.004595, Discriminator: 0.001874; Generator: 0.090841,\n",
      "D(x): 0.996, D(G(z)): 0.055\n",
      "2019-04-10 00:56:35,296 root         INFO     Train Epoch: 41 [5120/8000 (64%)]\tTotal Loss: 1.078655\n",
      "Reconstruction: 0.980389, Regularization: 0.005693, Discriminator: 0.001870; Generator: 0.090703,\n",
      "D(x): 0.997, D(G(z)): 0.055\n",
      "2019-04-10 00:56:35,404 root         INFO     Train Epoch: 41 [5632/8000 (70%)]\tTotal Loss: 1.104517\n",
      "Reconstruction: 1.006831, Regularization: 0.005664, Discriminator: 0.001978; Generator: 0.090044,\n",
      "D(x): 0.995, D(G(z)): 0.056\n",
      "2019-04-10 00:56:35,514 root         INFO     Train Epoch: 41 [6144/8000 (77%)]\tTotal Loss: 0.939871\n",
      "Reconstruction: 0.842992, Regularization: 0.004460, Discriminator: 0.002514; Generator: 0.089905,\n",
      "D(x): 0.983, D(G(z)): 0.056\n",
      "2019-04-10 00:56:35,624 root         INFO     Train Epoch: 41 [6656/8000 (83%)]\tTotal Loss: 1.017574\n",
      "Reconstruction: 0.921360, Regularization: 0.004245, Discriminator: 0.001865; Generator: 0.090105,\n",
      "D(x): 0.998, D(G(z)): 0.056\n",
      "2019-04-10 00:56:35,733 root         INFO     Train Epoch: 41 [7168/8000 (90%)]\tTotal Loss: 0.919884\n",
      "Reconstruction: 0.822917, Regularization: 0.005182, Discriminator: 0.001890; Generator: 0.089895,\n",
      "D(x): 0.998, D(G(z)): 0.057\n",
      "2019-04-10 00:56:35,844 root         INFO     Train Epoch: 41 [7680/8000 (96%)]\tTotal Loss: 0.916215\n",
      "Reconstruction: 0.817466, Regularization: 0.005636, Discriminator: 0.002064; Generator: 0.091048,\n",
      "D(x): 0.991, D(G(z)): 0.054\n",
      "2019-04-10 00:56:35,924 root         INFO     ====> Epoch: 41 Average loss: 1.1367\n",
      "2019-04-10 00:56:35,952 root         INFO     Train Epoch: 42 [0/8000 (0%)]\tTotal Loss: 0.878356\n",
      "Reconstruction: 0.781435, Regularization: 0.005050, Discriminator: 0.001961; Generator: 0.089909,\n",
      "D(x): 0.996, D(G(z)): 0.057\n",
      "2019-04-10 00:56:36,063 root         INFO     Train Epoch: 42 [512/8000 (6%)]\tTotal Loss: 0.812614\n",
      "Reconstruction: 0.714830, Regularization: 0.005885, Discriminator: 0.002776; Generator: 0.089124,\n",
      "D(x): 0.978, D(G(z)): 0.058\n",
      "2019-04-10 00:56:36,174 root         INFO     Train Epoch: 42 [1024/8000 (13%)]\tTotal Loss: 0.789356\n",
      "Reconstruction: 0.690312, Regularization: 0.006243, Discriminator: 0.001847; Generator: 0.090954,\n",
      "D(x): 0.997, D(G(z)): 0.055\n",
      "2019-04-10 00:56:36,285 root         INFO     Train Epoch: 42 [1536/8000 (19%)]\tTotal Loss: 0.808876\n",
      "Reconstruction: 0.711128, Regularization: 0.006058, Discriminator: 0.001965; Generator: 0.089725,\n",
      "D(x): 0.996, D(G(z)): 0.057\n",
      "2019-04-10 00:56:36,395 root         INFO     Train Epoch: 42 [2048/8000 (26%)]\tTotal Loss: 0.733568\n",
      "Reconstruction: 0.634328, Regularization: 0.007103, Discriminator: 0.002018; Generator: 0.090119,\n",
      "D(x): 0.993, D(G(z)): 0.056\n",
      "2019-04-10 00:56:36,504 root         INFO     Train Epoch: 42 [2560/8000 (32%)]\tTotal Loss: 0.760392\n",
      "Reconstruction: 0.660695, Regularization: 0.006477, Discriminator: 0.002539; Generator: 0.090681,\n",
      "D(x): 0.979, D(G(z)): 0.055\n",
      "2019-04-10 00:56:36,615 root         INFO     Train Epoch: 42 [3072/8000 (38%)]\tTotal Loss: 0.796915\n",
      "Reconstruction: 0.696944, Regularization: 0.007909, Discriminator: 0.001997; Generator: 0.090064,\n",
      "D(x): 0.994, D(G(z)): 0.056\n",
      "2019-04-10 00:56:36,724 root         INFO     Train Epoch: 42 [3584/8000 (45%)]\tTotal Loss: 0.768801\n",
      "Reconstruction: 0.669365, Regularization: 0.006382, Discriminator: 0.002430; Generator: 0.090624,\n",
      "D(x): 0.984, D(G(z)): 0.055\n",
      "2019-04-10 00:56:36,834 root         INFO     Train Epoch: 42 [4096/8000 (51%)]\tTotal Loss: 0.763004\n",
      "Reconstruction: 0.665810, Regularization: 0.006578, Discriminator: 0.001989; Generator: 0.088627,\n",
      "D(x): 0.997, D(G(z)): 0.059\n",
      "2019-04-10 00:56:36,944 root         INFO     Train Epoch: 42 [4608/8000 (58%)]\tTotal Loss: 0.632516\n",
      "Reconstruction: 0.534348, Regularization: 0.006835, Discriminator: 0.001948; Generator: 0.089385,\n",
      "D(x): 0.997, D(G(z)): 0.058\n",
      "2019-04-10 00:56:37,055 root         INFO     Train Epoch: 42 [5120/8000 (64%)]\tTotal Loss: 0.569130\n",
      "Reconstruction: 0.469986, Regularization: 0.008275, Discriminator: 0.001989; Generator: 0.088881,\n",
      "D(x): 0.997, D(G(z)): 0.059\n",
      "2019-04-10 00:56:37,165 root         INFO     Train Epoch: 42 [5632/8000 (70%)]\tTotal Loss: 0.584912\n",
      "Reconstruction: 0.484070, Regularization: 0.008272, Discriminator: 0.002387; Generator: 0.090183,\n",
      "D(x): 0.983, D(G(z)): 0.056\n",
      "2019-04-10 00:56:37,275 root         INFO     Train Epoch: 42 [6144/8000 (77%)]\tTotal Loss: 0.589172\n",
      "Reconstruction: 0.490395, Regularization: 0.008013, Discriminator: 0.002101; Generator: 0.088663,\n",
      "D(x): 0.994, D(G(z)): 0.059\n",
      "2019-04-10 00:56:37,386 root         INFO     Train Epoch: 42 [6656/8000 (83%)]\tTotal Loss: 0.562173\n",
      "Reconstruction: 0.460232, Regularization: 0.008933, Discriminator: 0.002198; Generator: 0.090810,\n",
      "D(x): 0.987, D(G(z)): 0.055\n",
      "2019-04-10 00:56:37,496 root         INFO     Train Epoch: 42 [7168/8000 (90%)]\tTotal Loss: 0.606949\n",
      "Reconstruction: 0.507537, Regularization: 0.008473, Discriminator: 0.002059; Generator: 0.088881,\n",
      "D(x): 0.994, D(G(z)): 0.058\n",
      "2019-04-10 00:56:37,606 root         INFO     Train Epoch: 42 [7680/8000 (96%)]\tTotal Loss: 0.545560\n",
      "Reconstruction: 0.444068, Regularization: 0.008766, Discriminator: 0.001938; Generator: 0.090789,\n",
      "D(x): 0.995, D(G(z)): 0.055\n",
      "2019-04-10 00:56:37,686 root         INFO     ====> Epoch: 42 Average loss: 0.6952\n",
      "2019-04-10 00:56:37,714 root         INFO     Train Epoch: 43 [0/8000 (0%)]\tTotal Loss: 0.594526\n",
      "Reconstruction: 0.493254, Regularization: 0.008571, Discriminator: 0.001936; Generator: 0.090765,\n",
      "D(x): 0.995, D(G(z)): 0.055\n",
      "2019-04-10 00:56:37,825 root         INFO     Train Epoch: 43 [512/8000 (6%)]\tTotal Loss: 0.528174\n",
      "Reconstruction: 0.427663, Regularization: 0.009407, Discriminator: 0.001995; Generator: 0.089109,\n",
      "D(x): 0.996, D(G(z)): 0.058\n",
      "2019-04-10 00:56:37,936 root         INFO     Train Epoch: 43 [1024/8000 (13%)]\tTotal Loss: 0.557344\n",
      "Reconstruction: 0.457185, Regularization: 0.007953, Discriminator: 0.001911; Generator: 0.090294,\n",
      "D(x): 0.997, D(G(z)): 0.056\n",
      "2019-04-10 00:56:38,046 root         INFO     Train Epoch: 43 [1536/8000 (19%)]\tTotal Loss: 0.596415\n",
      "Reconstruction: 0.495014, Regularization: 0.009323, Discriminator: 0.002061; Generator: 0.090016,\n",
      "D(x): 0.993, D(G(z)): 0.056\n",
      "2019-04-10 00:56:38,157 root         INFO     Train Epoch: 43 [2048/8000 (26%)]\tTotal Loss: 0.467721\n",
      "Reconstruction: 0.366607, Regularization: 0.008101, Discriminator: 0.001891; Generator: 0.091122,\n",
      "D(x): 0.996, D(G(z)): 0.055\n",
      "2019-04-10 00:56:38,268 root         INFO     Train Epoch: 43 [2560/8000 (32%)]\tTotal Loss: 0.518440\n",
      "Reconstruction: 0.417305, Regularization: 0.009415, Discriminator: 0.002272; Generator: 0.089449,\n",
      "D(x): 0.987, D(G(z)): 0.057\n",
      "2019-04-10 00:56:38,379 root         INFO     Train Epoch: 43 [3072/8000 (38%)]\tTotal Loss: 0.483828\n",
      "Reconstruction: 0.380808, Regularization: 0.010538, Discriminator: 0.001918; Generator: 0.090565,\n",
      "D(x): 0.996, D(G(z)): 0.056\n",
      "2019-04-10 00:56:38,489 root         INFO     Train Epoch: 43 [3584/8000 (45%)]\tTotal Loss: 0.486830\n",
      "Reconstruction: 0.382668, Regularization: 0.010427, Discriminator: 0.002202; Generator: 0.091533,\n",
      "D(x): 0.987, D(G(z)): 0.054\n",
      "2019-04-10 00:56:38,600 root         INFO     Train Epoch: 43 [4096/8000 (51%)]\tTotal Loss: 0.492670\n",
      "Reconstruction: 0.389143, Regularization: 0.009482, Discriminator: 0.002359; Generator: 0.091688,\n",
      "D(x): 0.984, D(G(z)): 0.054\n",
      "2019-04-10 00:56:38,710 root         INFO     Train Epoch: 43 [4608/8000 (58%)]\tTotal Loss: 0.467531\n",
      "Reconstruction: 0.365984, Regularization: 0.009055, Discriminator: 0.001925; Generator: 0.090567,\n",
      "D(x): 0.996, D(G(z)): 0.056\n",
      "2019-04-10 00:56:38,821 root         INFO     Train Epoch: 43 [5120/8000 (64%)]\tTotal Loss: 0.465510\n",
      "Reconstruction: 0.361895, Regularization: 0.010198, Discriminator: 0.002769; Generator: 0.090648,\n",
      "D(x): 0.974, D(G(z)): 0.055\n",
      "2019-04-10 00:56:38,931 root         INFO     Train Epoch: 43 [5632/8000 (70%)]\tTotal Loss: 0.427756\n",
      "Reconstruction: 0.325534, Regularization: 0.009530, Discriminator: 0.001906; Generator: 0.090786,\n",
      "D(x): 0.996, D(G(z)): 0.055\n",
      "2019-04-10 00:56:39,041 root         INFO     Train Epoch: 43 [6144/8000 (77%)]\tTotal Loss: 0.485626\n",
      "Reconstruction: 0.383570, Regularization: 0.010408, Discriminator: 0.001912; Generator: 0.089736,\n",
      "D(x): 0.998, D(G(z)): 0.057\n",
      "2019-04-10 00:56:39,150 root         INFO     Train Epoch: 43 [6656/8000 (83%)]\tTotal Loss: 0.427855\n",
      "Reconstruction: 0.323852, Regularization: 0.010254, Discriminator: 0.001934; Generator: 0.091817,\n",
      "D(x): 0.993, D(G(z)): 0.053\n",
      "2019-04-10 00:56:39,259 root         INFO     Train Epoch: 43 [7168/8000 (90%)]\tTotal Loss: 0.450233\n",
      "Reconstruction: 0.348447, Regularization: 0.009520, Discriminator: 0.002014; Generator: 0.090251,\n",
      "D(x): 0.994, D(G(z)): 0.056\n",
      "2019-04-10 00:56:39,368 root         INFO     Train Epoch: 43 [7680/8000 (96%)]\tTotal Loss: 0.422323\n",
      "Reconstruction: 0.318309, Regularization: 0.009911, Discriminator: 0.001855; Generator: 0.092247,\n",
      "D(x): 0.995, D(G(z)): 0.053\n",
      "2019-04-10 00:56:39,448 root         INFO     ====> Epoch: 43 Average loss: 0.4861\n",
      "2019-04-10 00:56:39,476 root         INFO     Train Epoch: 44 [0/8000 (0%)]\tTotal Loss: 0.389338\n",
      "Reconstruction: 0.284624, Regularization: 0.010872, Discriminator: 0.002308; Generator: 0.091534,\n",
      "D(x): 0.983, D(G(z)): 0.054\n",
      "2019-04-10 00:56:39,588 root         INFO     Train Epoch: 44 [512/8000 (6%)]\tTotal Loss: 0.423458\n",
      "Reconstruction: 0.320240, Regularization: 0.010305, Discriminator: 0.001991; Generator: 0.090922,\n",
      "D(x): 0.993, D(G(z)): 0.055\n",
      "2019-04-10 00:56:39,698 root         INFO     Train Epoch: 44 [1024/8000 (13%)]\tTotal Loss: 0.415450\n",
      "Reconstruction: 0.311661, Regularization: 0.010835, Discriminator: 0.001979; Generator: 0.090975,\n",
      "D(x): 0.993, D(G(z)): 0.055\n",
      "2019-04-10 00:56:39,807 root         INFO     Train Epoch: 44 [1536/8000 (19%)]\tTotal Loss: 0.404497\n",
      "Reconstruction: 0.301898, Regularization: 0.009852, Discriminator: 0.002304; Generator: 0.090443,\n",
      "D(x): 0.985, D(G(z)): 0.056\n",
      "2019-04-10 00:56:39,915 root         INFO     Train Epoch: 44 [2048/8000 (26%)]\tTotal Loss: 0.419037\n",
      "Reconstruction: 0.315814, Regularization: 0.009940, Discriminator: 0.002665; Generator: 0.090619,\n",
      "D(x): 0.978, D(G(z)): 0.056\n",
      "2019-04-10 00:56:40,024 root         INFO     Train Epoch: 44 [2560/8000 (32%)]\tTotal Loss: 0.396035\n",
      "Reconstruction: 0.291225, Regularization: 0.011101, Discriminator: 0.001918; Generator: 0.091791,\n",
      "D(x): 0.994, D(G(z)): 0.053\n",
      "2019-04-10 00:56:40,133 root         INFO     Train Epoch: 44 [3072/8000 (38%)]\tTotal Loss: 0.382727\n",
      "Reconstruction: 0.278922, Regularization: 0.010114, Discriminator: 0.002600; Generator: 0.091092,\n",
      "D(x): 0.976, D(G(z)): 0.055\n",
      "2019-04-10 00:56:40,243 root         INFO     Train Epoch: 44 [3584/8000 (45%)]\tTotal Loss: 0.399454\n",
      "Reconstruction: 0.295115, Regularization: 0.011682, Discriminator: 0.001930; Generator: 0.090727,\n",
      "D(x): 0.995, D(G(z)): 0.056\n",
      "2019-04-10 00:56:40,351 root         INFO     Train Epoch: 44 [4096/8000 (51%)]\tTotal Loss: 0.391603\n",
      "Reconstruction: 0.287665, Regularization: 0.010219, Discriminator: 0.001860; Generator: 0.091858,\n",
      "D(x): 0.996, D(G(z)): 0.054\n",
      "2019-04-10 00:56:40,459 root         INFO     Train Epoch: 44 [4608/8000 (58%)]\tTotal Loss: 0.395440\n",
      "Reconstruction: 0.290506, Regularization: 0.010451, Discriminator: 0.002625; Generator: 0.091858,\n",
      "D(x): 0.975, D(G(z)): 0.054\n",
      "2019-04-10 00:56:40,568 root         INFO     Train Epoch: 44 [5120/8000 (64%)]\tTotal Loss: 0.391539\n",
      "Reconstruction: 0.287874, Regularization: 0.010703, Discriminator: 0.002182; Generator: 0.090781,\n",
      "D(x): 0.988, D(G(z)): 0.056\n",
      "2019-04-10 00:56:40,676 root         INFO     Train Epoch: 44 [5632/8000 (70%)]\tTotal Loss: 0.381489\n",
      "Reconstruction: 0.278720, Regularization: 0.010082, Discriminator: 0.001978; Generator: 0.090709,\n",
      "D(x): 0.994, D(G(z)): 0.056\n",
      "2019-04-10 00:56:40,785 root         INFO     Train Epoch: 44 [6144/8000 (77%)]\tTotal Loss: 0.353433\n",
      "Reconstruction: 0.249395, Regularization: 0.010087, Discriminator: 0.001918; Generator: 0.092032,\n",
      "D(x): 0.994, D(G(z)): 0.053\n",
      "2019-04-10 00:56:40,895 root         INFO     Train Epoch: 44 [6656/8000 (83%)]\tTotal Loss: 0.368057\n",
      "Reconstruction: 0.263864, Regularization: 0.010266, Discriminator: 0.002496; Generator: 0.091431,\n",
      "D(x): 0.978, D(G(z)): 0.054\n",
      "2019-04-10 00:56:41,004 root         INFO     Train Epoch: 44 [7168/8000 (90%)]\tTotal Loss: 0.379076\n",
      "Reconstruction: 0.276124, Regularization: 0.010050, Discriminator: 0.002029; Generator: 0.090873,\n",
      "D(x): 0.992, D(G(z)): 0.055\n",
      "2019-04-10 00:56:41,113 root         INFO     Train Epoch: 44 [7680/8000 (96%)]\tTotal Loss: 0.370290\n",
      "Reconstruction: 0.268523, Regularization: 0.010124, Discriminator: 0.002160; Generator: 0.089482,\n",
      "D(x): 0.991, D(G(z)): 0.058\n",
      "2019-04-10 00:56:41,193 root         INFO     ====> Epoch: 44 Average loss: 0.3933\n",
      "2019-04-10 00:56:41,221 root         INFO     Train Epoch: 45 [0/8000 (0%)]\tTotal Loss: 0.364141\n",
      "Reconstruction: 0.259170, Regularization: 0.010420, Discriminator: 0.002695; Generator: 0.091857,\n",
      "D(x): 0.976, D(G(z)): 0.054\n",
      "2019-04-10 00:56:41,331 root         INFO     Train Epoch: 45 [512/8000 (6%)]\tTotal Loss: 0.358383\n",
      "Reconstruction: 0.256926, Regularization: 0.009600, Discriminator: 0.002054; Generator: 0.089802,\n",
      "D(x): 0.994, D(G(z)): 0.058\n",
      "2019-04-10 00:56:41,441 root         INFO     Train Epoch: 45 [1024/8000 (13%)]\tTotal Loss: 0.374548\n",
      "Reconstruction: 0.270204, Regularization: 0.011236, Discriminator: 0.002942; Generator: 0.090166,\n",
      "D(x): 0.970, D(G(z)): 0.056\n",
      "2019-04-10 00:56:41,550 root         INFO     Train Epoch: 45 [1536/8000 (19%)]\tTotal Loss: 0.335848\n",
      "Reconstruction: 0.232178, Regularization: 0.010584, Discriminator: 0.002149; Generator: 0.090936,\n",
      "D(x): 0.989, D(G(z)): 0.056\n",
      "2019-04-10 00:56:41,661 root         INFO     Train Epoch: 45 [2048/8000 (26%)]\tTotal Loss: 0.365260\n",
      "Reconstruction: 0.259618, Regularization: 0.010782, Discriminator: 0.002489; Generator: 0.092371,\n",
      "D(x): 0.978, D(G(z)): 0.053\n",
      "2019-04-10 00:56:41,771 root         INFO     Train Epoch: 45 [2560/8000 (32%)]\tTotal Loss: 0.352544\n",
      "Reconstruction: 0.250143, Regularization: 0.009275, Discriminator: 0.002142; Generator: 0.090984,\n",
      "D(x): 0.989, D(G(z)): 0.055\n",
      "2019-04-10 00:56:41,881 root         INFO     Train Epoch: 45 [3072/8000 (38%)]\tTotal Loss: 0.362500\n",
      "Reconstruction: 0.261616, Regularization: 0.009473, Discriminator: 0.002238; Generator: 0.089172,\n",
      "D(x): 0.989, D(G(z)): 0.058\n",
      "2019-04-10 00:56:41,992 root         INFO     Train Epoch: 45 [3584/8000 (45%)]\tTotal Loss: 0.340926\n",
      "Reconstruction: 0.237456, Regularization: 0.010300, Discriminator: 0.002276; Generator: 0.090893,\n",
      "D(x): 0.984, D(G(z)): 0.055\n",
      "2019-04-10 00:56:42,102 root         INFO     Train Epoch: 45 [4096/8000 (51%)]\tTotal Loss: 0.352913\n",
      "Reconstruction: 0.249438, Regularization: 0.009458, Discriminator: 0.002039; Generator: 0.091979,\n",
      "D(x): 0.991, D(G(z)): 0.054\n",
      "2019-04-10 00:56:42,213 root         INFO     Train Epoch: 45 [4608/8000 (58%)]\tTotal Loss: 0.358114\n",
      "Reconstruction: 0.255894, Regularization: 0.009511, Discriminator: 0.002254; Generator: 0.090455,\n",
      "D(x): 0.987, D(G(z)): 0.057\n",
      "2019-04-10 00:56:42,323 root         INFO     Train Epoch: 45 [5120/8000 (64%)]\tTotal Loss: 0.347258\n",
      "Reconstruction: 0.245270, Regularization: 0.009403, Discriminator: 0.002314; Generator: 0.090271,\n",
      "D(x): 0.985, D(G(z)): 0.057\n",
      "2019-04-10 00:56:42,434 root         INFO     Train Epoch: 45 [5632/8000 (70%)]\tTotal Loss: 0.342085\n",
      "Reconstruction: 0.240303, Regularization: 0.008837, Discriminator: 0.002152; Generator: 0.090793,\n",
      "D(x): 0.989, D(G(z)): 0.056\n",
      "2019-04-10 00:56:42,544 root         INFO     Train Epoch: 45 [6144/8000 (77%)]\tTotal Loss: 0.331570\n",
      "Reconstruction: 0.231583, Regularization: 0.009162, Discriminator: 0.002229; Generator: 0.088596,\n",
      "D(x): 0.990, D(G(z)): 0.060\n",
      "2019-04-10 00:56:42,655 root         INFO     Train Epoch: 45 [6656/8000 (83%)]\tTotal Loss: 0.330949\n",
      "Reconstruction: 0.228578, Regularization: 0.009186, Discriminator: 0.002517; Generator: 0.090668,\n",
      "D(x): 0.979, D(G(z)): 0.056\n",
      "2019-04-10 00:56:42,766 root         INFO     Train Epoch: 45 [7168/8000 (90%)]\tTotal Loss: 0.345711\n",
      "Reconstruction: 0.242827, Regularization: 0.010227, Discriminator: 0.002028; Generator: 0.090629,\n",
      "D(x): 0.993, D(G(z)): 0.056\n",
      "2019-04-10 00:56:42,876 root         INFO     Train Epoch: 45 [7680/8000 (96%)]\tTotal Loss: 0.338101\n",
      "Reconstruction: 0.236086, Regularization: 0.009137, Discriminator: 0.002377; Generator: 0.090502,\n",
      "D(x): 0.983, D(G(z)): 0.056\n",
      "2019-04-10 00:56:42,957 root         INFO     ====> Epoch: 45 Average loss: 0.3524\n",
      "2019-04-10 00:56:42,984 root         INFO     Train Epoch: 46 [0/8000 (0%)]\tTotal Loss: 0.350483\n",
      "Reconstruction: 0.248663, Regularization: 0.010017, Discriminator: 0.002711; Generator: 0.089093,\n",
      "D(x): 0.977, D(G(z)): 0.059\n",
      "2019-04-10 00:56:43,096 root         INFO     Train Epoch: 46 [512/8000 (6%)]\tTotal Loss: 0.355361\n",
      "Reconstruction: 0.252660, Regularization: 0.009196, Discriminator: 0.001971; Generator: 0.091535,\n",
      "D(x): 0.993, D(G(z)): 0.055\n",
      "2019-04-10 00:56:43,207 root         INFO     Train Epoch: 46 [1024/8000 (13%)]\tTotal Loss: 0.337956\n",
      "Reconstruction: 0.235031, Regularization: 0.009382, Discriminator: 0.002756; Generator: 0.090786,\n",
      "D(x): 0.974, D(G(z)): 0.056\n",
      "2019-04-10 00:56:43,319 root         INFO     Train Epoch: 46 [1536/8000 (19%)]\tTotal Loss: 0.346978\n",
      "Reconstruction: 0.243975, Regularization: 0.009501, Discriminator: 0.002122; Generator: 0.091380,\n",
      "D(x): 0.989, D(G(z)): 0.055\n",
      "2019-04-10 00:56:43,431 root         INFO     Train Epoch: 46 [2048/8000 (26%)]\tTotal Loss: 0.335161\n",
      "Reconstruction: 0.232632, Regularization: 0.008480, Discriminator: 0.002494; Generator: 0.091556,\n",
      "D(x): 0.978, D(G(z)): 0.055\n",
      "2019-04-10 00:56:43,542 root         INFO     Train Epoch: 46 [2560/8000 (32%)]\tTotal Loss: 0.339985\n",
      "Reconstruction: 0.236453, Regularization: 0.009004, Discriminator: 0.002611; Generator: 0.091917,\n",
      "D(x): 0.974, D(G(z)): 0.054\n",
      "2019-04-10 00:56:43,653 root         INFO     Train Epoch: 46 [3072/8000 (38%)]\tTotal Loss: 0.338513\n",
      "Reconstruction: 0.237786, Regularization: 0.009249, Discriminator: 0.003161; Generator: 0.088317,\n",
      "D(x): 0.966, D(G(z)): 0.060\n",
      "2019-04-10 00:56:43,765 root         INFO     Train Epoch: 46 [3584/8000 (45%)]\tTotal Loss: 0.339070\n",
      "Reconstruction: 0.237768, Regularization: 0.009754, Discriminator: 0.002201; Generator: 0.089346,\n",
      "D(x): 0.991, D(G(z)): 0.059\n",
      "2019-04-10 00:56:43,874 root         INFO     Train Epoch: 46 [4096/8000 (51%)]\tTotal Loss: 0.329110\n",
      "Reconstruction: 0.229967, Regularization: 0.008322, Discriminator: 0.002242; Generator: 0.088578,\n",
      "D(x): 0.991, D(G(z)): 0.060\n",
      "2019-04-10 00:56:43,983 root         INFO     Train Epoch: 46 [4608/8000 (58%)]\tTotal Loss: 0.323063\n",
      "Reconstruction: 0.223776, Regularization: 0.007948, Discriminator: 0.002592; Generator: 0.088747,\n",
      "D(x): 0.980, D(G(z)): 0.060\n",
      "2019-04-10 00:56:44,092 root         INFO     Train Epoch: 46 [5120/8000 (64%)]\tTotal Loss: 0.334776\n",
      "Reconstruction: 0.233864, Regularization: 0.008588, Discriminator: 0.002566; Generator: 0.089757,\n",
      "D(x): 0.979, D(G(z)): 0.057\n",
      "2019-04-10 00:56:44,201 root         INFO     Train Epoch: 46 [5632/8000 (70%)]\tTotal Loss: 0.338445\n",
      "Reconstruction: 0.237511, Regularization: 0.007985, Discriminator: 0.001940; Generator: 0.091009,\n",
      "D(x): 0.995, D(G(z)): 0.056\n",
      "2019-04-10 00:56:44,309 root         INFO     Train Epoch: 46 [6144/8000 (77%)]\tTotal Loss: 0.344852\n",
      "Reconstruction: 0.246102, Regularization: 0.008379, Discriminator: 0.002974; Generator: 0.087398,\n",
      "D(x): 0.976, D(G(z)): 0.064\n",
      "2019-04-10 00:56:44,417 root         INFO     Train Epoch: 46 [6656/8000 (83%)]\tTotal Loss: 0.349059\n",
      "Reconstruction: 0.251934, Regularization: 0.007727, Discriminator: 0.002598; Generator: 0.086800,\n",
      "D(x): 0.985, D(G(z)): 0.064\n",
      "2019-04-10 00:56:44,527 root         INFO     Train Epoch: 46 [7168/8000 (90%)]\tTotal Loss: 0.323904\n",
      "Reconstruction: 0.221563, Regularization: 0.007731, Discriminator: 0.002604; Generator: 0.092007,\n",
      "D(x): 0.975, D(G(z)): 0.054\n",
      "2019-04-10 00:56:44,638 root         INFO     Train Epoch: 46 [7680/8000 (96%)]\tTotal Loss: 0.332747\n",
      "Reconstruction: 0.234092, Regularization: 0.007483, Discriminator: 0.003758; Generator: 0.087415,\n",
      "D(x): 0.960, D(G(z)): 0.063\n",
      "2019-04-10 00:56:44,719 root         INFO     ====> Epoch: 46 Average loss: 0.3349\n",
      "2019-04-10 00:56:44,746 root         INFO     Train Epoch: 47 [0/8000 (0%)]\tTotal Loss: 0.336073\n",
      "Reconstruction: 0.235719, Regularization: 0.007442, Discriminator: 0.003883; Generator: 0.089029,\n",
      "D(x): 0.966, D(G(z)): 0.059\n",
      "2019-04-10 00:56:44,859 root         INFO     Train Epoch: 47 [512/8000 (6%)]\tTotal Loss: 0.340537\n",
      "Reconstruction: 0.241314, Regularization: 0.008568, Discriminator: 0.002145; Generator: 0.088510,\n",
      "D(x): 0.994, D(G(z)): 0.060\n",
      "2019-04-10 00:56:44,970 root         INFO     Train Epoch: 47 [1024/8000 (13%)]\tTotal Loss: 0.331830\n",
      "Reconstruction: 0.232944, Regularization: 0.007541, Discriminator: 0.002754; Generator: 0.088592,\n",
      "D(x): 0.977, D(G(z)): 0.060\n",
      "2019-04-10 00:56:45,082 root         INFO     Train Epoch: 47 [1536/8000 (19%)]\tTotal Loss: 0.327715\n",
      "Reconstruction: 0.228195, Regularization: 0.007194, Discriminator: 0.003272; Generator: 0.089054,\n",
      "D(x): 0.967, D(G(z)): 0.060\n",
      "2019-04-10 00:56:45,193 root         INFO     Train Epoch: 47 [2048/8000 (26%)]\tTotal Loss: 0.333338\n",
      "Reconstruction: 0.233431, Regularization: 0.007928, Discriminator: 0.004153; Generator: 0.087827,\n",
      "D(x): 0.949, D(G(z)): 0.062\n",
      "2019-04-10 00:56:45,305 root         INFO     Train Epoch: 47 [2560/8000 (32%)]\tTotal Loss: 0.328559\n",
      "Reconstruction: 0.228729, Regularization: 0.007700, Discriminator: 0.005450; Generator: 0.086680,\n",
      "D(x): 0.929, D(G(z)): 0.064\n",
      "2019-04-10 00:56:45,417 root         INFO     Train Epoch: 47 [3072/8000 (38%)]\tTotal Loss: 0.326255\n",
      "Reconstruction: 0.228346, Regularization: 0.007751, Discriminator: 0.004990; Generator: 0.085168,\n",
      "D(x): 0.935, D(G(z)): 0.067\n",
      "2019-04-10 00:56:45,528 root         INFO     Train Epoch: 47 [3584/8000 (45%)]\tTotal Loss: 0.317622\n",
      "Reconstruction: 0.222366, Regularization: 0.006269, Discriminator: 0.003855; Generator: 0.085132,\n",
      "D(x): 0.959, D(G(z)): 0.068\n",
      "2019-04-10 00:56:45,640 root         INFO     Train Epoch: 47 [4096/8000 (51%)]\tTotal Loss: 0.318277\n",
      "Reconstruction: 0.221818, Regularization: 0.006042, Discriminator: 0.004456; Generator: 0.085961,\n",
      "D(x): 0.950, D(G(z)): 0.065\n",
      "2019-04-10 00:56:45,752 root         INFO     Train Epoch: 47 [4608/8000 (58%)]\tTotal Loss: 0.324977\n",
      "Reconstruction: 0.227574, Regularization: 0.007762, Discriminator: 0.004131; Generator: 0.085509,\n",
      "D(x): 0.957, D(G(z)): 0.067\n",
      "2019-04-10 00:56:45,863 root         INFO     Train Epoch: 47 [5120/8000 (64%)]\tTotal Loss: 0.327153\n",
      "Reconstruction: 0.228865, Regularization: 0.007232, Discriminator: 0.004017; Generator: 0.087039,\n",
      "D(x): 0.956, D(G(z)): 0.063\n",
      "2019-04-10 00:56:45,975 root         INFO     Train Epoch: 47 [5632/8000 (70%)]\tTotal Loss: 0.334416\n",
      "Reconstruction: 0.237902, Regularization: 0.007822, Discriminator: 0.002404; Generator: 0.086289,\n",
      "D(x): 0.991, D(G(z)): 0.065\n",
      "2019-04-10 00:56:46,087 root         INFO     Train Epoch: 47 [6144/8000 (77%)]\tTotal Loss: 0.328846\n",
      "Reconstruction: 0.228643, Regularization: 0.005931, Discriminator: 0.005809; Generator: 0.088464,\n",
      "D(x): 0.927, D(G(z)): 0.061\n",
      "2019-04-10 00:56:46,198 root         INFO     Train Epoch: 47 [6656/8000 (83%)]\tTotal Loss: 0.320261\n",
      "Reconstruction: 0.222069, Regularization: 0.006655, Discriminator: 0.002788; Generator: 0.088749,\n",
      "D(x): 0.975, D(G(z)): 0.060\n",
      "2019-04-10 00:56:46,310 root         INFO     Train Epoch: 47 [7168/8000 (90%)]\tTotal Loss: 0.321981\n",
      "Reconstruction: 0.226269, Regularization: 0.005481, Discriminator: 0.003530; Generator: 0.086701,\n",
      "D(x): 0.962, D(G(z)): 0.064\n",
      "2019-04-10 00:56:46,422 root         INFO     Train Epoch: 47 [7680/8000 (96%)]\tTotal Loss: 0.318338\n",
      "Reconstruction: 0.217613, Regularization: 0.005253, Discriminator: 0.004458; Generator: 0.091014,\n",
      "D(x): 0.958, D(G(z)): 0.056\n",
      "2019-04-10 00:56:46,503 root         INFO     ====> Epoch: 47 Average loss: 0.3267\n",
      "2019-04-10 00:56:46,530 root         INFO     Train Epoch: 48 [0/8000 (0%)]\tTotal Loss: 0.322107\n",
      "Reconstruction: 0.225306, Regularization: 0.006362, Discriminator: 0.002555; Generator: 0.087884,\n",
      "D(x): 0.983, D(G(z)): 0.062\n",
      "2019-04-10 00:56:46,643 root         INFO     Train Epoch: 48 [512/8000 (6%)]\tTotal Loss: 0.330950\n",
      "Reconstruction: 0.231538, Regularization: 0.006716, Discriminator: 0.006283; Generator: 0.086412,\n",
      "D(x): 0.949, D(G(z)): 0.066\n",
      "2019-04-10 00:56:46,754 root         INFO     Train Epoch: 48 [1024/8000 (13%)]\tTotal Loss: 0.323226\n",
      "Reconstruction: 0.226439, Regularization: 0.005771, Discriminator: 0.004308; Generator: 0.086708,\n",
      "D(x): 0.944, D(G(z)): 0.064\n",
      "2019-04-10 00:56:46,866 root         INFO     Train Epoch: 48 [1536/8000 (19%)]\tTotal Loss: 0.319375\n",
      "Reconstruction: 0.226381, Regularization: 0.006440, Discriminator: 0.002783; Generator: 0.083771,\n",
      "D(x): 0.985, D(G(z)): 0.071\n",
      "2019-04-10 00:56:46,977 root         INFO     Train Epoch: 48 [2048/8000 (26%)]\tTotal Loss: 0.333349\n",
      "Reconstruction: 0.238524, Regularization: 0.004994, Discriminator: 0.002499; Generator: 0.087332,\n",
      "D(x): 0.986, D(G(z)): 0.063\n",
      "2019-04-10 00:56:47,089 root         INFO     Train Epoch: 48 [2560/8000 (32%)]\tTotal Loss: 0.324554\n",
      "Reconstruction: 0.228341, Regularization: 0.004829, Discriminator: 0.003739; Generator: 0.087646,\n",
      "D(x): 0.965, D(G(z)): 0.062\n",
      "2019-04-10 00:56:47,200 root         INFO     Train Epoch: 48 [3072/8000 (38%)]\tTotal Loss: 0.321415\n",
      "Reconstruction: 0.228792, Regularization: 0.004431, Discriminator: 0.002956; Generator: 0.085236,\n",
      "D(x): 0.979, D(G(z)): 0.068\n",
      "2019-04-10 00:56:47,312 root         INFO     Train Epoch: 48 [3584/8000 (45%)]\tTotal Loss: 0.317202\n",
      "Reconstruction: 0.224457, Regularization: 0.003986, Discriminator: 0.003774; Generator: 0.084985,\n",
      "D(x): 0.962, D(G(z)): 0.067\n",
      "2019-04-10 00:56:47,423 root         INFO     Train Epoch: 48 [4096/8000 (51%)]\tTotal Loss: 0.321265\n",
      "Reconstruction: 0.229773, Regularization: 0.004419, Discriminator: 0.005190; Generator: 0.081882,\n",
      "D(x): 0.931, D(G(z)): 0.075\n",
      "2019-04-10 00:56:47,535 root         INFO     Train Epoch: 48 [4608/8000 (58%)]\tTotal Loss: 0.328532\n",
      "Reconstruction: 0.238068, Regularization: 0.004884, Discriminator: 0.002799; Generator: 0.082782,\n",
      "D(x): 0.987, D(G(z)): 0.073\n",
      "2019-04-10 00:56:47,646 root         INFO     Train Epoch: 48 [5120/8000 (64%)]\tTotal Loss: 0.323957\n",
      "Reconstruction: 0.231555, Regularization: 0.004755, Discriminator: 0.003420; Generator: 0.084227,\n",
      "D(x): 0.974, D(G(z)): 0.072\n",
      "2019-04-10 00:56:47,758 root         INFO     Train Epoch: 48 [5632/8000 (70%)]\tTotal Loss: 0.319818\n",
      "Reconstruction: 0.229922, Regularization: 0.004816, Discriminator: 0.004225; Generator: 0.080856,\n",
      "D(x): 0.956, D(G(z)): 0.079\n",
      "2019-04-10 00:56:47,869 root         INFO     Train Epoch: 48 [6144/8000 (77%)]\tTotal Loss: 0.322526\n",
      "Reconstruction: 0.230639, Regularization: 0.004519, Discriminator: 0.004802; Generator: 0.082566,\n",
      "D(x): 0.946, D(G(z)): 0.074\n",
      "2019-04-10 00:56:47,981 root         INFO     Train Epoch: 48 [6656/8000 (83%)]\tTotal Loss: 0.328004\n",
      "Reconstruction: 0.233687, Regularization: 0.004908, Discriminator: 0.008395; Generator: 0.081014,\n",
      "D(x): 0.902, D(G(z)): 0.076\n",
      "2019-04-10 00:56:48,092 root         INFO     Train Epoch: 48 [7168/8000 (90%)]\tTotal Loss: 0.334368\n",
      "Reconstruction: 0.238784, Regularization: 0.005355, Discriminator: 0.005765; Generator: 0.084465,\n",
      "D(x): 0.928, D(G(z)): 0.070\n",
      "2019-04-10 00:56:48,203 root         INFO     Train Epoch: 48 [7680/8000 (96%)]\tTotal Loss: 0.327066\n",
      "Reconstruction: 0.236364, Regularization: 0.005226, Discriminator: 0.003523; Generator: 0.081953,\n",
      "D(x): 0.969, D(G(z)): 0.076\n",
      "2019-04-10 00:56:48,285 root         INFO     ====> Epoch: 48 Average loss: 0.3245\n",
      "2019-04-10 00:56:48,312 root         INFO     Train Epoch: 49 [0/8000 (0%)]\tTotal Loss: 0.333216\n",
      "Reconstruction: 0.240268, Regularization: 0.005122, Discriminator: 0.003014; Generator: 0.084813,\n",
      "D(x): 0.977, D(G(z)): 0.069\n",
      "2019-04-10 00:56:48,423 root         INFO     Train Epoch: 49 [512/8000 (6%)]\tTotal Loss: 0.320690\n",
      "Reconstruction: 0.230445, Regularization: 0.004364, Discriminator: 0.006837; Generator: 0.079043,\n",
      "D(x): 0.913, D(G(z)): 0.084\n",
      "2019-04-10 00:56:48,535 root         INFO     Train Epoch: 49 [1024/8000 (13%)]\tTotal Loss: 0.322966\n",
      "Reconstruction: 0.235253, Regularization: 0.003669, Discriminator: 0.003713; Generator: 0.080331,\n",
      "D(x): 0.967, D(G(z)): 0.079\n",
      "2019-04-10 00:56:48,646 root         INFO     Train Epoch: 49 [1536/8000 (19%)]\tTotal Loss: 0.321791\n",
      "Reconstruction: 0.235679, Regularization: 0.003605, Discriminator: 0.003976; Generator: 0.078532,\n",
      "D(x): 0.964, D(G(z)): 0.084\n",
      "2019-04-10 00:56:48,757 root         INFO     Train Epoch: 49 [2048/8000 (26%)]\tTotal Loss: 0.325626\n",
      "Reconstruction: 0.234878, Regularization: 0.003477, Discriminator: 0.005754; Generator: 0.081517,\n",
      "D(x): 0.940, D(G(z)): 0.078\n",
      "2019-04-10 00:56:48,867 root         INFO     Train Epoch: 49 [2560/8000 (32%)]\tTotal Loss: 0.329161\n",
      "Reconstruction: 0.237320, Regularization: 0.004392, Discriminator: 0.005128; Generator: 0.082321,\n",
      "D(x): 0.949, D(G(z)): 0.075\n",
      "2019-04-10 00:56:48,977 root         INFO     Train Epoch: 49 [3072/8000 (38%)]\tTotal Loss: 0.325071\n",
      "Reconstruction: 0.236192, Regularization: 0.003264, Discriminator: 0.004137; Generator: 0.081478,\n",
      "D(x): 0.956, D(G(z)): 0.079\n",
      "2019-04-10 00:56:49,087 root         INFO     Train Epoch: 49 [3584/8000 (45%)]\tTotal Loss: 0.333086\n",
      "Reconstruction: 0.240413, Regularization: 0.004079, Discriminator: 0.007243; Generator: 0.081350,\n",
      "D(x): 0.926, D(G(z)): 0.077\n",
      "2019-04-10 00:56:49,196 root         INFO     Train Epoch: 49 [4096/8000 (51%)]\tTotal Loss: 0.331185\n",
      "Reconstruction: 0.242876, Regularization: 0.004608, Discriminator: 0.003395; Generator: 0.080306,\n",
      "D(x): 0.977, D(G(z)): 0.081\n",
      "2019-04-10 00:56:49,306 root         INFO     Train Epoch: 49 [4608/8000 (58%)]\tTotal Loss: 0.335277\n",
      "Reconstruction: 0.243844, Regularization: 0.005132, Discriminator: 0.007248; Generator: 0.079053,\n",
      "D(x): 0.914, D(G(z)): 0.083\n",
      "2019-04-10 00:56:49,416 root         INFO     Train Epoch: 49 [5120/8000 (64%)]\tTotal Loss: 0.330446\n",
      "Reconstruction: 0.240754, Regularization: 0.003272, Discriminator: 0.006037; Generator: 0.080382,\n",
      "D(x): 0.923, D(G(z)): 0.081\n",
      "2019-04-10 00:56:49,526 root         INFO     Train Epoch: 49 [5632/8000 (70%)]\tTotal Loss: 0.329471\n",
      "Reconstruction: 0.242569, Regularization: 0.003063, Discriminator: 0.005093; Generator: 0.078745,\n",
      "D(x): 0.938, D(G(z)): 0.085\n",
      "2019-04-10 00:56:49,636 root         INFO     Train Epoch: 49 [6144/8000 (77%)]\tTotal Loss: 0.327535\n",
      "Reconstruction: 0.240963, Regularization: 0.003244, Discriminator: 0.004722; Generator: 0.078607,\n",
      "D(x): 0.946, D(G(z)): 0.084\n",
      "2019-04-10 00:56:49,746 root         INFO     Train Epoch: 49 [6656/8000 (83%)]\tTotal Loss: 0.332156\n",
      "Reconstruction: 0.244302, Regularization: 0.004181, Discriminator: 0.005380; Generator: 0.078293,\n",
      "D(x): 0.927, D(G(z)): 0.084\n",
      "2019-04-10 00:56:49,856 root         INFO     Train Epoch: 49 [7168/8000 (90%)]\tTotal Loss: 0.337231\n",
      "Reconstruction: 0.246513, Regularization: 0.003600, Discriminator: 0.008864; Generator: 0.078254,\n",
      "D(x): 0.909, D(G(z)): 0.086\n",
      "2019-04-10 00:56:49,967 root         INFO     Train Epoch: 49 [7680/8000 (96%)]\tTotal Loss: 0.330045\n",
      "Reconstruction: 0.243534, Regularization: 0.002714, Discriminator: 0.004307; Generator: 0.079490,\n",
      "D(x): 0.963, D(G(z)): 0.084\n",
      "2019-04-10 00:56:50,047 root         INFO     ====> Epoch: 49 Average loss: 0.3288\n",
      "2019-04-10 00:56:50,074 root         INFO     Train Epoch: 50 [0/8000 (0%)]\tTotal Loss: 0.330782\n",
      "Reconstruction: 0.242185, Regularization: 0.003470, Discriminator: 0.004466; Generator: 0.080662,\n",
      "D(x): 0.943, D(G(z)): 0.078\n",
      "2019-04-10 00:56:50,186 root         INFO     Train Epoch: 50 [512/8000 (6%)]\tTotal Loss: 0.334166\n",
      "Reconstruction: 0.250553, Regularization: 0.002654, Discriminator: 0.005144; Generator: 0.075816,\n",
      "D(x): 0.963, D(G(z)): 0.091\n",
      "2019-04-10 00:56:50,298 root         INFO     Train Epoch: 50 [1024/8000 (13%)]\tTotal Loss: 0.331535\n",
      "Reconstruction: 0.247189, Regularization: 0.002263, Discriminator: 0.007546; Generator: 0.074537,\n",
      "D(x): 0.926, D(G(z)): 0.099\n",
      "2019-04-10 00:56:50,409 root         INFO     Train Epoch: 50 [1536/8000 (19%)]\tTotal Loss: 0.331257\n",
      "Reconstruction: 0.246728, Regularization: 0.003071, Discriminator: 0.006166; Generator: 0.075292,\n",
      "D(x): 0.924, D(G(z)): 0.094\n",
      "2019-04-10 00:56:50,520 root         INFO     Train Epoch: 50 [2048/8000 (26%)]\tTotal Loss: 0.337312\n",
      "Reconstruction: 0.251297, Regularization: 0.003386, Discriminator: 0.009665; Generator: 0.072964,\n",
      "D(x): 0.911, D(G(z)): 0.104\n",
      "2019-04-10 00:56:50,631 root         INFO     Train Epoch: 50 [2560/8000 (32%)]\tTotal Loss: 0.339083\n",
      "Reconstruction: 0.254752, Regularization: 0.003204, Discriminator: 0.004494; Generator: 0.076634,\n",
      "D(x): 0.960, D(G(z)): 0.090\n",
      "2019-04-10 00:56:50,742 root         INFO     Train Epoch: 50 [3072/8000 (38%)]\tTotal Loss: 0.334889\n",
      "Reconstruction: 0.251748, Regularization: 0.003447, Discriminator: 0.004782; Generator: 0.074912,\n",
      "D(x): 0.953, D(G(z)): 0.094\n",
      "2019-04-10 00:56:50,853 root         INFO     Train Epoch: 50 [3584/8000 (45%)]\tTotal Loss: 0.333939\n",
      "Reconstruction: 0.253133, Regularization: 0.004044, Discriminator: 0.006177; Generator: 0.070584,\n",
      "D(x): 0.940, D(G(z)): 0.112\n",
      "2019-04-10 00:56:50,965 root         INFO     Train Epoch: 50 [4096/8000 (51%)]\tTotal Loss: 0.343618\n",
      "Reconstruction: 0.255484, Regularization: 0.003072, Discriminator: 0.010042; Generator: 0.075021,\n",
      "D(x): 0.882, D(G(z)): 0.095\n",
      "2019-04-10 00:56:51,076 root         INFO     Train Epoch: 50 [4608/8000 (58%)]\tTotal Loss: 0.344206\n",
      "Reconstruction: 0.254338, Regularization: 0.003038, Discriminator: 0.011337; Generator: 0.075493,\n",
      "D(x): 0.847, D(G(z)): 0.094\n",
      "2019-04-10 00:56:51,187 root         INFO     Train Epoch: 50 [5120/8000 (64%)]\tTotal Loss: 0.336049\n",
      "Reconstruction: 0.255089, Regularization: 0.002838, Discriminator: 0.007524; Generator: 0.070598,\n",
      "D(x): 0.918, D(G(z)): 0.109\n",
      "2019-04-10 00:56:51,298 root         INFO     Train Epoch: 50 [5632/8000 (70%)]\tTotal Loss: 0.343568\n",
      "Reconstruction: 0.253351, Regularization: 0.002642, Discriminator: 0.011903; Generator: 0.075672,\n",
      "D(x): 0.848, D(G(z)): 0.093\n",
      "2019-04-10 00:56:51,410 root         INFO     Train Epoch: 50 [6144/8000 (77%)]\tTotal Loss: 0.343179\n",
      "Reconstruction: 0.258289, Regularization: 0.002087, Discriminator: 0.007978; Generator: 0.074825,\n",
      "D(x): 0.923, D(G(z)): 0.095\n",
      "2019-04-10 00:56:51,521 root         INFO     Train Epoch: 50 [6656/8000 (83%)]\tTotal Loss: 0.344419\n",
      "Reconstruction: 0.259656, Regularization: 0.003282, Discriminator: 0.005734; Generator: 0.075747,\n",
      "D(x): 0.948, D(G(z)): 0.092\n",
      "2019-04-10 00:56:51,632 root         INFO     Train Epoch: 50 [7168/8000 (90%)]\tTotal Loss: 0.342013\n",
      "Reconstruction: 0.261787, Regularization: 0.002035, Discriminator: 0.008123; Generator: 0.070068,\n",
      "D(x): 0.935, D(G(z)): 0.111\n",
      "2019-04-10 00:56:51,744 root         INFO     Train Epoch: 50 [7680/8000 (96%)]\tTotal Loss: 0.340170\n",
      "Reconstruction: 0.257679, Regularization: 0.002039, Discriminator: 0.011948; Generator: 0.068504,\n",
      "D(x): 0.903, D(G(z)): 0.115\n",
      "2019-04-10 00:56:51,825 root         INFO     ====> Epoch: 50 Average loss: 0.3374\n",
      "2019-04-10 00:56:51,852 root         INFO     Train Epoch: 51 [0/8000 (0%)]\tTotal Loss: 0.345986\n",
      "Reconstruction: 0.259050, Regularization: 0.002225, Discriminator: 0.014945; Generator: 0.069767,\n",
      "D(x): 0.876, D(G(z)): 0.115\n",
      "2019-04-10 00:56:51,963 root         INFO     Train Epoch: 51 [512/8000 (6%)]\tTotal Loss: 0.346316\n",
      "Reconstruction: 0.261460, Regularization: 0.003669, Discriminator: 0.013935; Generator: 0.067252,\n",
      "D(x): 0.853, D(G(z)): 0.126\n",
      "2019-04-10 00:56:52,073 root         INFO     Train Epoch: 51 [1024/8000 (13%)]\tTotal Loss: 0.340226\n",
      "Reconstruction: 0.259414, Regularization: 0.002802, Discriminator: 0.008934; Generator: 0.069076,\n",
      "D(x): 0.914, D(G(z)): 0.117\n",
      "2019-04-10 00:56:52,184 root         INFO     Train Epoch: 51 [1536/8000 (19%)]\tTotal Loss: 0.341280\n",
      "Reconstruction: 0.262755, Regularization: 0.002227, Discriminator: 0.010754; Generator: 0.065544,\n",
      "D(x): 0.881, D(G(z)): 0.128\n",
      "2019-04-10 00:56:52,295 root         INFO     Train Epoch: 51 [2048/8000 (26%)]\tTotal Loss: 0.341307\n",
      "Reconstruction: 0.262508, Regularization: 0.002094, Discriminator: 0.008040; Generator: 0.068664,\n",
      "D(x): 0.912, D(G(z)): 0.116\n",
      "2019-04-10 00:56:52,406 root         INFO     Train Epoch: 51 [2560/8000 (32%)]\tTotal Loss: 0.345375\n",
      "Reconstruction: 0.264539, Regularization: 0.002538, Discriminator: 0.010511; Generator: 0.067786,\n",
      "D(x): 0.891, D(G(z)): 0.120\n",
      "2019-04-10 00:56:52,516 root         INFO     Train Epoch: 51 [3072/8000 (38%)]\tTotal Loss: 0.345417\n",
      "Reconstruction: 0.267419, Regularization: 0.002432, Discriminator: 0.007847; Generator: 0.067720,\n",
      "D(x): 0.926, D(G(z)): 0.121\n",
      "2019-04-10 00:56:52,626 root         INFO     Train Epoch: 51 [3584/8000 (45%)]\tTotal Loss: 0.352756\n",
      "Reconstruction: 0.271146, Regularization: 0.002657, Discriminator: 0.010289; Generator: 0.068664,\n",
      "D(x): 0.891, D(G(z)): 0.116\n",
      "2019-04-10 00:56:52,737 root         INFO     Train Epoch: 51 [4096/8000 (51%)]\tTotal Loss: 0.346740\n",
      "Reconstruction: 0.266241, Regularization: 0.001681, Discriminator: 0.006717; Generator: 0.072101,\n",
      "D(x): 0.926, D(G(z)): 0.103\n",
      "2019-04-10 00:56:52,846 root         INFO     Train Epoch: 51 [4608/8000 (58%)]\tTotal Loss: 0.343507\n",
      "Reconstruction: 0.268755, Regularization: 0.001505, Discriminator: 0.006824; Generator: 0.066422,\n",
      "D(x): 0.934, D(G(z)): 0.124\n",
      "2019-04-10 00:56:52,956 root         INFO     Train Epoch: 51 [5120/8000 (64%)]\tTotal Loss: 0.346243\n",
      "Reconstruction: 0.268897, Regularization: 0.002842, Discriminator: 0.008543; Generator: 0.065961,\n",
      "D(x): 0.914, D(G(z)): 0.126\n",
      "2019-04-10 00:56:53,065 root         INFO     Train Epoch: 51 [5632/8000 (70%)]\tTotal Loss: 0.356408\n",
      "Reconstruction: 0.272479, Regularization: 0.002439, Discriminator: 0.015539; Generator: 0.065952,\n",
      "D(x): 0.830, D(G(z)): 0.126\n",
      "2019-04-10 00:56:53,174 root         INFO     Train Epoch: 51 [6144/8000 (77%)]\tTotal Loss: 0.355457\n",
      "Reconstruction: 0.270837, Regularization: 0.003344, Discriminator: 0.016689; Generator: 0.064587,\n",
      "D(x): 0.842, D(G(z)): 0.132\n",
      "2019-04-10 00:56:53,286 root         INFO     Train Epoch: 51 [6656/8000 (83%)]\tTotal Loss: 0.347171\n",
      "Reconstruction: 0.274410, Regularization: 0.001568, Discriminator: 0.006746; Generator: 0.064447,\n",
      "D(x): 0.938, D(G(z)): 0.133\n",
      "2019-04-10 00:56:53,399 root         INFO     Train Epoch: 51 [7168/8000 (90%)]\tTotal Loss: 0.351478\n",
      "Reconstruction: 0.272642, Regularization: 0.001873, Discriminator: 0.014393; Generator: 0.062569,\n",
      "D(x): 0.862, D(G(z)): 0.140\n",
      "2019-04-10 00:56:53,512 root         INFO     Train Epoch: 51 [7680/8000 (96%)]\tTotal Loss: 0.360083\n",
      "Reconstruction: 0.277445, Regularization: 0.002463, Discriminator: 0.015811; Generator: 0.064364,\n",
      "D(x): 0.823, D(G(z)): 0.132\n",
      "2019-04-10 00:56:53,594 root         INFO     ====> Epoch: 51 Average loss: 0.3477\n",
      "2019-04-10 00:56:53,621 root         INFO     Train Epoch: 52 [0/8000 (0%)]\tTotal Loss: 0.350595\n",
      "Reconstruction: 0.276557, Regularization: 0.001929, Discriminator: 0.009134; Generator: 0.062975,\n",
      "D(x): 0.905, D(G(z)): 0.138\n",
      "2019-04-10 00:56:53,732 root         INFO     Train Epoch: 52 [512/8000 (6%)]\tTotal Loss: 0.359811\n",
      "Reconstruction: 0.277620, Regularization: 0.001279, Discriminator: 0.015429; Generator: 0.065483,\n",
      "D(x): 0.874, D(G(z)): 0.127\n",
      "2019-04-10 00:56:53,843 root         INFO     Train Epoch: 52 [1024/8000 (13%)]\tTotal Loss: 0.349781\n",
      "Reconstruction: 0.278398, Regularization: 0.002016, Discriminator: 0.005856; Generator: 0.063512,\n",
      "D(x): 0.965, D(G(z)): 0.137\n",
      "2019-04-10 00:56:53,954 root         INFO     Train Epoch: 52 [1536/8000 (19%)]\tTotal Loss: 0.354662\n",
      "Reconstruction: 0.280111, Regularization: 0.001612, Discriminator: 0.009103; Generator: 0.063836,\n",
      "D(x): 0.901, D(G(z)): 0.134\n",
      "2019-04-10 00:56:54,065 root         INFO     Train Epoch: 52 [2048/8000 (26%)]\tTotal Loss: 0.353715\n",
      "Reconstruction: 0.281513, Regularization: 0.001233, Discriminator: 0.012114; Generator: 0.058855,\n",
      "D(x): 0.885, D(G(z)): 0.157\n",
      "2019-04-10 00:56:54,174 root         INFO     Train Epoch: 52 [2560/8000 (32%)]\tTotal Loss: 0.360630\n",
      "Reconstruction: 0.281882, Regularization: 0.002979, Discriminator: 0.016369; Generator: 0.059399,\n",
      "D(x): 0.836, D(G(z)): 0.156\n",
      "2019-04-10 00:56:54,284 root         INFO     Train Epoch: 52 [3072/8000 (38%)]\tTotal Loss: 0.354740\n",
      "Reconstruction: 0.282016, Regularization: 0.001618, Discriminator: 0.010071; Generator: 0.061035,\n",
      "D(x): 0.893, D(G(z)): 0.147\n",
      "2019-04-10 00:56:54,395 root         INFO     Train Epoch: 52 [3584/8000 (45%)]\tTotal Loss: 0.371310\n",
      "Reconstruction: 0.286547, Regularization: 0.002705, Discriminator: 0.020294; Generator: 0.061764,\n",
      "D(x): 0.810, D(G(z)): 0.143\n",
      "2019-04-10 00:56:54,507 root         INFO     Train Epoch: 52 [4096/8000 (51%)]\tTotal Loss: 0.359028\n",
      "Reconstruction: 0.285196, Regularization: 0.001988, Discriminator: 0.016462; Generator: 0.055381,\n",
      "D(x): 0.852, D(G(z)): 0.174\n",
      "2019-04-10 00:56:54,617 root         INFO     Train Epoch: 52 [4608/8000 (58%)]\tTotal Loss: 0.364113\n",
      "Reconstruction: 0.284654, Regularization: 0.001608, Discriminator: 0.015728; Generator: 0.062124,\n",
      "D(x): 0.841, D(G(z)): 0.142\n",
      "2019-04-10 00:56:54,727 root         INFO     Train Epoch: 52 [5120/8000 (64%)]\tTotal Loss: 0.366798\n",
      "Reconstruction: 0.288608, Regularization: 0.002155, Discriminator: 0.017152; Generator: 0.058883,\n",
      "D(x): 0.815, D(G(z)): 0.156\n",
      "2019-04-10 00:56:54,838 root         INFO     Train Epoch: 52 [5632/8000 (70%)]\tTotal Loss: 0.354452\n",
      "Reconstruction: 0.287125, Regularization: 0.001297, Discriminator: 0.008044; Generator: 0.057986,\n",
      "D(x): 0.929, D(G(z)): 0.161\n",
      "2019-04-10 00:56:54,948 root         INFO     Train Epoch: 52 [6144/8000 (77%)]\tTotal Loss: 0.358943\n",
      "Reconstruction: 0.290406, Regularization: 0.001510, Discriminator: 0.011509; Generator: 0.055518,\n",
      "D(x): 0.866, D(G(z)): 0.174\n",
      "2019-04-10 00:56:55,058 root         INFO     Train Epoch: 52 [6656/8000 (83%)]\tTotal Loss: 0.359985\n",
      "Reconstruction: 0.291252, Regularization: 0.001379, Discriminator: 0.012213; Generator: 0.055141,\n",
      "D(x): 0.890, D(G(z)): 0.179\n",
      "2019-04-10 00:56:55,168 root         INFO     Train Epoch: 52 [7168/8000 (90%)]\tTotal Loss: 0.366145\n",
      "Reconstruction: 0.291837, Regularization: 0.002228, Discriminator: 0.013352; Generator: 0.058728,\n",
      "D(x): 0.867, D(G(z)): 0.157\n",
      "2019-04-10 00:56:55,278 root         INFO     Train Epoch: 52 [7680/8000 (96%)]\tTotal Loss: 0.365096\n",
      "Reconstruction: 0.291140, Regularization: 0.001818, Discriminator: 0.016462; Generator: 0.055676,\n",
      "D(x): 0.829, D(G(z)): 0.173\n",
      "2019-04-10 00:56:55,358 root         INFO     ====> Epoch: 52 Average loss: 0.3603\n",
      "2019-04-10 00:56:55,385 root         INFO     Train Epoch: 53 [0/8000 (0%)]\tTotal Loss: 0.375584\n",
      "Reconstruction: 0.293755, Regularization: 0.002479, Discriminator: 0.021761; Generator: 0.057589,\n",
      "D(x): 0.782, D(G(z)): 0.164\n",
      "2019-04-10 00:56:55,494 root         INFO     Train Epoch: 53 [512/8000 (6%)]\tTotal Loss: 0.367493\n",
      "Reconstruction: 0.294115, Regularization: 0.002202, Discriminator: 0.017583; Generator: 0.053594,\n",
      "D(x): 0.841, D(G(z)): 0.187\n",
      "2019-04-10 00:56:55,605 root         INFO     Train Epoch: 53 [1024/8000 (13%)]\tTotal Loss: 0.380644\n",
      "Reconstruction: 0.297574, Regularization: 0.003492, Discriminator: 0.022606; Generator: 0.056973,\n",
      "D(x): 0.791, D(G(z)): 0.168\n",
      "2019-04-10 00:56:55,715 root         INFO     Train Epoch: 53 [1536/8000 (19%)]\tTotal Loss: 0.377713\n",
      "Reconstruction: 0.296560, Regularization: 0.002139, Discriminator: 0.020891; Generator: 0.058122,\n",
      "D(x): 0.730, D(G(z)): 0.159\n",
      "2019-04-10 00:56:55,825 root         INFO     Train Epoch: 53 [2048/8000 (26%)]\tTotal Loss: 0.375195\n",
      "Reconstruction: 0.298844, Regularization: 0.002696, Discriminator: 0.018218; Generator: 0.055437,\n",
      "D(x): 0.821, D(G(z)): 0.174\n",
      "2019-04-10 00:56:55,935 root         INFO     Train Epoch: 53 [2560/8000 (32%)]\tTotal Loss: 0.368299\n",
      "Reconstruction: 0.299418, Regularization: 0.001561, Discriminator: 0.015615; Generator: 0.051704,\n",
      "D(x): 0.843, D(G(z)): 0.195\n",
      "2019-04-10 00:56:56,045 root         INFO     Train Epoch: 53 [3072/8000 (38%)]\tTotal Loss: 0.368553\n",
      "Reconstruction: 0.301661, Regularization: 0.001603, Discriminator: 0.011373; Generator: 0.053916,\n",
      "D(x): 0.893, D(G(z)): 0.183\n",
      "2019-04-10 00:56:56,153 root         INFO     Train Epoch: 53 [3584/8000 (45%)]\tTotal Loss: 0.378011\n",
      "Reconstruction: 0.301398, Regularization: 0.001821, Discriminator: 0.021038; Generator: 0.053753,\n",
      "D(x): 0.764, D(G(z)): 0.182\n",
      "2019-04-10 00:56:56,262 root         INFO     Train Epoch: 53 [4096/8000 (51%)]\tTotal Loss: 0.365751\n",
      "Reconstruction: 0.302279, Regularization: 0.001557, Discriminator: 0.010314; Generator: 0.051600,\n",
      "D(x): 0.920, D(G(z)): 0.196\n",
      "2019-04-10 00:56:56,372 root         INFO     Train Epoch: 53 [4608/8000 (58%)]\tTotal Loss: 0.365799\n",
      "Reconstruction: 0.303144, Regularization: 0.001370, Discriminator: 0.011143; Generator: 0.050142,\n",
      "D(x): 0.917, D(G(z)): 0.206\n",
      "2019-04-10 00:56:56,481 root         INFO     Train Epoch: 53 [5120/8000 (64%)]\tTotal Loss: 0.374233\n",
      "Reconstruction: 0.304286, Regularization: 0.000947, Discriminator: 0.016479; Generator: 0.052521,\n",
      "D(x): 0.823, D(G(z)): 0.189\n",
      "2019-04-10 00:56:56,590 root         INFO     Train Epoch: 53 [5632/8000 (70%)]\tTotal Loss: 0.377239\n",
      "Reconstruction: 0.307000, Regularization: 0.001728, Discriminator: 0.019254; Generator: 0.049257,\n",
      "D(x): 0.854, D(G(z)): 0.210\n",
      "2019-04-10 00:56:56,700 root         INFO     Train Epoch: 53 [6144/8000 (77%)]\tTotal Loss: 0.380044\n",
      "Reconstruction: 0.307040, Regularization: 0.001491, Discriminator: 0.019314; Generator: 0.052198,\n",
      "D(x): 0.781, D(G(z)): 0.190\n",
      "2019-04-10 00:56:56,811 root         INFO     Train Epoch: 53 [6656/8000 (83%)]\tTotal Loss: 0.380064\n",
      "Reconstruction: 0.308020, Regularization: 0.001949, Discriminator: 0.022519; Generator: 0.047576,\n",
      "D(x): 0.768, D(G(z)): 0.222\n",
      "2019-04-10 00:56:56,923 root         INFO     Train Epoch: 53 [7168/8000 (90%)]\tTotal Loss: 0.375496\n",
      "Reconstruction: 0.309603, Regularization: 0.001910, Discriminator: 0.015819; Generator: 0.048164,\n",
      "D(x): 0.845, D(G(z)): 0.218\n",
      "2019-04-10 00:56:57,032 root         INFO     Train Epoch: 53 [7680/8000 (96%)]\tTotal Loss: 0.388726\n",
      "Reconstruction: 0.311028, Regularization: 0.002416, Discriminator: 0.025253; Generator: 0.050028,\n",
      "D(x): 0.736, D(G(z)): 0.206\n",
      "2019-04-10 00:56:57,112 root         INFO     ====> Epoch: 53 Average loss: 0.3748\n",
      "2019-04-10 00:56:57,139 root         INFO     Train Epoch: 54 [0/8000 (0%)]\tTotal Loss: 0.379578\n",
      "Reconstruction: 0.311889, Regularization: 0.001665, Discriminator: 0.014249; Generator: 0.051774,\n",
      "D(x): 0.847, D(G(z)): 0.194\n",
      "2019-04-10 00:56:57,250 root         INFO     Train Epoch: 54 [512/8000 (6%)]\tTotal Loss: 0.389565\n",
      "Reconstruction: 0.312477, Regularization: 0.002293, Discriminator: 0.024938; Generator: 0.049857,\n",
      "D(x): 0.759, D(G(z)): 0.206\n",
      "2019-04-10 00:56:57,361 root         INFO     Train Epoch: 54 [1024/8000 (13%)]\tTotal Loss: 0.394847\n",
      "Reconstruction: 0.313090, Regularization: 0.002072, Discriminator: 0.033448; Generator: 0.046238,\n",
      "D(x): 0.790, D(G(z)): 0.232\n",
      "2019-04-10 00:56:57,472 root         INFO     Train Epoch: 54 [1536/8000 (19%)]\tTotal Loss: 0.387383\n",
      "Reconstruction: 0.315417, Regularization: 0.002487, Discriminator: 0.021303; Generator: 0.048176,\n",
      "D(x): 0.804, D(G(z)): 0.220\n",
      "2019-04-10 00:56:57,583 root         INFO     Train Epoch: 54 [2048/8000 (26%)]\tTotal Loss: 0.392534\n",
      "Reconstruction: 0.317007, Regularization: 0.002396, Discriminator: 0.029814; Generator: 0.043317,\n",
      "D(x): 0.759, D(G(z)): 0.253\n",
      "2019-04-10 00:56:57,695 root         INFO     Train Epoch: 54 [2560/8000 (32%)]\tTotal Loss: 0.384324\n",
      "Reconstruction: 0.316401, Regularization: 0.002218, Discriminator: 0.022582; Generator: 0.043124,\n",
      "D(x): 0.783, D(G(z)): 0.259\n",
      "2019-04-10 00:56:57,807 root         INFO     Train Epoch: 54 [3072/8000 (38%)]\tTotal Loss: 0.395916\n",
      "Reconstruction: 0.317851, Regularization: 0.001559, Discriminator: 0.030460; Generator: 0.046046,\n",
      "D(x): 0.704, D(G(z)): 0.233\n",
      "2019-04-10 00:56:57,919 root         INFO     Train Epoch: 54 [3584/8000 (45%)]\tTotal Loss: 0.386362\n",
      "Reconstruction: 0.319315, Regularization: 0.001921, Discriminator: 0.021596; Generator: 0.043530,\n",
      "D(x): 0.783, D(G(z)): 0.253\n",
      "2019-04-10 00:56:58,031 root         INFO     Train Epoch: 54 [4096/8000 (51%)]\tTotal Loss: 0.386329\n",
      "Reconstruction: 0.319906, Regularization: 0.001401, Discriminator: 0.020886; Generator: 0.044135,\n",
      "D(x): 0.807, D(G(z)): 0.248\n",
      "2019-04-10 00:56:58,143 root         INFO     Train Epoch: 54 [4608/8000 (58%)]\tTotal Loss: 0.394358\n",
      "Reconstruction: 0.319136, Regularization: 0.001661, Discriminator: 0.026984; Generator: 0.046577,\n",
      "D(x): 0.723, D(G(z)): 0.228\n",
      "2019-04-10 00:56:58,255 root         INFO     Train Epoch: 54 [5120/8000 (64%)]\tTotal Loss: 0.387169\n",
      "Reconstruction: 0.321092, Regularization: 0.001761, Discriminator: 0.020622; Generator: 0.043694,\n",
      "D(x): 0.800, D(G(z)): 0.249\n",
      "2019-04-10 00:56:58,367 root         INFO     Train Epoch: 54 [5632/8000 (70%)]\tTotal Loss: 0.391186\n",
      "Reconstruction: 0.323149, Regularization: 0.001468, Discriminator: 0.024097; Generator: 0.042471,\n",
      "D(x): 0.740, D(G(z)): 0.261\n",
      "2019-04-10 00:56:58,479 root         INFO     Train Epoch: 54 [6144/8000 (77%)]\tTotal Loss: 0.390647\n",
      "Reconstruction: 0.323638, Regularization: 0.001306, Discriminator: 0.022121; Generator: 0.043582,\n",
      "D(x): 0.744, D(G(z)): 0.250\n",
      "2019-04-10 00:56:58,587 root         INFO     Train Epoch: 54 [6656/8000 (83%)]\tTotal Loss: 0.393612\n",
      "Reconstruction: 0.325163, Regularization: 0.001322, Discriminator: 0.025814; Generator: 0.041312,\n",
      "D(x): 0.745, D(G(z)): 0.269\n",
      "2019-04-10 00:56:58,697 root         INFO     Train Epoch: 54 [7168/8000 (90%)]\tTotal Loss: 0.396226\n",
      "Reconstruction: 0.327123, Regularization: 0.001929, Discriminator: 0.024287; Generator: 0.042887,\n",
      "D(x): 0.750, D(G(z)): 0.256\n",
      "2019-04-10 00:56:58,807 root         INFO     Train Epoch: 54 [7680/8000 (96%)]\tTotal Loss: 0.412553\n",
      "Reconstruction: 0.328846, Regularization: 0.002274, Discriminator: 0.038730; Generator: 0.042704,\n",
      "D(x): 0.618, D(G(z)): 0.258\n",
      "2019-04-10 00:56:58,886 root         INFO     ====> Epoch: 54 Average loss: 0.3913\n",
      "2019-04-10 00:56:58,914 root         INFO     Train Epoch: 55 [0/8000 (0%)]\tTotal Loss: 0.393683\n",
      "Reconstruction: 0.328744, Regularization: 0.001363, Discriminator: 0.021435; Generator: 0.042140,\n",
      "D(x): 0.780, D(G(z)): 0.261\n",
      "2019-04-10 00:56:59,027 root         INFO     Train Epoch: 55 [512/8000 (6%)]\tTotal Loss: 0.405743\n",
      "Reconstruction: 0.329842, Regularization: 0.001407, Discriminator: 0.036093; Generator: 0.038402,\n",
      "D(x): 0.668, D(G(z)): 0.294\n",
      "2019-04-10 00:56:59,141 root         INFO     Train Epoch: 55 [1024/8000 (13%)]\tTotal Loss: 0.404111\n",
      "Reconstruction: 0.331848, Regularization: 0.002421, Discriminator: 0.030842; Generator: 0.039000,\n",
      "D(x): 0.749, D(G(z)): 0.290\n",
      "2019-04-10 00:56:59,254 root         INFO     Train Epoch: 55 [1536/8000 (19%)]\tTotal Loss: 0.406087\n",
      "Reconstruction: 0.332273, Regularization: 0.002054, Discriminator: 0.032883; Generator: 0.038877,\n",
      "D(x): 0.693, D(G(z)): 0.290\n",
      "2019-04-10 00:56:59,366 root         INFO     Train Epoch: 55 [2048/8000 (26%)]\tTotal Loss: 0.410719\n",
      "Reconstruction: 0.333208, Regularization: 0.001717, Discriminator: 0.035594; Generator: 0.040200,\n",
      "D(x): 0.656, D(G(z)): 0.278\n",
      "2019-04-10 00:56:59,478 root         INFO     Train Epoch: 55 [2560/8000 (32%)]\tTotal Loss: 0.409249\n",
      "Reconstruction: 0.334614, Regularization: 0.001486, Discriminator: 0.035580; Generator: 0.037569,\n",
      "D(x): 0.677, D(G(z)): 0.304\n",
      "2019-04-10 00:56:59,591 root         INFO     Train Epoch: 55 [3072/8000 (38%)]\tTotal Loss: 0.399714\n",
      "Reconstruction: 0.334794, Regularization: 0.001879, Discriminator: 0.024253; Generator: 0.038788,\n",
      "D(x): 0.767, D(G(z)): 0.292\n",
      "2019-04-10 00:56:59,703 root         INFO     Train Epoch: 55 [3584/8000 (45%)]\tTotal Loss: 0.406810\n",
      "Reconstruction: 0.335697, Regularization: 0.001376, Discriminator: 0.031521; Generator: 0.038216,\n",
      "D(x): 0.706, D(G(z)): 0.296\n",
      "2019-04-10 00:56:59,815 root         INFO     Train Epoch: 55 [4096/8000 (51%)]\tTotal Loss: 0.413375\n",
      "Reconstruction: 0.336367, Regularization: 0.002873, Discriminator: 0.037228; Generator: 0.036907,\n",
      "D(x): 0.618, D(G(z)): 0.308\n",
      "2019-04-10 00:56:59,928 root         INFO     Train Epoch: 55 [4608/8000 (58%)]\tTotal Loss: 0.409018\n",
      "Reconstruction: 0.336871, Regularization: 0.001907, Discriminator: 0.032219; Generator: 0.038021,\n",
      "D(x): 0.681, D(G(z)): 0.298\n",
      "2019-04-10 00:57:00,040 root         INFO     Train Epoch: 55 [5120/8000 (64%)]\tTotal Loss: 0.399834\n",
      "Reconstruction: 0.337756, Regularization: 0.001580, Discriminator: 0.026829; Generator: 0.033670,\n",
      "D(x): 0.790, D(G(z)): 0.342\n",
      "2019-04-10 00:57:00,153 root         INFO     Train Epoch: 55 [5632/8000 (70%)]\tTotal Loss: 0.412075\n",
      "Reconstruction: 0.339551, Regularization: 0.001755, Discriminator: 0.035108; Generator: 0.035661,\n",
      "D(x): 0.652, D(G(z)): 0.321\n",
      "2019-04-10 00:57:00,265 root         INFO     Train Epoch: 55 [6144/8000 (77%)]\tTotal Loss: 0.421101\n",
      "Reconstruction: 0.340518, Regularization: 0.001592, Discriminator: 0.042793; Generator: 0.036198,\n",
      "D(x): 0.554, D(G(z)): 0.316\n",
      "2019-04-10 00:57:00,377 root         INFO     Train Epoch: 55 [6656/8000 (83%)]\tTotal Loss: 0.406060\n",
      "Reconstruction: 0.340824, Regularization: 0.001363, Discriminator: 0.028563; Generator: 0.035310,\n",
      "D(x): 0.695, D(G(z)): 0.325\n",
      "2019-04-10 00:57:00,489 root         INFO     Train Epoch: 55 [7168/8000 (90%)]\tTotal Loss: 0.414850\n",
      "Reconstruction: 0.342043, Regularization: 0.001647, Discriminator: 0.034849; Generator: 0.036311,\n",
      "D(x): 0.710, D(G(z)): 0.315\n",
      "2019-04-10 00:57:00,601 root         INFO     Train Epoch: 55 [7680/8000 (96%)]\tTotal Loss: 0.412549\n",
      "Reconstruction: 0.342507, Regularization: 0.001656, Discriminator: 0.034812; Generator: 0.033573,\n",
      "D(x): 0.662, D(G(z)): 0.343\n",
      "2019-04-10 00:57:00,681 root         INFO     ====> Epoch: 55 Average loss: 0.4087\n",
      "2019-04-10 00:57:00,708 root         INFO     Train Epoch: 56 [0/8000 (0%)]\tTotal Loss: 0.423266\n",
      "Reconstruction: 0.343374, Regularization: 0.001299, Discriminator: 0.043388; Generator: 0.035205,\n",
      "D(x): 0.603, D(G(z)): 0.326\n",
      "2019-04-10 00:57:00,820 root         INFO     Train Epoch: 56 [512/8000 (6%)]\tTotal Loss: 0.420923\n",
      "Reconstruction: 0.345218, Regularization: 0.001804, Discriminator: 0.041419; Generator: 0.032482,\n",
      "D(x): 0.620, D(G(z)): 0.355\n",
      "2019-04-10 00:57:00,933 root         INFO     Train Epoch: 56 [1024/8000 (13%)]\tTotal Loss: 0.428107\n",
      "Reconstruction: 0.345097, Regularization: 0.001317, Discriminator: 0.047268; Generator: 0.034424,\n",
      "D(x): 0.603, D(G(z)): 0.334\n",
      "2019-04-10 00:57:01,046 root         INFO     Train Epoch: 56 [1536/8000 (19%)]\tTotal Loss: 0.421656\n",
      "Reconstruction: 0.347595, Regularization: 0.001955, Discriminator: 0.038526; Generator: 0.033580,\n",
      "D(x): 0.684, D(G(z)): 0.343\n",
      "2019-04-10 00:57:01,158 root         INFO     Train Epoch: 56 [2048/8000 (26%)]\tTotal Loss: 0.420398\n",
      "Reconstruction: 0.348048, Regularization: 0.001475, Discriminator: 0.036436; Generator: 0.034439,\n",
      "D(x): 0.617, D(G(z)): 0.333\n",
      "2019-04-10 00:57:01,271 root         INFO     Train Epoch: 56 [2560/8000 (32%)]\tTotal Loss: 0.420223\n",
      "Reconstruction: 0.349696, Regularization: 0.001890, Discriminator: 0.035993; Generator: 0.032644,\n",
      "D(x): 0.706, D(G(z)): 0.353\n",
      "2019-04-10 00:57:01,384 root         INFO     Train Epoch: 56 [3072/8000 (38%)]\tTotal Loss: 0.423730\n",
      "Reconstruction: 0.349668, Regularization: 0.001663, Discriminator: 0.040863; Generator: 0.031535,\n",
      "D(x): 0.566, D(G(z)): 0.365\n",
      "2019-04-10 00:57:01,496 root         INFO     Train Epoch: 56 [3584/8000 (45%)]\tTotal Loss: 0.416404\n",
      "Reconstruction: 0.350669, Regularization: 0.002073, Discriminator: 0.031755; Generator: 0.031907,\n",
      "D(x): 0.656, D(G(z)): 0.361\n",
      "2019-04-10 00:57:01,609 root         INFO     Train Epoch: 56 [4096/8000 (51%)]\tTotal Loss: 0.429532\n",
      "Reconstruction: 0.352272, Regularization: 0.001662, Discriminator: 0.043373; Generator: 0.032225,\n",
      "D(x): 0.596, D(G(z)): 0.358\n",
      "2019-04-10 00:57:01,721 root         INFO     Train Epoch: 56 [4608/8000 (58%)]\tTotal Loss: 0.424773\n",
      "Reconstruction: 0.352383, Regularization: 0.001196, Discriminator: 0.039605; Generator: 0.031589,\n",
      "D(x): 0.600, D(G(z)): 0.365\n",
      "2019-04-10 00:57:01,833 root         INFO     Train Epoch: 56 [5120/8000 (64%)]\tTotal Loss: 0.433983\n",
      "Reconstruction: 0.353662, Regularization: 0.001362, Discriminator: 0.046577; Generator: 0.032381,\n",
      "D(x): 0.511, D(G(z)): 0.356\n",
      "2019-04-10 00:57:01,946 root         INFO     Train Epoch: 56 [5632/8000 (70%)]\tTotal Loss: 0.431208\n",
      "Reconstruction: 0.354258, Regularization: 0.001344, Discriminator: 0.045430; Generator: 0.030176,\n",
      "D(x): 0.584, D(G(z)): 0.381\n",
      "2019-04-10 00:57:02,058 root         INFO     Train Epoch: 56 [6144/8000 (77%)]\tTotal Loss: 0.423961\n",
      "Reconstruction: 0.355014, Regularization: 0.001466, Discriminator: 0.036639; Generator: 0.030842,\n",
      "D(x): 0.616, D(G(z)): 0.373\n",
      "2019-04-10 00:57:02,171 root         INFO     Train Epoch: 56 [6656/8000 (83%)]\tTotal Loss: 0.436196\n",
      "Reconstruction: 0.355973, Regularization: 0.001446, Discriminator: 0.048244; Generator: 0.030533,\n",
      "D(x): 0.530, D(G(z)): 0.377\n",
      "2019-04-10 00:57:02,282 root         INFO     Train Epoch: 56 [7168/8000 (90%)]\tTotal Loss: 0.445513\n",
      "Reconstruction: 0.356802, Regularization: 0.001405, Discriminator: 0.057519; Generator: 0.029786,\n",
      "D(x): 0.528, D(G(z)): 0.387\n",
      "2019-04-10 00:57:02,394 root         INFO     Train Epoch: 56 [7680/8000 (96%)]\tTotal Loss: 0.416276\n",
      "Reconstruction: 0.357540, Regularization: 0.001477, Discriminator: 0.028924; Generator: 0.028335,\n",
      "D(x): 0.751, D(G(z)): 0.405\n",
      "2019-04-10 00:57:02,475 root         INFO     ====> Epoch: 56 Average loss: 0.4258\n",
      "2019-04-10 00:57:02,503 root         INFO     Train Epoch: 57 [0/8000 (0%)]\tTotal Loss: 0.439361\n",
      "Reconstruction: 0.357991, Regularization: 0.001267, Discriminator: 0.050603; Generator: 0.029500,\n",
      "D(x): 0.606, D(G(z)): 0.390\n",
      "2019-04-10 00:57:02,615 root         INFO     Train Epoch: 57 [512/8000 (6%)]\tTotal Loss: 0.434981\n",
      "Reconstruction: 0.358712, Regularization: 0.001897, Discriminator: 0.044991; Generator: 0.029382,\n",
      "D(x): 0.597, D(G(z)): 0.392\n",
      "2019-04-10 00:57:02,725 root         INFO     Train Epoch: 57 [1024/8000 (13%)]\tTotal Loss: 0.439955\n",
      "Reconstruction: 0.359294, Regularization: 0.002179, Discriminator: 0.049943; Generator: 0.028540,\n",
      "D(x): 0.519, D(G(z)): 0.402\n",
      "2019-04-10 00:57:02,835 root         INFO     Train Epoch: 57 [1536/8000 (19%)]\tTotal Loss: 0.449588\n",
      "Reconstruction: 0.359979, Regularization: 0.001410, Discriminator: 0.060326; Generator: 0.027873,\n",
      "D(x): 0.476, D(G(z)): 0.410\n",
      "2019-04-10 00:57:02,944 root         INFO     Train Epoch: 57 [2048/8000 (26%)]\tTotal Loss: 0.440447\n",
      "Reconstruction: 0.360636, Regularization: 0.001163, Discriminator: 0.049851; Generator: 0.028797,\n",
      "D(x): 0.475, D(G(z)): 0.399\n",
      "2019-04-10 00:57:03,055 root         INFO     Train Epoch: 57 [2560/8000 (32%)]\tTotal Loss: 0.431908\n",
      "Reconstruction: 0.361518, Regularization: 0.001542, Discriminator: 0.041248; Generator: 0.027600,\n",
      "D(x): 0.600, D(G(z)): 0.414\n",
      "2019-04-10 00:57:03,165 root         INFO     Train Epoch: 57 [3072/8000 (38%)]\tTotal Loss: 0.438673\n",
      "Reconstruction: 0.362596, Regularization: 0.001753, Discriminator: 0.045483; Generator: 0.028841,\n",
      "D(x): 0.511, D(G(z)): 0.398\n",
      "2019-04-10 00:57:03,275 root         INFO     Train Epoch: 57 [3584/8000 (45%)]\tTotal Loss: 0.438588\n",
      "Reconstruction: 0.363309, Regularization: 0.001378, Discriminator: 0.046360; Generator: 0.027541,\n",
      "D(x): 0.566, D(G(z)): 0.415\n",
      "2019-04-10 00:57:03,384 root         INFO     Train Epoch: 57 [4096/8000 (51%)]\tTotal Loss: 0.441230\n",
      "Reconstruction: 0.363969, Regularization: 0.001191, Discriminator: 0.047515; Generator: 0.028555,\n",
      "D(x): 0.555, D(G(z)): 0.401\n",
      "2019-04-10 00:57:03,494 root         INFO     Train Epoch: 57 [4608/8000 (58%)]\tTotal Loss: 0.437184\n",
      "Reconstruction: 0.364949, Regularization: 0.001617, Discriminator: 0.042410; Generator: 0.028207,\n",
      "D(x): 0.547, D(G(z)): 0.406\n",
      "2019-04-10 00:57:03,603 root         INFO     Train Epoch: 57 [5120/8000 (64%)]\tTotal Loss: 0.440545\n",
      "Reconstruction: 0.365924, Regularization: 0.002104, Discriminator: 0.045453; Generator: 0.027063,\n",
      "D(x): 0.587, D(G(z)): 0.421\n",
      "2019-04-10 00:57:03,714 root         INFO     Train Epoch: 57 [5632/8000 (70%)]\tTotal Loss: 0.450892\n",
      "Reconstruction: 0.366706, Regularization: 0.001216, Discriminator: 0.056516; Generator: 0.026454,\n",
      "D(x): 0.436, D(G(z)): 0.429\n",
      "2019-04-10 00:57:03,823 root         INFO     Train Epoch: 57 [6144/8000 (77%)]\tTotal Loss: 0.441835\n",
      "Reconstruction: 0.368013, Regularization: 0.001434, Discriminator: 0.045633; Generator: 0.026755,\n",
      "D(x): 0.587, D(G(z)): 0.425\n",
      "2019-04-10 00:57:03,933 root         INFO     Train Epoch: 57 [6656/8000 (83%)]\tTotal Loss: 0.447891\n",
      "Reconstruction: 0.368986, Regularization: 0.001795, Discriminator: 0.049193; Generator: 0.027918,\n",
      "D(x): 0.523, D(G(z)): 0.410\n",
      "2019-04-10 00:57:04,043 root         INFO     Train Epoch: 57 [7168/8000 (90%)]\tTotal Loss: 0.440271\n",
      "Reconstruction: 0.369911, Regularization: 0.001377, Discriminator: 0.042155; Generator: 0.026828,\n",
      "D(x): 0.583, D(G(z)): 0.424\n",
      "2019-04-10 00:57:04,153 root         INFO     Train Epoch: 57 [7680/8000 (96%)]\tTotal Loss: 0.446181\n",
      "Reconstruction: 0.370569, Regularization: 0.001358, Discriminator: 0.047682; Generator: 0.026572,\n",
      "D(x): 0.536, D(G(z)): 0.428\n",
      "2019-04-10 00:57:04,233 root         INFO     ====> Epoch: 57 Average loss: 0.4407\n",
      "2019-04-10 00:57:04,260 root         INFO     Train Epoch: 58 [0/8000 (0%)]\tTotal Loss: 0.445927\n",
      "Reconstruction: 0.371128, Regularization: 0.001134, Discriminator: 0.047377; Generator: 0.026288,\n",
      "D(x): 0.536, D(G(z)): 0.432\n",
      "2019-04-10 00:57:04,370 root         INFO     Train Epoch: 58 [512/8000 (6%)]\tTotal Loss: 0.456877\n",
      "Reconstruction: 0.371756, Regularization: 0.001526, Discriminator: 0.058472; Generator: 0.025123,\n",
      "D(x): 0.440, D(G(z)): 0.448\n",
      "2019-04-10 00:57:04,480 root         INFO     Train Epoch: 58 [1024/8000 (13%)]\tTotal Loss: 0.446605\n",
      "Reconstruction: 0.372698, Regularization: 0.001137, Discriminator: 0.046823; Generator: 0.025947,\n",
      "D(x): 0.537, D(G(z)): 0.436\n",
      "2019-04-10 00:57:04,589 root         INFO     Train Epoch: 58 [1536/8000 (19%)]\tTotal Loss: 0.453479\n",
      "Reconstruction: 0.373149, Regularization: 0.001253, Discriminator: 0.054847; Generator: 0.024230,\n",
      "D(x): 0.452, D(G(z)): 0.461\n",
      "2019-04-10 00:57:04,698 root         INFO     Train Epoch: 58 [2048/8000 (26%)]\tTotal Loss: 0.447143\n",
      "Reconstruction: 0.373718, Regularization: 0.001511, Discriminator: 0.047133; Generator: 0.024781,\n",
      "D(x): 0.567, D(G(z)): 0.453\n",
      "2019-04-10 00:57:04,808 root         INFO     Train Epoch: 58 [2560/8000 (32%)]\tTotal Loss: 0.454535\n",
      "Reconstruction: 0.374318, Regularization: 0.001813, Discriminator: 0.052994; Generator: 0.025410,\n",
      "D(x): 0.526, D(G(z)): 0.444\n",
      "2019-04-10 00:57:04,917 root         INFO     Train Epoch: 58 [3072/8000 (38%)]\tTotal Loss: 0.448912\n",
      "Reconstruction: 0.374815, Regularization: 0.001109, Discriminator: 0.048004; Generator: 0.024984,\n",
      "D(x): 0.567, D(G(z)): 0.450\n",
      "2019-04-10 00:57:05,026 root         INFO     Train Epoch: 58 [3584/8000 (45%)]\tTotal Loss: 0.447959\n",
      "Reconstruction: 0.375266, Regularization: 0.001295, Discriminator: 0.046604; Generator: 0.024793,\n",
      "D(x): 0.537, D(G(z)): 0.453\n",
      "2019-04-10 00:57:05,136 root         INFO     Train Epoch: 58 [4096/8000 (51%)]\tTotal Loss: 0.448010\n",
      "Reconstruction: 0.375884, Regularization: 0.000973, Discriminator: 0.046585; Generator: 0.024568,\n",
      "D(x): 0.565, D(G(z)): 0.456\n",
      "2019-04-10 00:57:05,245 root         INFO     Train Epoch: 58 [4608/8000 (58%)]\tTotal Loss: 0.459326\n",
      "Reconstruction: 0.376447, Regularization: 0.001522, Discriminator: 0.056661; Generator: 0.024696,\n",
      "D(x): 0.449, D(G(z)): 0.454\n",
      "2019-04-10 00:57:05,354 root         INFO     Train Epoch: 58 [5120/8000 (64%)]\tTotal Loss: 0.462645\n",
      "Reconstruction: 0.377052, Regularization: 0.001686, Discriminator: 0.059912; Generator: 0.023995,\n",
      "D(x): 0.420, D(G(z)): 0.464\n",
      "2019-04-10 00:57:05,463 root         INFO     Train Epoch: 58 [5632/8000 (70%)]\tTotal Loss: 0.464149\n",
      "Reconstruction: 0.377877, Regularization: 0.001745, Discriminator: 0.060594; Generator: 0.023934,\n",
      "D(x): 0.480, D(G(z)): 0.465\n",
      "2019-04-10 00:57:05,572 root         INFO     Train Epoch: 58 [6144/8000 (77%)]\tTotal Loss: 0.459651\n",
      "Reconstruction: 0.378304, Regularization: 0.001448, Discriminator: 0.056350; Generator: 0.023550,\n",
      "D(x): 0.444, D(G(z)): 0.471\n",
      "2019-04-10 00:57:05,683 root         INFO     Train Epoch: 58 [6656/8000 (83%)]\tTotal Loss: 0.457928\n",
      "Reconstruction: 0.379057, Regularization: 0.001719, Discriminator: 0.053489; Generator: 0.023663,\n",
      "D(x): 0.481, D(G(z)): 0.469\n",
      "2019-04-10 00:57:05,793 root         INFO     Train Epoch: 58 [7168/8000 (90%)]\tTotal Loss: 0.455885\n",
      "Reconstruction: 0.379703, Regularization: 0.001362, Discriminator: 0.050197; Generator: 0.024622,\n",
      "D(x): 0.497, D(G(z)): 0.455\n",
      "2019-04-10 00:57:05,903 root         INFO     Train Epoch: 58 [7680/8000 (96%)]\tTotal Loss: 0.460800\n",
      "Reconstruction: 0.380474, Regularization: 0.001476, Discriminator: 0.054250; Generator: 0.024600,\n",
      "D(x): 0.459, D(G(z)): 0.455\n",
      "2019-04-10 00:57:05,984 root         INFO     ====> Epoch: 58 Average loss: 0.4539\n",
      "2019-04-10 00:57:06,011 root         INFO     Train Epoch: 59 [0/8000 (0%)]\tTotal Loss: 0.456073\n",
      "Reconstruction: 0.380965, Regularization: 0.000965, Discriminator: 0.050078; Generator: 0.024065,\n",
      "D(x): 0.539, D(G(z)): 0.463\n",
      "2019-04-10 00:57:06,124 root         INFO     Train Epoch: 59 [512/8000 (6%)]\tTotal Loss: 0.459909\n",
      "Reconstruction: 0.381756, Regularization: 0.000997, Discriminator: 0.053325; Generator: 0.023831,\n",
      "D(x): 0.457, D(G(z)): 0.467\n",
      "2019-04-10 00:57:06,235 root         INFO     Train Epoch: 59 [1024/8000 (13%)]\tTotal Loss: 0.466635\n",
      "Reconstruction: 0.382596, Regularization: 0.001290, Discriminator: 0.059208; Generator: 0.023540,\n",
      "D(x): 0.458, D(G(z)): 0.471\n",
      "2019-04-10 00:57:06,347 root         INFO     Train Epoch: 59 [1536/8000 (19%)]\tTotal Loss: 0.464349\n",
      "Reconstruction: 0.383358, Regularization: 0.001116, Discriminator: 0.055374; Generator: 0.024501,\n",
      "D(x): 0.464, D(G(z)): 0.457\n",
      "2019-04-10 00:57:06,458 root         INFO     Train Epoch: 59 [2048/8000 (26%)]\tTotal Loss: 0.478618\n",
      "Reconstruction: 0.384013, Regularization: 0.001231, Discriminator: 0.069541; Generator: 0.023833,\n",
      "D(x): 0.321, D(G(z)): 0.467\n",
      "2019-04-10 00:57:06,570 root         INFO     Train Epoch: 59 [2560/8000 (32%)]\tTotal Loss: 0.461473\n",
      "Reconstruction: 0.384952, Regularization: 0.001280, Discriminator: 0.051686; Generator: 0.023555,\n",
      "D(x): 0.514, D(G(z)): 0.471\n",
      "2019-04-10 00:57:06,681 root         INFO     Train Epoch: 59 [3072/8000 (38%)]\tTotal Loss: 0.465140\n",
      "Reconstruction: 0.385506, Regularization: 0.001166, Discriminator: 0.055519; Generator: 0.022948,\n",
      "D(x): 0.429, D(G(z)): 0.480\n",
      "2019-04-10 00:57:06,793 root         INFO     Train Epoch: 59 [3584/8000 (45%)]\tTotal Loss: 0.463341\n",
      "Reconstruction: 0.386279, Regularization: 0.001212, Discriminator: 0.052936; Generator: 0.022914,\n",
      "D(x): 0.492, D(G(z)): 0.481\n",
      "2019-04-10 00:57:06,904 root         INFO     Train Epoch: 59 [4096/8000 (51%)]\tTotal Loss: 0.463203\n",
      "Reconstruction: 0.386980, Regularization: 0.001303, Discriminator: 0.052289; Generator: 0.022631,\n",
      "D(x): 0.480, D(G(z)): 0.485\n",
      "2019-04-10 00:57:07,015 root         INFO     Train Epoch: 59 [4608/8000 (58%)]\tTotal Loss: 0.474846\n",
      "Reconstruction: 0.387416, Regularization: 0.001193, Discriminator: 0.062947; Generator: 0.023290,\n",
      "D(x): 0.391, D(G(z)): 0.475\n",
      "2019-04-10 00:57:07,127 root         INFO     Train Epoch: 59 [5120/8000 (64%)]\tTotal Loss: 0.468670\n",
      "Reconstruction: 0.388218, Regularization: 0.001056, Discriminator: 0.056855; Generator: 0.022541,\n",
      "D(x): 0.473, D(G(z)): 0.486\n",
      "2019-04-10 00:57:07,238 root         INFO     Train Epoch: 59 [5632/8000 (70%)]\tTotal Loss: 0.475498\n",
      "Reconstruction: 0.388667, Regularization: 0.000883, Discriminator: 0.063392; Generator: 0.022556,\n",
      "D(x): 0.371, D(G(z)): 0.486\n",
      "2019-04-10 00:57:07,349 root         INFO     Train Epoch: 59 [6144/8000 (77%)]\tTotal Loss: 0.466855\n",
      "Reconstruction: 0.388961, Regularization: 0.000894, Discriminator: 0.054658; Generator: 0.022342,\n",
      "D(x): 0.450, D(G(z)): 0.489\n",
      "2019-04-10 00:57:07,461 root         INFO     Train Epoch: 59 [6656/8000 (83%)]\tTotal Loss: 0.466588\n",
      "Reconstruction: 0.389686, Regularization: 0.001088, Discriminator: 0.053384; Generator: 0.022430,\n",
      "D(x): 0.460, D(G(z)): 0.488\n",
      "2019-04-10 00:57:07,573 root         INFO     Train Epoch: 59 [7168/8000 (90%)]\tTotal Loss: 0.469536\n",
      "Reconstruction: 0.390376, Regularization: 0.001252, Discriminator: 0.055942; Generator: 0.021966,\n",
      "D(x): 0.432, D(G(z)): 0.495\n",
      "2019-04-10 00:57:07,684 root         INFO     Train Epoch: 59 [7680/8000 (96%)]\tTotal Loss: 0.475795\n",
      "Reconstruction: 0.390536, Regularization: 0.001374, Discriminator: 0.061792; Generator: 0.022093,\n",
      "D(x): 0.367, D(G(z)): 0.493\n",
      "2019-04-10 00:57:07,765 root         INFO     ====> Epoch: 59 Average loss: 0.4657\n",
      "2019-04-10 00:57:07,792 root         INFO     Train Epoch: 60 [0/8000 (0%)]\tTotal Loss: 0.463687\n",
      "Reconstruction: 0.390975, Regularization: 0.001055, Discriminator: 0.049330; Generator: 0.022328,\n",
      "D(x): 0.508, D(G(z)): 0.490\n",
      "2019-04-10 00:57:07,904 root         INFO     Train Epoch: 60 [512/8000 (6%)]\tTotal Loss: 0.461822\n",
      "Reconstruction: 0.391537, Regularization: 0.001260, Discriminator: 0.046960; Generator: 0.022065,\n",
      "D(x): 0.505, D(G(z)): 0.494\n",
      "2019-04-10 00:57:08,016 root         INFO     Train Epoch: 60 [1024/8000 (13%)]\tTotal Loss: 0.476901\n",
      "Reconstruction: 0.392049, Regularization: 0.001060, Discriminator: 0.061889; Generator: 0.021904,\n",
      "D(x): 0.360, D(G(z)): 0.496\n",
      "2019-04-10 00:57:08,127 root         INFO     Train Epoch: 60 [1536/8000 (19%)]\tTotal Loss: 0.462688\n",
      "Reconstruction: 0.392547, Regularization: 0.001512, Discriminator: 0.046763; Generator: 0.021866,\n",
      "D(x): 0.530, D(G(z)): 0.497\n",
      "2019-04-10 00:57:08,239 root         INFO     Train Epoch: 60 [2048/8000 (26%)]\tTotal Loss: 0.467129\n",
      "Reconstruction: 0.393052, Regularization: 0.001189, Discriminator: 0.050478; Generator: 0.022410,\n",
      "D(x): 0.489, D(G(z)): 0.488\n",
      "2019-04-10 00:57:08,350 root         INFO     Train Epoch: 60 [2560/8000 (32%)]\tTotal Loss: 0.466409\n",
      "Reconstruction: 0.393563, Regularization: 0.000784, Discriminator: 0.050193; Generator: 0.021869,\n",
      "D(x): 0.477, D(G(z)): 0.497\n",
      "2019-04-10 00:57:08,462 root         INFO     Train Epoch: 60 [3072/8000 (38%)]\tTotal Loss: 0.477609\n",
      "Reconstruction: 0.394320, Regularization: 0.001183, Discriminator: 0.060091; Generator: 0.022016,\n",
      "D(x): 0.401, D(G(z)): 0.494\n",
      "2019-04-10 00:57:08,573 root         INFO     Train Epoch: 60 [3584/8000 (45%)]\tTotal Loss: 0.479401\n",
      "Reconstruction: 0.394506, Regularization: 0.000807, Discriminator: 0.062152; Generator: 0.021936,\n",
      "D(x): 0.348, D(G(z)): 0.496\n",
      "2019-04-10 00:57:08,685 root         INFO     Train Epoch: 60 [4096/8000 (51%)]\tTotal Loss: 0.466867\n",
      "Reconstruction: 0.395467, Regularization: 0.001074, Discriminator: 0.048704; Generator: 0.021622,\n",
      "D(x): 0.507, D(G(z)): 0.501\n",
      "2019-04-10 00:57:08,796 root         INFO     Train Epoch: 60 [4608/8000 (58%)]\tTotal Loss: 0.476846\n",
      "Reconstruction: 0.395666, Regularization: 0.001126, Discriminator: 0.058680; Generator: 0.021373,\n",
      "D(x): 0.386, D(G(z)): 0.505\n",
      "2019-04-10 00:57:08,908 root         INFO     Train Epoch: 60 [5120/8000 (64%)]\tTotal Loss: 0.479181\n",
      "Reconstruction: 0.396623, Regularization: 0.001237, Discriminator: 0.059906; Generator: 0.021414,\n",
      "D(x): 0.390, D(G(z)): 0.504\n",
      "2019-04-10 00:57:09,019 root         INFO     Train Epoch: 60 [5632/8000 (70%)]\tTotal Loss: 0.475812\n",
      "Reconstruction: 0.397120, Regularization: 0.001489, Discriminator: 0.055462; Generator: 0.021741,\n",
      "D(x): 0.434, D(G(z)): 0.499\n",
      "2019-04-10 00:57:09,131 root         INFO     Train Epoch: 60 [6144/8000 (77%)]\tTotal Loss: 0.479046\n",
      "Reconstruction: 0.397734, Regularization: 0.001039, Discriminator: 0.058743; Generator: 0.021531,\n",
      "D(x): 0.384, D(G(z)): 0.502\n",
      "2019-04-10 00:57:09,242 root         INFO     Train Epoch: 60 [6656/8000 (83%)]\tTotal Loss: 0.477001\n",
      "Reconstruction: 0.398417, Regularization: 0.001226, Discriminator: 0.055730; Generator: 0.021629,\n",
      "D(x): 0.416, D(G(z)): 0.501\n",
      "2019-04-10 00:57:09,353 root         INFO     Train Epoch: 60 [7168/8000 (90%)]\tTotal Loss: 0.482824\n",
      "Reconstruction: 0.399070, Regularization: 0.001046, Discriminator: 0.061166; Generator: 0.021541,\n",
      "D(x): 0.379, D(G(z)): 0.502\n",
      "2019-04-10 00:57:09,465 root         INFO     Train Epoch: 60 [7680/8000 (96%)]\tTotal Loss: 0.478341\n",
      "Reconstruction: 0.399687, Regularization: 0.001489, Discriminator: 0.055494; Generator: 0.021670,\n",
      "D(x): 0.420, D(G(z)): 0.500\n",
      "2019-04-10 00:57:09,548 root         INFO     ====> Epoch: 60 Average loss: 0.4751\n",
      "2019-04-10 00:57:09,576 root         INFO     Train Epoch: 61 [0/8000 (0%)]\tTotal Loss: 0.479760\n",
      "Reconstruction: 0.400286, Regularization: 0.001206, Discriminator: 0.056759; Generator: 0.021509,\n",
      "D(x): 0.401, D(G(z)): 0.502\n",
      "2019-04-10 00:57:09,687 root         INFO     Train Epoch: 61 [512/8000 (6%)]\tTotal Loss: 0.481349\n",
      "Reconstruction: 0.400803, Regularization: 0.001193, Discriminator: 0.058188; Generator: 0.021165,\n",
      "D(x): 0.418, D(G(z)): 0.508\n",
      "2019-04-10 00:57:09,799 root         INFO     Train Epoch: 61 [1024/8000 (13%)]\tTotal Loss: 0.496098\n",
      "Reconstruction: 0.401353, Regularization: 0.001217, Discriminator: 0.071705; Generator: 0.021823,\n",
      "D(x): 0.277, D(G(z)): 0.497\n",
      "2019-04-10 00:57:09,911 root         INFO     Train Epoch: 61 [1536/8000 (19%)]\tTotal Loss: 0.485465\n",
      "Reconstruction: 0.401828, Regularization: 0.000813, Discriminator: 0.061254; Generator: 0.021570,\n",
      "D(x): 0.361, D(G(z)): 0.502\n",
      "2019-04-10 00:57:10,023 root         INFO     Train Epoch: 61 [2048/8000 (26%)]\tTotal Loss: 0.487406\n",
      "Reconstruction: 0.402673, Regularization: 0.001094, Discriminator: 0.062462; Generator: 0.021176,\n",
      "D(x): 0.351, D(G(z)): 0.508\n",
      "2019-04-10 00:57:10,134 root         INFO     Train Epoch: 61 [2560/8000 (32%)]\tTotal Loss: 0.481890\n",
      "Reconstruction: 0.403050, Regularization: 0.000837, Discriminator: 0.056772; Generator: 0.021231,\n",
      "D(x): 0.396, D(G(z)): 0.507\n",
      "2019-04-10 00:57:10,245 root         INFO     Train Epoch: 61 [3072/8000 (38%)]\tTotal Loss: 0.483186\n",
      "Reconstruction: 0.403796, Regularization: 0.000913, Discriminator: 0.057321; Generator: 0.021156,\n",
      "D(x): 0.418, D(G(z)): 0.508\n",
      "2019-04-10 00:57:10,353 root         INFO     Train Epoch: 61 [3584/8000 (45%)]\tTotal Loss: 0.485698\n",
      "Reconstruction: 0.404379, Regularization: 0.001090, Discriminator: 0.059117; Generator: 0.021111,\n",
      "D(x): 0.395, D(G(z)): 0.509\n",
      "2019-04-10 00:57:10,461 root         INFO     Train Epoch: 61 [4096/8000 (51%)]\tTotal Loss: 0.480977\n",
      "Reconstruction: 0.404740, Regularization: 0.000894, Discriminator: 0.054149; Generator: 0.021194,\n",
      "D(x): 0.425, D(G(z)): 0.508\n",
      "2019-04-10 00:57:10,570 root         INFO     Train Epoch: 61 [4608/8000 (58%)]\tTotal Loss: 0.488367\n",
      "Reconstruction: 0.405223, Regularization: 0.001022, Discriminator: 0.060956; Generator: 0.021166,\n",
      "D(x): 0.380, D(G(z)): 0.508\n",
      "2019-04-10 00:57:10,678 root         INFO     Train Epoch: 61 [5120/8000 (64%)]\tTotal Loss: 0.482841\n",
      "Reconstruction: 0.405754, Regularization: 0.001223, Discriminator: 0.055032; Generator: 0.020832,\n",
      "D(x): 0.412, D(G(z)): 0.513\n",
      "2019-04-10 00:57:10,786 root         INFO     Train Epoch: 61 [5632/8000 (70%)]\tTotal Loss: 0.485560\n",
      "Reconstruction: 0.406286, Regularization: 0.001169, Discriminator: 0.057099; Generator: 0.021006,\n",
      "D(x): 0.398, D(G(z)): 0.511\n",
      "2019-04-10 00:57:10,895 root         INFO     Train Epoch: 61 [6144/8000 (77%)]\tTotal Loss: 0.487829\n",
      "Reconstruction: 0.406364, Regularization: 0.000954, Discriminator: 0.059411; Generator: 0.021100,\n",
      "D(x): 0.357, D(G(z)): 0.509\n",
      "2019-04-10 00:57:11,004 root         INFO     Train Epoch: 61 [6656/8000 (83%)]\tTotal Loss: 0.485305\n",
      "Reconstruction: 0.407163, Regularization: 0.000761, Discriminator: 0.056499; Generator: 0.020882,\n",
      "D(x): 0.410, D(G(z)): 0.513\n",
      "2019-04-10 00:57:11,112 root         INFO     Train Epoch: 61 [7168/8000 (90%)]\tTotal Loss: 0.489854\n",
      "Reconstruction: 0.407416, Regularization: 0.000743, Discriminator: 0.060890; Generator: 0.020805,\n",
      "D(x): 0.358, D(G(z)): 0.514\n",
      "2019-04-10 00:57:11,221 root         INFO     Train Epoch: 61 [7680/8000 (96%)]\tTotal Loss: 0.482563\n",
      "Reconstruction: 0.407880, Regularization: 0.001172, Discriminator: 0.052653; Generator: 0.020858,\n",
      "D(x): 0.426, D(G(z)): 0.513\n",
      "2019-04-10 00:57:11,300 root         INFO     ====> Epoch: 61 Average loss: 0.4846\n",
      "2019-04-10 00:57:11,327 root         INFO     Train Epoch: 62 [0/8000 (0%)]\tTotal Loss: 0.489193\n",
      "Reconstruction: 0.408415, Regularization: 0.000550, Discriminator: 0.059167; Generator: 0.021060,\n",
      "D(x): 0.380, D(G(z)): 0.510\n",
      "2019-04-10 00:57:11,437 root         INFO     Train Epoch: 62 [512/8000 (6%)]\tTotal Loss: 0.492682\n",
      "Reconstruction: 0.408657, Regularization: 0.000762, Discriminator: 0.062391; Generator: 0.020873,\n",
      "D(x): 0.338, D(G(z)): 0.513\n",
      "2019-04-10 00:57:11,547 root         INFO     Train Epoch: 62 [1024/8000 (13%)]\tTotal Loss: 0.493539\n",
      "Reconstruction: 0.409464, Regularization: 0.000898, Discriminator: 0.062560; Generator: 0.020616,\n",
      "D(x): 0.335, D(G(z)): 0.517\n",
      "2019-04-10 00:57:11,656 root         INFO     Train Epoch: 62 [1536/8000 (19%)]\tTotal Loss: 0.492751\n",
      "Reconstruction: 0.409637, Regularization: 0.001206, Discriminator: 0.060885; Generator: 0.021023,\n",
      "D(x): 0.383, D(G(z)): 0.510\n",
      "2019-04-10 00:57:11,765 root         INFO     Train Epoch: 62 [2048/8000 (26%)]\tTotal Loss: 0.491657\n",
      "Reconstruction: 0.410337, Regularization: 0.000899, Discriminator: 0.059426; Generator: 0.020995,\n",
      "D(x): 0.358, D(G(z)): 0.511\n",
      "2019-04-10 00:57:11,874 root         INFO     Train Epoch: 62 [2560/8000 (32%)]\tTotal Loss: 0.487041\n",
      "Reconstruction: 0.410889, Regularization: 0.000909, Discriminator: 0.054399; Generator: 0.020844,\n",
      "D(x): 0.427, D(G(z)): 0.513\n",
      "2019-04-10 00:57:11,984 root         INFO     Train Epoch: 62 [3072/8000 (38%)]\tTotal Loss: 0.489507\n",
      "Reconstruction: 0.411361, Regularization: 0.000637, Discriminator: 0.056493; Generator: 0.021016,\n",
      "D(x): 0.416, D(G(z)): 0.510\n",
      "2019-04-10 00:57:12,093 root         INFO     Train Epoch: 62 [3584/8000 (45%)]\tTotal Loss: 0.487954\n",
      "Reconstruction: 0.411892, Regularization: 0.000994, Discriminator: 0.054020; Generator: 0.021048,\n",
      "D(x): 0.391, D(G(z)): 0.510\n",
      "2019-04-10 00:57:12,202 root         INFO     Train Epoch: 62 [4096/8000 (51%)]\tTotal Loss: 0.490399\n",
      "Reconstruction: 0.412819, Regularization: 0.000957, Discriminator: 0.055463; Generator: 0.021160,\n",
      "D(x): 0.400, D(G(z)): 0.508\n",
      "2019-04-10 00:57:12,312 root         INFO     Train Epoch: 62 [4608/8000 (58%)]\tTotal Loss: 0.496131\n",
      "Reconstruction: 0.413364, Regularization: 0.000636, Discriminator: 0.061067; Generator: 0.021064,\n",
      "D(x): 0.372, D(G(z)): 0.510\n",
      "2019-04-10 00:57:12,421 root         INFO     Train Epoch: 62 [5120/8000 (64%)]\tTotal Loss: 0.496303\n",
      "Reconstruction: 0.413465, Regularization: 0.001026, Discriminator: 0.060743; Generator: 0.021069,\n",
      "D(x): 0.344, D(G(z)): 0.510\n",
      "2019-04-10 00:57:12,531 root         INFO     Train Epoch: 62 [5632/8000 (70%)]\tTotal Loss: 0.497265\n",
      "Reconstruction: 0.414399, Regularization: 0.001261, Discriminator: 0.060762; Generator: 0.020843,\n",
      "D(x): 0.346, D(G(z)): 0.513\n",
      "2019-04-10 00:57:12,640 root         INFO     Train Epoch: 62 [6144/8000 (77%)]\tTotal Loss: 0.494703\n",
      "Reconstruction: 0.414707, Regularization: 0.000824, Discriminator: 0.058109; Generator: 0.021062,\n",
      "D(x): 0.367, D(G(z)): 0.510\n",
      "2019-04-10 00:57:12,750 root         INFO     Train Epoch: 62 [6656/8000 (83%)]\tTotal Loss: 0.495850\n",
      "Reconstruction: 0.415200, Regularization: 0.001096, Discriminator: 0.058866; Generator: 0.020688,\n",
      "D(x): 0.398, D(G(z)): 0.516\n",
      "2019-04-10 00:57:12,859 root         INFO     Train Epoch: 62 [7168/8000 (90%)]\tTotal Loss: 0.492097\n",
      "Reconstruction: 0.415499, Regularization: 0.000701, Discriminator: 0.055005; Generator: 0.020893,\n",
      "D(x): 0.398, D(G(z)): 0.512\n",
      "2019-04-10 00:57:12,968 root         INFO     Train Epoch: 62 [7680/8000 (96%)]\tTotal Loss: 0.497075\n",
      "Reconstruction: 0.416067, Regularization: 0.000667, Discriminator: 0.059164; Generator: 0.021177,\n",
      "D(x): 0.338, D(G(z)): 0.508\n",
      "2019-04-10 00:57:13,047 root         INFO     ====> Epoch: 62 Average loss: 0.4926\n",
      "2019-04-10 00:57:13,074 root         INFO     Train Epoch: 63 [0/8000 (0%)]\tTotal Loss: 0.496226\n",
      "Reconstruction: 0.416235, Regularization: 0.000703, Discriminator: 0.058190; Generator: 0.021096,\n",
      "D(x): 0.373, D(G(z)): 0.509\n",
      "2019-04-10 00:57:13,182 root         INFO     Train Epoch: 63 [512/8000 (6%)]\tTotal Loss: 0.495026\n",
      "Reconstruction: 0.416870, Regularization: 0.000927, Discriminator: 0.056344; Generator: 0.020886,\n",
      "D(x): 0.380, D(G(z)): 0.513\n",
      "2019-04-10 00:57:13,289 root         INFO     Train Epoch: 63 [1024/8000 (13%)]\tTotal Loss: 0.496711\n",
      "Reconstruction: 0.417429, Regularization: 0.000897, Discriminator: 0.057405; Generator: 0.020980,\n",
      "D(x): 0.360, D(G(z)): 0.511\n",
      "2019-04-10 00:57:13,396 root         INFO     Train Epoch: 63 [1536/8000 (19%)]\tTotal Loss: 0.490066\n",
      "Reconstruction: 0.417495, Regularization: 0.000693, Discriminator: 0.050966; Generator: 0.020912,\n",
      "D(x): 0.436, D(G(z)): 0.512\n",
      "2019-04-10 00:57:13,503 root         INFO     Train Epoch: 63 [2048/8000 (26%)]\tTotal Loss: 0.495458\n",
      "Reconstruction: 0.418247, Regularization: 0.000749, Discriminator: 0.055702; Generator: 0.020760,\n",
      "D(x): 0.384, D(G(z)): 0.515\n",
      "2019-04-10 00:57:13,610 root         INFO     Train Epoch: 63 [2560/8000 (32%)]\tTotal Loss: 0.499016\n",
      "Reconstruction: 0.418888, Regularization: 0.000716, Discriminator: 0.058518; Generator: 0.020894,\n",
      "D(x): 0.354, D(G(z)): 0.512\n",
      "2019-04-10 00:57:13,717 root         INFO     Train Epoch: 63 [3072/8000 (38%)]\tTotal Loss: 0.494876\n",
      "Reconstruction: 0.419155, Regularization: 0.000775, Discriminator: 0.054328; Generator: 0.020618,\n",
      "D(x): 0.407, D(G(z)): 0.517\n",
      "2019-04-10 00:57:13,826 root         INFO     Train Epoch: 63 [3584/8000 (45%)]\tTotal Loss: 0.498521\n",
      "Reconstruction: 0.419664, Regularization: 0.000995, Discriminator: 0.056906; Generator: 0.020955,\n",
      "D(x): 0.359, D(G(z)): 0.511\n",
      "2019-04-10 00:57:13,937 root         INFO     Train Epoch: 63 [4096/8000 (51%)]\tTotal Loss: 0.500294\n",
      "Reconstruction: 0.419979, Regularization: 0.000903, Discriminator: 0.058745; Generator: 0.020667,\n",
      "D(x): 0.359, D(G(z)): 0.516\n",
      "2019-04-10 00:57:14,047 root         INFO     Train Epoch: 63 [4608/8000 (58%)]\tTotal Loss: 0.501516\n",
      "Reconstruction: 0.420542, Regularization: 0.000968, Discriminator: 0.059005; Generator: 0.021001,\n",
      "D(x): 0.342, D(G(z)): 0.511\n",
      "2019-04-10 00:57:14,160 root         INFO     Train Epoch: 63 [5120/8000 (64%)]\tTotal Loss: 0.492190\n",
      "Reconstruction: 0.420913, Regularization: 0.000526, Discriminator: 0.049921; Generator: 0.020830,\n",
      "D(x): 0.438, D(G(z)): 0.513\n",
      "2019-04-10 00:57:14,271 root         INFO     Train Epoch: 63 [5632/8000 (70%)]\tTotal Loss: 0.504550\n",
      "Reconstruction: 0.421300, Regularization: 0.000896, Discriminator: 0.061559; Generator: 0.020795,\n",
      "D(x): 0.320, D(G(z)): 0.514\n",
      "2019-04-10 00:57:14,383 root         INFO     Train Epoch: 63 [6144/8000 (77%)]\tTotal Loss: 0.503685\n",
      "Reconstruction: 0.421970, Regularization: 0.000934, Discriminator: 0.059836; Generator: 0.020945,\n",
      "D(x): 0.350, D(G(z)): 0.512\n",
      "2019-04-10 00:57:14,494 root         INFO     Train Epoch: 63 [6656/8000 (83%)]\tTotal Loss: 0.503548\n",
      "Reconstruction: 0.422210, Regularization: 0.000730, Discriminator: 0.059520; Generator: 0.021088,\n",
      "D(x): 0.335, D(G(z)): 0.509\n",
      "2019-04-10 00:57:14,605 root         INFO     Train Epoch: 63 [7168/8000 (90%)]\tTotal Loss: 0.502667\n",
      "Reconstruction: 0.422821, Regularization: 0.000851, Discriminator: 0.057830; Generator: 0.021165,\n",
      "D(x): 0.353, D(G(z)): 0.508\n",
      "2019-04-10 00:57:14,717 root         INFO     Train Epoch: 63 [7680/8000 (96%)]\tTotal Loss: 0.504428\n",
      "Reconstruction: 0.423683, Regularization: 0.001475, Discriminator: 0.058110; Generator: 0.021160,\n",
      "D(x): 0.349, D(G(z)): 0.508\n",
      "2019-04-10 00:57:14,798 root         INFO     ====> Epoch: 63 Average loss: 0.4999\n",
      "2019-04-10 00:57:14,826 root         INFO     Train Epoch: 64 [0/8000 (0%)]\tTotal Loss: 0.497561\n",
      "Reconstruction: 0.423719, Regularization: 0.000613, Discriminator: 0.052166; Generator: 0.021064,\n",
      "D(x): 0.409, D(G(z)): 0.510\n",
      "2019-04-10 00:57:14,937 root         INFO     Train Epoch: 64 [512/8000 (6%)]\tTotal Loss: 0.506447\n",
      "Reconstruction: 0.424673, Regularization: 0.000889, Discriminator: 0.059814; Generator: 0.021071,\n",
      "D(x): 0.336, D(G(z)): 0.510\n",
      "2019-04-10 00:57:15,047 root         INFO     Train Epoch: 64 [1024/8000 (13%)]\tTotal Loss: 0.502559\n",
      "Reconstruction: 0.424911, Regularization: 0.000654, Discriminator: 0.055668; Generator: 0.021326,\n",
      "D(x): 0.373, D(G(z)): 0.505\n",
      "2019-04-10 00:57:15,157 root         INFO     Train Epoch: 64 [1536/8000 (19%)]\tTotal Loss: 0.504886\n",
      "Reconstruction: 0.425718, Regularization: 0.000843, Discriminator: 0.057058; Generator: 0.021267,\n",
      "D(x): 0.367, D(G(z)): 0.506\n",
      "2019-04-10 00:57:15,267 root         INFO     Train Epoch: 64 [2048/8000 (26%)]\tTotal Loss: 0.503260\n",
      "Reconstruction: 0.426136, Regularization: 0.001169, Discriminator: 0.054820; Generator: 0.021135,\n",
      "D(x): 0.384, D(G(z)): 0.508\n",
      "2019-04-10 00:57:15,377 root         INFO     Train Epoch: 64 [2560/8000 (32%)]\tTotal Loss: 0.508751\n",
      "Reconstruction: 0.426592, Regularization: 0.000849, Discriminator: 0.060087; Generator: 0.021224,\n",
      "D(x): 0.326, D(G(z)): 0.507\n",
      "2019-04-10 00:57:15,487 root         INFO     Train Epoch: 64 [3072/8000 (38%)]\tTotal Loss: 0.510058\n",
      "Reconstruction: 0.427153, Regularization: 0.000675, Discriminator: 0.060932; Generator: 0.021297,\n",
      "D(x): 0.308, D(G(z)): 0.506\n",
      "2019-04-10 00:57:15,597 root         INFO     Train Epoch: 64 [3584/8000 (45%)]\tTotal Loss: 0.509826\n",
      "Reconstruction: 0.427426, Regularization: 0.001173, Discriminator: 0.059880; Generator: 0.021347,\n",
      "D(x): 0.336, D(G(z)): 0.505\n",
      "2019-04-10 00:57:15,708 root         INFO     Train Epoch: 64 [4096/8000 (51%)]\tTotal Loss: 0.504422\n",
      "Reconstruction: 0.427985, Regularization: 0.000606, Discriminator: 0.054591; Generator: 0.021241,\n",
      "D(x): 0.376, D(G(z)): 0.507\n",
      "2019-04-10 00:57:15,817 root         INFO     Train Epoch: 64 [4608/8000 (58%)]\tTotal Loss: 0.508176\n",
      "Reconstruction: 0.428372, Regularization: 0.000497, Discriminator: 0.058138; Generator: 0.021170,\n",
      "D(x): 0.338, D(G(z)): 0.508\n",
      "2019-04-10 00:57:15,927 root         INFO     Train Epoch: 64 [5120/8000 (64%)]\tTotal Loss: 0.508234\n",
      "Reconstruction: 0.428959, Regularization: 0.000769, Discriminator: 0.057384; Generator: 0.021123,\n",
      "D(x): 0.348, D(G(z)): 0.509\n",
      "2019-04-10 00:57:16,037 root         INFO     Train Epoch: 64 [5632/8000 (70%)]\tTotal Loss: 0.509542\n",
      "Reconstruction: 0.429796, Regularization: 0.000885, Discriminator: 0.057662; Generator: 0.021199,\n",
      "D(x): 0.354, D(G(z)): 0.507\n",
      "2019-04-10 00:57:16,147 root         INFO     Train Epoch: 64 [6144/8000 (77%)]\tTotal Loss: 0.513777\n",
      "Reconstruction: 0.429932, Regularization: 0.000749, Discriminator: 0.061764; Generator: 0.021333,\n",
      "D(x): 0.311, D(G(z)): 0.505\n",
      "2019-04-10 00:57:16,258 root         INFO     Train Epoch: 64 [6656/8000 (83%)]\tTotal Loss: 0.505062\n",
      "Reconstruction: 0.430282, Regularization: 0.000595, Discriminator: 0.052904; Generator: 0.021281,\n",
      "D(x): 0.392, D(G(z)): 0.506\n",
      "2019-04-10 00:57:16,368 root         INFO     Train Epoch: 64 [7168/8000 (90%)]\tTotal Loss: 0.508229\n",
      "Reconstruction: 0.430811, Regularization: 0.000499, Discriminator: 0.055571; Generator: 0.021348,\n",
      "D(x): 0.355, D(G(z)): 0.505\n",
      "2019-04-10 00:57:16,477 root         INFO     Train Epoch: 64 [7680/8000 (96%)]\tTotal Loss: 0.508561\n",
      "Reconstruction: 0.431096, Regularization: 0.000615, Discriminator: 0.055545; Generator: 0.021304,\n",
      "D(x): 0.361, D(G(z)): 0.506\n",
      "2019-04-10 00:57:16,558 root         INFO     ====> Epoch: 64 Average loss: 0.5076\n",
      "2019-04-10 00:57:16,585 root         INFO     Train Epoch: 65 [0/8000 (0%)]\tTotal Loss: 0.513764\n",
      "Reconstruction: 0.431365, Regularization: 0.000641, Discriminator: 0.060376; Generator: 0.021382,\n",
      "D(x): 0.322, D(G(z)): 0.505\n",
      "2019-04-10 00:57:16,697 root         INFO     Train Epoch: 65 [512/8000 (6%)]\tTotal Loss: 0.508306\n",
      "Reconstruction: 0.431690, Regularization: 0.000626, Discriminator: 0.054542; Generator: 0.021448,\n",
      "D(x): 0.370, D(G(z)): 0.503\n",
      "2019-04-10 00:57:16,808 root         INFO     Train Epoch: 65 [1024/8000 (13%)]\tTotal Loss: 0.511155\n",
      "Reconstruction: 0.431998, Regularization: 0.000537, Discriminator: 0.057259; Generator: 0.021360,\n",
      "D(x): 0.348, D(G(z)): 0.505\n",
      "2019-04-10 00:57:16,918 root         INFO     Train Epoch: 65 [1536/8000 (19%)]\tTotal Loss: 0.509271\n",
      "Reconstruction: 0.432230, Regularization: 0.000545, Discriminator: 0.055019; Generator: 0.021477,\n",
      "D(x): 0.360, D(G(z)): 0.503\n",
      "2019-04-10 00:57:17,028 root         INFO     Train Epoch: 65 [2048/8000 (26%)]\tTotal Loss: 0.510832\n",
      "Reconstruction: 0.432740, Regularization: 0.000536, Discriminator: 0.056108; Generator: 0.021448,\n",
      "D(x): 0.363, D(G(z)): 0.503\n",
      "2019-04-10 00:57:17,138 root         INFO     Train Epoch: 65 [2560/8000 (32%)]\tTotal Loss: 0.513664\n",
      "Reconstruction: 0.433289, Regularization: 0.000519, Discriminator: 0.058397; Generator: 0.021459,\n",
      "D(x): 0.328, D(G(z)): 0.503\n",
      "2019-04-10 00:57:17,248 root         INFO     Train Epoch: 65 [3072/8000 (38%)]\tTotal Loss: 0.515718\n",
      "Reconstruction: 0.433462, Regularization: 0.000662, Discriminator: 0.060139; Generator: 0.021456,\n",
      "D(x): 0.326, D(G(z)): 0.503\n",
      "2019-04-10 00:57:17,359 root         INFO     Train Epoch: 65 [3584/8000 (45%)]\tTotal Loss: 0.508574\n",
      "Reconstruction: 0.434160, Regularization: 0.000478, Discriminator: 0.052374; Generator: 0.021561,\n",
      "D(x): 0.401, D(G(z)): 0.502\n",
      "2019-04-10 00:57:17,469 root         INFO     Train Epoch: 65 [4096/8000 (51%)]\tTotal Loss: 0.513940\n",
      "Reconstruction: 0.434458, Regularization: 0.000640, Discriminator: 0.057251; Generator: 0.021591,\n",
      "D(x): 0.341, D(G(z)): 0.501\n",
      "2019-04-10 00:57:17,580 root         INFO     Train Epoch: 65 [4608/8000 (58%)]\tTotal Loss: 0.513077\n",
      "Reconstruction: 0.434757, Regularization: 0.000612, Discriminator: 0.056193; Generator: 0.021515,\n",
      "D(x): 0.360, D(G(z)): 0.502\n",
      "2019-04-10 00:57:17,691 root         INFO     Train Epoch: 65 [5120/8000 (64%)]\tTotal Loss: 0.512972\n",
      "Reconstruction: 0.435189, Regularization: 0.000645, Discriminator: 0.055405; Generator: 0.021733,\n",
      "D(x): 0.361, D(G(z)): 0.499\n",
      "2019-04-10 00:57:17,802 root         INFO     Train Epoch: 65 [5632/8000 (70%)]\tTotal Loss: 0.513011\n",
      "Reconstruction: 0.435644, Regularization: 0.000624, Discriminator: 0.055096; Generator: 0.021647,\n",
      "D(x): 0.363, D(G(z)): 0.500\n",
      "2019-04-10 00:57:17,914 root         INFO     Train Epoch: 65 [6144/8000 (77%)]\tTotal Loss: 0.519309\n",
      "Reconstruction: 0.436186, Regularization: 0.000923, Discriminator: 0.060641; Generator: 0.021558,\n",
      "D(x): 0.305, D(G(z)): 0.502\n",
      "2019-04-10 00:57:18,026 root         INFO     Train Epoch: 65 [6656/8000 (83%)]\tTotal Loss: 0.516198\n",
      "Reconstruction: 0.436720, Regularization: 0.000720, Discriminator: 0.057022; Generator: 0.021736,\n",
      "D(x): 0.339, D(G(z)): 0.499\n",
      "2019-04-10 00:57:18,138 root         INFO     Train Epoch: 65 [7168/8000 (90%)]\tTotal Loss: 0.515703\n",
      "Reconstruction: 0.437151, Regularization: 0.000623, Discriminator: 0.056104; Generator: 0.021825,\n",
      "D(x): 0.356, D(G(z)): 0.497\n",
      "2019-04-10 00:57:18,250 root         INFO     Train Epoch: 65 [7680/8000 (96%)]\tTotal Loss: 0.518492\n",
      "Reconstruction: 0.437738, Regularization: 0.000549, Discriminator: 0.058400; Generator: 0.021805,\n",
      "D(x): 0.327, D(G(z)): 0.498\n",
      "2019-04-10 00:57:18,333 root         INFO     ====> Epoch: 65 Average loss: 0.5138\n",
      "2019-04-10 00:57:18,360 root         INFO     Train Epoch: 66 [0/8000 (0%)]\tTotal Loss: 0.517500\n",
      "Reconstruction: 0.438070, Regularization: 0.000643, Discriminator: 0.056927; Generator: 0.021861,\n",
      "D(x): 0.342, D(G(z)): 0.497\n",
      "2019-04-10 00:57:18,472 root         INFO     Train Epoch: 66 [512/8000 (6%)]\tTotal Loss: 0.516420\n",
      "Reconstruction: 0.438442, Regularization: 0.000590, Discriminator: 0.055463; Generator: 0.021924,\n",
      "D(x): 0.359, D(G(z)): 0.496\n",
      "2019-04-10 00:57:18,584 root         INFO     Train Epoch: 66 [1024/8000 (13%)]\tTotal Loss: 0.521152\n",
      "Reconstruction: 0.439001, Regularization: 0.000754, Discriminator: 0.059466; Generator: 0.021931,\n",
      "D(x): 0.320, D(G(z)): 0.496\n",
      "2019-04-10 00:57:18,695 root         INFO     Train Epoch: 66 [1536/8000 (19%)]\tTotal Loss: 0.519072\n",
      "Reconstruction: 0.439714, Regularization: 0.000683, Discriminator: 0.056722; Generator: 0.021953,\n",
      "D(x): 0.340, D(G(z)): 0.495\n",
      "2019-04-10 00:57:18,806 root         INFO     Train Epoch: 66 [2048/8000 (26%)]\tTotal Loss: 0.518883\n",
      "Reconstruction: 0.440121, Regularization: 0.000614, Discriminator: 0.056236; Generator: 0.021912,\n",
      "D(x): 0.345, D(G(z)): 0.496\n",
      "2019-04-10 00:57:18,917 root         INFO     Train Epoch: 66 [2560/8000 (32%)]\tTotal Loss: 0.516825\n",
      "Reconstruction: 0.440588, Regularization: 0.000439, Discriminator: 0.053889; Generator: 0.021909,\n",
      "D(x): 0.369, D(G(z)): 0.496\n",
      "2019-04-10 00:57:19,028 root         INFO     Train Epoch: 66 [3072/8000 (38%)]\tTotal Loss: 0.520601\n",
      "Reconstruction: 0.441010, Regularization: 0.000530, Discriminator: 0.057020; Generator: 0.022041,\n",
      "D(x): 0.332, D(G(z)): 0.494\n",
      "2019-04-10 00:57:19,139 root         INFO     Train Epoch: 66 [3584/8000 (45%)]\tTotal Loss: 0.517776\n",
      "Reconstruction: 0.441505, Regularization: 0.000625, Discriminator: 0.053591; Generator: 0.022056,\n",
      "D(x): 0.372, D(G(z)): 0.494\n",
      "2019-04-10 00:57:19,249 root         INFO     Train Epoch: 66 [4096/8000 (51%)]\tTotal Loss: 0.523689\n",
      "Reconstruction: 0.442185, Regularization: 0.000478, Discriminator: 0.058858; Generator: 0.022168,\n",
      "D(x): 0.320, D(G(z)): 0.492\n",
      "2019-04-10 00:57:19,359 root         INFO     Train Epoch: 66 [4608/8000 (58%)]\tTotal Loss: 0.523590\n",
      "Reconstruction: 0.442766, Regularization: 0.000549, Discriminator: 0.058041; Generator: 0.022234,\n",
      "D(x): 0.338, D(G(z)): 0.491\n",
      "2019-04-10 00:57:19,468 root         INFO     Train Epoch: 66 [5120/8000 (64%)]\tTotal Loss: 0.521946\n",
      "Reconstruction: 0.442960, Regularization: 0.000726, Discriminator: 0.055979; Generator: 0.022282,\n",
      "D(x): 0.340, D(G(z)): 0.490\n",
      "2019-04-10 00:57:19,577 root         INFO     Train Epoch: 66 [5632/8000 (70%)]\tTotal Loss: 0.521727\n",
      "Reconstruction: 0.443330, Regularization: 0.000407, Discriminator: 0.055736; Generator: 0.022254,\n",
      "D(x): 0.349, D(G(z)): 0.491\n",
      "2019-04-10 00:57:19,686 root         INFO     Train Epoch: 66 [6144/8000 (77%)]\tTotal Loss: 0.521974\n",
      "Reconstruction: 0.443720, Regularization: 0.000667, Discriminator: 0.055235; Generator: 0.022352,\n",
      "D(x): 0.348, D(G(z)): 0.489\n",
      "2019-04-10 00:57:19,796 root         INFO     Train Epoch: 66 [6656/8000 (83%)]\tTotal Loss: 0.524128\n",
      "Reconstruction: 0.444314, Regularization: 0.000600, Discriminator: 0.056922; Generator: 0.022291,\n",
      "D(x): 0.328, D(G(z)): 0.490\n",
      "2019-04-10 00:57:19,905 root         INFO     Train Epoch: 66 [7168/8000 (90%)]\tTotal Loss: 0.526610\n",
      "Reconstruction: 0.444794, Regularization: 0.000590, Discriminator: 0.058935; Generator: 0.022291,\n",
      "D(x): 0.313, D(G(z)): 0.490\n",
      "2019-04-10 00:57:20,014 root         INFO     Train Epoch: 66 [7680/8000 (96%)]\tTotal Loss: 0.524352\n",
      "Reconstruction: 0.445083, Regularization: 0.000449, Discriminator: 0.056437; Generator: 0.022384,\n",
      "D(x): 0.334, D(G(z)): 0.489\n",
      "2019-04-10 00:57:20,095 root         INFO     ====> Epoch: 66 Average loss: 0.5210\n",
      "2019-04-10 00:57:20,122 root         INFO     Train Epoch: 67 [0/8000 (0%)]\tTotal Loss: 0.523175\n",
      "Reconstruction: 0.445377, Regularization: 0.000446, Discriminator: 0.054952; Generator: 0.022401,\n",
      "D(x): 0.351, D(G(z)): 0.488\n",
      "2019-04-10 00:57:20,235 root         INFO     Train Epoch: 67 [512/8000 (6%)]\tTotal Loss: 0.526394\n",
      "Reconstruction: 0.445773, Regularization: 0.000662, Discriminator: 0.057521; Generator: 0.022438,\n",
      "D(x): 0.320, D(G(z)): 0.488\n",
      "2019-04-10 00:57:20,343 root         INFO     Train Epoch: 67 [1024/8000 (13%)]\tTotal Loss: 0.523351\n",
      "Reconstruction: 0.446048, Regularization: 0.000563, Discriminator: 0.054261; Generator: 0.022479,\n",
      "D(x): 0.354, D(G(z)): 0.487\n",
      "2019-04-10 00:57:20,451 root         INFO     Train Epoch: 67 [1536/8000 (19%)]\tTotal Loss: 0.527815\n",
      "Reconstruction: 0.446574, Regularization: 0.000558, Discriminator: 0.058240; Generator: 0.022443,\n",
      "D(x): 0.318, D(G(z)): 0.488\n",
      "2019-04-10 00:57:20,560 root         INFO     Train Epoch: 67 [2048/8000 (26%)]\tTotal Loss: 0.524515\n",
      "Reconstruction: 0.446984, Regularization: 0.000524, Discriminator: 0.054346; Generator: 0.022662,\n",
      "D(x): 0.351, D(G(z)): 0.484\n",
      "2019-04-10 00:57:20,671 root         INFO     Train Epoch: 67 [2560/8000 (32%)]\tTotal Loss: 0.524948\n",
      "Reconstruction: 0.447351, Regularization: 0.000539, Discriminator: 0.054501; Generator: 0.022557,\n",
      "D(x): 0.354, D(G(z)): 0.486\n",
      "2019-04-10 00:57:20,781 root         INFO     Train Epoch: 67 [3072/8000 (38%)]\tTotal Loss: 0.525804\n",
      "Reconstruction: 0.447686, Regularization: 0.000510, Discriminator: 0.054900; Generator: 0.022708,\n",
      "D(x): 0.344, D(G(z)): 0.484\n",
      "2019-04-10 00:57:20,890 root         INFO     Train Epoch: 67 [3584/8000 (45%)]\tTotal Loss: 0.527538\n",
      "Reconstruction: 0.448044, Regularization: 0.000597, Discriminator: 0.056217; Generator: 0.022680,\n",
      "D(x): 0.337, D(G(z)): 0.484\n",
      "2019-04-10 00:57:20,999 root         INFO     Train Epoch: 67 [4096/8000 (51%)]\tTotal Loss: 0.527517\n",
      "Reconstruction: 0.448446, Regularization: 0.000699, Discriminator: 0.055634; Generator: 0.022738,\n",
      "D(x): 0.341, D(G(z)): 0.483\n",
      "2019-04-10 00:57:21,106 root         INFO     Train Epoch: 67 [4608/8000 (58%)]\tTotal Loss: 0.526423\n",
      "Reconstruction: 0.448763, Regularization: 0.000476, Discriminator: 0.054366; Generator: 0.022817,\n",
      "D(x): 0.348, D(G(z)): 0.482\n",
      "2019-04-10 00:57:21,213 root         INFO     Train Epoch: 67 [5120/8000 (64%)]\tTotal Loss: 0.529139\n",
      "Reconstruction: 0.449143, Regularization: 0.000520, Discriminator: 0.056561; Generator: 0.022915,\n",
      "D(x): 0.331, D(G(z)): 0.480\n",
      "2019-04-10 00:57:21,321 root         INFO     Train Epoch: 67 [5632/8000 (70%)]\tTotal Loss: 0.529180\n",
      "Reconstruction: 0.449525, Regularization: 0.000485, Discriminator: 0.056223; Generator: 0.022948,\n",
      "D(x): 0.328, D(G(z)): 0.480\n",
      "2019-04-10 00:57:21,430 root         INFO     Train Epoch: 67 [6144/8000 (77%)]\tTotal Loss: 0.529521\n",
      "Reconstruction: 0.449848, Regularization: 0.000662, Discriminator: 0.056108; Generator: 0.022903,\n",
      "D(x): 0.327, D(G(z)): 0.481\n",
      "2019-04-10 00:57:21,542 root         INFO     Train Epoch: 67 [6656/8000 (83%)]\tTotal Loss: 0.529606\n",
      "Reconstruction: 0.450273, Regularization: 0.000528, Discriminator: 0.055924; Generator: 0.022882,\n",
      "D(x): 0.332, D(G(z)): 0.481\n",
      "2019-04-10 00:57:21,653 root         INFO     Train Epoch: 67 [7168/8000 (90%)]\tTotal Loss: 0.526663\n",
      "Reconstruction: 0.450579, Regularization: 0.000478, Discriminator: 0.052624; Generator: 0.022982,\n",
      "D(x): 0.365, D(G(z)): 0.479\n",
      "2019-04-10 00:57:21,763 root         INFO     Train Epoch: 67 [7680/8000 (96%)]\tTotal Loss: 0.529078\n",
      "Reconstruction: 0.450939, Regularization: 0.000516, Discriminator: 0.054585; Generator: 0.023038,\n",
      "D(x): 0.344, D(G(z)): 0.478\n",
      "2019-04-10 00:57:21,844 root         INFO     ====> Epoch: 67 Average loss: 0.5271\n",
      "2019-04-10 00:57:21,872 root         INFO     Train Epoch: 68 [0/8000 (0%)]\tTotal Loss: 0.529157\n",
      "Reconstruction: 0.451333, Regularization: 0.000576, Discriminator: 0.054145; Generator: 0.023103,\n",
      "D(x): 0.347, D(G(z)): 0.477\n",
      "2019-04-10 00:57:21,988 root         INFO     Train Epoch: 68 [512/8000 (6%)]\tTotal Loss: 0.529224\n",
      "Reconstruction: 0.451725, Regularization: 0.000459, Discriminator: 0.053926; Generator: 0.023114,\n",
      "D(x): 0.350, D(G(z)): 0.477\n",
      "2019-04-10 00:57:22,102 root         INFO     Train Epoch: 68 [1024/8000 (13%)]\tTotal Loss: 0.532518\n",
      "Reconstruction: 0.452165, Regularization: 0.000423, Discriminator: 0.056828; Generator: 0.023102,\n",
      "D(x): 0.323, D(G(z)): 0.477\n",
      "2019-04-10 00:57:22,215 root         INFO     Train Epoch: 68 [1536/8000 (19%)]\tTotal Loss: 0.530705\n",
      "Reconstruction: 0.452643, Regularization: 0.000532, Discriminator: 0.054265; Generator: 0.023265,\n",
      "D(x): 0.343, D(G(z)): 0.475\n",
      "2019-04-10 00:57:22,330 root         INFO     Train Epoch: 68 [2048/8000 (26%)]\tTotal Loss: 0.531449\n",
      "Reconstruction: 0.453161, Regularization: 0.000346, Discriminator: 0.054654; Generator: 0.023288,\n",
      "D(x): 0.342, D(G(z)): 0.475\n",
      "2019-04-10 00:57:22,443 root         INFO     Train Epoch: 68 [2560/8000 (32%)]\tTotal Loss: 0.533788\n",
      "Reconstruction: 0.453537, Regularization: 0.000462, Discriminator: 0.056515; Generator: 0.023274,\n",
      "D(x): 0.324, D(G(z)): 0.475\n",
      "2019-04-10 00:57:22,558 root         INFO     Train Epoch: 68 [3072/8000 (38%)]\tTotal Loss: 0.531033\n",
      "Reconstruction: 0.454067, Regularization: 0.000375, Discriminator: 0.053258; Generator: 0.023333,\n",
      "D(x): 0.353, D(G(z)): 0.474\n",
      "2019-04-10 00:57:22,681 root         INFO     Train Epoch: 68 [3584/8000 (45%)]\tTotal Loss: 0.531754\n",
      "Reconstruction: 0.454531, Regularization: 0.000348, Discriminator: 0.053541; Generator: 0.023335,\n",
      "D(x): 0.352, D(G(z)): 0.474\n",
      "2019-04-10 00:57:22,795 root         INFO     Train Epoch: 68 [4096/8000 (51%)]\tTotal Loss: 0.533331\n",
      "Reconstruction: 0.454988, Regularization: 0.000596, Discriminator: 0.054387; Generator: 0.023360,\n",
      "D(x): 0.340, D(G(z)): 0.474\n",
      "2019-04-10 00:57:22,908 root         INFO     Train Epoch: 68 [4608/8000 (58%)]\tTotal Loss: 0.534320\n",
      "Reconstruction: 0.455355, Regularization: 0.000357, Discriminator: 0.054993; Generator: 0.023616,\n",
      "D(x): 0.331, D(G(z)): 0.470\n",
      "2019-04-10 00:57:23,021 root         INFO     Train Epoch: 68 [5120/8000 (64%)]\tTotal Loss: 0.534344\n",
      "Reconstruction: 0.455944, Regularization: 0.000360, Discriminator: 0.054496; Generator: 0.023545,\n",
      "D(x): 0.339, D(G(z)): 0.471\n",
      "2019-04-10 00:57:23,132 root         INFO     Train Epoch: 68 [5632/8000 (70%)]\tTotal Loss: 0.533676\n",
      "Reconstruction: 0.456396, Regularization: 0.000360, Discriminator: 0.053385; Generator: 0.023535,\n",
      "D(x): 0.351, D(G(z)): 0.471\n",
      "2019-04-10 00:57:23,243 root         INFO     Train Epoch: 68 [6144/8000 (77%)]\tTotal Loss: 0.536454\n",
      "Reconstruction: 0.456894, Regularization: 0.000534, Discriminator: 0.055481; Generator: 0.023546,\n",
      "D(x): 0.330, D(G(z)): 0.471\n",
      "2019-04-10 00:57:23,354 root         INFO     Train Epoch: 68 [6656/8000 (83%)]\tTotal Loss: 0.535401\n",
      "Reconstruction: 0.457281, Regularization: 0.000628, Discriminator: 0.053712; Generator: 0.023779,\n",
      "D(x): 0.343, D(G(z)): 0.467\n",
      "2019-04-10 00:57:23,466 root         INFO     Train Epoch: 68 [7168/8000 (90%)]\tTotal Loss: 0.535777\n",
      "Reconstruction: 0.457703, Regularization: 0.000379, Discriminator: 0.053893; Generator: 0.023802,\n",
      "D(x): 0.347, D(G(z)): 0.467\n",
      "2019-04-10 00:57:23,578 root         INFO     Train Epoch: 68 [7680/8000 (96%)]\tTotal Loss: 0.535475\n",
      "Reconstruction: 0.457974, Regularization: 0.000450, Discriminator: 0.053185; Generator: 0.023865,\n",
      "D(x): 0.346, D(G(z)): 0.466\n",
      "2019-04-10 00:57:23,659 root         INFO     ====> Epoch: 68 Average loss: 0.5333\n",
      "2019-04-10 00:57:23,686 root         INFO     Train Epoch: 69 [0/8000 (0%)]\tTotal Loss: 0.537736\n",
      "Reconstruction: 0.458380, Regularization: 0.000393, Discriminator: 0.055079; Generator: 0.023884,\n",
      "D(x): 0.326, D(G(z)): 0.466\n",
      "2019-04-10 00:57:23,797 root         INFO     Train Epoch: 69 [512/8000 (6%)]\tTotal Loss: 0.539101\n",
      "Reconstruction: 0.458693, Regularization: 0.000384, Discriminator: 0.056048; Generator: 0.023977,\n",
      "D(x): 0.317, D(G(z)): 0.464\n",
      "2019-04-10 00:57:23,907 root         INFO     Train Epoch: 69 [1024/8000 (13%)]\tTotal Loss: 0.536657\n",
      "Reconstruction: 0.459135, Regularization: 0.000432, Discriminator: 0.053034; Generator: 0.024056,\n",
      "D(x): 0.346, D(G(z)): 0.463\n",
      "2019-04-10 00:57:24,017 root         INFO     Train Epoch: 69 [1536/8000 (19%)]\tTotal Loss: 0.537127\n",
      "Reconstruction: 0.459514, Regularization: 0.000505, Discriminator: 0.053061; Generator: 0.024047,\n",
      "D(x): 0.347, D(G(z)): 0.463\n",
      "2019-04-10 00:57:24,127 root         INFO     Train Epoch: 69 [2048/8000 (26%)]\tTotal Loss: 0.537797\n",
      "Reconstruction: 0.459718, Regularization: 0.000274, Discriminator: 0.053766; Generator: 0.024039,\n",
      "D(x): 0.339, D(G(z)): 0.463\n",
      "2019-04-10 00:57:24,238 root         INFO     Train Epoch: 69 [2560/8000 (32%)]\tTotal Loss: 0.538211\n",
      "Reconstruction: 0.460206, Regularization: 0.000598, Discriminator: 0.053261; Generator: 0.024146,\n",
      "D(x): 0.345, D(G(z)): 0.462\n",
      "2019-04-10 00:57:24,348 root         INFO     Train Epoch: 69 [3072/8000 (38%)]\tTotal Loss: 0.539117\n",
      "Reconstruction: 0.460433, Regularization: 0.000623, Discriminator: 0.053997; Generator: 0.024064,\n",
      "D(x): 0.337, D(G(z)): 0.463\n",
      "2019-04-10 00:57:24,458 root         INFO     Train Epoch: 69 [3584/8000 (45%)]\tTotal Loss: 0.538648\n",
      "Reconstruction: 0.460873, Regularization: 0.000365, Discriminator: 0.053309; Generator: 0.024101,\n",
      "D(x): 0.344, D(G(z)): 0.462\n",
      "2019-04-10 00:57:24,568 root         INFO     Train Epoch: 69 [4096/8000 (51%)]\tTotal Loss: 0.540084\n",
      "Reconstruction: 0.461213, Regularization: 0.000525, Discriminator: 0.054083; Generator: 0.024262,\n",
      "D(x): 0.334, D(G(z)): 0.460\n",
      "2019-04-10 00:57:24,678 root         INFO     Train Epoch: 69 [4608/8000 (58%)]\tTotal Loss: 0.541977\n",
      "Reconstruction: 0.461747, Regularization: 0.000516, Discriminator: 0.055524; Generator: 0.024190,\n",
      "D(x): 0.319, D(G(z)): 0.461\n",
      "2019-04-10 00:57:24,789 root         INFO     Train Epoch: 69 [5120/8000 (64%)]\tTotal Loss: 0.539169\n",
      "Reconstruction: 0.462010, Regularization: 0.000290, Discriminator: 0.052597; Generator: 0.024272,\n",
      "D(x): 0.348, D(G(z)): 0.460\n",
      "2019-04-10 00:57:24,899 root         INFO     Train Epoch: 69 [5632/8000 (70%)]\tTotal Loss: 0.539570\n",
      "Reconstruction: 0.462268, Regularization: 0.000457, Discriminator: 0.052509; Generator: 0.024336,\n",
      "D(x): 0.348, D(G(z)): 0.459\n",
      "2019-04-10 00:57:25,009 root         INFO     Train Epoch: 69 [6144/8000 (77%)]\tTotal Loss: 0.539489\n",
      "Reconstruction: 0.462590, Regularization: 0.000507, Discriminator: 0.052060; Generator: 0.024333,\n",
      "D(x): 0.355, D(G(z)): 0.459\n",
      "2019-04-10 00:57:25,119 root         INFO     Train Epoch: 69 [6656/8000 (83%)]\tTotal Loss: 0.542015\n",
      "Reconstruction: 0.462978, Regularization: 0.000432, Discriminator: 0.054252; Generator: 0.024353,\n",
      "D(x): 0.330, D(G(z)): 0.459\n",
      "2019-04-10 00:57:25,229 root         INFO     Train Epoch: 69 [7168/8000 (90%)]\tTotal Loss: 0.542020\n",
      "Reconstruction: 0.463414, Regularization: 0.000259, Discriminator: 0.053976; Generator: 0.024371,\n",
      "D(x): 0.334, D(G(z)): 0.458\n",
      "2019-04-10 00:57:25,339 root         INFO     Train Epoch: 69 [7680/8000 (96%)]\tTotal Loss: 0.543455\n",
      "Reconstruction: 0.463835, Regularization: 0.000399, Discriminator: 0.054690; Generator: 0.024531,\n",
      "D(x): 0.323, D(G(z)): 0.456\n",
      "2019-04-10 00:57:25,419 root         INFO     ====> Epoch: 69 Average loss: 0.5394\n",
      "2019-04-10 00:57:25,446 root         INFO     Train Epoch: 70 [0/8000 (0%)]\tTotal Loss: 0.542308\n",
      "Reconstruction: 0.464030, Regularization: 0.000492, Discriminator: 0.053246; Generator: 0.024539,\n",
      "D(x): 0.338, D(G(z)): 0.456\n",
      "2019-04-10 00:57:25,555 root         INFO     Train Epoch: 70 [512/8000 (6%)]\tTotal Loss: 0.542082\n",
      "Reconstruction: 0.464197, Regularization: 0.000449, Discriminator: 0.052811; Generator: 0.024625,\n",
      "D(x): 0.343, D(G(z)): 0.455\n",
      "2019-04-10 00:57:25,666 root         INFO     Train Epoch: 70 [1024/8000 (13%)]\tTotal Loss: 0.542119\n",
      "Reconstruction: 0.464852, Regularization: 0.000285, Discriminator: 0.052327; Generator: 0.024655,\n",
      "D(x): 0.349, D(G(z)): 0.454\n",
      "2019-04-10 00:57:25,776 root         INFO     Train Epoch: 70 [1536/8000 (19%)]\tTotal Loss: 0.543418\n",
      "Reconstruction: 0.465033, Regularization: 0.000392, Discriminator: 0.053304; Generator: 0.024689,\n",
      "D(x): 0.335, D(G(z)): 0.454\n",
      "2019-04-10 00:57:25,885 root         INFO     Train Epoch: 70 [2048/8000 (26%)]\tTotal Loss: 0.542318\n",
      "Reconstruction: 0.465550, Regularization: 0.000416, Discriminator: 0.051669; Generator: 0.024683,\n",
      "D(x): 0.354, D(G(z)): 0.454\n",
      "2019-04-10 00:57:25,996 root         INFO     Train Epoch: 70 [2560/8000 (32%)]\tTotal Loss: 0.543814\n",
      "Reconstruction: 0.465758, Regularization: 0.000265, Discriminator: 0.053033; Generator: 0.024758,\n",
      "D(x): 0.338, D(G(z)): 0.453\n",
      "2019-04-10 00:57:26,105 root         INFO     Train Epoch: 70 [3072/8000 (38%)]\tTotal Loss: 0.542282\n",
      "Reconstruction: 0.466284, Regularization: 0.000339, Discriminator: 0.050834; Generator: 0.024824,\n",
      "D(x): 0.362, D(G(z)): 0.452\n",
      "2019-04-10 00:57:26,216 root         INFO     Train Epoch: 70 [3584/8000 (45%)]\tTotal Loss: 0.545017\n",
      "Reconstruction: 0.466447, Regularization: 0.000386, Discriminator: 0.053243; Generator: 0.024941,\n",
      "D(x): 0.335, D(G(z)): 0.450\n",
      "2019-04-10 00:57:26,325 root         INFO     Train Epoch: 70 [4096/8000 (51%)]\tTotal Loss: 0.544799\n",
      "Reconstruction: 0.466816, Regularization: 0.000526, Discriminator: 0.052338; Generator: 0.025119,\n",
      "D(x): 0.341, D(G(z)): 0.448\n",
      "2019-04-10 00:57:26,434 root         INFO     Train Epoch: 70 [4608/8000 (58%)]\tTotal Loss: 0.543897\n",
      "Reconstruction: 0.467342, Regularization: 0.000343, Discriminator: 0.051164; Generator: 0.025047,\n",
      "D(x): 0.357, D(G(z)): 0.449\n",
      "2019-04-10 00:57:26,545 root         INFO     Train Epoch: 70 [5120/8000 (64%)]\tTotal Loss: 0.545621\n",
      "Reconstruction: 0.467954, Regularization: 0.000393, Discriminator: 0.052215; Generator: 0.025059,\n",
      "D(x): 0.344, D(G(z)): 0.448\n",
      "2019-04-10 00:57:26,654 root         INFO     Train Epoch: 70 [5632/8000 (70%)]\tTotal Loss: 0.546869\n",
      "Reconstruction: 0.468225, Regularization: 0.000560, Discriminator: 0.052953; Generator: 0.025131,\n",
      "D(x): 0.338, D(G(z)): 0.447\n",
      "2019-04-10 00:57:26,762 root         INFO     Train Epoch: 70 [6144/8000 (77%)]\tTotal Loss: 0.546446\n",
      "Reconstruction: 0.468668, Regularization: 0.000398, Discriminator: 0.052132; Generator: 0.025248,\n",
      "D(x): 0.344, D(G(z)): 0.446\n",
      "2019-04-10 00:57:26,870 root         INFO     Train Epoch: 70 [6656/8000 (83%)]\tTotal Loss: 0.546170\n",
      "Reconstruction: 0.468946, Regularization: 0.000431, Discriminator: 0.051532; Generator: 0.025262,\n",
      "D(x): 0.349, D(G(z)): 0.446\n",
      "2019-04-10 00:57:26,977 root         INFO     Train Epoch: 70 [7168/8000 (90%)]\tTotal Loss: 0.547417\n",
      "Reconstruction: 0.469282, Regularization: 0.000321, Discriminator: 0.052528; Generator: 0.025286,\n",
      "D(x): 0.340, D(G(z)): 0.445\n",
      "2019-04-10 00:57:27,085 root         INFO     Train Epoch: 70 [7680/8000 (96%)]\tTotal Loss: 0.547193\n",
      "Reconstruction: 0.469837, Regularization: 0.000486, Discriminator: 0.051620; Generator: 0.025250,\n",
      "D(x): 0.349, D(G(z)): 0.446\n",
      "2019-04-10 00:57:27,164 root         INFO     ====> Epoch: 70 Average loss: 0.5448\n",
      "2019-04-10 00:57:27,191 root         INFO     Train Epoch: 71 [0/8000 (0%)]\tTotal Loss: 0.547230\n",
      "Reconstruction: 0.469874, Regularization: 0.000313, Discriminator: 0.051766; Generator: 0.025277,\n",
      "D(x): 0.348, D(G(z)): 0.445\n",
      "2019-04-10 00:57:27,303 root         INFO     Train Epoch: 71 [512/8000 (6%)]\tTotal Loss: 0.549377\n",
      "Reconstruction: 0.470553, Regularization: 0.000233, Discriminator: 0.053251; Generator: 0.025340,\n",
      "D(x): 0.329, D(G(z)): 0.444\n",
      "2019-04-10 00:57:27,413 root         INFO     Train Epoch: 71 [1024/8000 (13%)]\tTotal Loss: 0.547349\n",
      "Reconstruction: 0.470838, Regularization: 0.000339, Discriminator: 0.050699; Generator: 0.025473,\n",
      "D(x): 0.358, D(G(z)): 0.443\n",
      "2019-04-10 00:57:27,524 root         INFO     Train Epoch: 71 [1536/8000 (19%)]\tTotal Loss: 0.548958\n",
      "Reconstruction: 0.471185, Regularization: 0.000262, Discriminator: 0.051893; Generator: 0.025617,\n",
      "D(x): 0.342, D(G(z)): 0.441\n",
      "2019-04-10 00:57:27,634 root         INFO     Train Epoch: 71 [2048/8000 (26%)]\tTotal Loss: 0.548569\n",
      "Reconstruction: 0.471575, Regularization: 0.000316, Discriminator: 0.051074; Generator: 0.025603,\n",
      "D(x): 0.351, D(G(z)): 0.441\n",
      "2019-04-10 00:57:27,744 root         INFO     Train Epoch: 71 [2560/8000 (32%)]\tTotal Loss: 0.549920\n",
      "Reconstruction: 0.471960, Regularization: 0.000302, Discriminator: 0.052045; Generator: 0.025614,\n",
      "D(x): 0.341, D(G(z)): 0.441\n",
      "2019-04-10 00:57:27,855 root         INFO     Train Epoch: 71 [3072/8000 (38%)]\tTotal Loss: 0.549108\n",
      "Reconstruction: 0.472519, Regularization: 0.000263, Discriminator: 0.050671; Generator: 0.025654,\n",
      "D(x): 0.355, D(G(z)): 0.440\n",
      "2019-04-10 00:57:27,965 root         INFO     Train Epoch: 71 [3584/8000 (45%)]\tTotal Loss: 0.551884\n",
      "Reconstruction: 0.472661, Regularization: 0.000372, Discriminator: 0.053095; Generator: 0.025757,\n",
      "D(x): 0.329, D(G(z)): 0.439\n",
      "2019-04-10 00:57:28,076 root         INFO     Train Epoch: 71 [4096/8000 (51%)]\tTotal Loss: 0.550382\n",
      "Reconstruction: 0.473183, Regularization: 0.000255, Discriminator: 0.051081; Generator: 0.025863,\n",
      "D(x): 0.349, D(G(z)): 0.437\n",
      "2019-04-10 00:57:28,184 root         INFO     Train Epoch: 71 [4608/8000 (58%)]\tTotal Loss: 0.551558\n",
      "Reconstruction: 0.473540, Regularization: 0.000413, Discriminator: 0.051667; Generator: 0.025939,\n",
      "D(x): 0.342, D(G(z)): 0.436\n",
      "2019-04-10 00:57:28,292 root         INFO     Train Epoch: 71 [5120/8000 (64%)]\tTotal Loss: 0.550780\n",
      "Reconstruction: 0.473861, Regularization: 0.000441, Discriminator: 0.050551; Generator: 0.025927,\n",
      "D(x): 0.354, D(G(z)): 0.436\n",
      "2019-04-10 00:57:28,400 root         INFO     Train Epoch: 71 [5632/8000 (70%)]\tTotal Loss: 0.550793\n",
      "Reconstruction: 0.474273, Regularization: 0.000311, Discriminator: 0.050302; Generator: 0.025907,\n",
      "D(x): 0.357, D(G(z)): 0.436\n",
      "2019-04-10 00:57:28,508 root         INFO     Train Epoch: 71 [6144/8000 (77%)]\tTotal Loss: 0.551605\n",
      "Reconstruction: 0.474672, Regularization: 0.000290, Discriminator: 0.050648; Generator: 0.025995,\n",
      "D(x): 0.353, D(G(z)): 0.435\n",
      "2019-04-10 00:57:28,615 root         INFO     Train Epoch: 71 [6656/8000 (83%)]\tTotal Loss: 0.553264\n",
      "Reconstruction: 0.475024, Regularization: 0.000427, Discriminator: 0.051740; Generator: 0.026073,\n",
      "D(x): 0.341, D(G(z)): 0.434\n",
      "2019-04-10 00:57:28,722 root         INFO     Train Epoch: 71 [7168/8000 (90%)]\tTotal Loss: 0.553012\n",
      "Reconstruction: 0.475190, Regularization: 0.000346, Discriminator: 0.051378; Generator: 0.026099,\n",
      "D(x): 0.343, D(G(z)): 0.434\n",
      "2019-04-10 00:57:28,829 root         INFO     Train Epoch: 71 [7680/8000 (96%)]\tTotal Loss: 0.552891\n",
      "Reconstruction: 0.475696, Regularization: 0.000408, Discriminator: 0.050685; Generator: 0.026102,\n",
      "D(x): 0.350, D(G(z)): 0.434\n",
      "2019-04-10 00:57:28,909 root         INFO     ====> Epoch: 71 Average loss: 0.5507\n",
      "2019-04-10 00:57:28,936 root         INFO     Train Epoch: 72 [0/8000 (0%)]\tTotal Loss: 0.553436\n",
      "Reconstruction: 0.475664, Regularization: 0.000380, Discriminator: 0.051186; Generator: 0.026207,\n",
      "D(x): 0.344, D(G(z)): 0.432\n",
      "2019-04-10 00:57:29,049 root         INFO     Train Epoch: 72 [512/8000 (6%)]\tTotal Loss: 0.554046\n",
      "Reconstruction: 0.476155, Regularization: 0.000365, Discriminator: 0.051249; Generator: 0.026278,\n",
      "D(x): 0.343, D(G(z)): 0.431\n",
      "2019-04-10 00:57:29,161 root         INFO     Train Epoch: 72 [1024/8000 (13%)]\tTotal Loss: 0.553337\n",
      "Reconstruction: 0.476368, Regularization: 0.000327, Discriminator: 0.050235; Generator: 0.026407,\n",
      "D(x): 0.353, D(G(z)): 0.430\n",
      "2019-04-10 00:57:29,272 root         INFO     Train Epoch: 72 [1536/8000 (19%)]\tTotal Loss: 0.555253\n",
      "Reconstruction: 0.476680, Regularization: 0.000461, Discriminator: 0.051685; Generator: 0.026427,\n",
      "D(x): 0.337, D(G(z)): 0.429\n",
      "2019-04-10 00:57:29,383 root         INFO     Train Epoch: 72 [2048/8000 (26%)]\tTotal Loss: 0.554632\n",
      "Reconstruction: 0.477089, Regularization: 0.000371, Discriminator: 0.050815; Generator: 0.026358,\n",
      "D(x): 0.347, D(G(z)): 0.430\n",
      "2019-04-10 00:57:29,494 root         INFO     Train Epoch: 72 [2560/8000 (32%)]\tTotal Loss: 0.554513\n",
      "Reconstruction: 0.477249, Regularization: 0.000339, Discriminator: 0.050445; Generator: 0.026481,\n",
      "D(x): 0.350, D(G(z)): 0.429\n",
      "2019-04-10 00:57:29,606 root         INFO     Train Epoch: 72 [3072/8000 (38%)]\tTotal Loss: 0.554212\n",
      "Reconstruction: 0.477536, Regularization: 0.000338, Discriminator: 0.049753; Generator: 0.026585,\n",
      "D(x): 0.356, D(G(z)): 0.427\n",
      "2019-04-10 00:57:29,717 root         INFO     Train Epoch: 72 [3584/8000 (45%)]\tTotal Loss: 0.555392\n",
      "Reconstruction: 0.477810, Regularization: 0.000310, Discriminator: 0.050575; Generator: 0.026697,\n",
      "D(x): 0.346, D(G(z)): 0.426\n",
      "2019-04-10 00:57:29,828 root         INFO     Train Epoch: 72 [4096/8000 (51%)]\tTotal Loss: 0.554953\n",
      "Reconstruction: 0.478179, Regularization: 0.000252, Discriminator: 0.049840; Generator: 0.026682,\n",
      "D(x): 0.355, D(G(z)): 0.426\n",
      "2019-04-10 00:57:29,939 root         INFO     Train Epoch: 72 [4608/8000 (58%)]\tTotal Loss: 0.555629\n",
      "Reconstruction: 0.478600, Regularization: 0.000378, Discriminator: 0.049943; Generator: 0.026709,\n",
      "D(x): 0.354, D(G(z)): 0.425\n",
      "2019-04-10 00:57:30,049 root         INFO     Train Epoch: 72 [5120/8000 (64%)]\tTotal Loss: 0.556595\n",
      "Reconstruction: 0.478929, Regularization: 0.000289, Discriminator: 0.050572; Generator: 0.026805,\n",
      "D(x): 0.345, D(G(z)): 0.424\n",
      "2019-04-10 00:57:30,160 root         INFO     Train Epoch: 72 [5632/8000 (70%)]\tTotal Loss: 0.556655\n",
      "Reconstruction: 0.479098, Regularization: 0.000219, Discriminator: 0.050464; Generator: 0.026875,\n",
      "D(x): 0.346, D(G(z)): 0.423\n",
      "2019-04-10 00:57:30,270 root         INFO     Train Epoch: 72 [6144/8000 (77%)]\tTotal Loss: 0.557387\n",
      "Reconstruction: 0.479371, Regularization: 0.000348, Discriminator: 0.050752; Generator: 0.026916,\n",
      "D(x): 0.343, D(G(z)): 0.423\n",
      "2019-04-10 00:57:30,381 root         INFO     Train Epoch: 72 [6656/8000 (83%)]\tTotal Loss: 0.555948\n",
      "Reconstruction: 0.479601, Regularization: 0.000280, Discriminator: 0.048987; Generator: 0.027080,\n",
      "D(x): 0.361, D(G(z)): 0.420\n",
      "2019-04-10 00:57:30,492 root         INFO     Train Epoch: 72 [7168/8000 (90%)]\tTotal Loss: 0.556168\n",
      "Reconstruction: 0.479931, Regularization: 0.000282, Discriminator: 0.048850; Generator: 0.027105,\n",
      "D(x): 0.362, D(G(z)): 0.420\n",
      "2019-04-10 00:57:30,602 root         INFO     Train Epoch: 72 [7680/8000 (96%)]\tTotal Loss: 0.556955\n",
      "Reconstruction: 0.480245, Regularization: 0.000280, Discriminator: 0.049324; Generator: 0.027106,\n",
      "D(x): 0.357, D(G(z)): 0.420\n",
      "2019-04-10 00:57:30,684 root         INFO     ====> Epoch: 72 Average loss: 0.5557\n",
      "2019-04-10 00:57:30,711 root         INFO     Train Epoch: 73 [0/8000 (0%)]\tTotal Loss: 0.557614\n",
      "Reconstruction: 0.480359, Regularization: 0.000327, Discriminator: 0.049816; Generator: 0.027112,\n",
      "D(x): 0.351, D(G(z)): 0.420\n",
      "2019-04-10 00:57:30,823 root         INFO     Train Epoch: 73 [512/8000 (6%)]\tTotal Loss: 0.558323\n",
      "Reconstruction: 0.480816, Regularization: 0.000346, Discriminator: 0.049953; Generator: 0.027208,\n",
      "D(x): 0.349, D(G(z)): 0.419\n",
      "2019-04-10 00:57:30,935 root         INFO     Train Epoch: 73 [1024/8000 (13%)]\tTotal Loss: 0.558683\n",
      "Reconstruction: 0.481085, Regularization: 0.000530, Discriminator: 0.049819; Generator: 0.027249,\n",
      "D(x): 0.350, D(G(z)): 0.418\n",
      "2019-04-10 00:57:31,047 root         INFO     Train Epoch: 73 [1536/8000 (19%)]\tTotal Loss: 0.559261\n",
      "Reconstruction: 0.481351, Regularization: 0.000265, Discriminator: 0.050270; Generator: 0.027375,\n",
      "D(x): 0.345, D(G(z)): 0.416\n",
      "2019-04-10 00:57:31,158 root         INFO     Train Epoch: 73 [2048/8000 (26%)]\tTotal Loss: 0.558553\n",
      "Reconstruction: 0.481615, Regularization: 0.000276, Discriminator: 0.049243; Generator: 0.027420,\n",
      "D(x): 0.356, D(G(z)): 0.416\n",
      "2019-04-10 00:57:31,269 root         INFO     Train Epoch: 73 [2560/8000 (32%)]\tTotal Loss: 0.559642\n",
      "Reconstruction: 0.481927, Regularization: 0.000367, Discriminator: 0.049833; Generator: 0.027516,\n",
      "D(x): 0.347, D(G(z)): 0.415\n",
      "2019-04-10 00:57:31,379 root         INFO     Train Epoch: 73 [3072/8000 (38%)]\tTotal Loss: 0.559617\n",
      "Reconstruction: 0.482281, Regularization: 0.000364, Discriminator: 0.049369; Generator: 0.027604,\n",
      "D(x): 0.352, D(G(z)): 0.413\n",
      "2019-04-10 00:57:31,490 root         INFO     Train Epoch: 73 [3584/8000 (45%)]\tTotal Loss: 0.560097\n",
      "Reconstruction: 0.482457, Regularization: 0.000348, Discriminator: 0.049646; Generator: 0.027646,\n",
      "D(x): 0.349, D(G(z)): 0.413\n",
      "2019-04-10 00:57:31,600 root         INFO     Train Epoch: 73 [4096/8000 (51%)]\tTotal Loss: 0.561266\n",
      "Reconstruction: 0.482920, Regularization: 0.000342, Discriminator: 0.050315; Generator: 0.027689,\n",
      "D(x): 0.341, D(G(z)): 0.412\n",
      "2019-04-10 00:57:31,711 root         INFO     Train Epoch: 73 [4608/8000 (58%)]\tTotal Loss: 0.560740\n",
      "Reconstruction: 0.482946, Regularization: 0.000326, Discriminator: 0.049678; Generator: 0.027789,\n",
      "D(x): 0.347, D(G(z)): 0.411\n",
      "2019-04-10 00:57:31,821 root         INFO     Train Epoch: 73 [5120/8000 (64%)]\tTotal Loss: 0.560599\n",
      "Reconstruction: 0.483596, Regularization: 0.000225, Discriminator: 0.048869; Generator: 0.027909,\n",
      "D(x): 0.355, D(G(z)): 0.409\n",
      "2019-04-10 00:57:31,930 root         INFO     Train Epoch: 73 [5632/8000 (70%)]\tTotal Loss: 0.562370\n",
      "Reconstruction: 0.483877, Regularization: 0.000308, Discriminator: 0.050171; Generator: 0.028014,\n",
      "D(x): 0.340, D(G(z)): 0.408\n",
      "2019-04-10 00:57:32,039 root         INFO     Train Epoch: 73 [6144/8000 (77%)]\tTotal Loss: 0.561651\n",
      "Reconstruction: 0.484053, Regularization: 0.000347, Discriminator: 0.049138; Generator: 0.028113,\n",
      "D(x): 0.350, D(G(z)): 0.407\n",
      "2019-04-10 00:57:32,148 root         INFO     Train Epoch: 73 [6656/8000 (83%)]\tTotal Loss: 0.562016\n",
      "Reconstruction: 0.484367, Regularization: 0.000303, Discriminator: 0.049173; Generator: 0.028174,\n",
      "D(x): 0.350, D(G(z)): 0.406\n",
      "2019-04-10 00:57:32,258 root         INFO     Train Epoch: 73 [7168/8000 (90%)]\tTotal Loss: 0.562511\n",
      "Reconstruction: 0.484754, Regularization: 0.000322, Discriminator: 0.049183; Generator: 0.028252,\n",
      "D(x): 0.349, D(G(z)): 0.405\n",
      "2019-04-10 00:57:32,367 root         INFO     Train Epoch: 73 [7680/8000 (96%)]\tTotal Loss: 0.562257\n",
      "Reconstruction: 0.485008, Regularization: 0.000340, Discriminator: 0.048588; Generator: 0.028321,\n",
      "D(x): 0.355, D(G(z)): 0.404\n",
      "2019-04-10 00:57:32,450 root         INFO     ====> Epoch: 73 Average loss: 0.5603\n",
      "2019-04-10 00:57:32,477 root         INFO     Train Epoch: 74 [0/8000 (0%)]\tTotal Loss: 0.563082\n",
      "Reconstruction: 0.485306, Regularization: 0.000445, Discriminator: 0.048972; Generator: 0.028358,\n",
      "D(x): 0.350, D(G(z)): 0.404\n",
      "2019-04-10 00:57:32,588 root         INFO     Train Epoch: 74 [512/8000 (6%)]\tTotal Loss: 0.562817\n",
      "Reconstruction: 0.485437, Regularization: 0.000243, Discriminator: 0.048671; Generator: 0.028466,\n",
      "D(x): 0.353, D(G(z)): 0.402\n",
      "2019-04-10 00:57:32,698 root         INFO     Train Epoch: 74 [1024/8000 (13%)]\tTotal Loss: 0.563270\n",
      "Reconstruction: 0.485722, Regularization: 0.000264, Discriminator: 0.048720; Generator: 0.028564,\n",
      "D(x): 0.352, D(G(z)): 0.401\n",
      "2019-04-10 00:57:32,808 root         INFO     Train Epoch: 74 [1536/8000 (19%)]\tTotal Loss: 0.563589\n",
      "Reconstruction: 0.486019, Regularization: 0.000260, Discriminator: 0.048668; Generator: 0.028642,\n",
      "D(x): 0.351, D(G(z)): 0.400\n",
      "2019-04-10 00:57:32,916 root         INFO     Train Epoch: 74 [2048/8000 (26%)]\tTotal Loss: 0.564459\n",
      "Reconstruction: 0.486473, Regularization: 0.000384, Discriminator: 0.048868; Generator: 0.028734,\n",
      "D(x): 0.349, D(G(z)): 0.399\n",
      "2019-04-10 00:57:33,024 root         INFO     Train Epoch: 74 [2560/8000 (32%)]\tTotal Loss: 0.564602\n",
      "Reconstruction: 0.486725, Regularization: 0.000259, Discriminator: 0.048807; Generator: 0.028811,\n",
      "D(x): 0.349, D(G(z)): 0.398\n",
      "2019-04-10 00:57:33,133 root         INFO     Train Epoch: 74 [3072/8000 (38%)]\tTotal Loss: 0.564129\n",
      "Reconstruction: 0.486894, Regularization: 0.000246, Discriminator: 0.048030; Generator: 0.028959,\n",
      "D(x): 0.356, D(G(z)): 0.396\n",
      "2019-04-10 00:57:33,241 root         INFO     Train Epoch: 74 [3584/8000 (45%)]\tTotal Loss: 0.565505\n",
      "Reconstruction: 0.487386, Regularization: 0.000288, Discriminator: 0.048752; Generator: 0.029078,\n",
      "D(x): 0.347, D(G(z)): 0.394\n",
      "2019-04-10 00:57:33,349 root         INFO     Train Epoch: 74 [4096/8000 (51%)]\tTotal Loss: 0.565862\n",
      "Reconstruction: 0.487488, Regularization: 0.000544, Discriminator: 0.048661; Generator: 0.029168,\n",
      "D(x): 0.348, D(G(z)): 0.393\n",
      "2019-04-10 00:57:33,457 root         INFO     Train Epoch: 74 [4608/8000 (58%)]\tTotal Loss: 0.566036\n",
      "Reconstruction: 0.488044, Regularization: 0.000189, Discriminator: 0.048565; Generator: 0.029239,\n",
      "D(x): 0.348, D(G(z)): 0.392\n",
      "2019-04-10 00:57:33,565 root         INFO     Train Epoch: 74 [5120/8000 (64%)]\tTotal Loss: 0.565586\n",
      "Reconstruction: 0.488000, Regularization: 0.000251, Discriminator: 0.048013; Generator: 0.029322,\n",
      "D(x): 0.354, D(G(z)): 0.391\n",
      "2019-04-10 00:57:33,672 root         INFO     Train Epoch: 74 [5632/8000 (70%)]\tTotal Loss: 0.566412\n",
      "Reconstruction: 0.488450, Regularization: 0.000283, Discriminator: 0.048261; Generator: 0.029418,\n",
      "D(x): 0.350, D(G(z)): 0.390\n",
      "2019-04-10 00:57:33,780 root         INFO     Train Epoch: 74 [6144/8000 (77%)]\tTotal Loss: 0.566880\n",
      "Reconstruction: 0.488521, Regularization: 0.000423, Discriminator: 0.048391; Generator: 0.029544,\n",
      "D(x): 0.348, D(G(z)): 0.389\n",
      "2019-04-10 00:57:33,888 root         INFO     Train Epoch: 74 [6656/8000 (83%)]\tTotal Loss: 0.566507\n",
      "Reconstruction: 0.488785, Regularization: 0.000215, Discriminator: 0.047823; Generator: 0.029684,\n",
      "D(x): 0.353, D(G(z)): 0.387\n",
      "2019-04-10 00:57:33,997 root         INFO     Train Epoch: 74 [7168/8000 (90%)]\tTotal Loss: 0.567263\n",
      "Reconstruction: 0.489064, Regularization: 0.000233, Discriminator: 0.048158; Generator: 0.029807,\n",
      "D(x): 0.349, D(G(z)): 0.385\n",
      "2019-04-10 00:57:34,105 root         INFO     Train Epoch: 74 [7680/8000 (96%)]\tTotal Loss: 0.567849\n",
      "Reconstruction: 0.489338, Regularization: 0.000287, Discriminator: 0.048312; Generator: 0.029912,\n",
      "D(x): 0.346, D(G(z)): 0.384\n",
      "2019-04-10 00:57:34,185 root         INFO     ====> Epoch: 74 Average loss: 0.5653\n",
      "2019-04-10 00:57:34,212 root         INFO     Train Epoch: 75 [0/8000 (0%)]\tTotal Loss: 0.567286\n",
      "Reconstruction: 0.489362, Regularization: 0.000164, Discriminator: 0.047781; Generator: 0.029980,\n",
      "D(x): 0.352, D(G(z)): 0.383\n",
      "2019-04-10 00:57:34,324 root         INFO     Train Epoch: 75 [512/8000 (6%)]\tTotal Loss: 0.568174\n",
      "Reconstruction: 0.489747, Regularization: 0.000271, Discriminator: 0.048097; Generator: 0.030059,\n",
      "D(x): 0.348, D(G(z)): 0.382\n",
      "2019-04-10 00:57:34,434 root         INFO     Train Epoch: 75 [1024/8000 (13%)]\tTotal Loss: 0.568134\n",
      "Reconstruction: 0.489873, Regularization: 0.000339, Discriminator: 0.047744; Generator: 0.030178,\n",
      "D(x): 0.351, D(G(z)): 0.381\n",
      "2019-04-10 00:57:34,545 root         INFO     Train Epoch: 75 [1536/8000 (19%)]\tTotal Loss: 0.568496\n",
      "Reconstruction: 0.489957, Regularization: 0.000395, Discriminator: 0.047839; Generator: 0.030305,\n",
      "D(x): 0.349, D(G(z)): 0.379\n",
      "2019-04-10 00:57:34,656 root         INFO     Train Epoch: 75 [2048/8000 (26%)]\tTotal Loss: 0.568648\n",
      "Reconstruction: 0.490257, Regularization: 0.000308, Discriminator: 0.047647; Generator: 0.030436,\n",
      "D(x): 0.350, D(G(z)): 0.378\n",
      "2019-04-10 00:57:34,768 root         INFO     Train Epoch: 75 [2560/8000 (32%)]\tTotal Loss: 0.568614\n",
      "Reconstruction: 0.490491, Regularization: 0.000181, Discriminator: 0.047406; Generator: 0.030536,\n",
      "D(x): 0.352, D(G(z)): 0.376\n",
      "2019-04-10 00:57:34,879 root         INFO     Train Epoch: 75 [3072/8000 (38%)]\tTotal Loss: 0.569319\n",
      "Reconstruction: 0.490671, Regularization: 0.000293, Discriminator: 0.047725; Generator: 0.030630,\n",
      "D(x): 0.348, D(G(z)): 0.375\n",
      "2019-04-10 00:57:34,989 root         INFO     Train Epoch: 75 [3584/8000 (45%)]\tTotal Loss: 0.569341\n",
      "Reconstruction: 0.490805, Regularization: 0.000201, Discriminator: 0.047586; Generator: 0.030748,\n",
      "D(x): 0.348, D(G(z)): 0.374\n",
      "2019-04-10 00:57:35,100 root         INFO     Train Epoch: 75 [4096/8000 (51%)]\tTotal Loss: 0.569596\n",
      "Reconstruction: 0.490885, Regularization: 0.000215, Discriminator: 0.047626; Generator: 0.030870,\n",
      "D(x): 0.347, D(G(z)): 0.372\n",
      "2019-04-10 00:57:35,211 root         INFO     Train Epoch: 75 [4608/8000 (58%)]\tTotal Loss: 0.569630\n",
      "Reconstruction: 0.490971, Regularization: 0.000241, Discriminator: 0.047408; Generator: 0.031011,\n",
      "D(x): 0.349, D(G(z)): 0.371\n",
      "2019-04-10 00:57:35,321 root         INFO     Train Epoch: 75 [5120/8000 (64%)]\tTotal Loss: 0.570167\n",
      "Reconstruction: 0.491238, Regularization: 0.000304, Discriminator: 0.047544; Generator: 0.031082,\n",
      "D(x): 0.347, D(G(z)): 0.370\n",
      "2019-04-10 00:57:35,432 root         INFO     Train Epoch: 75 [5632/8000 (70%)]\tTotal Loss: 0.570101\n",
      "Reconstruction: 0.491273, Regularization: 0.000193, Discriminator: 0.047439; Generator: 0.031197,\n",
      "D(x): 0.347, D(G(z)): 0.369\n",
      "2019-04-10 00:57:35,543 root         INFO     Train Epoch: 75 [6144/8000 (77%)]\tTotal Loss: 0.570323\n",
      "Reconstruction: 0.491375, Regularization: 0.000336, Discriminator: 0.047310; Generator: 0.031302,\n",
      "D(x): 0.348, D(G(z)): 0.367\n",
      "2019-04-10 00:57:35,655 root         INFO     Train Epoch: 75 [6656/8000 (83%)]\tTotal Loss: 0.570512\n",
      "Reconstruction: 0.491477, Regularization: 0.000333, Discriminator: 0.047288; Generator: 0.031413,\n",
      "D(x): 0.347, D(G(z)): 0.366\n",
      "2019-04-10 00:57:35,766 root         INFO     Train Epoch: 75 [7168/8000 (90%)]\tTotal Loss: 0.570526\n",
      "Reconstruction: 0.491549, Regularization: 0.000314, Discriminator: 0.047139; Generator: 0.031524,\n",
      "D(x): 0.348, D(G(z)): 0.365\n",
      "2019-04-10 00:57:35,877 root         INFO     Train Epoch: 75 [7680/8000 (96%)]\tTotal Loss: 0.570726\n",
      "Reconstruction: 0.491622, Regularization: 0.000271, Discriminator: 0.047214; Generator: 0.031619,\n",
      "D(x): 0.347, D(G(z)): 0.364\n",
      "2019-04-10 00:57:35,957 root         INFO     ====> Epoch: 75 Average loss: 0.5694\n",
      "2019-04-10 00:57:35,985 root         INFO     Train Epoch: 76 [0/8000 (0%)]\tTotal Loss: 0.570879\n",
      "Reconstruction: 0.491670, Regularization: 0.000289, Discriminator: 0.047225; Generator: 0.031695,\n",
      "D(x): 0.346, D(G(z)): 0.363\n",
      "2019-04-10 00:57:36,096 root         INFO     Train Epoch: 76 [512/8000 (6%)]\tTotal Loss: 0.570628\n",
      "Reconstruction: 0.491571, Regularization: 0.000139, Discriminator: 0.047061; Generator: 0.031856,\n",
      "D(x): 0.347, D(G(z)): 0.361\n",
      "2019-04-10 00:57:36,208 root         INFO     Train Epoch: 76 [1024/8000 (13%)]\tTotal Loss: 0.570851\n",
      "Reconstruction: 0.491667, Regularization: 0.000216, Discriminator: 0.047015; Generator: 0.031953,\n",
      "D(x): 0.347, D(G(z)): 0.360\n",
      "2019-04-10 00:57:36,320 root         INFO     Train Epoch: 76 [1536/8000 (19%)]\tTotal Loss: 0.570981\n",
      "Reconstruction: 0.491712, Regularization: 0.000240, Discriminator: 0.047013; Generator: 0.032016,\n",
      "D(x): 0.347, D(G(z)): 0.359\n",
      "2019-04-10 00:57:36,431 root         INFO     Train Epoch: 76 [2048/8000 (26%)]\tTotal Loss: 0.570920\n",
      "Reconstruction: 0.491861, Regularization: 0.000172, Discriminator: 0.046853; Generator: 0.032034,\n",
      "D(x): 0.348, D(G(z)): 0.359\n",
      "2019-04-10 00:57:36,543 root         INFO     Train Epoch: 76 [2560/8000 (32%)]\tTotal Loss: 0.570826\n",
      "Reconstruction: 0.491628, Regularization: 0.000271, Discriminator: 0.046848; Generator: 0.032079,\n",
      "D(x): 0.348, D(G(z)): 0.358\n",
      "2019-04-10 00:57:36,654 root         INFO     Train Epoch: 76 [3072/8000 (38%)]\tTotal Loss: 0.571017\n",
      "Reconstruction: 0.491869, Regularization: 0.000247, Discriminator: 0.046754; Generator: 0.032146,\n",
      "D(x): 0.349, D(G(z)): 0.357\n",
      "2019-04-10 00:57:36,765 root         INFO     Train Epoch: 76 [3584/8000 (45%)]\tTotal Loss: 0.570874\n",
      "Reconstruction: 0.491698, Regularization: 0.000171, Discriminator: 0.046760; Generator: 0.032245,\n",
      "D(x): 0.348, D(G(z)): 0.356\n",
      "2019-04-10 00:57:36,877 root         INFO     Train Epoch: 76 [4096/8000 (51%)]\tTotal Loss: 0.570831\n",
      "Reconstruction: 0.491687, Regularization: 0.000203, Discriminator: 0.046675; Generator: 0.032265,\n",
      "D(x): 0.349, D(G(z)): 0.356\n",
      "2019-04-10 00:57:36,988 root         INFO     Train Epoch: 76 [4608/8000 (58%)]\tTotal Loss: 0.570978\n",
      "Reconstruction: 0.491752, Regularization: 0.000207, Discriminator: 0.046698; Generator: 0.032321,\n",
      "D(x): 0.348, D(G(z)): 0.355\n",
      "2019-04-10 00:57:37,099 root         INFO     Train Epoch: 76 [5120/8000 (64%)]\tTotal Loss: 0.571118\n",
      "Reconstruction: 0.491793, Regularization: 0.000302, Discriminator: 0.046638; Generator: 0.032385,\n",
      "D(x): 0.348, D(G(z)): 0.355\n",
      "2019-04-10 00:57:37,211 root         INFO     Train Epoch: 76 [5632/8000 (70%)]\tTotal Loss: 0.571089\n",
      "Reconstruction: 0.491714, Regularization: 0.000275, Discriminator: 0.046652; Generator: 0.032448,\n",
      "D(x): 0.348, D(G(z)): 0.354\n",
      "2019-04-10 00:57:37,322 root         INFO     Train Epoch: 76 [6144/8000 (77%)]\tTotal Loss: 0.571089\n",
      "Reconstruction: 0.491730, Regularization: 0.000268, Discriminator: 0.046523; Generator: 0.032568,\n",
      "D(x): 0.349, D(G(z)): 0.353\n",
      "2019-04-10 00:57:37,433 root         INFO     Train Epoch: 76 [6656/8000 (83%)]\tTotal Loss: 0.571002\n",
      "Reconstruction: 0.491561, Regularization: 0.000303, Discriminator: 0.046511; Generator: 0.032627,\n",
      "D(x): 0.348, D(G(z)): 0.352\n",
      "2019-04-10 00:57:37,545 root         INFO     Train Epoch: 76 [7168/8000 (90%)]\tTotal Loss: 0.571198\n",
      "Reconstruction: 0.491660, Regularization: 0.000351, Discriminator: 0.046474; Generator: 0.032712,\n",
      "D(x): 0.348, D(G(z)): 0.351\n",
      "2019-04-10 00:57:37,657 root         INFO     Train Epoch: 76 [7680/8000 (96%)]\tTotal Loss: 0.571303\n",
      "Reconstruction: 0.491760, Regularization: 0.000330, Discriminator: 0.046415; Generator: 0.032797,\n",
      "D(x): 0.348, D(G(z)): 0.350\n",
      "2019-04-10 00:57:37,738 root         INFO     ====> Epoch: 76 Average loss: 0.5710\n",
      "2019-04-10 00:57:37,765 root         INFO     Train Epoch: 77 [0/8000 (0%)]\tTotal Loss: 0.571264\n",
      "Reconstruction: 0.491585, Regularization: 0.000450, Discriminator: 0.046403; Generator: 0.032826,\n",
      "D(x): 0.348, D(G(z)): 0.350\n",
      "2019-04-10 00:57:37,876 root         INFO     Train Epoch: 77 [512/8000 (6%)]\tTotal Loss: 0.571064\n",
      "Reconstruction: 0.491607, Regularization: 0.000225, Discriminator: 0.046360; Generator: 0.032873,\n",
      "D(x): 0.349, D(G(z)): 0.349\n",
      "2019-04-10 00:57:37,987 root         INFO     Train Epoch: 77 [1024/8000 (13%)]\tTotal Loss: 0.571100\n",
      "Reconstruction: 0.491578, Regularization: 0.000318, Discriminator: 0.046305; Generator: 0.032899,\n",
      "D(x): 0.349, D(G(z)): 0.349\n",
      "2019-04-10 00:57:38,098 root         INFO     Train Epoch: 77 [1536/8000 (19%)]\tTotal Loss: 0.570882\n",
      "Reconstruction: 0.491489, Regularization: 0.000198, Discriminator: 0.046259; Generator: 0.032936,\n",
      "D(x): 0.349, D(G(z)): 0.349\n",
      "2019-04-10 00:57:38,207 root         INFO     Train Epoch: 77 [2048/8000 (26%)]\tTotal Loss: 0.570887\n",
      "Reconstruction: 0.491439, Regularization: 0.000239, Discriminator: 0.046231; Generator: 0.032977,\n",
      "D(x): 0.349, D(G(z)): 0.348\n",
      "2019-04-10 00:57:38,317 root         INFO     Train Epoch: 77 [2560/8000 (32%)]\tTotal Loss: 0.570909\n",
      "Reconstruction: 0.491452, Regularization: 0.000246, Discriminator: 0.046199; Generator: 0.033013,\n",
      "D(x): 0.350, D(G(z)): 0.348\n",
      "2019-04-10 00:57:38,426 root         INFO     Train Epoch: 77 [3072/8000 (38%)]\tTotal Loss: 0.570908\n",
      "Reconstruction: 0.491488, Regularization: 0.000241, Discriminator: 0.046115; Generator: 0.033064,\n",
      "D(x): 0.350, D(G(z)): 0.347\n",
      "2019-04-10 00:57:38,535 root         INFO     Train Epoch: 77 [3584/8000 (45%)]\tTotal Loss: 0.570965\n",
      "Reconstruction: 0.491378, Regularization: 0.000376, Discriminator: 0.046076; Generator: 0.033135,\n",
      "D(x): 0.350, D(G(z)): 0.346\n",
      "2019-04-10 00:57:38,645 root         INFO     Train Epoch: 77 [4096/8000 (51%)]\tTotal Loss: 0.570977\n",
      "Reconstruction: 0.491437, Regularization: 0.000284, Discriminator: 0.046034; Generator: 0.033222,\n",
      "D(x): 0.350, D(G(z)): 0.345\n",
      "2019-04-10 00:57:38,754 root         INFO     Train Epoch: 77 [4608/8000 (58%)]\tTotal Loss: 0.570702\n",
      "Reconstruction: 0.491237, Regularization: 0.000186, Discriminator: 0.046038; Generator: 0.033242,\n",
      "D(x): 0.350, D(G(z)): 0.345\n",
      "2019-04-10 00:57:38,864 root         INFO     Train Epoch: 77 [5120/8000 (64%)]\tTotal Loss: 0.570976\n",
      "Reconstruction: 0.491303, Regularization: 0.000370, Discriminator: 0.046009; Generator: 0.033294,\n",
      "D(x): 0.350, D(G(z)): 0.345\n",
      "2019-04-10 00:57:38,973 root         INFO     Train Epoch: 77 [5632/8000 (70%)]\tTotal Loss: 0.570890\n",
      "Reconstruction: 0.491437, Regularization: 0.000202, Discriminator: 0.045922; Generator: 0.033328,\n",
      "D(x): 0.351, D(G(z)): 0.344\n",
      "2019-04-10 00:57:39,082 root         INFO     Train Epoch: 77 [6144/8000 (77%)]\tTotal Loss: 0.570890\n",
      "Reconstruction: 0.491447, Regularization: 0.000265, Discriminator: 0.045831; Generator: 0.033347,\n",
      "D(x): 0.352, D(G(z)): 0.344\n",
      "2019-04-10 00:57:39,191 root         INFO     Train Epoch: 77 [6656/8000 (83%)]\tTotal Loss: 0.570726\n",
      "Reconstruction: 0.491230, Regularization: 0.000300, Discriminator: 0.045831; Generator: 0.033365,\n",
      "D(x): 0.352, D(G(z)): 0.344\n",
      "2019-04-10 00:57:39,301 root         INFO     Train Epoch: 77 [7168/8000 (90%)]\tTotal Loss: 0.570899\n",
      "Reconstruction: 0.491468, Regularization: 0.000237, Discriminator: 0.045823; Generator: 0.033370,\n",
      "D(x): 0.352, D(G(z)): 0.344\n",
      "2019-04-10 00:57:39,410 root         INFO     Train Epoch: 77 [7680/8000 (96%)]\tTotal Loss: 0.570776\n",
      "Reconstruction: 0.491283, Regularization: 0.000286, Discriminator: 0.045770; Generator: 0.033437,\n",
      "D(x): 0.352, D(G(z)): 0.343\n",
      "2019-04-10 00:57:39,490 root         INFO     ====> Epoch: 77 Average loss: 0.5709\n",
      "2019-04-10 00:57:39,517 root         INFO     Train Epoch: 78 [0/8000 (0%)]\tTotal Loss: 0.570713\n",
      "Reconstruction: 0.491221, Regularization: 0.000290, Discriminator: 0.045785; Generator: 0.033417,\n",
      "D(x): 0.352, D(G(z)): 0.343\n",
      "2019-04-10 00:57:39,630 root         INFO     Train Epoch: 78 [512/8000 (6%)]\tTotal Loss: 0.570840\n",
      "Reconstruction: 0.491242, Regularization: 0.000321, Discriminator: 0.045765; Generator: 0.033513,\n",
      "D(x): 0.351, D(G(z)): 0.342\n",
      "2019-04-10 00:57:39,743 root         INFO     Train Epoch: 78 [1024/8000 (13%)]\tTotal Loss: 0.570698\n",
      "Reconstruction: 0.491272, Regularization: 0.000308, Discriminator: 0.045592; Generator: 0.033527,\n",
      "D(x): 0.353, D(G(z)): 0.342\n",
      "2019-04-10 00:57:39,855 root         INFO     Train Epoch: 78 [1536/8000 (19%)]\tTotal Loss: 0.570572\n",
      "Reconstruction: 0.491165, Regularization: 0.000294, Discriminator: 0.045602; Generator: 0.033511,\n",
      "D(x): 0.353, D(G(z)): 0.342\n",
      "2019-04-10 00:57:39,967 root         INFO     Train Epoch: 78 [2048/8000 (26%)]\tTotal Loss: 0.570598\n",
      "Reconstruction: 0.491230, Regularization: 0.000248, Discriminator: 0.045560; Generator: 0.033560,\n",
      "D(x): 0.354, D(G(z)): 0.342\n",
      "2019-04-10 00:57:40,077 root         INFO     Train Epoch: 78 [2560/8000 (32%)]\tTotal Loss: 0.570281\n",
      "Reconstruction: 0.491023, Regularization: 0.000200, Discriminator: 0.045492; Generator: 0.033566,\n",
      "D(x): 0.354, D(G(z)): 0.342\n",
      "2019-04-10 00:57:40,187 root         INFO     Train Epoch: 78 [3072/8000 (38%)]\tTotal Loss: 0.570353\n",
      "Reconstruction: 0.490959, Regularization: 0.000302, Discriminator: 0.045537; Generator: 0.033554,\n",
      "D(x): 0.354, D(G(z)): 0.342\n",
      "2019-04-10 00:57:40,298 root         INFO     Train Epoch: 78 [3584/8000 (45%)]\tTotal Loss: 0.570224\n",
      "Reconstruction: 0.490821, Regularization: 0.000218, Discriminator: 0.045618; Generator: 0.033568,\n",
      "D(x): 0.353, D(G(z)): 0.342\n",
      "2019-04-10 00:57:40,408 root         INFO     Train Epoch: 78 [4096/8000 (51%)]\tTotal Loss: 0.570164\n",
      "Reconstruction: 0.490809, Regularization: 0.000395, Discriminator: 0.045352; Generator: 0.033608,\n",
      "D(x): 0.356, D(G(z)): 0.341\n",
      "2019-04-10 00:57:40,519 root         INFO     Train Epoch: 78 [4608/8000 (58%)]\tTotal Loss: 0.569971\n",
      "Reconstruction: 0.490908, Regularization: 0.000225, Discriminator: 0.045266; Generator: 0.033572,\n",
      "D(x): 0.357, D(G(z)): 0.342\n",
      "2019-04-10 00:57:40,629 root         INFO     Train Epoch: 78 [5120/8000 (64%)]\tTotal Loss: 0.569841\n",
      "Reconstruction: 0.490634, Regularization: 0.000381, Discriminator: 0.045171; Generator: 0.033655,\n",
      "D(x): 0.357, D(G(z)): 0.341\n",
      "2019-04-10 00:57:40,739 root         INFO     Train Epoch: 78 [5632/8000 (70%)]\tTotal Loss: 0.569894\n",
      "Reconstruction: 0.490512, Regularization: 0.000322, Discriminator: 0.045374; Generator: 0.033687,\n",
      "D(x): 0.355, D(G(z)): 0.340\n",
      "2019-04-10 00:57:40,849 root         INFO     Train Epoch: 78 [6144/8000 (77%)]\tTotal Loss: 0.569663\n",
      "Reconstruction: 0.490502, Regularization: 0.000251, Discriminator: 0.045129; Generator: 0.033781,\n",
      "D(x): 0.357, D(G(z)): 0.339\n",
      "2019-04-10 00:57:40,960 root         INFO     Train Epoch: 78 [6656/8000 (83%)]\tTotal Loss: 0.569402\n",
      "Reconstruction: 0.490262, Regularization: 0.000202, Discriminator: 0.045151; Generator: 0.033787,\n",
      "D(x): 0.357, D(G(z)): 0.339\n",
      "2019-04-10 00:57:41,070 root         INFO     Train Epoch: 78 [7168/8000 (90%)]\tTotal Loss: 0.569640\n",
      "Reconstruction: 0.490298, Regularization: 0.000342, Discriminator: 0.045173; Generator: 0.033826,\n",
      "D(x): 0.356, D(G(z)): 0.339\n",
      "2019-04-10 00:57:41,180 root         INFO     Train Epoch: 78 [7680/8000 (96%)]\tTotal Loss: 0.569183\n",
      "Reconstruction: 0.490163, Regularization: 0.000195, Discriminator: 0.044994; Generator: 0.033831,\n",
      "D(x): 0.358, D(G(z)): 0.339\n",
      "2019-04-10 00:57:41,261 root         INFO     ====> Epoch: 78 Average loss: 0.5701\n",
      "2019-04-10 00:57:41,288 root         INFO     Train Epoch: 79 [0/8000 (0%)]\tTotal Loss: 0.568726\n",
      "Reconstruction: 0.489876, Regularization: 0.000159, Discriminator: 0.044860; Generator: 0.033831,\n",
      "D(x): 0.360, D(G(z)): 0.339\n",
      "2019-04-10 00:57:41,401 root         INFO     Train Epoch: 79 [512/8000 (6%)]\tTotal Loss: 0.568638\n",
      "Reconstruction: 0.489787, Regularization: 0.000288, Discriminator: 0.044685; Generator: 0.033878,\n",
      "D(x): 0.362, D(G(z)): 0.338\n",
      "2019-04-10 00:57:41,513 root         INFO     Train Epoch: 79 [1024/8000 (13%)]\tTotal Loss: 0.568839\n",
      "Reconstruction: 0.489495, Regularization: 0.000238, Discriminator: 0.045251; Generator: 0.033855,\n",
      "D(x): 0.355, D(G(z)): 0.338\n",
      "2019-04-10 00:57:41,625 root         INFO     Train Epoch: 79 [1536/8000 (19%)]\tTotal Loss: 0.568019\n",
      "Reconstruction: 0.489112, Regularization: 0.000220, Discriminator: 0.044896; Generator: 0.033789,\n",
      "D(x): 0.360, D(G(z)): 0.339\n",
      "2019-04-10 00:57:41,737 root         INFO     Train Epoch: 79 [2048/8000 (26%)]\tTotal Loss: 0.567785\n",
      "Reconstruction: 0.489123, Regularization: 0.000154, Discriminator: 0.044687; Generator: 0.033821,\n",
      "D(x): 0.362, D(G(z)): 0.339\n",
      "2019-04-10 00:57:41,850 root         INFO     Train Epoch: 79 [2560/8000 (32%)]\tTotal Loss: 0.567637\n",
      "Reconstruction: 0.488808, Regularization: 0.000350, Discriminator: 0.044642; Generator: 0.033836,\n",
      "D(x): 0.363, D(G(z)): 0.339\n",
      "2019-04-10 00:57:41,961 root         INFO     Train Epoch: 79 [3072/8000 (38%)]\tTotal Loss: 0.567198\n",
      "Reconstruction: 0.488554, Regularization: 0.000269, Discriminator: 0.044577; Generator: 0.033797,\n",
      "D(x): 0.363, D(G(z)): 0.339\n",
      "2019-04-10 00:57:42,073 root         INFO     Train Epoch: 79 [3584/8000 (45%)]\tTotal Loss: 0.567055\n",
      "Reconstruction: 0.488474, Regularization: 0.000419, Discriminator: 0.044387; Generator: 0.033774,\n",
      "D(x): 0.366, D(G(z)): 0.339\n",
      "2019-04-10 00:57:42,185 root         INFO     Train Epoch: 79 [4096/8000 (51%)]\tTotal Loss: 0.566319\n",
      "Reconstruction: 0.488063, Regularization: 0.000244, Discriminator: 0.044308; Generator: 0.033705,\n",
      "D(x): 0.367, D(G(z)): 0.340\n",
      "2019-04-10 00:57:42,297 root         INFO     Train Epoch: 79 [4608/8000 (58%)]\tTotal Loss: 0.565936\n",
      "Reconstruction: 0.487567, Regularization: 0.000259, Discriminator: 0.044502; Generator: 0.033609,\n",
      "D(x): 0.366, D(G(z)): 0.341\n",
      "2019-04-10 00:57:42,407 root         INFO     Train Epoch: 79 [5120/8000 (64%)]\tTotal Loss: 0.565629\n",
      "Reconstruction: 0.487406, Regularization: 0.000303, Discriminator: 0.044403; Generator: 0.033517,\n",
      "D(x): 0.367, D(G(z)): 0.342\n",
      "2019-04-10 00:57:42,517 root         INFO     Train Epoch: 79 [5632/8000 (70%)]\tTotal Loss: 0.565306\n",
      "Reconstruction: 0.487234, Regularization: 0.000251, Discriminator: 0.044391; Generator: 0.033430,\n",
      "D(x): 0.368, D(G(z)): 0.343\n",
      "2019-04-10 00:57:42,627 root         INFO     Train Epoch: 79 [6144/8000 (77%)]\tTotal Loss: 0.564960\n",
      "Reconstruction: 0.486866, Regularization: 0.000205, Discriminator: 0.044561; Generator: 0.033327,\n",
      "D(x): 0.366, D(G(z)): 0.344\n",
      "2019-04-10 00:57:42,737 root         INFO     Train Epoch: 79 [6656/8000 (83%)]\tTotal Loss: 0.564003\n",
      "Reconstruction: 0.486527, Regularization: 0.000184, Discriminator: 0.044140; Generator: 0.033151,\n",
      "D(x): 0.373, D(G(z)): 0.346\n",
      "2019-04-10 00:57:42,848 root         INFO     Train Epoch: 79 [7168/8000 (90%)]\tTotal Loss: 0.563162\n",
      "Reconstruction: 0.486069, Regularization: 0.000226, Discriminator: 0.043875; Generator: 0.032991,\n",
      "D(x): 0.377, D(G(z)): 0.348\n",
      "2019-04-10 00:57:42,958 root         INFO     Train Epoch: 79 [7680/8000 (96%)]\tTotal Loss: 0.562701\n",
      "Reconstruction: 0.485862, Regularization: 0.000256, Discriminator: 0.043789; Generator: 0.032795,\n",
      "D(x): 0.379, D(G(z)): 0.350\n",
      "2019-04-10 00:57:43,039 root         INFO     ====> Epoch: 79 Average loss: 0.5662\n",
      "2019-04-10 00:57:43,067 root         INFO     Train Epoch: 80 [0/8000 (0%)]\tTotal Loss: 0.562259\n",
      "Reconstruction: 0.485543, Regularization: 0.000203, Discriminator: 0.043859; Generator: 0.032654,\n",
      "D(x): 0.379, D(G(z)): 0.352\n",
      "2019-04-10 00:57:43,179 root         INFO     Train Epoch: 80 [512/8000 (6%)]\tTotal Loss: 0.561558\n",
      "Reconstruction: 0.485282, Regularization: 0.000186, Discriminator: 0.043721; Generator: 0.032369,\n",
      "D(x): 0.383, D(G(z)): 0.355\n",
      "2019-04-10 00:57:43,291 root         INFO     Train Epoch: 80 [1024/8000 (13%)]\tTotal Loss: 0.560864\n",
      "Reconstruction: 0.484996, Regularization: 0.000326, Discriminator: 0.043405; Generator: 0.032138,\n",
      "D(x): 0.388, D(G(z)): 0.358\n",
      "2019-04-10 00:57:43,403 root         INFO     Train Epoch: 80 [1536/8000 (19%)]\tTotal Loss: 0.560275\n",
      "Reconstruction: 0.484792, Regularization: 0.000193, Discriminator: 0.043446; Generator: 0.031845,\n",
      "D(x): 0.390, D(G(z)): 0.361\n",
      "2019-04-10 00:57:43,515 root         INFO     Train Epoch: 80 [2048/8000 (26%)]\tTotal Loss: 0.559839\n",
      "Reconstruction: 0.484601, Regularization: 0.000278, Discriminator: 0.043365; Generator: 0.031595,\n",
      "D(x): 0.393, D(G(z)): 0.364\n",
      "2019-04-10 00:57:43,626 root         INFO     Train Epoch: 80 [2560/8000 (32%)]\tTotal Loss: 0.559187\n",
      "Reconstruction: 0.484292, Regularization: 0.000247, Discriminator: 0.043260; Generator: 0.031388,\n",
      "D(x): 0.395, D(G(z)): 0.366\n",
      "2019-04-10 00:57:43,738 root         INFO     Train Epoch: 80 [3072/8000 (38%)]\tTotal Loss: 0.558173\n",
      "Reconstruction: 0.484097, Regularization: 0.000208, Discriminator: 0.042823; Generator: 0.031044,\n",
      "D(x): 0.404, D(G(z)): 0.370\n",
      "2019-04-10 00:57:43,849 root         INFO     Train Epoch: 80 [3584/8000 (45%)]\tTotal Loss: 0.557735\n",
      "Reconstruction: 0.483777, Regularization: 0.000313, Discriminator: 0.042889; Generator: 0.030756,\n",
      "D(x): 0.405, D(G(z)): 0.374\n",
      "2019-04-10 00:57:43,961 root         INFO     Train Epoch: 80 [4096/8000 (51%)]\tTotal Loss: 0.557221\n",
      "Reconstruction: 0.483355, Regularization: 0.000148, Discriminator: 0.043182; Generator: 0.030537,\n",
      "D(x): 0.403, D(G(z)): 0.376\n",
      "2019-04-10 00:57:44,073 root         INFO     Train Epoch: 80 [4608/8000 (58%)]\tTotal Loss: 0.556436\n",
      "Reconstruction: 0.482903, Regularization: 0.000322, Discriminator: 0.042856; Generator: 0.030355,\n",
      "D(x): 0.409, D(G(z)): 0.379\n",
      "2019-04-10 00:57:44,183 root         INFO     Train Epoch: 80 [5120/8000 (64%)]\tTotal Loss: 0.555964\n",
      "Reconstruction: 0.482717, Regularization: 0.000382, Discriminator: 0.042690; Generator: 0.030175,\n",
      "D(x): 0.412, D(G(z)): 0.381\n",
      "2019-04-10 00:57:44,292 root         INFO     Train Epoch: 80 [5632/8000 (70%)]\tTotal Loss: 0.555107\n",
      "Reconstruction: 0.482319, Regularization: 0.000205, Discriminator: 0.042547; Generator: 0.030036,\n",
      "D(x): 0.415, D(G(z)): 0.382\n",
      "2019-04-10 00:57:44,402 root         INFO     Train Epoch: 80 [6144/8000 (77%)]\tTotal Loss: 0.554462\n",
      "Reconstruction: 0.481713, Regularization: 0.000235, Discriminator: 0.042517; Generator: 0.029997,\n",
      "D(x): 0.416, D(G(z)): 0.383\n",
      "2019-04-10 00:57:44,511 root         INFO     Train Epoch: 80 [6656/8000 (83%)]\tTotal Loss: 0.553534\n",
      "Reconstruction: 0.481144, Regularization: 0.000233, Discriminator: 0.042179; Generator: 0.029979,\n",
      "D(x): 0.421, D(G(z)): 0.383\n",
      "2019-04-10 00:57:44,621 root         INFO     Train Epoch: 80 [7168/8000 (90%)]\tTotal Loss: 0.553146\n",
      "Reconstruction: 0.480628, Regularization: 0.000208, Discriminator: 0.042359; Generator: 0.029950,\n",
      "D(x): 0.419, D(G(z)): 0.384\n",
      "2019-04-10 00:57:44,731 root         INFO     Train Epoch: 80 [7680/8000 (96%)]\tTotal Loss: 0.552080\n",
      "Reconstruction: 0.479899, Regularization: 0.000247, Discriminator: 0.041952; Generator: 0.029982,\n",
      "D(x): 0.424, D(G(z)): 0.383\n",
      "2019-04-10 00:57:44,812 root         INFO     ====> Epoch: 80 Average loss: 0.5571\n",
      "2019-04-10 00:57:44,839 root         INFO     Train Epoch: 81 [0/8000 (0%)]\tTotal Loss: 0.551755\n",
      "Reconstruction: 0.479390, Regularization: 0.000171, Discriminator: 0.042228; Generator: 0.029966,\n",
      "D(x): 0.420, D(G(z)): 0.383\n",
      "2019-04-10 00:57:44,952 root         INFO     Train Epoch: 81 [512/8000 (6%)]\tTotal Loss: 0.550990\n",
      "Reconstruction: 0.478683, Regularization: 0.000256, Discriminator: 0.042229; Generator: 0.029822,\n",
      "D(x): 0.421, D(G(z)): 0.385\n",
      "2019-04-10 00:57:45,064 root         INFO     Train Epoch: 81 [1024/8000 (13%)]\tTotal Loss: 0.549458\n",
      "Reconstruction: 0.477807, Regularization: 0.000261, Discriminator: 0.041675; Generator: 0.029714,\n",
      "D(x): 0.430, D(G(z)): 0.386\n",
      "2019-04-10 00:57:45,176 root         INFO     Train Epoch: 81 [1536/8000 (19%)]\tTotal Loss: 0.549086\n",
      "Reconstruction: 0.477538, Regularization: 0.000265, Discriminator: 0.041626; Generator: 0.029657,\n",
      "D(x): 0.431, D(G(z)): 0.387\n",
      "2019-04-10 00:57:45,288 root         INFO     Train Epoch: 81 [2048/8000 (26%)]\tTotal Loss: 0.548390\n",
      "Reconstruction: 0.477018, Regularization: 0.000257, Discriminator: 0.041347; Generator: 0.029769,\n",
      "D(x): 0.434, D(G(z)): 0.386\n",
      "2019-04-10 00:57:45,399 root         INFO     Train Epoch: 81 [2560/8000 (32%)]\tTotal Loss: 0.548519\n",
      "Reconstruction: 0.476756, Regularization: 0.000293, Discriminator: 0.041670; Generator: 0.029799,\n",
      "D(x): 0.429, D(G(z)): 0.385\n",
      "2019-04-10 00:57:45,510 root         INFO     Train Epoch: 81 [3072/8000 (38%)]\tTotal Loss: 0.547455\n",
      "Reconstruction: 0.476302, Regularization: 0.000281, Discriminator: 0.041095; Generator: 0.029778,\n",
      "D(x): 0.437, D(G(z)): 0.386\n",
      "2019-04-10 00:57:45,622 root         INFO     Train Epoch: 81 [3584/8000 (45%)]\tTotal Loss: 0.547346\n",
      "Reconstruction: 0.475923, Regularization: 0.000148, Discriminator: 0.041455; Generator: 0.029821,\n",
      "D(x): 0.432, D(G(z)): 0.385\n",
      "2019-04-10 00:57:45,733 root         INFO     Train Epoch: 81 [4096/8000 (51%)]\tTotal Loss: 0.546381\n",
      "Reconstruction: 0.475440, Regularization: 0.000343, Discriminator: 0.040900; Generator: 0.029697,\n",
      "D(x): 0.441, D(G(z)): 0.387\n",
      "2019-04-10 00:57:45,845 root         INFO     Train Epoch: 81 [4608/8000 (58%)]\tTotal Loss: 0.546445\n",
      "Reconstruction: 0.474752, Regularization: 0.000256, Discriminator: 0.041590; Generator: 0.029847,\n",
      "D(x): 0.430, D(G(z)): 0.385\n",
      "2019-04-10 00:57:45,956 root         INFO     Train Epoch: 81 [5120/8000 (64%)]\tTotal Loss: 0.545159\n",
      "Reconstruction: 0.474340, Regularization: 0.000246, Discriminator: 0.040698; Generator: 0.029875,\n",
      "D(x): 0.442, D(G(z)): 0.384\n",
      "2019-04-10 00:57:46,067 root         INFO     Train Epoch: 81 [5632/8000 (70%)]\tTotal Loss: 0.544949\n",
      "Reconstruction: 0.473405, Regularization: 0.000410, Discriminator: 0.041229; Generator: 0.029905,\n",
      "D(x): 0.435, D(G(z)): 0.384\n",
      "2019-04-10 00:57:46,178 root         INFO     Train Epoch: 81 [6144/8000 (77%)]\tTotal Loss: 0.543623\n",
      "Reconstruction: 0.472438, Regularization: 0.000228, Discriminator: 0.041097; Generator: 0.029860,\n",
      "D(x): 0.437, D(G(z)): 0.385\n",
      "2019-04-10 00:57:46,289 root         INFO     Train Epoch: 81 [6656/8000 (83%)]\tTotal Loss: 0.542305\n",
      "Reconstruction: 0.471841, Regularization: 0.000295, Discriminator: 0.040683; Generator: 0.029487,\n",
      "D(x): 0.446, D(G(z)): 0.389\n",
      "2019-04-10 00:57:46,400 root         INFO     Train Epoch: 81 [7168/8000 (90%)]\tTotal Loss: 0.541438\n",
      "Reconstruction: 0.470906, Regularization: 0.000220, Discriminator: 0.040604; Generator: 0.029709,\n",
      "D(x): 0.445, D(G(z)): 0.386\n",
      "2019-04-10 00:57:46,510 root         INFO     Train Epoch: 81 [7680/8000 (96%)]\tTotal Loss: 0.541170\n",
      "Reconstruction: 0.470075, Regularization: 0.000291, Discriminator: 0.041232; Generator: 0.029571,\n",
      "D(x): 0.438, D(G(z)): 0.388\n",
      "2019-04-10 00:57:46,592 root         INFO     ====> Epoch: 81 Average loss: 0.5462\n",
      "2019-04-10 00:57:46,619 root         INFO     Train Epoch: 82 [0/8000 (0%)]\tTotal Loss: 0.539903\n",
      "Reconstruction: 0.469451, Regularization: 0.000180, Discriminator: 0.040480; Generator: 0.029792,\n",
      "D(x): 0.446, D(G(z)): 0.385\n",
      "2019-04-10 00:57:46,731 root         INFO     Train Epoch: 82 [512/8000 (6%)]\tTotal Loss: 0.539149\n",
      "Reconstruction: 0.468484, Regularization: 0.000251, Discriminator: 0.040501; Generator: 0.029913,\n",
      "D(x): 0.445, D(G(z)): 0.384\n",
      "2019-04-10 00:57:46,843 root         INFO     Train Epoch: 82 [1024/8000 (13%)]\tTotal Loss: 0.538467\n",
      "Reconstruction: 0.467538, Regularization: 0.000136, Discriminator: 0.040841; Generator: 0.029952,\n",
      "D(x): 0.440, D(G(z)): 0.383\n",
      "2019-04-10 00:57:46,954 root         INFO     Train Epoch: 82 [1536/8000 (19%)]\tTotal Loss: 0.536545\n",
      "Reconstruction: 0.466492, Regularization: 0.000126, Discriminator: 0.040433; Generator: 0.029494,\n",
      "D(x): 0.450, D(G(z)): 0.389\n",
      "2019-04-10 00:57:47,065 root         INFO     Train Epoch: 82 [2048/8000 (26%)]\tTotal Loss: 0.534882\n",
      "Reconstruction: 0.465745, Regularization: 0.000210, Discriminator: 0.039668; Generator: 0.029259,\n",
      "D(x): 0.463, D(G(z)): 0.392\n",
      "2019-04-10 00:57:47,177 root         INFO     Train Epoch: 82 [2560/8000 (32%)]\tTotal Loss: 0.533771\n",
      "Reconstruction: 0.464914, Regularization: 0.000248, Discriminator: 0.039350; Generator: 0.029259,\n",
      "D(x): 0.468, D(G(z)): 0.392\n",
      "2019-04-10 00:57:47,288 root         INFO     Train Epoch: 82 [3072/8000 (38%)]\tTotal Loss: 0.534014\n",
      "Reconstruction: 0.464128, Regularization: 0.000242, Discriminator: 0.040038; Generator: 0.029606,\n",
      "D(x): 0.454, D(G(z)): 0.388\n",
      "2019-04-10 00:57:47,399 root         INFO     Train Epoch: 82 [3584/8000 (45%)]\tTotal Loss: 0.533374\n",
      "Reconstruction: 0.463539, Regularization: 0.000253, Discriminator: 0.040420; Generator: 0.029162,\n",
      "D(x): 0.453, D(G(z)): 0.393\n",
      "2019-04-10 00:57:47,510 root         INFO     Train Epoch: 82 [4096/8000 (51%)]\tTotal Loss: 0.532149\n",
      "Reconstruction: 0.462831, Regularization: 0.000116, Discriminator: 0.039929; Generator: 0.029273,\n",
      "D(x): 0.459, D(G(z)): 0.392\n",
      "2019-04-10 00:57:47,622 root         INFO     Train Epoch: 82 [4608/8000 (58%)]\tTotal Loss: 0.531245\n",
      "Reconstruction: 0.461722, Regularization: 0.000348, Discriminator: 0.039450; Generator: 0.029725,\n",
      "D(x): 0.463, D(G(z)): 0.386\n",
      "2019-04-10 00:57:47,733 root         INFO     Train Epoch: 82 [5120/8000 (64%)]\tTotal Loss: 0.529878\n",
      "Reconstruction: 0.460304, Regularization: 0.000238, Discriminator: 0.040190; Generator: 0.029147,\n",
      "D(x): 0.457, D(G(z)): 0.393\n",
      "2019-04-10 00:57:47,844 root         INFO     Train Epoch: 82 [5632/8000 (70%)]\tTotal Loss: 0.528521\n",
      "Reconstruction: 0.459561, Regularization: 0.000328, Discriminator: 0.039572; Generator: 0.029059,\n",
      "D(x): 0.468, D(G(z)): 0.395\n",
      "2019-04-10 00:57:47,955 root         INFO     Train Epoch: 82 [6144/8000 (77%)]\tTotal Loss: 0.526968\n",
      "Reconstruction: 0.458015, Regularization: 0.000271, Discriminator: 0.039655; Generator: 0.029027,\n",
      "D(x): 0.466, D(G(z)): 0.395\n",
      "2019-04-10 00:57:48,066 root         INFO     Train Epoch: 82 [6656/8000 (83%)]\tTotal Loss: 0.526905\n",
      "Reconstruction: 0.457035, Regularization: 0.000238, Discriminator: 0.040084; Generator: 0.029548,\n",
      "D(x): 0.455, D(G(z)): 0.388\n",
      "2019-04-10 00:57:48,177 root         INFO     Train Epoch: 82 [7168/8000 (90%)]\tTotal Loss: 0.524141\n",
      "Reconstruction: 0.455081, Regularization: 0.000165, Discriminator: 0.040146; Generator: 0.028749,\n",
      "D(x): 0.461, D(G(z)): 0.399\n",
      "2019-04-10 00:57:48,288 root         INFO     Train Epoch: 82 [7680/8000 (96%)]\tTotal Loss: 0.521894\n",
      "Reconstruction: 0.453622, Regularization: 0.000339, Discriminator: 0.039282; Generator: 0.028651,\n",
      "D(x): 0.477, D(G(z)): 0.400\n",
      "2019-04-10 00:57:48,369 root         INFO     ====> Epoch: 82 Average loss: 0.5317\n",
      "2019-04-10 00:57:48,397 root         INFO     Train Epoch: 83 [0/8000 (0%)]\tTotal Loss: 0.521195\n",
      "Reconstruction: 0.453317, Regularization: 0.000153, Discriminator: 0.038829; Generator: 0.028895,\n",
      "D(x): 0.479, D(G(z)): 0.397\n",
      "2019-04-10 00:57:48,508 root         INFO     Train Epoch: 83 [512/8000 (6%)]\tTotal Loss: 0.520007\n",
      "Reconstruction: 0.451497, Regularization: 0.000215, Discriminator: 0.039667; Generator: 0.028628,\n",
      "D(x): 0.470, D(G(z)): 0.400\n",
      "2019-04-10 00:57:48,619 root         INFO     Train Epoch: 83 [1024/8000 (13%)]\tTotal Loss: 0.519209\n",
      "Reconstruction: 0.450366, Regularization: 0.000228, Discriminator: 0.039959; Generator: 0.028656,\n",
      "D(x): 0.465, D(G(z)): 0.400\n",
      "2019-04-10 00:57:48,729 root         INFO     Train Epoch: 83 [1536/8000 (19%)]\tTotal Loss: 0.518707\n",
      "Reconstruction: 0.449812, Regularization: 0.000347, Discriminator: 0.040023; Generator: 0.028525,\n",
      "D(x): 0.467, D(G(z)): 0.401\n",
      "2019-04-10 00:57:48,840 root         INFO     Train Epoch: 83 [2048/8000 (26%)]\tTotal Loss: 0.517563\n",
      "Reconstruction: 0.448334, Regularization: 0.000218, Discriminator: 0.040240; Generator: 0.028771,\n",
      "D(x): 0.461, D(G(z)): 0.398\n",
      "2019-04-10 00:57:48,950 root         INFO     Train Epoch: 83 [2560/8000 (32%)]\tTotal Loss: 0.516273\n",
      "Reconstruction: 0.447997, Regularization: 0.000149, Discriminator: 0.039559; Generator: 0.028568,\n",
      "D(x): 0.472, D(G(z)): 0.401\n",
      "2019-04-10 00:57:49,061 root         INFO     Train Epoch: 83 [3072/8000 (38%)]\tTotal Loss: 0.514821\n",
      "Reconstruction: 0.446487, Regularization: 0.000167, Discriminator: 0.039416; Generator: 0.028751,\n",
      "D(x): 0.474, D(G(z)): 0.399\n",
      "2019-04-10 00:57:49,171 root         INFO     Train Epoch: 83 [3584/8000 (45%)]\tTotal Loss: 0.512523\n",
      "Reconstruction: 0.445191, Regularization: 0.000265, Discriminator: 0.038889; Generator: 0.028178,\n",
      "D(x): 0.488, D(G(z)): 0.406\n",
      "2019-04-10 00:57:49,281 root         INFO     Train Epoch: 83 [4096/8000 (51%)]\tTotal Loss: 0.511130\n",
      "Reconstruction: 0.443393, Regularization: 0.000302, Discriminator: 0.039353; Generator: 0.028083,\n",
      "D(x): 0.481, D(G(z)): 0.407\n",
      "2019-04-10 00:57:49,392 root         INFO     Train Epoch: 83 [4608/8000 (58%)]\tTotal Loss: 0.509506\n",
      "Reconstruction: 0.441688, Regularization: 0.000291, Discriminator: 0.039644; Generator: 0.027884,\n",
      "D(x): 0.480, D(G(z)): 0.410\n",
      "2019-04-10 00:57:49,502 root         INFO     Train Epoch: 83 [5120/8000 (64%)]\tTotal Loss: 0.508886\n",
      "Reconstruction: 0.440647, Regularization: 0.000195, Discriminator: 0.040487; Generator: 0.027558,\n",
      "D(x): 0.469, D(G(z)): 0.414\n",
      "2019-04-10 00:57:49,612 root         INFO     Train Epoch: 83 [5632/8000 (70%)]\tTotal Loss: 0.506049\n",
      "Reconstruction: 0.438921, Regularization: 0.000268, Discriminator: 0.038892; Generator: 0.027968,\n",
      "D(x): 0.488, D(G(z)): 0.409\n",
      "2019-04-10 00:57:49,723 root         INFO     Train Epoch: 83 [6144/8000 (77%)]\tTotal Loss: 0.505115\n",
      "Reconstruction: 0.438060, Regularization: 0.000318, Discriminator: 0.039013; Generator: 0.027724,\n",
      "D(x): 0.491, D(G(z)): 0.412\n",
      "2019-04-10 00:57:49,833 root         INFO     Train Epoch: 83 [6656/8000 (83%)]\tTotal Loss: 0.503505\n",
      "Reconstruction: 0.436619, Regularization: 0.000228, Discriminator: 0.038909; Generator: 0.027749,\n",
      "D(x): 0.492, D(G(z)): 0.411\n",
      "2019-04-10 00:57:49,943 root         INFO     Train Epoch: 83 [7168/8000 (90%)]\tTotal Loss: 0.503017\n",
      "Reconstruction: 0.435271, Regularization: 0.000195, Discriminator: 0.039926; Generator: 0.027625,\n",
      "D(x): 0.478, D(G(z)): 0.413\n",
      "2019-04-10 00:57:50,053 root         INFO     Train Epoch: 83 [7680/8000 (96%)]\tTotal Loss: 0.500490\n",
      "Reconstruction: 0.434126, Regularization: 0.000228, Discriminator: 0.038962; Generator: 0.027173,\n",
      "D(x): 0.497, D(G(z)): 0.419\n",
      "2019-04-10 00:57:50,134 root         INFO     ====> Epoch: 83 Average loss: 0.5115\n",
      "2019-04-10 00:57:50,161 root         INFO     Train Epoch: 84 [0/8000 (0%)]\tTotal Loss: 0.500292\n",
      "Reconstruction: 0.433656, Regularization: 0.000274, Discriminator: 0.038784; Generator: 0.027579,\n",
      "D(x): 0.496, D(G(z)): 0.414\n",
      "2019-04-10 00:57:50,273 root         INFO     Train Epoch: 84 [512/8000 (6%)]\tTotal Loss: 0.500090\n",
      "Reconstruction: 0.432301, Regularization: 0.000186, Discriminator: 0.040899; Generator: 0.026703,\n",
      "D(x): 0.473, D(G(z)): 0.425\n",
      "2019-04-10 00:57:50,384 root         INFO     Train Epoch: 84 [1024/8000 (13%)]\tTotal Loss: 0.497622\n",
      "Reconstruction: 0.431179, Regularization: 0.000313, Discriminator: 0.039370; Generator: 0.026760,\n",
      "D(x): 0.497, D(G(z)): 0.425\n",
      "2019-04-10 00:57:50,495 root         INFO     Train Epoch: 84 [1536/8000 (19%)]\tTotal Loss: 0.495954\n",
      "Reconstruction: 0.429178, Regularization: 0.000298, Discriminator: 0.039528; Generator: 0.026949,\n",
      "D(x): 0.493, D(G(z)): 0.422\n",
      "2019-04-10 00:57:50,606 root         INFO     Train Epoch: 84 [2048/8000 (26%)]\tTotal Loss: 0.494521\n",
      "Reconstruction: 0.427812, Regularization: 0.000302, Discriminator: 0.039654; Generator: 0.026753,\n",
      "D(x): 0.494, D(G(z)): 0.425\n",
      "2019-04-10 00:57:50,717 root         INFO     Train Epoch: 84 [2560/8000 (32%)]\tTotal Loss: 0.492293\n",
      "Reconstruction: 0.426010, Regularization: 0.000268, Discriminator: 0.039852; Generator: 0.026164,\n",
      "D(x): 0.496, D(G(z)): 0.433\n",
      "2019-04-10 00:57:50,828 root         INFO     Train Epoch: 84 [3072/8000 (38%)]\tTotal Loss: 0.490254\n",
      "Reconstruction: 0.424781, Regularization: 0.000152, Discriminator: 0.038689; Generator: 0.026633,\n",
      "D(x): 0.509, D(G(z)): 0.426\n",
      "2019-04-10 00:57:50,939 root         INFO     Train Epoch: 84 [3584/8000 (45%)]\tTotal Loss: 0.489698\n",
      "Reconstruction: 0.422280, Regularization: 0.000197, Discriminator: 0.040600; Generator: 0.026621,\n",
      "D(x): 0.480, D(G(z)): 0.427\n",
      "2019-04-10 00:57:51,050 root         INFO     Train Epoch: 84 [4096/8000 (51%)]\tTotal Loss: 0.487225\n",
      "Reconstruction: 0.420830, Regularization: 0.000342, Discriminator: 0.039729; Generator: 0.026324,\n",
      "D(x): 0.497, D(G(z)): 0.431\n",
      "2019-04-10 00:57:51,161 root         INFO     Train Epoch: 84 [4608/8000 (58%)]\tTotal Loss: 0.484840\n",
      "Reconstruction: 0.418858, Regularization: 0.000333, Discriminator: 0.039327; Generator: 0.026322,\n",
      "D(x): 0.505, D(G(z)): 0.431\n",
      "2019-04-10 00:57:51,272 root         INFO     Train Epoch: 84 [5120/8000 (64%)]\tTotal Loss: 0.482956\n",
      "Reconstruction: 0.417845, Regularization: 0.000266, Discriminator: 0.039540; Generator: 0.025305,\n",
      "D(x): 0.512, D(G(z)): 0.445\n",
      "2019-04-10 00:57:51,384 root         INFO     Train Epoch: 84 [5632/8000 (70%)]\tTotal Loss: 0.482105\n",
      "Reconstruction: 0.416371, Regularization: 0.000155, Discriminator: 0.040198; Generator: 0.025381,\n",
      "D(x): 0.500, D(G(z)): 0.444\n",
      "2019-04-10 00:57:51,495 root         INFO     Train Epoch: 84 [6144/8000 (77%)]\tTotal Loss: 0.482055\n",
      "Reconstruction: 0.415106, Regularization: 0.000209, Discriminator: 0.040995; Generator: 0.025745,\n",
      "D(x): 0.483, D(G(z)): 0.439\n",
      "2019-04-10 00:57:51,606 root         INFO     Train Epoch: 84 [6656/8000 (83%)]\tTotal Loss: 0.480982\n",
      "Reconstruction: 0.414604, Regularization: 0.000248, Discriminator: 0.040552; Generator: 0.025579,\n",
      "D(x): 0.491, D(G(z)): 0.441\n",
      "2019-04-10 00:57:51,717 root         INFO     Train Epoch: 84 [7168/8000 (90%)]\tTotal Loss: 0.479523\n",
      "Reconstruction: 0.413828, Regularization: 0.000279, Discriminator: 0.039597; Generator: 0.025818,\n",
      "D(x): 0.505, D(G(z)): 0.438\n",
      "2019-04-10 00:57:51,828 root         INFO     Train Epoch: 84 [7680/8000 (96%)]\tTotal Loss: 0.478770\n",
      "Reconstruction: 0.412203, Regularization: 0.000227, Discriminator: 0.040931; Generator: 0.025409,\n",
      "D(x): 0.489, D(G(z)): 0.443\n",
      "2019-04-10 00:57:51,909 root         INFO     ====> Epoch: 84 Average loss: 0.4884\n",
      "2019-04-10 00:57:51,937 root         INFO     Train Epoch: 85 [0/8000 (0%)]\tTotal Loss: 0.477117\n",
      "Reconstruction: 0.411493, Regularization: 0.000188, Discriminator: 0.040481; Generator: 0.024955,\n",
      "D(x): 0.502, D(G(z)): 0.450\n",
      "2019-04-10 00:57:52,048 root         INFO     Train Epoch: 85 [512/8000 (6%)]\tTotal Loss: 0.476917\n",
      "Reconstruction: 0.410210, Regularization: 0.000198, Discriminator: 0.041377; Generator: 0.025133,\n",
      "D(x): 0.487, D(G(z)): 0.447\n",
      "2019-04-10 00:57:52,160 root         INFO     Train Epoch: 85 [1024/8000 (13%)]\tTotal Loss: 0.473738\n",
      "Reconstruction: 0.408545, Regularization: 0.000214, Discriminator: 0.040512; Generator: 0.024468,\n",
      "D(x): 0.511, D(G(z)): 0.457\n",
      "2019-04-10 00:57:52,271 root         INFO     Train Epoch: 85 [1536/8000 (19%)]\tTotal Loss: 0.472633\n",
      "Reconstruction: 0.406585, Regularization: 0.000184, Discriminator: 0.041191; Generator: 0.024673,\n",
      "D(x): 0.494, D(G(z)): 0.454\n",
      "2019-04-10 00:57:52,383 root         INFO     Train Epoch: 85 [2048/8000 (26%)]\tTotal Loss: 0.470992\n",
      "Reconstruction: 0.404979, Regularization: 0.000220, Discriminator: 0.041229; Generator: 0.024563,\n",
      "D(x): 0.497, D(G(z)): 0.456\n",
      "2019-04-10 00:57:52,494 root         INFO     Train Epoch: 85 [2560/8000 (32%)]\tTotal Loss: 0.469024\n",
      "Reconstruction: 0.402996, Regularization: 0.000208, Discriminator: 0.041678; Generator: 0.024142,\n",
      "D(x): 0.497, D(G(z)): 0.462\n",
      "2019-04-10 00:57:52,606 root         INFO     Train Epoch: 85 [3072/8000 (38%)]\tTotal Loss: 0.467751\n",
      "Reconstruction: 0.401548, Regularization: 0.000230, Discriminator: 0.041844; Generator: 0.024129,\n",
      "D(x): 0.494, D(G(z)): 0.462\n",
      "2019-04-10 00:57:52,717 root         INFO     Train Epoch: 85 [3584/8000 (45%)]\tTotal Loss: 0.464895\n",
      "Reconstruction: 0.399689, Regularization: 0.000248, Discriminator: 0.041042; Generator: 0.023917,\n",
      "D(x): 0.509, D(G(z)): 0.465\n",
      "2019-04-10 00:57:52,829 root         INFO     Train Epoch: 85 [4096/8000 (51%)]\tTotal Loss: 0.464083\n",
      "Reconstruction: 0.398120, Regularization: 0.000185, Discriminator: 0.042026; Generator: 0.023752,\n",
      "D(x): 0.496, D(G(z)): 0.468\n",
      "2019-04-10 00:57:52,940 root         INFO     Train Epoch: 85 [4608/8000 (58%)]\tTotal Loss: 0.463980\n",
      "Reconstruction: 0.397315, Regularization: 0.000261, Discriminator: 0.042985; Generator: 0.023419,\n",
      "D(x): 0.486, D(G(z)): 0.473\n",
      "2019-04-10 00:57:53,051 root         INFO     Train Epoch: 85 [5120/8000 (64%)]\tTotal Loss: 0.462733\n",
      "Reconstruction: 0.396463, Regularization: 0.000179, Discriminator: 0.042267; Generator: 0.023824,\n",
      "D(x): 0.489, D(G(z)): 0.467\n",
      "2019-04-10 00:57:53,163 root         INFO     Train Epoch: 85 [5632/8000 (70%)]\tTotal Loss: 0.462119\n",
      "Reconstruction: 0.395632, Regularization: 0.000204, Discriminator: 0.042898; Generator: 0.023385,\n",
      "D(x): 0.486, D(G(z)): 0.473\n",
      "2019-04-10 00:57:53,274 root         INFO     Train Epoch: 85 [6144/8000 (77%)]\tTotal Loss: 0.461429\n",
      "Reconstruction: 0.394310, Regularization: 0.000206, Discriminator: 0.043322; Generator: 0.023590,\n",
      "D(x): 0.477, D(G(z)): 0.470\n",
      "2019-04-10 00:57:53,386 root         INFO     Train Epoch: 85 [6656/8000 (83%)]\tTotal Loss: 0.457880\n",
      "Reconstruction: 0.393200, Regularization: 0.000265, Discriminator: 0.041462; Generator: 0.022953,\n",
      "D(x): 0.515, D(G(z)): 0.480\n",
      "2019-04-10 00:57:53,497 root         INFO     Train Epoch: 85 [7168/8000 (90%)]\tTotal Loss: 0.456786\n",
      "Reconstruction: 0.392054, Regularization: 0.000202, Discriminator: 0.041581; Generator: 0.022949,\n",
      "D(x): 0.512, D(G(z)): 0.480\n",
      "2019-04-10 00:57:53,608 root         INFO     Train Epoch: 85 [7680/8000 (96%)]\tTotal Loss: 0.456038\n",
      "Reconstruction: 0.390356, Regularization: 0.000179, Discriminator: 0.042549; Generator: 0.022953,\n",
      "D(x): 0.499, D(G(z)): 0.480\n",
      "2019-04-10 00:57:53,689 root         INFO     ====> Epoch: 85 Average loss: 0.4654\n",
      "2019-04-10 00:57:53,717 root         INFO     Train Epoch: 86 [0/8000 (0%)]\tTotal Loss: 0.453928\n",
      "Reconstruction: 0.389424, Regularization: 0.000236, Discriminator: 0.041607; Generator: 0.022661,\n",
      "D(x): 0.518, D(G(z)): 0.484\n",
      "2019-04-10 00:57:53,828 root         INFO     Train Epoch: 86 [512/8000 (6%)]\tTotal Loss: 0.453085\n",
      "Reconstruction: 0.388159, Regularization: 0.000294, Discriminator: 0.041956; Generator: 0.022677,\n",
      "D(x): 0.513, D(G(z)): 0.484\n",
      "2019-04-10 00:57:53,938 root         INFO     Train Epoch: 86 [1024/8000 (13%)]\tTotal Loss: 0.451245\n",
      "Reconstruction: 0.386426, Regularization: 0.000273, Discriminator: 0.042112; Generator: 0.022435,\n",
      "D(x): 0.513, D(G(z)): 0.488\n",
      "2019-04-10 00:57:54,050 root         INFO     Train Epoch: 86 [1536/8000 (19%)]\tTotal Loss: 0.452674\n",
      "Reconstruction: 0.385499, Regularization: 0.000258, Discriminator: 0.044350; Generator: 0.022567,\n",
      "D(x): 0.478, D(G(z)): 0.486\n",
      "2019-04-10 00:57:54,161 root         INFO     Train Epoch: 86 [2048/8000 (26%)]\tTotal Loss: 0.448909\n",
      "Reconstruction: 0.383825, Regularization: 0.000279, Discriminator: 0.042493; Generator: 0.022311,\n",
      "D(x): 0.512, D(G(z)): 0.490\n",
      "2019-04-10 00:57:54,273 root         INFO     Train Epoch: 86 [2560/8000 (32%)]\tTotal Loss: 0.448439\n",
      "Reconstruction: 0.382108, Regularization: 0.000233, Discriminator: 0.043918; Generator: 0.022181,\n",
      "D(x): 0.489, D(G(z)): 0.492\n",
      "2019-04-10 00:57:54,384 root         INFO     Train Epoch: 86 [3072/8000 (38%)]\tTotal Loss: 0.446004\n",
      "Reconstruction: 0.380626, Regularization: 0.000191, Discriminator: 0.043085; Generator: 0.022102,\n",
      "D(x): 0.502, D(G(z)): 0.493\n",
      "2019-04-10 00:57:54,496 root         INFO     Train Epoch: 86 [3584/8000 (45%)]\tTotal Loss: 0.446063\n",
      "Reconstruction: 0.379509, Regularization: 0.000326, Discriminator: 0.044609; Generator: 0.021620,\n",
      "D(x): 0.489, D(G(z)): 0.501\n",
      "2019-04-10 00:57:54,608 root         INFO     Train Epoch: 86 [4096/8000 (51%)]\tTotal Loss: 0.443992\n",
      "Reconstruction: 0.378551, Regularization: 0.000251, Discriminator: 0.043638; Generator: 0.021552,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:57:54,719 root         INFO     Train Epoch: 86 [4608/8000 (58%)]\tTotal Loss: 0.442211\n",
      "Reconstruction: 0.377382, Regularization: 0.000273, Discriminator: 0.043330; Generator: 0.021227,\n",
      "D(x): 0.511, D(G(z)): 0.507\n",
      "2019-04-10 00:57:54,830 root         INFO     Train Epoch: 86 [5120/8000 (64%)]\tTotal Loss: 0.441491\n",
      "Reconstruction: 0.375906, Regularization: 0.000275, Discriminator: 0.044212; Generator: 0.021098,\n",
      "D(x): 0.502, D(G(z)): 0.509\n",
      "2019-04-10 00:57:54,942 root         INFO     Train Epoch: 86 [5632/8000 (70%)]\tTotal Loss: 0.440119\n",
      "Reconstruction: 0.375747, Regularization: 0.000289, Discriminator: 0.043016; Generator: 0.021067,\n",
      "D(x): 0.519, D(G(z)): 0.510\n",
      "2019-04-10 00:57:55,053 root         INFO     Train Epoch: 86 [6144/8000 (77%)]\tTotal Loss: 0.439318\n",
      "Reconstruction: 0.374487, Regularization: 0.000263, Discriminator: 0.043329; Generator: 0.021240,\n",
      "D(x): 0.511, D(G(z)): 0.507\n",
      "2019-04-10 00:57:55,165 root         INFO     Train Epoch: 86 [6656/8000 (83%)]\tTotal Loss: 0.440502\n",
      "Reconstruction: 0.374466, Regularization: 0.000281, Discriminator: 0.044453; Generator: 0.021303,\n",
      "D(x): 0.493, D(G(z)): 0.506\n",
      "2019-04-10 00:57:55,276 root         INFO     Train Epoch: 86 [7168/8000 (90%)]\tTotal Loss: 0.438006\n",
      "Reconstruction: 0.373064, Regularization: 0.000252, Discriminator: 0.043745; Generator: 0.020945,\n",
      "D(x): 0.509, D(G(z)): 0.512\n",
      "2019-04-10 00:57:55,388 root         INFO     Train Epoch: 86 [7680/8000 (96%)]\tTotal Loss: 0.437233\n",
      "Reconstruction: 0.372043, Regularization: 0.000287, Discriminator: 0.044156; Generator: 0.020747,\n",
      "D(x): 0.506, D(G(z)): 0.515\n",
      "2019-04-10 00:57:55,469 root         INFO     ====> Epoch: 86 Average loss: 0.4447\n",
      "2019-04-10 00:57:55,497 root         INFO     Train Epoch: 87 [0/8000 (0%)]\tTotal Loss: 0.436046\n",
      "Reconstruction: 0.371737, Regularization: 0.000235, Discriminator: 0.043366; Generator: 0.020708,\n",
      "D(x): 0.519, D(G(z)): 0.515\n",
      "2019-04-10 00:57:55,609 root         INFO     Train Epoch: 87 [512/8000 (6%)]\tTotal Loss: 0.435927\n",
      "Reconstruction: 0.370155, Regularization: 0.000223, Discriminator: 0.044871; Generator: 0.020679,\n",
      "D(x): 0.501, D(G(z)): 0.516\n",
      "2019-04-10 00:57:55,720 root         INFO     Train Epoch: 87 [1024/8000 (13%)]\tTotal Loss: 0.434614\n",
      "Reconstruction: 0.369611, Regularization: 0.000282, Discriminator: 0.044358; Generator: 0.020363,\n",
      "D(x): 0.513, D(G(z)): 0.521\n",
      "2019-04-10 00:57:55,832 root         INFO     Train Epoch: 87 [1536/8000 (19%)]\tTotal Loss: 0.433264\n",
      "Reconstruction: 0.368213, Regularization: 0.000156, Discriminator: 0.044475; Generator: 0.020419,\n",
      "D(x): 0.510, D(G(z)): 0.520\n",
      "2019-04-10 00:57:55,943 root         INFO     Train Epoch: 87 [2048/8000 (26%)]\tTotal Loss: 0.431738\n",
      "Reconstruction: 0.366660, Regularization: 0.000162, Discriminator: 0.044787; Generator: 0.020129,\n",
      "D(x): 0.507, D(G(z)): 0.525\n",
      "2019-04-10 00:57:56,055 root         INFO     Train Epoch: 87 [2560/8000 (32%)]\tTotal Loss: 0.430260\n",
      "Reconstruction: 0.365712, Regularization: 0.000239, Discriminator: 0.044220; Generator: 0.020090,\n",
      "D(x): 0.514, D(G(z)): 0.526\n",
      "2019-04-10 00:57:56,166 root         INFO     Train Epoch: 87 [3072/8000 (38%)]\tTotal Loss: 0.428570\n",
      "Reconstruction: 0.364730, Regularization: 0.000242, Discriminator: 0.043416; Generator: 0.020182,\n",
      "D(x): 0.527, D(G(z)): 0.524\n",
      "2019-04-10 00:57:56,276 root         INFO     Train Epoch: 87 [3584/8000 (45%)]\tTotal Loss: 0.428742\n",
      "Reconstruction: 0.363409, Regularization: 0.000269, Discriminator: 0.045143; Generator: 0.019921,\n",
      "D(x): 0.506, D(G(z)): 0.529\n",
      "2019-04-10 00:57:56,386 root         INFO     Train Epoch: 87 [4096/8000 (51%)]\tTotal Loss: 0.425634\n",
      "Reconstruction: 0.362345, Regularization: 0.000254, Discriminator: 0.043091; Generator: 0.019943,\n",
      "D(x): 0.539, D(G(z)): 0.528\n",
      "2019-04-10 00:57:56,495 root         INFO     Train Epoch: 87 [4608/8000 (58%)]\tTotal Loss: 0.425253\n",
      "Reconstruction: 0.361104, Regularization: 0.000280, Discriminator: 0.044039; Generator: 0.019830,\n",
      "D(x): 0.525, D(G(z)): 0.530\n",
      "2019-04-10 00:57:56,604 root         INFO     Train Epoch: 87 [5120/8000 (64%)]\tTotal Loss: 0.424521\n",
      "Reconstruction: 0.360044, Regularization: 0.000279, Discriminator: 0.044374; Generator: 0.019823,\n",
      "D(x): 0.522, D(G(z)): 0.530\n",
      "2019-04-10 00:57:56,714 root         INFO     Train Epoch: 87 [5632/8000 (70%)]\tTotal Loss: 0.423715\n",
      "Reconstruction: 0.359350, Regularization: 0.000298, Discriminator: 0.044452; Generator: 0.019615,\n",
      "D(x): 0.520, D(G(z)): 0.534\n",
      "2019-04-10 00:57:56,823 root         INFO     Train Epoch: 87 [6144/8000 (77%)]\tTotal Loss: 0.423336\n",
      "Reconstruction: 0.358548, Regularization: 0.000278, Discriminator: 0.044836; Generator: 0.019675,\n",
      "D(x): 0.514, D(G(z)): 0.533\n",
      "2019-04-10 00:57:56,932 root         INFO     Train Epoch: 87 [6656/8000 (83%)]\tTotal Loss: 0.421348\n",
      "Reconstruction: 0.357331, Regularization: 0.000273, Discriminator: 0.044159; Generator: 0.019585,\n",
      "D(x): 0.525, D(G(z)): 0.534\n",
      "2019-04-10 00:57:57,043 root         INFO     Train Epoch: 87 [7168/8000 (90%)]\tTotal Loss: 0.421610\n",
      "Reconstruction: 0.357086, Regularization: 0.000303, Discriminator: 0.044614; Generator: 0.019608,\n",
      "D(x): 0.518, D(G(z)): 0.534\n",
      "2019-04-10 00:57:57,152 root         INFO     Train Epoch: 87 [7680/8000 (96%)]\tTotal Loss: 0.420894\n",
      "Reconstruction: 0.356006, Regularization: 0.000245, Discriminator: 0.045063; Generator: 0.019580,\n",
      "D(x): 0.510, D(G(z)): 0.534\n",
      "2019-04-10 00:57:57,232 root         INFO     ====> Epoch: 87 Average loss: 0.4279\n",
      "2019-04-10 00:57:57,260 root         INFO     Train Epoch: 88 [0/8000 (0%)]\tTotal Loss: 0.421278\n",
      "Reconstruction: 0.356674, Regularization: 0.000223, Discriminator: 0.045118; Generator: 0.019262,\n",
      "D(x): 0.516, D(G(z)): 0.540\n",
      "2019-04-10 00:57:57,372 root         INFO     Train Epoch: 88 [512/8000 (6%)]\tTotal Loss: 0.419766\n",
      "Reconstruction: 0.355447, Regularization: 0.000173, Discriminator: 0.044859; Generator: 0.019287,\n",
      "D(x): 0.519, D(G(z)): 0.539\n",
      "2019-04-10 00:57:57,483 root         INFO     Train Epoch: 88 [1024/8000 (13%)]\tTotal Loss: 0.420120\n",
      "Reconstruction: 0.355390, Regularization: 0.000287, Discriminator: 0.045250; Generator: 0.019193,\n",
      "D(x): 0.518, D(G(z)): 0.541\n",
      "2019-04-10 00:57:57,594 root         INFO     Train Epoch: 88 [1536/8000 (19%)]\tTotal Loss: 0.419123\n",
      "Reconstruction: 0.353922, Regularization: 0.000281, Discriminator: 0.045419; Generator: 0.019500,\n",
      "D(x): 0.508, D(G(z)): 0.536\n",
      "2019-04-10 00:57:57,705 root         INFO     Train Epoch: 88 [2048/8000 (26%)]\tTotal Loss: 0.418427\n",
      "Reconstruction: 0.353628, Regularization: 0.000253, Discriminator: 0.045055; Generator: 0.019492,\n",
      "D(x): 0.513, D(G(z)): 0.536\n",
      "2019-04-10 00:57:57,816 root         INFO     Train Epoch: 88 [2560/8000 (32%)]\tTotal Loss: 0.417452\n",
      "Reconstruction: 0.352924, Regularization: 0.000241, Discriminator: 0.044945; Generator: 0.019342,\n",
      "D(x): 0.517, D(G(z)): 0.539\n",
      "2019-04-10 00:57:57,927 root         INFO     Train Epoch: 88 [3072/8000 (38%)]\tTotal Loss: 0.416034\n",
      "Reconstruction: 0.352102, Regularization: 0.000262, Discriminator: 0.044077; Generator: 0.019593,\n",
      "D(x): 0.527, D(G(z)): 0.534\n",
      "2019-04-10 00:57:58,036 root         INFO     Train Epoch: 88 [3584/8000 (45%)]\tTotal Loss: 0.414409\n",
      "Reconstruction: 0.350640, Regularization: 0.000278, Discriminator: 0.044241; Generator: 0.019249,\n",
      "D(x): 0.531, D(G(z)): 0.540\n",
      "2019-04-10 00:57:58,146 root         INFO     Train Epoch: 88 [4096/8000 (51%)]\tTotal Loss: 0.415324\n",
      "Reconstruction: 0.350575, Regularization: 0.000343, Discriminator: 0.045314; Generator: 0.019091,\n",
      "D(x): 0.517, D(G(z)): 0.543\n",
      "2019-04-10 00:57:58,256 root         INFO     Train Epoch: 88 [4608/8000 (58%)]\tTotal Loss: 0.413169\n",
      "Reconstruction: 0.348797, Regularization: 0.000276, Discriminator: 0.045134; Generator: 0.018962,\n",
      "D(x): 0.522, D(G(z)): 0.545\n",
      "2019-04-10 00:57:58,366 root         INFO     Train Epoch: 88 [5120/8000 (64%)]\tTotal Loss: 0.412744\n",
      "Reconstruction: 0.348365, Regularization: 0.000255, Discriminator: 0.045533; Generator: 0.018591,\n",
      "D(x): 0.521, D(G(z)): 0.552\n",
      "2019-04-10 00:57:58,475 root         INFO     Train Epoch: 88 [5632/8000 (70%)]\tTotal Loss: 0.410907\n",
      "Reconstruction: 0.347497, Regularization: 0.000382, Discriminator: 0.044529; Generator: 0.018499,\n",
      "D(x): 0.541, D(G(z)): 0.553\n",
      "2019-04-10 00:57:58,585 root         INFO     Train Epoch: 88 [6144/8000 (77%)]\tTotal Loss: 0.411575\n",
      "Reconstruction: 0.346928, Regularization: 0.000336, Discriminator: 0.045568; Generator: 0.018743,\n",
      "D(x): 0.519, D(G(z)): 0.549\n",
      "2019-04-10 00:57:58,695 root         INFO     Train Epoch: 88 [6656/8000 (83%)]\tTotal Loss: 0.409733\n",
      "Reconstruction: 0.344860, Regularization: 0.000289, Discriminator: 0.045796; Generator: 0.018788,\n",
      "D(x): 0.513, D(G(z)): 0.548\n",
      "2019-04-10 00:57:58,805 root         INFO     Train Epoch: 88 [7168/8000 (90%)]\tTotal Loss: 0.410161\n",
      "Reconstruction: 0.345489, Regularization: 0.000335, Discriminator: 0.045487; Generator: 0.018849,\n",
      "D(x): 0.517, D(G(z)): 0.547\n",
      "2019-04-10 00:57:58,913 root         INFO     Train Epoch: 88 [7680/8000 (96%)]\tTotal Loss: 0.409545\n",
      "Reconstruction: 0.345168, Regularization: 0.000208, Discriminator: 0.045371; Generator: 0.018798,\n",
      "D(x): 0.520, D(G(z)): 0.548\n",
      "2019-04-10 00:57:58,992 root         INFO     ====> Epoch: 88 Average loss: 0.4150\n",
      "2019-04-10 00:57:59,020 root         INFO     Train Epoch: 89 [0/8000 (0%)]\tTotal Loss: 0.409383\n",
      "Reconstruction: 0.344359, Regularization: 0.000255, Discriminator: 0.045886; Generator: 0.018883,\n",
      "D(x): 0.510, D(G(z)): 0.546\n",
      "2019-04-10 00:57:59,132 root         INFO     Train Epoch: 89 [512/8000 (6%)]\tTotal Loss: 0.408329\n",
      "Reconstruction: 0.344438, Regularization: 0.000314, Discriminator: 0.044860; Generator: 0.018716,\n",
      "D(x): 0.530, D(G(z)): 0.549\n",
      "2019-04-10 00:57:59,242 root         INFO     Train Epoch: 89 [1024/8000 (13%)]\tTotal Loss: 0.407642\n",
      "Reconstruction: 0.343908, Regularization: 0.000245, Discriminator: 0.044755; Generator: 0.018734,\n",
      "D(x): 0.531, D(G(z)): 0.549\n",
      "2019-04-10 00:57:59,353 root         INFO     Train Epoch: 89 [1536/8000 (19%)]\tTotal Loss: 0.407593\n",
      "Reconstruction: 0.343896, Regularization: 0.000257, Discriminator: 0.044765; Generator: 0.018674,\n",
      "D(x): 0.532, D(G(z)): 0.550\n",
      "2019-04-10 00:57:59,463 root         INFO     Train Epoch: 89 [2048/8000 (26%)]\tTotal Loss: 0.406869\n",
      "Reconstruction: 0.342671, Regularization: 0.000227, Discriminator: 0.045300; Generator: 0.018671,\n",
      "D(x): 0.523, D(G(z)): 0.550\n",
      "2019-04-10 00:57:59,574 root         INFO     Train Epoch: 89 [2560/8000 (32%)]\tTotal Loss: 0.406456\n",
      "Reconstruction: 0.342185, Regularization: 0.000253, Discriminator: 0.045307; Generator: 0.018712,\n",
      "D(x): 0.522, D(G(z)): 0.549\n",
      "2019-04-10 00:57:59,684 root         INFO     Train Epoch: 89 [3072/8000 (38%)]\tTotal Loss: 0.406588\n",
      "Reconstruction: 0.342582, Regularization: 0.000287, Discriminator: 0.045036; Generator: 0.018684,\n",
      "D(x): 0.527, D(G(z)): 0.550\n",
      "2019-04-10 00:57:59,795 root         INFO     Train Epoch: 89 [3584/8000 (45%)]\tTotal Loss: 0.405666\n",
      "Reconstruction: 0.341384, Regularization: 0.000249, Discriminator: 0.045243; Generator: 0.018789,\n",
      "D(x): 0.521, D(G(z)): 0.548\n",
      "2019-04-10 00:57:59,905 root         INFO     Train Epoch: 89 [4096/8000 (51%)]\tTotal Loss: 0.405750\n",
      "Reconstruction: 0.341448, Regularization: 0.000219, Discriminator: 0.045288; Generator: 0.018795,\n",
      "D(x): 0.520, D(G(z)): 0.548\n",
      "2019-04-10 00:58:00,016 root         INFO     Train Epoch: 89 [4608/8000 (58%)]\tTotal Loss: 0.406328\n",
      "Reconstruction: 0.341956, Regularization: 0.000305, Discriminator: 0.045144; Generator: 0.018924,\n",
      "D(x): 0.520, D(G(z)): 0.546\n",
      "2019-04-10 00:58:00,126 root         INFO     Train Epoch: 89 [5120/8000 (64%)]\tTotal Loss: 0.404016\n",
      "Reconstruction: 0.340137, Regularization: 0.000274, Discriminator: 0.044702; Generator: 0.018903,\n",
      "D(x): 0.528, D(G(z)): 0.546\n",
      "2019-04-10 00:58:00,237 root         INFO     Train Epoch: 89 [5632/8000 (70%)]\tTotal Loss: 0.404784\n",
      "Reconstruction: 0.340793, Regularization: 0.000476, Discriminator: 0.044588; Generator: 0.018928,\n",
      "D(x): 0.529, D(G(z)): 0.546\n",
      "2019-04-10 00:58:00,348 root         INFO     Train Epoch: 89 [6144/8000 (77%)]\tTotal Loss: 0.403094\n",
      "Reconstruction: 0.339479, Regularization: 0.000206, Discriminator: 0.044570; Generator: 0.018838,\n",
      "D(x): 0.531, D(G(z)): 0.547\n",
      "2019-04-10 00:58:00,458 root         INFO     Train Epoch: 89 [6656/8000 (83%)]\tTotal Loss: 0.404113\n",
      "Reconstruction: 0.340300, Regularization: 0.000310, Discriminator: 0.044615; Generator: 0.018888,\n",
      "D(x): 0.530, D(G(z)): 0.546\n",
      "2019-04-10 00:58:00,568 root         INFO     Train Epoch: 89 [7168/8000 (90%)]\tTotal Loss: 0.402165\n",
      "Reconstruction: 0.338575, Regularization: 0.000280, Discriminator: 0.044413; Generator: 0.018897,\n",
      "D(x): 0.533, D(G(z)): 0.546\n",
      "2019-04-10 00:58:00,677 root         INFO     Train Epoch: 89 [7680/8000 (96%)]\tTotal Loss: 0.403260\n",
      "Reconstruction: 0.338970, Regularization: 0.000442, Discriminator: 0.044855; Generator: 0.018994,\n",
      "D(x): 0.524, D(G(z)): 0.545\n",
      "2019-04-10 00:58:00,757 root         INFO     ====> Epoch: 89 Average loss: 0.4056\n",
      "2019-04-10 00:58:00,785 root         INFO     Train Epoch: 90 [0/8000 (0%)]\tTotal Loss: 0.400956\n",
      "Reconstruction: 0.337266, Regularization: 0.000268, Discriminator: 0.044485; Generator: 0.018938,\n",
      "D(x): 0.531, D(G(z)): 0.546\n",
      "2019-04-10 00:58:00,897 root         INFO     Train Epoch: 90 [512/8000 (6%)]\tTotal Loss: 0.400791\n",
      "Reconstruction: 0.337574, Regularization: 0.000349, Discriminator: 0.044051; Generator: 0.018818,\n",
      "D(x): 0.540, D(G(z)): 0.548\n",
      "2019-04-10 00:58:01,008 root         INFO     Train Epoch: 90 [1024/8000 (13%)]\tTotal Loss: 0.401393\n",
      "Reconstruction: 0.337741, Regularization: 0.000234, Discriminator: 0.044535; Generator: 0.018881,\n",
      "D(x): 0.531, D(G(z)): 0.547\n",
      "2019-04-10 00:58:01,119 root         INFO     Train Epoch: 90 [1536/8000 (19%)]\tTotal Loss: 0.400818\n",
      "Reconstruction: 0.336642, Regularization: 0.000237, Discriminator: 0.044981; Generator: 0.018957,\n",
      "D(x): 0.522, D(G(z)): 0.545\n",
      "2019-04-10 00:58:01,230 root         INFO     Train Epoch: 90 [2048/8000 (26%)]\tTotal Loss: 0.399804\n",
      "Reconstruction: 0.336003, Regularization: 0.000382, Discriminator: 0.044483; Generator: 0.018936,\n",
      "D(x): 0.531, D(G(z)): 0.546\n",
      "2019-04-10 00:58:01,341 root         INFO     Train Epoch: 90 [2560/8000 (32%)]\tTotal Loss: 0.399778\n",
      "Reconstruction: 0.336166, Regularization: 0.000271, Discriminator: 0.044463; Generator: 0.018879,\n",
      "D(x): 0.532, D(G(z)): 0.547\n",
      "2019-04-10 00:58:01,452 root         INFO     Train Epoch: 90 [3072/8000 (38%)]\tTotal Loss: 0.399737\n",
      "Reconstruction: 0.335987, Regularization: 0.000333, Discriminator: 0.044594; Generator: 0.018822,\n",
      "D(x): 0.531, D(G(z)): 0.548\n",
      "2019-04-10 00:58:01,563 root         INFO     Train Epoch: 90 [3584/8000 (45%)]\tTotal Loss: 0.398619\n",
      "Reconstruction: 0.335166, Regularization: 0.000325, Discriminator: 0.044321; Generator: 0.018807,\n",
      "D(x): 0.536, D(G(z)): 0.548\n",
      "2019-04-10 00:58:01,674 root         INFO     Train Epoch: 90 [4096/8000 (51%)]\tTotal Loss: 0.398382\n",
      "Reconstruction: 0.334760, Regularization: 0.000302, Discriminator: 0.044407; Generator: 0.018913,\n",
      "D(x): 0.532, D(G(z)): 0.546\n",
      "2019-04-10 00:58:01,785 root         INFO     Train Epoch: 90 [4608/8000 (58%)]\tTotal Loss: 0.398016\n",
      "Reconstruction: 0.334567, Regularization: 0.000329, Discriminator: 0.044133; Generator: 0.018987,\n",
      "D(x): 0.535, D(G(z)): 0.545\n",
      "2019-04-10 00:58:01,896 root         INFO     Train Epoch: 90 [5120/8000 (64%)]\tTotal Loss: 0.397802\n",
      "Reconstruction: 0.334254, Regularization: 0.000369, Discriminator: 0.044183; Generator: 0.018996,\n",
      "D(x): 0.534, D(G(z)): 0.545\n",
      "2019-04-10 00:58:02,007 root         INFO     Train Epoch: 90 [5632/8000 (70%)]\tTotal Loss: 0.397972\n",
      "Reconstruction: 0.334469, Regularization: 0.000248, Discriminator: 0.044239; Generator: 0.019016,\n",
      "D(x): 0.533, D(G(z)): 0.544\n",
      "2019-04-10 00:58:02,118 root         INFO     Train Epoch: 90 [6144/8000 (77%)]\tTotal Loss: 0.397035\n",
      "Reconstruction: 0.333594, Regularization: 0.000259, Discriminator: 0.044183; Generator: 0.018999,\n",
      "D(x): 0.534, D(G(z)): 0.544\n",
      "2019-04-10 00:58:02,229 root         INFO     Train Epoch: 90 [6656/8000 (83%)]\tTotal Loss: 0.397894\n",
      "Reconstruction: 0.334539, Regularization: 0.000290, Discriminator: 0.044039; Generator: 0.019027,\n",
      "D(x): 0.536, D(G(z)): 0.544\n",
      "2019-04-10 00:58:02,339 root         INFO     Train Epoch: 90 [7168/8000 (90%)]\tTotal Loss: 0.397777\n",
      "Reconstruction: 0.334287, Regularization: 0.000294, Discriminator: 0.044139; Generator: 0.019058,\n",
      "D(x): 0.533, D(G(z)): 0.543\n",
      "2019-04-10 00:58:02,449 root         INFO     Train Epoch: 90 [7680/8000 (96%)]\tTotal Loss: 0.397356\n",
      "Reconstruction: 0.333892, Regularization: 0.000267, Discriminator: 0.044045; Generator: 0.019153,\n",
      "D(x): 0.533, D(G(z)): 0.542\n",
      "2019-04-10 00:58:02,529 root         INFO     ====> Epoch: 90 Average loss: 0.3988\n",
      "2019-04-10 00:58:02,556 root         INFO     Train Epoch: 91 [0/8000 (0%)]\tTotal Loss: 0.397349\n",
      "Reconstruction: 0.333802, Regularization: 0.000271, Discriminator: 0.044087; Generator: 0.019188,\n",
      "D(x): 0.532, D(G(z)): 0.541\n",
      "2019-04-10 00:58:02,668 root         INFO     Train Epoch: 91 [512/8000 (6%)]\tTotal Loss: 0.397457\n",
      "Reconstruction: 0.334081, Regularization: 0.000220, Discriminator: 0.043925; Generator: 0.019231,\n",
      "D(x): 0.534, D(G(z)): 0.540\n",
      "2019-04-10 00:58:02,779 root         INFO     Train Epoch: 91 [1024/8000 (13%)]\tTotal Loss: 0.397203\n",
      "Reconstruction: 0.333747, Regularization: 0.000267, Discriminator: 0.043947; Generator: 0.019243,\n",
      "D(x): 0.533, D(G(z)): 0.540\n",
      "2019-04-10 00:58:02,890 root         INFO     Train Epoch: 91 [1536/8000 (19%)]\tTotal Loss: 0.397210\n",
      "Reconstruction: 0.333744, Regularization: 0.000397, Discriminator: 0.043838; Generator: 0.019231,\n",
      "D(x): 0.535, D(G(z)): 0.540\n",
      "2019-04-10 00:58:03,002 root         INFO     Train Epoch: 91 [2048/8000 (26%)]\tTotal Loss: 0.397492\n",
      "Reconstruction: 0.334115, Regularization: 0.000277, Discriminator: 0.043838; Generator: 0.019261,\n",
      "D(x): 0.534, D(G(z)): 0.540\n",
      "2019-04-10 00:58:03,113 root         INFO     Train Epoch: 91 [2560/8000 (32%)]\tTotal Loss: 0.396899\n",
      "Reconstruction: 0.333501, Regularization: 0.000293, Discriminator: 0.043835; Generator: 0.019270,\n",
      "D(x): 0.534, D(G(z)): 0.540\n",
      "2019-04-10 00:58:03,224 root         INFO     Train Epoch: 91 [3072/8000 (38%)]\tTotal Loss: 0.397053\n",
      "Reconstruction: 0.333613, Regularization: 0.000347, Discriminator: 0.043750; Generator: 0.019343,\n",
      "D(x): 0.534, D(G(z)): 0.539\n",
      "2019-04-10 00:58:03,335 root         INFO     Train Epoch: 91 [3584/8000 (45%)]\tTotal Loss: 0.398301\n",
      "Reconstruction: 0.334911, Regularization: 0.000247, Discriminator: 0.043704; Generator: 0.019439,\n",
      "D(x): 0.533, D(G(z)): 0.537\n",
      "2019-04-10 00:58:03,446 root         INFO     Train Epoch: 91 [4096/8000 (51%)]\tTotal Loss: 0.397415\n",
      "Reconstruction: 0.333976, Regularization: 0.000274, Discriminator: 0.043678; Generator: 0.019487,\n",
      "D(x): 0.533, D(G(z)): 0.536\n",
      "2019-04-10 00:58:03,557 root         INFO     Train Epoch: 91 [4608/8000 (58%)]\tTotal Loss: 0.397569\n",
      "Reconstruction: 0.334211, Regularization: 0.000258, Discriminator: 0.043594; Generator: 0.019506,\n",
      "D(x): 0.534, D(G(z)): 0.536\n",
      "2019-04-10 00:58:03,668 root         INFO     Train Epoch: 91 [5120/8000 (64%)]\tTotal Loss: 0.396463\n",
      "Reconstruction: 0.332890, Regularization: 0.000382, Discriminator: 0.043605; Generator: 0.019585,\n",
      "D(x): 0.532, D(G(z)): 0.534\n",
      "2019-04-10 00:58:03,779 root         INFO     Train Epoch: 91 [5632/8000 (70%)]\tTotal Loss: 0.396701\n",
      "Reconstruction: 0.333311, Regularization: 0.000321, Discriminator: 0.043528; Generator: 0.019541,\n",
      "D(x): 0.534, D(G(z)): 0.535\n",
      "2019-04-10 00:58:03,891 root         INFO     Train Epoch: 91 [6144/8000 (77%)]\tTotal Loss: 0.397320\n",
      "Reconstruction: 0.333976, Regularization: 0.000302, Discriminator: 0.043475; Generator: 0.019567,\n",
      "D(x): 0.535, D(G(z)): 0.535\n",
      "2019-04-10 00:58:04,002 root         INFO     Train Epoch: 91 [6656/8000 (83%)]\tTotal Loss: 0.396738\n",
      "Reconstruction: 0.333357, Regularization: 0.000312, Discriminator: 0.043494; Generator: 0.019575,\n",
      "D(x): 0.534, D(G(z)): 0.535\n",
      "2019-04-10 00:58:04,113 root         INFO     Train Epoch: 91 [7168/8000 (90%)]\tTotal Loss: 0.397293\n",
      "Reconstruction: 0.334081, Regularization: 0.000252, Discriminator: 0.043396; Generator: 0.019564,\n",
      "D(x): 0.536, D(G(z)): 0.535\n",
      "2019-04-10 00:58:04,224 root         INFO     Train Epoch: 91 [7680/8000 (96%)]\tTotal Loss: 0.396220\n",
      "Reconstruction: 0.332929, Regularization: 0.000283, Discriminator: 0.043382; Generator: 0.019626,\n",
      "D(x): 0.535, D(G(z)): 0.534\n",
      "2019-04-10 00:58:04,305 root         INFO     ====> Epoch: 91 Average loss: 0.3971\n",
      "2019-04-10 00:58:04,332 root         INFO     Train Epoch: 92 [0/8000 (0%)]\tTotal Loss: 0.395802\n",
      "Reconstruction: 0.332517, Regularization: 0.000257, Discriminator: 0.043372; Generator: 0.019655,\n",
      "D(x): 0.535, D(G(z)): 0.533\n",
      "2019-04-10 00:58:04,444 root         INFO     Train Epoch: 92 [512/8000 (6%)]\tTotal Loss: 0.395409\n",
      "Reconstruction: 0.332223, Regularization: 0.000205, Discriminator: 0.043305; Generator: 0.019676,\n",
      "D(x): 0.535, D(G(z)): 0.533\n",
      "2019-04-10 00:58:04,555 root         INFO     Train Epoch: 92 [1024/8000 (13%)]\tTotal Loss: 0.396360\n",
      "Reconstruction: 0.333064, Regularization: 0.000247, Discriminator: 0.043298; Generator: 0.019751,\n",
      "D(x): 0.534, D(G(z)): 0.532\n",
      "2019-04-10 00:58:04,667 root         INFO     Train Epoch: 92 [1536/8000 (19%)]\tTotal Loss: 0.395563\n",
      "Reconstruction: 0.332289, Regularization: 0.000316, Discriminator: 0.043249; Generator: 0.019709,\n",
      "D(x): 0.536, D(G(z)): 0.532\n",
      "2019-04-10 00:58:04,778 root         INFO     Train Epoch: 92 [2048/8000 (26%)]\tTotal Loss: 0.396046\n",
      "Reconstruction: 0.332761, Regularization: 0.000335, Discriminator: 0.043235; Generator: 0.019716,\n",
      "D(x): 0.536, D(G(z)): 0.532\n",
      "2019-04-10 00:58:04,890 root         INFO     Train Epoch: 92 [2560/8000 (32%)]\tTotal Loss: 0.395919\n",
      "Reconstruction: 0.332718, Regularization: 0.000280, Discriminator: 0.043164; Generator: 0.019757,\n",
      "D(x): 0.536, D(G(z)): 0.531\n",
      "2019-04-10 00:58:05,003 root         INFO     Train Epoch: 92 [3072/8000 (38%)]\tTotal Loss: 0.395610\n",
      "Reconstruction: 0.332272, Regularization: 0.000308, Discriminator: 0.043267; Generator: 0.019763,\n",
      "D(x): 0.534, D(G(z)): 0.531\n",
      "2019-04-10 00:58:05,115 root         INFO     Train Epoch: 92 [3584/8000 (45%)]\tTotal Loss: 0.397063\n",
      "Reconstruction: 0.333739, Regularization: 0.000298, Discriminator: 0.043185; Generator: 0.019841,\n",
      "D(x): 0.534, D(G(z)): 0.530\n",
      "2019-04-10 00:58:05,227 root         INFO     Train Epoch: 92 [4096/8000 (51%)]\tTotal Loss: 0.395629\n",
      "Reconstruction: 0.332416, Regularization: 0.000245, Discriminator: 0.043073; Generator: 0.019896,\n",
      "D(x): 0.535, D(G(z)): 0.529\n",
      "2019-04-10 00:58:05,339 root         INFO     Train Epoch: 92 [4608/8000 (58%)]\tTotal Loss: 0.396367\n",
      "Reconstruction: 0.333082, Regularization: 0.000195, Discriminator: 0.043140; Generator: 0.019950,\n",
      "D(x): 0.533, D(G(z)): 0.528\n",
      "2019-04-10 00:58:05,452 root         INFO     Train Epoch: 92 [5120/8000 (64%)]\tTotal Loss: 0.396602\n",
      "Reconstruction: 0.333173, Regularization: 0.000247, Discriminator: 0.043151; Generator: 0.020031,\n",
      "D(x): 0.531, D(G(z)): 0.527\n",
      "2019-04-10 00:58:05,564 root         INFO     Train Epoch: 92 [5632/8000 (70%)]\tTotal Loss: 0.395828\n",
      "Reconstruction: 0.332315, Regularization: 0.000292, Discriminator: 0.043162; Generator: 0.020059,\n",
      "D(x): 0.530, D(G(z)): 0.526\n",
      "2019-04-10 00:58:05,676 root         INFO     Train Epoch: 92 [6144/8000 (77%)]\tTotal Loss: 0.397067\n",
      "Reconstruction: 0.333874, Regularization: 0.000270, Discriminator: 0.042858; Generator: 0.020064,\n",
      "D(x): 0.536, D(G(z)): 0.526\n",
      "2019-04-10 00:58:05,788 root         INFO     Train Epoch: 92 [6656/8000 (83%)]\tTotal Loss: 0.396604\n",
      "Reconstruction: 0.333401, Regularization: 0.000233, Discriminator: 0.042905; Generator: 0.020065,\n",
      "D(x): 0.535, D(G(z)): 0.526\n",
      "2019-04-10 00:58:05,900 root         INFO     Train Epoch: 92 [7168/8000 (90%)]\tTotal Loss: 0.397430\n",
      "Reconstruction: 0.334017, Regularization: 0.000276, Discriminator: 0.043021; Generator: 0.020115,\n",
      "D(x): 0.532, D(G(z)): 0.525\n",
      "2019-04-10 00:58:06,012 root         INFO     Train Epoch: 92 [7680/8000 (96%)]\tTotal Loss: 0.397649\n",
      "Reconstruction: 0.334075, Regularization: 0.000249, Discriminator: 0.043131; Generator: 0.020194,\n",
      "D(x): 0.529, D(G(z)): 0.524\n",
      "2019-04-10 00:58:06,094 root         INFO     ====> Epoch: 92 Average loss: 0.3966\n",
      "2019-04-10 00:58:06,121 root         INFO     Train Epoch: 93 [0/8000 (0%)]\tTotal Loss: 0.396622\n",
      "Reconstruction: 0.333249, Regularization: 0.000307, Discriminator: 0.042859; Generator: 0.020206,\n",
      "D(x): 0.533, D(G(z)): 0.524\n",
      "2019-04-10 00:58:06,233 root         INFO     Train Epoch: 93 [512/8000 (6%)]\tTotal Loss: 0.398559\n",
      "Reconstruction: 0.335183, Regularization: 0.000266, Discriminator: 0.042872; Generator: 0.020238,\n",
      "D(x): 0.532, D(G(z)): 0.523\n",
      "2019-04-10 00:58:06,342 root         INFO     Train Epoch: 93 [1024/8000 (13%)]\tTotal Loss: 0.398199\n",
      "Reconstruction: 0.334574, Regularization: 0.000294, Discriminator: 0.043056; Generator: 0.020275,\n",
      "D(x): 0.528, D(G(z)): 0.523\n",
      "2019-04-10 00:58:06,455 root         INFO     Train Epoch: 93 [1536/8000 (19%)]\tTotal Loss: 0.398642\n",
      "Reconstruction: 0.335286, Regularization: 0.000309, Discriminator: 0.042747; Generator: 0.020300,\n",
      "D(x): 0.533, D(G(z)): 0.522\n",
      "2019-04-10 00:58:06,567 root         INFO     Train Epoch: 93 [2048/8000 (26%)]\tTotal Loss: 0.399075\n",
      "Reconstruction: 0.335627, Regularization: 0.000319, Discriminator: 0.042864; Generator: 0.020266,\n",
      "D(x): 0.532, D(G(z)): 0.523\n",
      "2019-04-10 00:58:06,680 root         INFO     Train Epoch: 93 [2560/8000 (32%)]\tTotal Loss: 0.398809\n",
      "Reconstruction: 0.335540, Regularization: 0.000243, Discriminator: 0.042713; Generator: 0.020313,\n",
      "D(x): 0.533, D(G(z)): 0.522\n",
      "2019-04-10 00:58:06,792 root         INFO     Train Epoch: 93 [3072/8000 (38%)]\tTotal Loss: 0.399863\n",
      "Reconstruction: 0.336509, Regularization: 0.000263, Discriminator: 0.042708; Generator: 0.020384,\n",
      "D(x): 0.532, D(G(z)): 0.521\n",
      "2019-04-10 00:58:06,904 root         INFO     Train Epoch: 93 [3584/8000 (45%)]\tTotal Loss: 0.399793\n",
      "Reconstruction: 0.336554, Regularization: 0.000198, Discriminator: 0.042651; Generator: 0.020390,\n",
      "D(x): 0.533, D(G(z)): 0.521\n",
      "2019-04-10 00:58:07,016 root         INFO     Train Epoch: 93 [4096/8000 (51%)]\tTotal Loss: 0.400056\n",
      "Reconstruction: 0.336294, Regularization: 0.000300, Discriminator: 0.043077; Generator: 0.020384,\n",
      "D(x): 0.526, D(G(z)): 0.521\n",
      "2019-04-10 00:58:07,129 root         INFO     Train Epoch: 93 [4608/8000 (58%)]\tTotal Loss: 0.400881\n",
      "Reconstruction: 0.337441, Regularization: 0.000282, Discriminator: 0.042733; Generator: 0.020425,\n",
      "D(x): 0.531, D(G(z)): 0.520\n",
      "2019-04-10 00:58:07,239 root         INFO     Train Epoch: 93 [5120/8000 (64%)]\tTotal Loss: 0.400718\n",
      "Reconstruction: 0.337166, Regularization: 0.000215, Discriminator: 0.042927; Generator: 0.020410,\n",
      "D(x): 0.528, D(G(z)): 0.520\n",
      "2019-04-10 00:58:07,348 root         INFO     Train Epoch: 93 [5632/8000 (70%)]\tTotal Loss: 0.401351\n",
      "Reconstruction: 0.337829, Regularization: 0.000239, Discriminator: 0.042827; Generator: 0.020456,\n",
      "D(x): 0.529, D(G(z)): 0.520\n",
      "2019-04-10 00:58:07,456 root         INFO     Train Epoch: 93 [6144/8000 (77%)]\tTotal Loss: 0.402815\n",
      "Reconstruction: 0.339024, Regularization: 0.000266, Discriminator: 0.043053; Generator: 0.020471,\n",
      "D(x): 0.525, D(G(z)): 0.519\n",
      "2019-04-10 00:58:07,566 root         INFO     Train Epoch: 93 [6656/8000 (83%)]\tTotal Loss: 0.403154\n",
      "Reconstruction: 0.339371, Regularization: 0.000377, Discriminator: 0.042877; Generator: 0.020528,\n",
      "D(x): 0.527, D(G(z)): 0.518\n",
      "2019-04-10 00:58:07,677 root         INFO     Train Epoch: 93 [7168/8000 (90%)]\tTotal Loss: 0.402784\n",
      "Reconstruction: 0.339023, Regularization: 0.000265, Discriminator: 0.042937; Generator: 0.020559,\n",
      "D(x): 0.525, D(G(z)): 0.518\n",
      "2019-04-10 00:58:07,788 root         INFO     Train Epoch: 93 [7680/8000 (96%)]\tTotal Loss: 0.402569\n",
      "Reconstruction: 0.339158, Regularization: 0.000235, Discriminator: 0.042597; Generator: 0.020579,\n",
      "D(x): 0.531, D(G(z)): 0.518\n",
      "2019-04-10 00:58:07,869 root         INFO     ====> Epoch: 93 Average loss: 0.4004\n",
      "2019-04-10 00:58:07,897 root         INFO     Train Epoch: 94 [0/8000 (0%)]\tTotal Loss: 0.403575\n",
      "Reconstruction: 0.340027, Regularization: 0.000313, Discriminator: 0.042669; Generator: 0.020565,\n",
      "D(x): 0.530, D(G(z)): 0.518\n",
      "2019-04-10 00:58:08,011 root         INFO     Train Epoch: 94 [512/8000 (6%)]\tTotal Loss: 0.403245\n",
      "Reconstruction: 0.340000, Regularization: 0.000191, Discriminator: 0.042507; Generator: 0.020547,\n",
      "D(x): 0.533, D(G(z)): 0.518\n",
      "2019-04-10 00:58:08,124 root         INFO     Train Epoch: 94 [1024/8000 (13%)]\tTotal Loss: 0.403526\n",
      "Reconstruction: 0.339785, Regularization: 0.000280, Discriminator: 0.042884; Generator: 0.020577,\n",
      "D(x): 0.526, D(G(z)): 0.518\n",
      "2019-04-10 00:58:08,236 root         INFO     Train Epoch: 94 [1536/8000 (19%)]\tTotal Loss: 0.403567\n",
      "Reconstruction: 0.340335, Regularization: 0.000225, Discriminator: 0.042413; Generator: 0.020593,\n",
      "D(x): 0.533, D(G(z)): 0.517\n",
      "2019-04-10 00:58:08,348 root         INFO     Train Epoch: 94 [2048/8000 (26%)]\tTotal Loss: 0.404462\n",
      "Reconstruction: 0.340904, Regularization: 0.000261, Discriminator: 0.042710; Generator: 0.020586,\n",
      "D(x): 0.529, D(G(z)): 0.517\n",
      "2019-04-10 00:58:08,461 root         INFO     Train Epoch: 94 [2560/8000 (32%)]\tTotal Loss: 0.404751\n",
      "Reconstruction: 0.341058, Regularization: 0.000309, Discriminator: 0.042737; Generator: 0.020648,\n",
      "D(x): 0.527, D(G(z)): 0.516\n",
      "2019-04-10 00:58:08,573 root         INFO     Train Epoch: 94 [3072/8000 (38%)]\tTotal Loss: 0.404846\n",
      "Reconstruction: 0.340839, Regularization: 0.000347, Discriminator: 0.042965; Generator: 0.020695,\n",
      "D(x): 0.522, D(G(z)): 0.516\n",
      "2019-04-10 00:58:08,685 root         INFO     Train Epoch: 94 [3584/8000 (45%)]\tTotal Loss: 0.405548\n",
      "Reconstruction: 0.341688, Regularization: 0.000325, Discriminator: 0.042814; Generator: 0.020721,\n",
      "D(x): 0.524, D(G(z)): 0.515\n",
      "2019-04-10 00:58:08,797 root         INFO     Train Epoch: 94 [4096/8000 (51%)]\tTotal Loss: 0.406651\n",
      "Reconstruction: 0.342823, Regularization: 0.000285, Discriminator: 0.042808; Generator: 0.020735,\n",
      "D(x): 0.524, D(G(z)): 0.515\n",
      "2019-04-10 00:58:08,908 root         INFO     Train Epoch: 94 [4608/8000 (58%)]\tTotal Loss: 0.406346\n",
      "Reconstruction: 0.342618, Regularization: 0.000294, Discriminator: 0.042673; Generator: 0.020761,\n",
      "D(x): 0.526, D(G(z)): 0.515\n",
      "2019-04-10 00:58:09,019 root         INFO     Train Epoch: 94 [5120/8000 (64%)]\tTotal Loss: 0.407191\n",
      "Reconstruction: 0.343481, Regularization: 0.000316, Discriminator: 0.042645; Generator: 0.020749,\n",
      "D(x): 0.527, D(G(z)): 0.515\n",
      "2019-04-10 00:58:09,131 root         INFO     Train Epoch: 94 [5632/8000 (70%)]\tTotal Loss: 0.406576\n",
      "Reconstruction: 0.342607, Regularization: 0.000277, Discriminator: 0.042985; Generator: 0.020707,\n",
      "D(x): 0.522, D(G(z)): 0.516\n",
      "2019-04-10 00:58:09,242 root         INFO     Train Epoch: 94 [6144/8000 (77%)]\tTotal Loss: 0.406741\n",
      "Reconstruction: 0.343001, Regularization: 0.000194, Discriminator: 0.042813; Generator: 0.020732,\n",
      "D(x): 0.524, D(G(z)): 0.515\n",
      "2019-04-10 00:58:09,353 root         INFO     Train Epoch: 94 [6656/8000 (83%)]\tTotal Loss: 0.408223\n",
      "Reconstruction: 0.344561, Regularization: 0.000301, Discriminator: 0.042615; Generator: 0.020747,\n",
      "D(x): 0.527, D(G(z)): 0.515\n",
      "2019-04-10 00:58:09,464 root         INFO     Train Epoch: 94 [7168/8000 (90%)]\tTotal Loss: 0.408294\n",
      "Reconstruction: 0.344380, Regularization: 0.000345, Discriminator: 0.042757; Generator: 0.020812,\n",
      "D(x): 0.524, D(G(z)): 0.514\n",
      "2019-04-10 00:58:09,575 root         INFO     Train Epoch: 94 [7680/8000 (96%)]\tTotal Loss: 0.408558\n",
      "Reconstruction: 0.344854, Regularization: 0.000244, Discriminator: 0.042662; Generator: 0.020798,\n",
      "D(x): 0.526, D(G(z)): 0.514\n",
      "2019-04-10 00:58:09,656 root         INFO     ====> Epoch: 94 Average loss: 0.4059\n",
      "2019-04-10 00:58:09,684 root         INFO     Train Epoch: 95 [0/8000 (0%)]\tTotal Loss: 0.408093\n",
      "Reconstruction: 0.344479, Regularization: 0.000210, Discriminator: 0.042612; Generator: 0.020791,\n",
      "D(x): 0.526, D(G(z)): 0.514\n",
      "2019-04-10 00:58:09,797 root         INFO     Train Epoch: 95 [512/8000 (6%)]\tTotal Loss: 0.409382\n",
      "Reconstruction: 0.345349, Regularization: 0.000306, Discriminator: 0.042968; Generator: 0.020760,\n",
      "D(x): 0.521, D(G(z)): 0.515\n",
      "2019-04-10 00:58:09,910 root         INFO     Train Epoch: 95 [1024/8000 (13%)]\tTotal Loss: 0.409906\n",
      "Reconstruction: 0.346189, Regularization: 0.000270, Discriminator: 0.042702; Generator: 0.020745,\n",
      "D(x): 0.526, D(G(z)): 0.515\n",
      "2019-04-10 00:58:10,022 root         INFO     Train Epoch: 95 [1536/8000 (19%)]\tTotal Loss: 0.410988\n",
      "Reconstruction: 0.347373, Regularization: 0.000259, Discriminator: 0.042540; Generator: 0.020816,\n",
      "D(x): 0.527, D(G(z)): 0.514\n",
      "2019-04-10 00:58:10,135 root         INFO     Train Epoch: 95 [2048/8000 (26%)]\tTotal Loss: 0.411140\n",
      "Reconstruction: 0.347101, Regularization: 0.000277, Discriminator: 0.042887; Generator: 0.020875,\n",
      "D(x): 0.521, D(G(z)): 0.513\n",
      "2019-04-10 00:58:10,247 root         INFO     Train Epoch: 95 [2560/8000 (32%)]\tTotal Loss: 0.411428\n",
      "Reconstruction: 0.347328, Regularization: 0.000282, Discriminator: 0.042900; Generator: 0.020917,\n",
      "D(x): 0.520, D(G(z)): 0.512\n",
      "2019-04-10 00:58:10,357 root         INFO     Train Epoch: 95 [3072/8000 (38%)]\tTotal Loss: 0.411860\n",
      "Reconstruction: 0.347668, Regularization: 0.000285, Discriminator: 0.042976; Generator: 0.020930,\n",
      "D(x): 0.518, D(G(z)): 0.512\n",
      "2019-04-10 00:58:10,466 root         INFO     Train Epoch: 95 [3584/8000 (45%)]\tTotal Loss: 0.412020\n",
      "Reconstruction: 0.348436, Regularization: 0.000193, Discriminator: 0.042441; Generator: 0.020951,\n",
      "D(x): 0.527, D(G(z)): 0.511\n",
      "2019-04-10 00:58:10,576 root         INFO     Train Epoch: 95 [4096/8000 (51%)]\tTotal Loss: 0.412512\n",
      "Reconstruction: 0.348372, Regularization: 0.000301, Discriminator: 0.042940; Generator: 0.020899,\n",
      "D(x): 0.519, D(G(z)): 0.512\n",
      "2019-04-10 00:58:10,686 root         INFO     Train Epoch: 95 [4608/8000 (58%)]\tTotal Loss: 0.414171\n",
      "Reconstruction: 0.349783, Regularization: 0.000431, Discriminator: 0.043038; Generator: 0.020919,\n",
      "D(x): 0.517, D(G(z)): 0.512\n",
      "2019-04-10 00:58:10,795 root         INFO     Train Epoch: 95 [5120/8000 (64%)]\tTotal Loss: 0.414379\n",
      "Reconstruction: 0.350034, Regularization: 0.000297, Discriminator: 0.043085; Generator: 0.020962,\n",
      "D(x): 0.516, D(G(z)): 0.511\n",
      "2019-04-10 00:58:10,904 root         INFO     Train Epoch: 95 [5632/8000 (70%)]\tTotal Loss: 0.414825\n",
      "Reconstruction: 0.350529, Regularization: 0.000304, Discriminator: 0.042973; Generator: 0.021019,\n",
      "D(x): 0.517, D(G(z)): 0.510\n",
      "2019-04-10 00:58:11,013 root         INFO     Train Epoch: 95 [6144/8000 (77%)]\tTotal Loss: 0.416303\n",
      "Reconstruction: 0.351813, Regularization: 0.000295, Discriminator: 0.043126; Generator: 0.021068,\n",
      "D(x): 0.513, D(G(z)): 0.510\n",
      "2019-04-10 00:58:11,122 root         INFO     Train Epoch: 95 [6656/8000 (83%)]\tTotal Loss: 0.416759\n",
      "Reconstruction: 0.352752, Regularization: 0.000245, Discriminator: 0.042718; Generator: 0.021044,\n",
      "D(x): 0.521, D(G(z)): 0.510\n",
      "2019-04-10 00:58:11,233 root         INFO     Train Epoch: 95 [7168/8000 (90%)]\tTotal Loss: 0.416475\n",
      "Reconstruction: 0.352228, Regularization: 0.000254, Discriminator: 0.042906; Generator: 0.021088,\n",
      "D(x): 0.516, D(G(z)): 0.509\n",
      "2019-04-10 00:58:11,344 root         INFO     Train Epoch: 95 [7680/8000 (96%)]\tTotal Loss: 0.418046\n",
      "Reconstruction: 0.353739, Regularization: 0.000269, Discriminator: 0.042930; Generator: 0.021108,\n",
      "D(x): 0.516, D(G(z)): 0.509\n",
      "2019-04-10 00:58:11,425 root         INFO     ====> Epoch: 95 Average loss: 0.4133\n",
      "2019-04-10 00:58:11,452 root         INFO     Train Epoch: 96 [0/8000 (0%)]\tTotal Loss: 0.418924\n",
      "Reconstruction: 0.354705, Regularization: 0.000273, Discriminator: 0.042857; Generator: 0.021089,\n",
      "D(x): 0.517, D(G(z)): 0.509\n",
      "2019-04-10 00:58:11,562 root         INFO     Train Epoch: 96 [512/8000 (6%)]\tTotal Loss: 0.418661\n",
      "Reconstruction: 0.354538, Regularization: 0.000272, Discriminator: 0.042704; Generator: 0.021146,\n",
      "D(x): 0.519, D(G(z)): 0.508\n",
      "2019-04-10 00:58:11,672 root         INFO     Train Epoch: 96 [1024/8000 (13%)]\tTotal Loss: 0.419874\n",
      "Reconstruction: 0.355399, Regularization: 0.000273, Discriminator: 0.043051; Generator: 0.021151,\n",
      "D(x): 0.513, D(G(z)): 0.508\n",
      "2019-04-10 00:58:11,783 root         INFO     Train Epoch: 96 [1536/8000 (19%)]\tTotal Loss: 0.421052\n",
      "Reconstruction: 0.356425, Regularization: 0.000280, Discriminator: 0.043151; Generator: 0.021197,\n",
      "D(x): 0.511, D(G(z)): 0.507\n",
      "2019-04-10 00:58:11,893 root         INFO     Train Epoch: 96 [2048/8000 (26%)]\tTotal Loss: 0.421246\n",
      "Reconstruction: 0.356851, Regularization: 0.000372, Discriminator: 0.042783; Generator: 0.021240,\n",
      "D(x): 0.516, D(G(z)): 0.507\n",
      "2019-04-10 00:58:12,003 root         INFO     Train Epoch: 96 [2560/8000 (32%)]\tTotal Loss: 0.422464\n",
      "Reconstruction: 0.358115, Regularization: 0.000329, Discriminator: 0.042847; Generator: 0.021172,\n",
      "D(x): 0.516, D(G(z)): 0.508\n",
      "2019-04-10 00:58:12,113 root         INFO     Train Epoch: 96 [3072/8000 (38%)]\tTotal Loss: 0.422826\n",
      "Reconstruction: 0.358359, Regularization: 0.000288, Discriminator: 0.042941; Generator: 0.021237,\n",
      "D(x): 0.513, D(G(z)): 0.507\n",
      "2019-04-10 00:58:12,223 root         INFO     Train Epoch: 96 [3584/8000 (45%)]\tTotal Loss: 0.423378\n",
      "Reconstruction: 0.359121, Regularization: 0.000296, Discriminator: 0.042726; Generator: 0.021235,\n",
      "D(x): 0.517, D(G(z)): 0.507\n",
      "2019-04-10 00:58:12,333 root         INFO     Train Epoch: 96 [4096/8000 (51%)]\tTotal Loss: 0.424539\n",
      "Reconstruction: 0.360484, Regularization: 0.000208, Discriminator: 0.042650; Generator: 0.021197,\n",
      "D(x): 0.519, D(G(z)): 0.507\n",
      "2019-04-10 00:58:12,443 root         INFO     Train Epoch: 96 [4608/8000 (58%)]\tTotal Loss: 0.425528\n",
      "Reconstruction: 0.360893, Regularization: 0.000259, Discriminator: 0.043108; Generator: 0.021268,\n",
      "D(x): 0.510, D(G(z)): 0.506\n",
      "2019-04-10 00:58:12,553 root         INFO     Train Epoch: 96 [5120/8000 (64%)]\tTotal Loss: 0.426165\n",
      "Reconstruction: 0.361409, Regularization: 0.000288, Discriminator: 0.043197; Generator: 0.021270,\n",
      "D(x): 0.509, D(G(z)): 0.506\n",
      "2019-04-10 00:58:12,663 root         INFO     Train Epoch: 96 [5632/8000 (70%)]\tTotal Loss: 0.426189\n",
      "Reconstruction: 0.361144, Regularization: 0.000342, Discriminator: 0.043403; Generator: 0.021300,\n",
      "D(x): 0.505, D(G(z)): 0.506\n",
      "2019-04-10 00:58:12,773 root         INFO     Train Epoch: 96 [6144/8000 (77%)]\tTotal Loss: 0.426450\n",
      "Reconstruction: 0.361842, Regularization: 0.000315, Discriminator: 0.042953; Generator: 0.021339,\n",
      "D(x): 0.512, D(G(z)): 0.505\n",
      "2019-04-10 00:58:12,884 root         INFO     Train Epoch: 96 [6656/8000 (83%)]\tTotal Loss: 0.427394\n",
      "Reconstruction: 0.362653, Regularization: 0.000274, Discriminator: 0.043145; Generator: 0.021323,\n",
      "D(x): 0.509, D(G(z)): 0.505\n",
      "2019-04-10 00:58:12,995 root         INFO     Train Epoch: 96 [7168/8000 (90%)]\tTotal Loss: 0.427108\n",
      "Reconstruction: 0.362967, Regularization: 0.000193, Discriminator: 0.042635; Generator: 0.021313,\n",
      "D(x): 0.517, D(G(z)): 0.506\n",
      "2019-04-10 00:58:13,107 root         INFO     Train Epoch: 96 [7680/8000 (96%)]\tTotal Loss: 0.428753\n",
      "Reconstruction: 0.364459, Regularization: 0.000248, Discriminator: 0.042769; Generator: 0.021277,\n",
      "D(x): 0.516, D(G(z)): 0.506\n",
      "2019-04-10 00:58:13,187 root         INFO     ====> Epoch: 96 Average loss: 0.4239\n",
      "2019-04-10 00:58:13,215 root         INFO     Train Epoch: 97 [0/8000 (0%)]\tTotal Loss: 0.429042\n",
      "Reconstruction: 0.363961, Regularization: 0.000355, Discriminator: 0.043417; Generator: 0.021310,\n",
      "D(x): 0.505, D(G(z)): 0.506\n",
      "2019-04-10 00:58:13,327 root         INFO     Train Epoch: 97 [512/8000 (6%)]\tTotal Loss: 0.429738\n",
      "Reconstruction: 0.364719, Regularization: 0.000276, Discriminator: 0.043385; Generator: 0.021357,\n",
      "D(x): 0.504, D(G(z)): 0.505\n",
      "2019-04-10 00:58:13,438 root         INFO     Train Epoch: 97 [1024/8000 (13%)]\tTotal Loss: 0.429300\n",
      "Reconstruction: 0.364325, Regularization: 0.000251, Discriminator: 0.043300; Generator: 0.021423,\n",
      "D(x): 0.504, D(G(z)): 0.504\n",
      "2019-04-10 00:58:13,550 root         INFO     Train Epoch: 97 [1536/8000 (19%)]\tTotal Loss: 0.430539\n",
      "Reconstruction: 0.365998, Regularization: 0.000291, Discriminator: 0.042802; Generator: 0.021447,\n",
      "D(x): 0.512, D(G(z)): 0.503\n",
      "2019-04-10 00:58:13,661 root         INFO     Train Epoch: 97 [2048/8000 (26%)]\tTotal Loss: 0.431911\n",
      "Reconstruction: 0.366957, Regularization: 0.000215, Discriminator: 0.043236; Generator: 0.021503,\n",
      "D(x): 0.504, D(G(z)): 0.503\n",
      "2019-04-10 00:58:13,772 root         INFO     Train Epoch: 97 [2560/8000 (32%)]\tTotal Loss: 0.431267\n",
      "Reconstruction: 0.366675, Regularization: 0.000268, Discriminator: 0.042854; Generator: 0.021470,\n",
      "D(x): 0.511, D(G(z)): 0.503\n",
      "2019-04-10 00:58:13,882 root         INFO     Train Epoch: 97 [3072/8000 (38%)]\tTotal Loss: 0.433864\n",
      "Reconstruction: 0.369105, Regularization: 0.000275, Discriminator: 0.043030; Generator: 0.021454,\n",
      "D(x): 0.509, D(G(z)): 0.503\n",
      "2019-04-10 00:58:13,993 root         INFO     Train Epoch: 97 [3584/8000 (45%)]\tTotal Loss: 0.433829\n",
      "Reconstruction: 0.368895, Regularization: 0.000309, Discriminator: 0.043135; Generator: 0.021489,\n",
      "D(x): 0.506, D(G(z)): 0.503\n",
      "2019-04-10 00:58:14,108 root         INFO     Train Epoch: 97 [4096/8000 (51%)]\tTotal Loss: 0.434212\n",
      "Reconstruction: 0.369078, Regularization: 0.000272, Discriminator: 0.043288; Generator: 0.021574,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:58:14,219 root         INFO     Train Epoch: 97 [4608/8000 (58%)]\tTotal Loss: 0.436049\n",
      "Reconstruction: 0.370700, Regularization: 0.000289, Discriminator: 0.043477; Generator: 0.021582,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 00:58:14,329 root         INFO     Train Epoch: 97 [5120/8000 (64%)]\tTotal Loss: 0.436234\n",
      "Reconstruction: 0.371425, Regularization: 0.000259, Discriminator: 0.042990; Generator: 0.021559,\n",
      "D(x): 0.507, D(G(z)): 0.502\n",
      "2019-04-10 00:58:14,440 root         INFO     Train Epoch: 97 [5632/8000 (70%)]\tTotal Loss: 0.437168\n",
      "Reconstruction: 0.371846, Regularization: 0.000266, Discriminator: 0.043476; Generator: 0.021580,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 00:58:14,550 root         INFO     Train Epoch: 97 [6144/8000 (77%)]\tTotal Loss: 0.437217\n",
      "Reconstruction: 0.372289, Regularization: 0.000207, Discriminator: 0.043176; Generator: 0.021546,\n",
      "D(x): 0.505, D(G(z)): 0.502\n",
      "2019-04-10 00:58:14,660 root         INFO     Train Epoch: 97 [6656/8000 (83%)]\tTotal Loss: 0.439299\n",
      "Reconstruction: 0.374176, Regularization: 0.000319, Discriminator: 0.043202; Generator: 0.021602,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 00:58:14,770 root         INFO     Train Epoch: 97 [7168/8000 (90%)]\tTotal Loss: 0.440142\n",
      "Reconstruction: 0.374900, Regularization: 0.000201, Discriminator: 0.043338; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:14,884 root         INFO     Train Epoch: 97 [7680/8000 (96%)]\tTotal Loss: 0.441958\n",
      "Reconstruction: 0.376686, Regularization: 0.000230, Discriminator: 0.043317; Generator: 0.021724,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:14,966 root         INFO     ====> Epoch: 97 Average loss: 0.4347\n",
      "2019-04-10 00:58:14,993 root         INFO     Train Epoch: 98 [0/8000 (0%)]\tTotal Loss: 0.442590\n",
      "Reconstruction: 0.376755, Regularization: 0.000281, Discriminator: 0.043784; Generator: 0.021770,\n",
      "D(x): 0.491, D(G(z)): 0.498\n",
      "2019-04-10 00:58:15,106 root         INFO     Train Epoch: 98 [512/8000 (6%)]\tTotal Loss: 0.442904\n",
      "Reconstruction: 0.377091, Regularization: 0.000327, Discriminator: 0.043671; Generator: 0.021815,\n",
      "D(x): 0.493, D(G(z)): 0.498\n",
      "2019-04-10 00:58:15,218 root         INFO     Train Epoch: 98 [1024/8000 (13%)]\tTotal Loss: 0.442679\n",
      "Reconstruction: 0.377241, Regularization: 0.000211, Discriminator: 0.043449; Generator: 0.021778,\n",
      "D(x): 0.496, D(G(z)): 0.498\n",
      "2019-04-10 00:58:15,331 root         INFO     Train Epoch: 98 [1536/8000 (19%)]\tTotal Loss: 0.444720\n",
      "Reconstruction: 0.379086, Regularization: 0.000302, Discriminator: 0.043481; Generator: 0.021850,\n",
      "D(x): 0.495, D(G(z)): 0.497\n",
      "2019-04-10 00:58:15,443 root         INFO     Train Epoch: 98 [2048/8000 (26%)]\tTotal Loss: 0.444278\n",
      "Reconstruction: 0.379000, Regularization: 0.000232, Discriminator: 0.043190; Generator: 0.021856,\n",
      "D(x): 0.499, D(G(z)): 0.497\n",
      "2019-04-10 00:58:15,554 root         INFO     Train Epoch: 98 [2560/8000 (32%)]\tTotal Loss: 0.445289\n",
      "Reconstruction: 0.379814, Regularization: 0.000241, Discriminator: 0.043329; Generator: 0.021905,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-10 00:58:15,666 root         INFO     Train Epoch: 98 [3072/8000 (38%)]\tTotal Loss: 0.446398\n",
      "Reconstruction: 0.381005, Regularization: 0.000203, Discriminator: 0.043304; Generator: 0.021886,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-10 00:58:15,776 root         INFO     Train Epoch: 98 [3584/8000 (45%)]\tTotal Loss: 0.447146\n",
      "Reconstruction: 0.381241, Regularization: 0.000325, Discriminator: 0.043659; Generator: 0.021920,\n",
      "D(x): 0.491, D(G(z)): 0.496\n",
      "2019-04-10 00:58:15,887 root         INFO     Train Epoch: 98 [4096/8000 (51%)]\tTotal Loss: 0.447693\n",
      "Reconstruction: 0.382383, Regularization: 0.000259, Discriminator: 0.043199; Generator: 0.021852,\n",
      "D(x): 0.499, D(G(z)): 0.497\n",
      "2019-04-10 00:58:15,998 root         INFO     Train Epoch: 98 [4608/8000 (58%)]\tTotal Loss: 0.448819\n",
      "Reconstruction: 0.383205, Regularization: 0.000293, Discriminator: 0.043450; Generator: 0.021871,\n",
      "D(x): 0.495, D(G(z)): 0.497\n",
      "2019-04-10 00:58:16,109 root         INFO     Train Epoch: 98 [5120/8000 (64%)]\tTotal Loss: 0.449065\n",
      "Reconstruction: 0.383445, Regularization: 0.000223, Discriminator: 0.043459; Generator: 0.021939,\n",
      "D(x): 0.494, D(G(z)): 0.496\n",
      "2019-04-10 00:58:16,220 root         INFO     Train Epoch: 98 [5632/8000 (70%)]\tTotal Loss: 0.449865\n",
      "Reconstruction: 0.384373, Regularization: 0.000260, Discriminator: 0.043263; Generator: 0.021969,\n",
      "D(x): 0.496, D(G(z)): 0.495\n",
      "2019-04-10 00:58:16,332 root         INFO     Train Epoch: 98 [6144/8000 (77%)]\tTotal Loss: 0.449169\n",
      "Reconstruction: 0.383896, Regularization: 0.000172, Discriminator: 0.043105; Generator: 0.021997,\n",
      "D(x): 0.498, D(G(z)): 0.495\n",
      "2019-04-10 00:58:16,443 root         INFO     Train Epoch: 98 [6656/8000 (83%)]\tTotal Loss: 0.450971\n",
      "Reconstruction: 0.384636, Regularization: 0.000421, Discriminator: 0.043882; Generator: 0.022032,\n",
      "D(x): 0.486, D(G(z)): 0.494\n",
      "2019-04-10 00:58:16,554 root         INFO     Train Epoch: 98 [7168/8000 (90%)]\tTotal Loss: 0.451334\n",
      "Reconstruction: 0.385514, Regularization: 0.000260, Discriminator: 0.043517; Generator: 0.022044,\n",
      "D(x): 0.491, D(G(z)): 0.494\n",
      "2019-04-10 00:58:16,664 root         INFO     Train Epoch: 98 [7680/8000 (96%)]\tTotal Loss: 0.452476\n",
      "Reconstruction: 0.386785, Regularization: 0.000186, Discriminator: 0.043427; Generator: 0.022079,\n",
      "D(x): 0.492, D(G(z)): 0.493\n",
      "2019-04-10 00:58:16,746 root         INFO     ====> Epoch: 98 Average loss: 0.4474\n",
      "2019-04-10 00:58:16,773 root         INFO     Train Epoch: 99 [0/8000 (0%)]\tTotal Loss: 0.453350\n",
      "Reconstruction: 0.387043, Regularization: 0.000329, Discriminator: 0.043922; Generator: 0.022056,\n",
      "D(x): 0.485, D(G(z)): 0.494\n",
      "2019-04-10 00:58:16,885 root         INFO     Train Epoch: 99 [512/8000 (6%)]\tTotal Loss: 0.454334\n",
      "Reconstruction: 0.388095, Regularization: 0.000261, Discriminator: 0.043873; Generator: 0.022106,\n",
      "D(x): 0.485, D(G(z)): 0.493\n",
      "2019-04-10 00:58:16,997 root         INFO     Train Epoch: 99 [1024/8000 (13%)]\tTotal Loss: 0.455823\n",
      "Reconstruction: 0.389722, Regularization: 0.000266, Discriminator: 0.043748; Generator: 0.022086,\n",
      "D(x): 0.487, D(G(z)): 0.493\n",
      "2019-04-10 00:58:17,109 root         INFO     Train Epoch: 99 [1536/8000 (19%)]\tTotal Loss: 0.456566\n",
      "Reconstruction: 0.390908, Regularization: 0.000225, Discriminator: 0.043285; Generator: 0.022146,\n",
      "D(x): 0.493, D(G(z)): 0.492\n",
      "2019-04-10 00:58:17,221 root         INFO     Train Epoch: 99 [2048/8000 (26%)]\tTotal Loss: 0.456128\n",
      "Reconstruction: 0.390227, Regularization: 0.000184, Discriminator: 0.043555; Generator: 0.022162,\n",
      "D(x): 0.489, D(G(z)): 0.492\n",
      "2019-04-10 00:58:17,334 root         INFO     Train Epoch: 99 [2560/8000 (32%)]\tTotal Loss: 0.456970\n",
      "Reconstruction: 0.390817, Regularization: 0.000281, Discriminator: 0.043708; Generator: 0.022164,\n",
      "D(x): 0.486, D(G(z)): 0.492\n",
      "2019-04-10 00:58:17,446 root         INFO     Train Epoch: 99 [3072/8000 (38%)]\tTotal Loss: 0.457998\n",
      "Reconstruction: 0.391781, Regularization: 0.000261, Discriminator: 0.043740; Generator: 0.022215,\n",
      "D(x): 0.485, D(G(z)): 0.491\n",
      "2019-04-10 00:58:17,558 root         INFO     Train Epoch: 99 [3584/8000 (45%)]\tTotal Loss: 0.459086\n",
      "Reconstruction: 0.393042, Regularization: 0.000237, Discriminator: 0.043539; Generator: 0.022268,\n",
      "D(x): 0.487, D(G(z)): 0.490\n",
      "2019-04-10 00:58:17,667 root         INFO     Train Epoch: 99 [4096/8000 (51%)]\tTotal Loss: 0.460618\n",
      "Reconstruction: 0.394546, Regularization: 0.000213, Discriminator: 0.043638; Generator: 0.022221,\n",
      "D(x): 0.487, D(G(z)): 0.491\n",
      "2019-04-10 00:58:17,776 root         INFO     Train Epoch: 99 [4608/8000 (58%)]\tTotal Loss: 0.459918\n",
      "Reconstruction: 0.393723, Regularization: 0.000209, Discriminator: 0.043717; Generator: 0.022270,\n",
      "D(x): 0.485, D(G(z)): 0.490\n",
      "2019-04-10 00:58:17,885 root         INFO     Train Epoch: 99 [5120/8000 (64%)]\tTotal Loss: 0.462202\n",
      "Reconstruction: 0.395791, Regularization: 0.000287, Discriminator: 0.043796; Generator: 0.022328,\n",
      "D(x): 0.482, D(G(z)): 0.489\n",
      "2019-04-10 00:58:17,993 root         INFO     Train Epoch: 99 [5632/8000 (70%)]\tTotal Loss: 0.462460\n",
      "Reconstruction: 0.396158, Regularization: 0.000254, Discriminator: 0.043651; Generator: 0.022397,\n",
      "D(x): 0.484, D(G(z)): 0.488\n",
      "2019-04-10 00:58:18,102 root         INFO     Train Epoch: 99 [6144/8000 (77%)]\tTotal Loss: 0.462312\n",
      "Reconstruction: 0.396338, Regularization: 0.000173, Discriminator: 0.043278; Generator: 0.022524,\n",
      "D(x): 0.488, D(G(z)): 0.486\n",
      "2019-04-10 00:58:18,211 root         INFO     Train Epoch: 99 [6656/8000 (83%)]\tTotal Loss: 0.463134\n",
      "Reconstruction: 0.396805, Regularization: 0.000165, Discriminator: 0.043741; Generator: 0.022423,\n",
      "D(x): 0.482, D(G(z)): 0.488\n",
      "2019-04-10 00:58:18,320 root         INFO     Train Epoch: 99 [7168/8000 (90%)]\tTotal Loss: 0.463596\n",
      "Reconstruction: 0.397344, Regularization: 0.000208, Discriminator: 0.043729; Generator: 0.022314,\n",
      "D(x): 0.484, D(G(z)): 0.490\n",
      "2019-04-10 00:58:18,429 root         INFO     Train Epoch: 99 [7680/8000 (96%)]\tTotal Loss: 0.465298\n",
      "Reconstruction: 0.398982, Regularization: 0.000270, Discriminator: 0.043664; Generator: 0.022382,\n",
      "D(x): 0.484, D(G(z)): 0.489\n",
      "2019-04-10 00:58:18,509 root         INFO     ====> Epoch: 99 Average loss: 0.4595\n",
      "2019-04-10 00:58:18,536 root         INFO     Train Epoch: 100 [0/8000 (0%)]\tTotal Loss: 0.464938\n",
      "Reconstruction: 0.398945, Regularization: 0.000199, Discriminator: 0.043433; Generator: 0.022362,\n",
      "D(x): 0.488, D(G(z)): 0.489\n",
      "2019-04-10 00:58:18,647 root         INFO     Train Epoch: 100 [512/8000 (6%)]\tTotal Loss: 0.465956\n",
      "Reconstruction: 0.399934, Regularization: 0.000148, Discriminator: 0.043451; Generator: 0.022424,\n",
      "D(x): 0.486, D(G(z)): 0.488\n",
      "2019-04-10 00:58:18,757 root         INFO     Train Epoch: 100 [1024/8000 (13%)]\tTotal Loss: 0.465567\n",
      "Reconstruction: 0.399414, Regularization: 0.000160, Discriminator: 0.043619; Generator: 0.022374,\n",
      "D(x): 0.484, D(G(z)): 0.489\n",
      "2019-04-10 00:58:18,868 root         INFO     Train Epoch: 100 [1536/8000 (19%)]\tTotal Loss: 0.467376\n",
      "Reconstruction: 0.401007, Regularization: 0.000185, Discriminator: 0.043746; Generator: 0.022438,\n",
      "D(x): 0.482, D(G(z)): 0.488\n",
      "2019-04-10 00:58:18,979 root         INFO     Train Epoch: 100 [2048/8000 (26%)]\tTotal Loss: 0.467467\n",
      "Reconstruction: 0.400955, Regularization: 0.000291, Discriminator: 0.043774; Generator: 0.022447,\n",
      "D(x): 0.481, D(G(z)): 0.488\n",
      "2019-04-10 00:58:19,090 root         INFO     Train Epoch: 100 [2560/8000 (32%)]\tTotal Loss: 0.468937\n",
      "Reconstruction: 0.402172, Regularization: 0.000294, Discriminator: 0.043938; Generator: 0.022533,\n",
      "D(x): 0.477, D(G(z)): 0.486\n",
      "2019-04-10 00:58:19,201 root         INFO     Train Epoch: 100 [3072/8000 (38%)]\tTotal Loss: 0.469139\n",
      "Reconstruction: 0.402656, Regularization: 0.000181, Discriminator: 0.043773; Generator: 0.022530,\n",
      "D(x): 0.480, D(G(z)): 0.486\n",
      "2019-04-10 00:58:19,312 root         INFO     Train Epoch: 100 [3584/8000 (45%)]\tTotal Loss: 0.470042\n",
      "Reconstruction: 0.403492, Regularization: 0.000213, Discriminator: 0.043717; Generator: 0.022620,\n",
      "D(x): 0.479, D(G(z)): 0.485\n",
      "2019-04-10 00:58:19,423 root         INFO     Train Epoch: 100 [4096/8000 (51%)]\tTotal Loss: 0.470065\n",
      "Reconstruction: 0.403659, Regularization: 0.000224, Discriminator: 0.043621; Generator: 0.022560,\n",
      "D(x): 0.482, D(G(z)): 0.486\n",
      "2019-04-10 00:58:19,534 root         INFO     Train Epoch: 100 [4608/8000 (58%)]\tTotal Loss: 0.471084\n",
      "Reconstruction: 0.404786, Regularization: 0.000138, Discriminator: 0.043592; Generator: 0.022568,\n",
      "D(x): 0.482, D(G(z)): 0.486\n",
      "2019-04-10 00:58:19,645 root         INFO     Train Epoch: 100 [5120/8000 (64%)]\tTotal Loss: 0.471817\n",
      "Reconstruction: 0.405277, Regularization: 0.000220, Discriminator: 0.043724; Generator: 0.022595,\n",
      "D(x): 0.480, D(G(z)): 0.485\n",
      "2019-04-10 00:58:19,755 root         INFO     Train Epoch: 100 [5632/8000 (70%)]\tTotal Loss: 0.472681\n",
      "Reconstruction: 0.406255, Regularization: 0.000195, Discriminator: 0.043635; Generator: 0.022596,\n",
      "D(x): 0.481, D(G(z)): 0.485\n",
      "2019-04-10 00:58:19,866 root         INFO     Train Epoch: 100 [6144/8000 (77%)]\tTotal Loss: 0.473040\n",
      "Reconstruction: 0.406459, Regularization: 0.000172, Discriminator: 0.043722; Generator: 0.022687,\n",
      "D(x): 0.478, D(G(z)): 0.484\n",
      "2019-04-10 00:58:19,977 root         INFO     Train Epoch: 100 [6656/8000 (83%)]\tTotal Loss: 0.473338\n",
      "Reconstruction: 0.406720, Regularization: 0.000217, Discriminator: 0.043683; Generator: 0.022719,\n",
      "D(x): 0.478, D(G(z)): 0.483\n",
      "2019-04-10 00:58:20,087 root         INFO     Train Epoch: 100 [7168/8000 (90%)]\tTotal Loss: 0.473389\n",
      "Reconstruction: 0.406918, Regularization: 0.000160, Discriminator: 0.043606; Generator: 0.022705,\n",
      "D(x): 0.480, D(G(z)): 0.484\n",
      "2019-04-10 00:58:20,196 root         INFO     Train Epoch: 100 [7680/8000 (96%)]\tTotal Loss: 0.474060\n",
      "Reconstruction: 0.407547, Regularization: 0.000156, Discriminator: 0.043696; Generator: 0.022661,\n",
      "D(x): 0.479, D(G(z)): 0.484\n",
      "2019-04-10 00:58:20,278 root         INFO     ====> Epoch: 100 Average loss: 0.4703\n",
      "2019-04-10 00:58:20,305 root         INFO     Train Epoch: 101 [0/8000 (0%)]\tTotal Loss: 0.474467\n",
      "Reconstruction: 0.407861, Regularization: 0.000207, Discriminator: 0.043664; Generator: 0.022735,\n",
      "D(x): 0.478, D(G(z)): 0.483\n",
      "2019-04-10 00:58:20,416 root         INFO     Train Epoch: 101 [512/8000 (6%)]\tTotal Loss: 0.474183\n",
      "Reconstruction: 0.407640, Regularization: 0.000170, Discriminator: 0.043650; Generator: 0.022723,\n",
      "D(x): 0.479, D(G(z)): 0.483\n",
      "2019-04-10 00:58:20,526 root         INFO     Train Epoch: 101 [1024/8000 (13%)]\tTotal Loss: 0.475366\n",
      "Reconstruction: 0.408848, Regularization: 0.000264, Discriminator: 0.043477; Generator: 0.022776,\n",
      "D(x): 0.481, D(G(z)): 0.482\n",
      "2019-04-10 00:58:20,637 root         INFO     Train Epoch: 101 [1536/8000 (19%)]\tTotal Loss: 0.475723\n",
      "Reconstruction: 0.409086, Regularization: 0.000163, Discriminator: 0.043636; Generator: 0.022838,\n",
      "D(x): 0.477, D(G(z)): 0.482\n",
      "2019-04-10 00:58:20,746 root         INFO     Train Epoch: 101 [2048/8000 (26%)]\tTotal Loss: 0.476197\n",
      "Reconstruction: 0.409586, Regularization: 0.000173, Discriminator: 0.043566; Generator: 0.022872,\n",
      "D(x): 0.478, D(G(z)): 0.481\n",
      "2019-04-10 00:58:20,855 root         INFO     Train Epoch: 101 [2560/8000 (32%)]\tTotal Loss: 0.476084\n",
      "Reconstruction: 0.409336, Regularization: 0.000302, Discriminator: 0.043571; Generator: 0.022875,\n",
      "D(x): 0.478, D(G(z)): 0.481\n",
      "2019-04-10 00:58:20,965 root         INFO     Train Epoch: 101 [3072/8000 (38%)]\tTotal Loss: 0.476558\n",
      "Reconstruction: 0.409995, Regularization: 0.000151, Discriminator: 0.043468; Generator: 0.022944,\n",
      "D(x): 0.478, D(G(z)): 0.480\n",
      "2019-04-10 00:58:21,074 root         INFO     Train Epoch: 101 [3584/8000 (45%)]\tTotal Loss: 0.477477\n",
      "Reconstruction: 0.410848, Regularization: 0.000182, Discriminator: 0.043447; Generator: 0.022999,\n",
      "D(x): 0.478, D(G(z)): 0.479\n",
      "2019-04-10 00:58:21,183 root         INFO     Train Epoch: 101 [4096/8000 (51%)]\tTotal Loss: 0.477286\n",
      "Reconstruction: 0.410665, Regularization: 0.000214, Discriminator: 0.043436; Generator: 0.022971,\n",
      "D(x): 0.479, D(G(z)): 0.479\n",
      "2019-04-10 00:58:21,292 root         INFO     Train Epoch: 101 [4608/8000 (58%)]\tTotal Loss: 0.477737\n",
      "Reconstruction: 0.410989, Regularization: 0.000214, Discriminator: 0.043392; Generator: 0.023143,\n",
      "D(x): 0.477, D(G(z)): 0.477\n",
      "2019-04-10 00:58:21,401 root         INFO     Train Epoch: 101 [5120/8000 (64%)]\tTotal Loss: 0.477214\n",
      "Reconstruction: 0.410411, Regularization: 0.000231, Discriminator: 0.043352; Generator: 0.023220,\n",
      "D(x): 0.476, D(G(z)): 0.476\n",
      "2019-04-10 00:58:21,510 root         INFO     Train Epoch: 101 [5632/8000 (70%)]\tTotal Loss: 0.477218\n",
      "Reconstruction: 0.410548, Regularization: 0.000211, Discriminator: 0.043249; Generator: 0.023210,\n",
      "D(x): 0.478, D(G(z)): 0.476\n",
      "2019-04-10 00:58:21,619 root         INFO     Train Epoch: 101 [6144/8000 (77%)]\tTotal Loss: 0.476527\n",
      "Reconstruction: 0.409790, Regularization: 0.000263, Discriminator: 0.043240; Generator: 0.023234,\n",
      "D(x): 0.478, D(G(z)): 0.475\n",
      "2019-04-10 00:58:21,728 root         INFO     Train Epoch: 101 [6656/8000 (83%)]\tTotal Loss: 0.475945\n",
      "Reconstruction: 0.409203, Regularization: 0.000184, Discriminator: 0.043227; Generator: 0.023330,\n",
      "D(x): 0.477, D(G(z)): 0.474\n",
      "2019-04-10 00:58:21,839 root         INFO     Train Epoch: 101 [7168/8000 (90%)]\tTotal Loss: 0.475385\n",
      "Reconstruction: 0.408691, Regularization: 0.000195, Discriminator: 0.043171; Generator: 0.023328,\n",
      "D(x): 0.478, D(G(z)): 0.474\n",
      "2019-04-10 00:58:21,950 root         INFO     Train Epoch: 101 [7680/8000 (96%)]\tTotal Loss: 0.475291\n",
      "Reconstruction: 0.408521, Regularization: 0.000153, Discriminator: 0.043133; Generator: 0.023483,\n",
      "D(x): 0.476, D(G(z)): 0.472\n",
      "2019-04-10 00:58:22,031 root         INFO     ====> Epoch: 101 Average loss: 0.4761\n",
      "2019-04-10 00:58:22,059 root         INFO     Train Epoch: 102 [0/8000 (0%)]\tTotal Loss: 0.473887\n",
      "Reconstruction: 0.407219, Regularization: 0.000247, Discriminator: 0.043037; Generator: 0.023384,\n",
      "D(x): 0.479, D(G(z)): 0.473\n",
      "2019-04-10 00:58:22,170 root         INFO     Train Epoch: 102 [512/8000 (6%)]\tTotal Loss: 0.474181\n",
      "Reconstruction: 0.407447, Regularization: 0.000236, Discriminator: 0.042966; Generator: 0.023531,\n",
      "D(x): 0.478, D(G(z)): 0.471\n",
      "2019-04-10 00:58:22,280 root         INFO     Train Epoch: 102 [1024/8000 (13%)]\tTotal Loss: 0.474487\n",
      "Reconstruction: 0.407664, Regularization: 0.000280, Discriminator: 0.042960; Generator: 0.023583,\n",
      "D(x): 0.477, D(G(z)): 0.470\n",
      "2019-04-10 00:58:22,391 root         INFO     Train Epoch: 102 [1536/8000 (19%)]\tTotal Loss: 0.473046\n",
      "Reconstruction: 0.406117, Regularization: 0.000189, Discriminator: 0.043038; Generator: 0.023701,\n",
      "D(x): 0.475, D(G(z)): 0.468\n",
      "2019-04-10 00:58:22,501 root         INFO     Train Epoch: 102 [2048/8000 (26%)]\tTotal Loss: 0.472890\n",
      "Reconstruction: 0.406299, Regularization: 0.000248, Discriminator: 0.042722; Generator: 0.023621,\n",
      "D(x): 0.481, D(G(z)): 0.470\n",
      "2019-04-10 00:58:22,611 root         INFO     Train Epoch: 102 [2560/8000 (32%)]\tTotal Loss: 0.472084\n",
      "Reconstruction: 0.405637, Regularization: 0.000232, Discriminator: 0.042732; Generator: 0.023482,\n",
      "D(x): 0.482, D(G(z)): 0.472\n",
      "2019-04-10 00:58:22,722 root         INFO     Train Epoch: 102 [3072/8000 (38%)]\tTotal Loss: 0.470812\n",
      "Reconstruction: 0.404353, Regularization: 0.000171, Discriminator: 0.042698; Generator: 0.023589,\n",
      "D(x): 0.481, D(G(z)): 0.470\n",
      "2019-04-10 00:58:22,832 root         INFO     Train Epoch: 102 [3584/8000 (45%)]\tTotal Loss: 0.469698\n",
      "Reconstruction: 0.402872, Regularization: 0.000234, Discriminator: 0.043125; Generator: 0.023466,\n",
      "D(x): 0.477, D(G(z)): 0.472\n",
      "2019-04-10 00:58:22,943 root         INFO     Train Epoch: 102 [4096/8000 (51%)]\tTotal Loss: 0.468218\n",
      "Reconstruction: 0.401576, Regularization: 0.000170, Discriminator: 0.043210; Generator: 0.023262,\n",
      "D(x): 0.478, D(G(z)): 0.475\n",
      "2019-04-10 00:58:23,054 root         INFO     Train Epoch: 102 [4608/8000 (58%)]\tTotal Loss: 0.466700\n",
      "Reconstruction: 0.400521, Regularization: 0.000247, Discriminator: 0.042728; Generator: 0.023204,\n",
      "D(x): 0.486, D(G(z)): 0.476\n",
      "2019-04-10 00:58:23,164 root         INFO     Train Epoch: 102 [5120/8000 (64%)]\tTotal Loss: 0.464951\n",
      "Reconstruction: 0.398629, Regularization: 0.000242, Discriminator: 0.042772; Generator: 0.023307,\n",
      "D(x): 0.484, D(G(z)): 0.474\n",
      "2019-04-10 00:58:23,275 root         INFO     Train Epoch: 102 [5632/8000 (70%)]\tTotal Loss: 0.464055\n",
      "Reconstruction: 0.398126, Regularization: 0.000282, Discriminator: 0.042540; Generator: 0.023107,\n",
      "D(x): 0.491, D(G(z)): 0.477\n",
      "2019-04-10 00:58:23,385 root         INFO     Train Epoch: 102 [6144/8000 (77%)]\tTotal Loss: 0.462466\n",
      "Reconstruction: 0.396399, Regularization: 0.000215, Discriminator: 0.042865; Generator: 0.022986,\n",
      "D(x): 0.487, D(G(z)): 0.479\n",
      "2019-04-10 00:58:23,497 root         INFO     Train Epoch: 102 [6656/8000 (83%)]\tTotal Loss: 0.461833\n",
      "Reconstruction: 0.395726, Regularization: 0.000259, Discriminator: 0.042708; Generator: 0.023139,\n",
      "D(x): 0.488, D(G(z)): 0.477\n",
      "2019-04-10 00:58:23,608 root         INFO     Train Epoch: 102 [7168/8000 (90%)]\tTotal Loss: 0.460412\n",
      "Reconstruction: 0.393579, Regularization: 0.000215, Discriminator: 0.043564; Generator: 0.023054,\n",
      "D(x): 0.476, D(G(z)): 0.478\n",
      "2019-04-10 00:58:23,719 root         INFO     Train Epoch: 102 [7680/8000 (96%)]\tTotal Loss: 0.457394\n",
      "Reconstruction: 0.391830, Regularization: 0.000208, Discriminator: 0.042791; Generator: 0.022565,\n",
      "D(x): 0.495, D(G(z)): 0.486\n",
      "2019-04-10 00:58:23,800 root         INFO     ====> Epoch: 102 Average loss: 0.4677\n",
      "2019-04-10 00:58:23,827 root         INFO     Train Epoch: 103 [0/8000 (0%)]\tTotal Loss: 0.456899\n",
      "Reconstruction: 0.391195, Regularization: 0.000238, Discriminator: 0.042766; Generator: 0.022700,\n",
      "D(x): 0.493, D(G(z)): 0.484\n",
      "2019-04-10 00:58:23,940 root         INFO     Train Epoch: 103 [512/8000 (6%)]\tTotal Loss: 0.456707\n",
      "Reconstruction: 0.390756, Regularization: 0.000371, Discriminator: 0.043044; Generator: 0.022537,\n",
      "D(x): 0.491, D(G(z)): 0.486\n",
      "2019-04-10 00:58:24,051 root         INFO     Train Epoch: 103 [1024/8000 (13%)]\tTotal Loss: 0.454341\n",
      "Reconstruction: 0.388549, Regularization: 0.000227, Discriminator: 0.043019; Generator: 0.022547,\n",
      "D(x): 0.492, D(G(z)): 0.486\n",
      "2019-04-10 00:58:24,163 root         INFO     Train Epoch: 103 [1536/8000 (19%)]\tTotal Loss: 0.452263\n",
      "Reconstruction: 0.386670, Regularization: 0.000236, Discriminator: 0.042937; Generator: 0.022419,\n",
      "D(x): 0.495, D(G(z)): 0.488\n",
      "2019-04-10 00:58:24,275 root         INFO     Train Epoch: 103 [2048/8000 (26%)]\tTotal Loss: 0.451253\n",
      "Reconstruction: 0.385265, Regularization: 0.000228, Discriminator: 0.043169; Generator: 0.022591,\n",
      "D(x): 0.489, D(G(z)): 0.485\n",
      "2019-04-10 00:58:24,387 root         INFO     Train Epoch: 103 [2560/8000 (32%)]\tTotal Loss: 0.448913\n",
      "Reconstruction: 0.383311, Regularization: 0.000274, Discriminator: 0.043118; Generator: 0.022210,\n",
      "D(x): 0.495, D(G(z)): 0.491\n",
      "2019-04-10 00:58:24,499 root         INFO     Train Epoch: 103 [3072/8000 (38%)]\tTotal Loss: 0.448391\n",
      "Reconstruction: 0.382811, Regularization: 0.000295, Discriminator: 0.043271; Generator: 0.022013,\n",
      "D(x): 0.496, D(G(z)): 0.494\n",
      "2019-04-10 00:58:24,610 root         INFO     Train Epoch: 103 [3584/8000 (45%)]\tTotal Loss: 0.446936\n",
      "Reconstruction: 0.381562, Regularization: 0.000274, Discriminator: 0.042926; Generator: 0.022175,\n",
      "D(x): 0.499, D(G(z)): 0.492\n",
      "2019-04-10 00:58:24,720 root         INFO     Train Epoch: 103 [4096/8000 (51%)]\tTotal Loss: 0.445488\n",
      "Reconstruction: 0.380256, Regularization: 0.000208, Discriminator: 0.042960; Generator: 0.022063,\n",
      "D(x): 0.500, D(G(z)): 0.494\n",
      "2019-04-10 00:58:24,830 root         INFO     Train Epoch: 103 [4608/8000 (58%)]\tTotal Loss: 0.444126\n",
      "Reconstruction: 0.379018, Regularization: 0.000244, Discriminator: 0.042942; Generator: 0.021921,\n",
      "D(x): 0.503, D(G(z)): 0.496\n",
      "2019-04-10 00:58:24,939 root         INFO     Train Epoch: 103 [5120/8000 (64%)]\tTotal Loss: 0.442703\n",
      "Reconstruction: 0.376818, Regularization: 0.000232, Discriminator: 0.043666; Generator: 0.021988,\n",
      "D(x): 0.490, D(G(z)): 0.495\n",
      "2019-04-10 00:58:25,048 root         INFO     Train Epoch: 103 [5632/8000 (70%)]\tTotal Loss: 0.440383\n",
      "Reconstruction: 0.375195, Regularization: 0.000295, Discriminator: 0.043357; Generator: 0.021536,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:25,157 root         INFO     Train Epoch: 103 [6144/8000 (77%)]\tTotal Loss: 0.438265\n",
      "Reconstruction: 0.373725, Regularization: 0.000252, Discriminator: 0.043078; Generator: 0.021210,\n",
      "D(x): 0.512, D(G(z)): 0.507\n",
      "2019-04-10 00:58:25,267 root         INFO     Train Epoch: 103 [6656/8000 (83%)]\tTotal Loss: 0.437959\n",
      "Reconstruction: 0.372684, Regularization: 0.000254, Discriminator: 0.043632; Generator: 0.021389,\n",
      "D(x): 0.500, D(G(z)): 0.504\n",
      "2019-04-10 00:58:25,376 root         INFO     Train Epoch: 103 [7168/8000 (90%)]\tTotal Loss: 0.436708\n",
      "Reconstruction: 0.371647, Regularization: 0.000173, Discriminator: 0.043567; Generator: 0.021320,\n",
      "D(x): 0.502, D(G(z)): 0.505\n",
      "2019-04-10 00:58:25,485 root         INFO     Train Epoch: 103 [7680/8000 (96%)]\tTotal Loss: 0.436047\n",
      "Reconstruction: 0.371197, Regularization: 0.000293, Discriminator: 0.043244; Generator: 0.021314,\n",
      "D(x): 0.508, D(G(z)): 0.506\n",
      "2019-04-10 00:58:25,565 root         INFO     ====> Epoch: 103 Average loss: 0.4456\n",
      "2019-04-10 00:58:25,592 root         INFO     Train Epoch: 104 [0/8000 (0%)]\tTotal Loss: 0.435582\n",
      "Reconstruction: 0.370128, Regularization: 0.000197, Discriminator: 0.043901; Generator: 0.021356,\n",
      "D(x): 0.496, D(G(z)): 0.505\n",
      "2019-04-10 00:58:25,701 root         INFO     Train Epoch: 104 [512/8000 (6%)]\tTotal Loss: 0.434000\n",
      "Reconstruction: 0.368356, Regularization: 0.000188, Discriminator: 0.044093; Generator: 0.021363,\n",
      "D(x): 0.493, D(G(z)): 0.505\n",
      "2019-04-10 00:58:25,810 root         INFO     Train Epoch: 104 [1024/8000 (13%)]\tTotal Loss: 0.432267\n",
      "Reconstruction: 0.367675, Regularization: 0.000239, Discriminator: 0.043210; Generator: 0.021142,\n",
      "D(x): 0.511, D(G(z)): 0.508\n",
      "2019-04-10 00:58:25,918 root         INFO     Train Epoch: 104 [1536/8000 (19%)]\tTotal Loss: 0.431699\n",
      "Reconstruction: 0.367027, Regularization: 0.000192, Discriminator: 0.043480; Generator: 0.020999,\n",
      "D(x): 0.509, D(G(z)): 0.511\n",
      "2019-04-10 00:58:26,027 root         INFO     Train Epoch: 104 [2048/8000 (26%)]\tTotal Loss: 0.429619\n",
      "Reconstruction: 0.364825, Regularization: 0.000196, Discriminator: 0.043675; Generator: 0.020924,\n",
      "D(x): 0.507, D(G(z)): 0.512\n",
      "2019-04-10 00:58:26,135 root         INFO     Train Epoch: 104 [2560/8000 (32%)]\tTotal Loss: 0.429101\n",
      "Reconstruction: 0.364432, Regularization: 0.000235, Discriminator: 0.043596; Generator: 0.020837,\n",
      "D(x): 0.510, D(G(z)): 0.513\n",
      "2019-04-10 00:58:26,242 root         INFO     Train Epoch: 104 [3072/8000 (38%)]\tTotal Loss: 0.428658\n",
      "Reconstruction: 0.363958, Regularization: 0.000259, Discriminator: 0.043506; Generator: 0.020935,\n",
      "D(x): 0.509, D(G(z)): 0.512\n",
      "2019-04-10 00:58:26,351 root         INFO     Train Epoch: 104 [3584/8000 (45%)]\tTotal Loss: 0.427396\n",
      "Reconstruction: 0.362707, Regularization: 0.000279, Discriminator: 0.043635; Generator: 0.020775,\n",
      "D(x): 0.510, D(G(z)): 0.514\n",
      "2019-04-10 00:58:26,459 root         INFO     Train Epoch: 104 [4096/8000 (51%)]\tTotal Loss: 0.425949\n",
      "Reconstruction: 0.361037, Regularization: 0.000188, Discriminator: 0.043893; Generator: 0.020832,\n",
      "D(x): 0.505, D(G(z)): 0.513\n",
      "2019-04-10 00:58:26,567 root         INFO     Train Epoch: 104 [4608/8000 (58%)]\tTotal Loss: 0.425878\n",
      "Reconstruction: 0.361433, Regularization: 0.000214, Discriminator: 0.043572; Generator: 0.020659,\n",
      "D(x): 0.513, D(G(z)): 0.516\n",
      "2019-04-10 00:58:26,674 root         INFO     Train Epoch: 104 [5120/8000 (64%)]\tTotal Loss: 0.426670\n",
      "Reconstruction: 0.361929, Regularization: 0.000214, Discriminator: 0.043749; Generator: 0.020779,\n",
      "D(x): 0.508, D(G(z)): 0.514\n",
      "2019-04-10 00:58:26,781 root         INFO     Train Epoch: 104 [5632/8000 (70%)]\tTotal Loss: 0.425220\n",
      "Reconstruction: 0.360418, Regularization: 0.000282, Discriminator: 0.043737; Generator: 0.020784,\n",
      "D(x): 0.508, D(G(z)): 0.514\n",
      "2019-04-10 00:58:26,888 root         INFO     Train Epoch: 104 [6144/8000 (77%)]\tTotal Loss: 0.423981\n",
      "Reconstruction: 0.359295, Regularization: 0.000251, Discriminator: 0.043705; Generator: 0.020730,\n",
      "D(x): 0.510, D(G(z)): 0.515\n",
      "2019-04-10 00:58:26,995 root         INFO     Train Epoch: 104 [6656/8000 (83%)]\tTotal Loss: 0.423008\n",
      "Reconstruction: 0.358320, Regularization: 0.000251, Discriminator: 0.043641; Generator: 0.020796,\n",
      "D(x): 0.509, D(G(z)): 0.514\n",
      "2019-04-10 00:58:27,102 root         INFO     Train Epoch: 104 [7168/8000 (90%)]\tTotal Loss: 0.422386\n",
      "Reconstruction: 0.357988, Regularization: 0.000282, Discriminator: 0.043530; Generator: 0.020587,\n",
      "D(x): 0.515, D(G(z)): 0.517\n",
      "2019-04-10 00:58:27,209 root         INFO     Train Epoch: 104 [7680/8000 (96%)]\tTotal Loss: 0.423062\n",
      "Reconstruction: 0.358568, Regularization: 0.000242, Discriminator: 0.043600; Generator: 0.020651,\n",
      "D(x): 0.513, D(G(z)): 0.516\n",
      "2019-04-10 00:58:27,288 root         INFO     ====> Epoch: 104 Average loss: 0.4271\n",
      "2019-04-10 00:58:27,315 root         INFO     Train Epoch: 105 [0/8000 (0%)]\tTotal Loss: 0.421830\n",
      "Reconstruction: 0.357293, Regularization: 0.000212, Discriminator: 0.043550; Generator: 0.020774,\n",
      "D(x): 0.511, D(G(z)): 0.514\n",
      "2019-04-10 00:58:27,422 root         INFO     Train Epoch: 105 [512/8000 (6%)]\tTotal Loss: 0.421394\n",
      "Reconstruction: 0.356866, Regularization: 0.000256, Discriminator: 0.043524; Generator: 0.020748,\n",
      "D(x): 0.512, D(G(z)): 0.515\n",
      "2019-04-10 00:58:27,529 root         INFO     Train Epoch: 105 [1024/8000 (13%)]\tTotal Loss: 0.420459\n",
      "Reconstruction: 0.355995, Regularization: 0.000259, Discriminator: 0.043492; Generator: 0.020713,\n",
      "D(x): 0.513, D(G(z)): 0.515\n",
      "2019-04-10 00:58:27,636 root         INFO     Train Epoch: 105 [1536/8000 (19%)]\tTotal Loss: 0.418797\n",
      "Reconstruction: 0.354405, Regularization: 0.000298, Discriminator: 0.043308; Generator: 0.020787,\n",
      "D(x): 0.515, D(G(z)): 0.514\n",
      "2019-04-10 00:58:27,742 root         INFO     Train Epoch: 105 [2048/8000 (26%)]\tTotal Loss: 0.420185\n",
      "Reconstruction: 0.355666, Regularization: 0.000236, Discriminator: 0.043468; Generator: 0.020815,\n",
      "D(x): 0.512, D(G(z)): 0.514\n",
      "2019-04-10 00:58:27,852 root         INFO     Train Epoch: 105 [2560/8000 (32%)]\tTotal Loss: 0.420016\n",
      "Reconstruction: 0.355611, Regularization: 0.000264, Discriminator: 0.043477; Generator: 0.020664,\n",
      "D(x): 0.514, D(G(z)): 0.516\n",
      "2019-04-10 00:58:27,961 root         INFO     Train Epoch: 105 [3072/8000 (38%)]\tTotal Loss: 0.419003\n",
      "Reconstruction: 0.354815, Regularization: 0.000228, Discriminator: 0.043403; Generator: 0.020557,\n",
      "D(x): 0.517, D(G(z)): 0.518\n",
      "2019-04-10 00:58:28,070 root         INFO     Train Epoch: 105 [3584/8000 (45%)]\tTotal Loss: 0.418779\n",
      "Reconstruction: 0.354500, Regularization: 0.000262, Discriminator: 0.043386; Generator: 0.020631,\n",
      "D(x): 0.516, D(G(z)): 0.517\n",
      "2019-04-10 00:58:28,180 root         INFO     Train Epoch: 105 [4096/8000 (51%)]\tTotal Loss: 0.418920\n",
      "Reconstruction: 0.354498, Regularization: 0.000241, Discriminator: 0.043399; Generator: 0.020782,\n",
      "D(x): 0.513, D(G(z)): 0.514\n",
      "2019-04-10 00:58:28,289 root         INFO     Train Epoch: 105 [4608/8000 (58%)]\tTotal Loss: 0.418952\n",
      "Reconstruction: 0.354451, Regularization: 0.000244, Discriminator: 0.043390; Generator: 0.020867,\n",
      "D(x): 0.512, D(G(z)): 0.513\n",
      "2019-04-10 00:58:28,398 root         INFO     Train Epoch: 105 [5120/8000 (64%)]\tTotal Loss: 0.420513\n",
      "Reconstruction: 0.356108, Regularization: 0.000238, Discriminator: 0.043359; Generator: 0.020808,\n",
      "D(x): 0.514, D(G(z)): 0.514\n",
      "2019-04-10 00:58:28,507 root         INFO     Train Epoch: 105 [5632/8000 (70%)]\tTotal Loss: 0.419147\n",
      "Reconstruction: 0.354814, Regularization: 0.000178, Discriminator: 0.043338; Generator: 0.020817,\n",
      "D(x): 0.514, D(G(z)): 0.514\n",
      "2019-04-10 00:58:28,616 root         INFO     Train Epoch: 105 [6144/8000 (77%)]\tTotal Loss: 0.417866\n",
      "Reconstruction: 0.353464, Regularization: 0.000311, Discriminator: 0.043339; Generator: 0.020752,\n",
      "D(x): 0.515, D(G(z)): 0.515\n",
      "2019-04-10 00:58:28,725 root         INFO     Train Epoch: 105 [6656/8000 (83%)]\tTotal Loss: 0.418330\n",
      "Reconstruction: 0.353894, Regularization: 0.000289, Discriminator: 0.043322; Generator: 0.020824,\n",
      "D(x): 0.514, D(G(z)): 0.514\n",
      "2019-04-10 00:58:28,835 root         INFO     Train Epoch: 105 [7168/8000 (90%)]\tTotal Loss: 0.419281\n",
      "Reconstruction: 0.354865, Regularization: 0.000248, Discriminator: 0.043265; Generator: 0.020902,\n",
      "D(x): 0.514, D(G(z)): 0.512\n",
      "2019-04-10 00:58:28,944 root         INFO     Train Epoch: 105 [7680/8000 (96%)]\tTotal Loss: 0.420133\n",
      "Reconstruction: 0.355750, Regularization: 0.000222, Discriminator: 0.043277; Generator: 0.020884,\n",
      "D(x): 0.514, D(G(z)): 0.513\n",
      "2019-04-10 00:58:29,024 root         INFO     ====> Epoch: 105 Average loss: 0.4196\n",
      "2019-04-10 00:58:29,051 root         INFO     Train Epoch: 106 [0/8000 (0%)]\tTotal Loss: 0.420145\n",
      "Reconstruction: 0.355794, Regularization: 0.000219, Discriminator: 0.043219; Generator: 0.020914,\n",
      "D(x): 0.514, D(G(z)): 0.512\n",
      "2019-04-10 00:58:29,160 root         INFO     Train Epoch: 106 [512/8000 (6%)]\tTotal Loss: 0.418828\n",
      "Reconstruction: 0.354404, Regularization: 0.000206, Discriminator: 0.043243; Generator: 0.020975,\n",
      "D(x): 0.513, D(G(z)): 0.511\n",
      "2019-04-10 00:58:29,269 root         INFO     Train Epoch: 106 [1024/8000 (13%)]\tTotal Loss: 0.420318\n",
      "Reconstruction: 0.355685, Regularization: 0.000308, Discriminator: 0.043318; Generator: 0.021007,\n",
      "D(x): 0.511, D(G(z)): 0.511\n",
      "2019-04-10 00:58:29,377 root         INFO     Train Epoch: 106 [1536/8000 (19%)]\tTotal Loss: 0.420908\n",
      "Reconstruction: 0.356379, Regularization: 0.000265, Discriminator: 0.043281; Generator: 0.020984,\n",
      "D(x): 0.512, D(G(z)): 0.511\n",
      "2019-04-10 00:58:29,486 root         INFO     Train Epoch: 106 [2048/8000 (26%)]\tTotal Loss: 0.419654\n",
      "Reconstruction: 0.355250, Regularization: 0.000193, Discriminator: 0.043197; Generator: 0.021013,\n",
      "D(x): 0.513, D(G(z)): 0.510\n",
      "2019-04-10 00:58:29,596 root         INFO     Train Epoch: 106 [2560/8000 (32%)]\tTotal Loss: 0.420727\n",
      "Reconstruction: 0.356263, Regularization: 0.000262, Discriminator: 0.043220; Generator: 0.020983,\n",
      "D(x): 0.513, D(G(z)): 0.511\n",
      "2019-04-10 00:58:29,705 root         INFO     Train Epoch: 106 [3072/8000 (38%)]\tTotal Loss: 0.421301\n",
      "Reconstruction: 0.356898, Regularization: 0.000264, Discriminator: 0.043124; Generator: 0.021015,\n",
      "D(x): 0.514, D(G(z)): 0.510\n",
      "2019-04-10 00:58:29,815 root         INFO     Train Epoch: 106 [3584/8000 (45%)]\tTotal Loss: 0.421151\n",
      "Reconstruction: 0.356607, Regularization: 0.000229, Discriminator: 0.043223; Generator: 0.021091,\n",
      "D(x): 0.511, D(G(z)): 0.509\n",
      "2019-04-10 00:58:29,924 root         INFO     Train Epoch: 106 [4096/8000 (51%)]\tTotal Loss: 0.422136\n",
      "Reconstruction: 0.357598, Regularization: 0.000263, Discriminator: 0.043143; Generator: 0.021132,\n",
      "D(x): 0.512, D(G(z)): 0.509\n",
      "2019-04-10 00:58:30,033 root         INFO     Train Epoch: 106 [4608/8000 (58%)]\tTotal Loss: 0.422389\n",
      "Reconstruction: 0.357692, Regularization: 0.000271, Discriminator: 0.043282; Generator: 0.021145,\n",
      "D(x): 0.509, D(G(z)): 0.508\n",
      "2019-04-10 00:58:30,142 root         INFO     Train Epoch: 106 [5120/8000 (64%)]\tTotal Loss: 0.421762\n",
      "Reconstruction: 0.357168, Regularization: 0.000248, Discriminator: 0.043247; Generator: 0.021099,\n",
      "D(x): 0.511, D(G(z)): 0.509\n",
      "2019-04-10 00:58:30,251 root         INFO     Train Epoch: 106 [5632/8000 (70%)]\tTotal Loss: 0.423121\n",
      "Reconstruction: 0.358676, Regularization: 0.000250, Discriminator: 0.043102; Generator: 0.021093,\n",
      "D(x): 0.513, D(G(z)): 0.509\n",
      "2019-04-10 00:58:30,360 root         INFO     Train Epoch: 106 [6144/8000 (77%)]\tTotal Loss: 0.424106\n",
      "Reconstruction: 0.359622, Regularization: 0.000241, Discriminator: 0.043062; Generator: 0.021180,\n",
      "D(x): 0.512, D(G(z)): 0.508\n",
      "2019-04-10 00:58:30,469 root         INFO     Train Epoch: 106 [6656/8000 (83%)]\tTotal Loss: 0.425595\n",
      "Reconstruction: 0.360868, Regularization: 0.000291, Discriminator: 0.043223; Generator: 0.021213,\n",
      "D(x): 0.509, D(G(z)): 0.507\n",
      "2019-04-10 00:58:30,579 root         INFO     Train Epoch: 106 [7168/8000 (90%)]\tTotal Loss: 0.425897\n",
      "Reconstruction: 0.361334, Regularization: 0.000227, Discriminator: 0.043076; Generator: 0.021261,\n",
      "D(x): 0.511, D(G(z)): 0.506\n",
      "2019-04-10 00:58:30,688 root         INFO     Train Epoch: 106 [7680/8000 (96%)]\tTotal Loss: 0.427118\n",
      "Reconstruction: 0.362469, Regularization: 0.000252, Discriminator: 0.043112; Generator: 0.021285,\n",
      "D(x): 0.510, D(G(z)): 0.506\n",
      "2019-04-10 00:58:30,768 root         INFO     ====> Epoch: 106 Average loss: 0.4222\n",
      "2019-04-10 00:58:30,795 root         INFO     Train Epoch: 107 [0/8000 (0%)]\tTotal Loss: 0.425577\n",
      "Reconstruction: 0.360877, Regularization: 0.000262, Discriminator: 0.043136; Generator: 0.021302,\n",
      "D(x): 0.509, D(G(z)): 0.506\n",
      "2019-04-10 00:58:30,905 root         INFO     Train Epoch: 107 [512/8000 (6%)]\tTotal Loss: 0.427785\n",
      "Reconstruction: 0.363168, Regularization: 0.000209, Discriminator: 0.043175; Generator: 0.021232,\n",
      "D(x): 0.509, D(G(z)): 0.507\n",
      "2019-04-10 00:58:31,013 root         INFO     Train Epoch: 107 [1024/8000 (13%)]\tTotal Loss: 0.427852\n",
      "Reconstruction: 0.363230, Regularization: 0.000216, Discriminator: 0.043144; Generator: 0.021262,\n",
      "D(x): 0.509, D(G(z)): 0.506\n",
      "2019-04-10 00:58:31,122 root         INFO     Train Epoch: 107 [1536/8000 (19%)]\tTotal Loss: 0.427763\n",
      "Reconstruction: 0.363086, Regularization: 0.000250, Discriminator: 0.043104; Generator: 0.021322,\n",
      "D(x): 0.509, D(G(z)): 0.505\n",
      "2019-04-10 00:58:31,230 root         INFO     Train Epoch: 107 [2048/8000 (26%)]\tTotal Loss: 0.428571\n",
      "Reconstruction: 0.363903, Regularization: 0.000214, Discriminator: 0.043087; Generator: 0.021368,\n",
      "D(x): 0.509, D(G(z)): 0.505\n",
      "2019-04-10 00:58:31,338 root         INFO     Train Epoch: 107 [2560/8000 (32%)]\tTotal Loss: 0.430430\n",
      "Reconstruction: 0.365565, Regularization: 0.000231, Discriminator: 0.043197; Generator: 0.021437,\n",
      "D(x): 0.506, D(G(z)): 0.504\n",
      "2019-04-10 00:58:31,447 root         INFO     Train Epoch: 107 [3072/8000 (38%)]\tTotal Loss: 0.430462\n",
      "Reconstruction: 0.365541, Regularization: 0.000241, Discriminator: 0.043238; Generator: 0.021441,\n",
      "D(x): 0.505, D(G(z)): 0.504\n",
      "2019-04-10 00:58:31,555 root         INFO     Train Epoch: 107 [3584/8000 (45%)]\tTotal Loss: 0.430162\n",
      "Reconstruction: 0.365442, Regularization: 0.000240, Discriminator: 0.043059; Generator: 0.021420,\n",
      "D(x): 0.508, D(G(z)): 0.504\n",
      "2019-04-10 00:58:31,664 root         INFO     Train Epoch: 107 [4096/8000 (51%)]\tTotal Loss: 0.431269\n",
      "Reconstruction: 0.366338, Regularization: 0.000273, Discriminator: 0.043221; Generator: 0.021437,\n",
      "D(x): 0.505, D(G(z)): 0.504\n",
      "2019-04-10 00:58:31,772 root         INFO     Train Epoch: 107 [4608/8000 (58%)]\tTotal Loss: 0.432188\n",
      "Reconstruction: 0.367274, Regularization: 0.000194, Discriminator: 0.043276; Generator: 0.021443,\n",
      "D(x): 0.504, D(G(z)): 0.503\n",
      "2019-04-10 00:58:31,881 root         INFO     Train Epoch: 107 [5120/8000 (64%)]\tTotal Loss: 0.431930\n",
      "Reconstruction: 0.366936, Regularization: 0.000223, Discriminator: 0.043249; Generator: 0.021522,\n",
      "D(x): 0.504, D(G(z)): 0.502\n",
      "2019-04-10 00:58:31,989 root         INFO     Train Epoch: 107 [5632/8000 (70%)]\tTotal Loss: 0.433634\n",
      "Reconstruction: 0.368598, Regularization: 0.000248, Discriminator: 0.043307; Generator: 0.021480,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:32,098 root         INFO     Train Epoch: 107 [6144/8000 (77%)]\tTotal Loss: 0.435772\n",
      "Reconstruction: 0.370704, Regularization: 0.000211, Discriminator: 0.043324; Generator: 0.021532,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:32,206 root         INFO     Train Epoch: 107 [6656/8000 (83%)]\tTotal Loss: 0.435962\n",
      "Reconstruction: 0.370931, Regularization: 0.000255, Discriminator: 0.043272; Generator: 0.021504,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:32,314 root         INFO     Train Epoch: 107 [7168/8000 (90%)]\tTotal Loss: 0.436027\n",
      "Reconstruction: 0.370755, Regularization: 0.000236, Discriminator: 0.043441; Generator: 0.021595,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 00:58:32,423 root         INFO     Train Epoch: 107 [7680/8000 (96%)]\tTotal Loss: 0.437597\n",
      "Reconstruction: 0.372573, Regularization: 0.000245, Discriminator: 0.043231; Generator: 0.021549,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:58:32,502 root         INFO     ====> Epoch: 107 Average loss: 0.4316\n",
      "2019-04-10 00:58:32,529 root         INFO     Train Epoch: 108 [0/8000 (0%)]\tTotal Loss: 0.438252\n",
      "Reconstruction: 0.373211, Regularization: 0.000223, Discriminator: 0.043275; Generator: 0.021543,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:58:32,638 root         INFO     Train Epoch: 108 [512/8000 (6%)]\tTotal Loss: 0.439145\n",
      "Reconstruction: 0.373938, Regularization: 0.000254, Discriminator: 0.043267; Generator: 0.021686,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:58:32,747 root         INFO     Train Epoch: 108 [1024/8000 (13%)]\tTotal Loss: 0.440615\n",
      "Reconstruction: 0.375173, Regularization: 0.000281, Discriminator: 0.043423; Generator: 0.021738,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 00:58:32,855 root         INFO     Train Epoch: 108 [1536/8000 (19%)]\tTotal Loss: 0.441045\n",
      "Reconstruction: 0.375817, Regularization: 0.000228, Discriminator: 0.043335; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:58:32,964 root         INFO     Train Epoch: 108 [2048/8000 (26%)]\tTotal Loss: 0.442580\n",
      "Reconstruction: 0.377004, Regularization: 0.000250, Discriminator: 0.043582; Generator: 0.021744,\n",
      "D(x): 0.495, D(G(z)): 0.499\n",
      "2019-04-10 00:58:33,072 root         INFO     Train Epoch: 108 [2560/8000 (32%)]\tTotal Loss: 0.442165\n",
      "Reconstruction: 0.376967, Regularization: 0.000221, Discriminator: 0.043258; Generator: 0.021719,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:58:33,179 root         INFO     Train Epoch: 108 [3072/8000 (38%)]\tTotal Loss: 0.442737\n",
      "Reconstruction: 0.377439, Regularization: 0.000231, Discriminator: 0.043351; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:33,288 root         INFO     Train Epoch: 108 [3584/8000 (45%)]\tTotal Loss: 0.444612\n",
      "Reconstruction: 0.379243, Regularization: 0.000265, Discriminator: 0.043428; Generator: 0.021676,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 00:58:33,396 root         INFO     Train Epoch: 108 [4096/8000 (51%)]\tTotal Loss: 0.443826\n",
      "Reconstruction: 0.378354, Regularization: 0.000294, Discriminator: 0.043440; Generator: 0.021739,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 00:58:33,504 root         INFO     Train Epoch: 108 [4608/8000 (58%)]\tTotal Loss: 0.443384\n",
      "Reconstruction: 0.378161, Regularization: 0.000279, Discriminator: 0.043179; Generator: 0.021765,\n",
      "D(x): 0.501, D(G(z)): 0.498\n",
      "2019-04-10 00:58:33,611 root         INFO     Train Epoch: 108 [5120/8000 (64%)]\tTotal Loss: 0.444564\n",
      "Reconstruction: 0.378985, Regularization: 0.000259, Discriminator: 0.043490; Generator: 0.021831,\n",
      "D(x): 0.495, D(G(z)): 0.497\n",
      "2019-04-10 00:58:33,719 root         INFO     Train Epoch: 108 [5632/8000 (70%)]\tTotal Loss: 0.445917\n",
      "Reconstruction: 0.380465, Regularization: 0.000204, Discriminator: 0.043437; Generator: 0.021812,\n",
      "D(x): 0.496, D(G(z)): 0.498\n",
      "2019-04-10 00:58:33,827 root         INFO     Train Epoch: 108 [6144/8000 (77%)]\tTotal Loss: 0.447080\n",
      "Reconstruction: 0.381718, Regularization: 0.000263, Discriminator: 0.043280; Generator: 0.021819,\n",
      "D(x): 0.498, D(G(z)): 0.497\n",
      "2019-04-10 00:58:33,934 root         INFO     Train Epoch: 108 [6656/8000 (83%)]\tTotal Loss: 0.448599\n",
      "Reconstruction: 0.383108, Regularization: 0.000196, Discriminator: 0.043474; Generator: 0.021822,\n",
      "D(x): 0.495, D(G(z)): 0.497\n",
      "2019-04-10 00:58:34,041 root         INFO     Train Epoch: 108 [7168/8000 (90%)]\tTotal Loss: 0.448816\n",
      "Reconstruction: 0.383144, Regularization: 0.000241, Discriminator: 0.043539; Generator: 0.021892,\n",
      "D(x): 0.493, D(G(z)): 0.496\n",
      "2019-04-10 00:58:34,148 root         INFO     Train Epoch: 108 [7680/8000 (96%)]\tTotal Loss: 0.450484\n",
      "Reconstruction: 0.384894, Regularization: 0.000290, Discriminator: 0.043348; Generator: 0.021952,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:34,228 root         INFO     ====> Epoch: 108 Average loss: 0.4440\n",
      "2019-04-10 00:58:34,256 root         INFO     Train Epoch: 109 [0/8000 (0%)]\tTotal Loss: 0.450014\n",
      "Reconstruction: 0.384435, Regularization: 0.000244, Discriminator: 0.043363; Generator: 0.021972,\n",
      "D(x): 0.494, D(G(z)): 0.495\n",
      "2019-04-10 00:58:34,368 root         INFO     Train Epoch: 109 [512/8000 (6%)]\tTotal Loss: 0.450753\n",
      "Reconstruction: 0.385258, Regularization: 0.000213, Discriminator: 0.043361; Generator: 0.021921,\n",
      "D(x): 0.495, D(G(z)): 0.496\n",
      "2019-04-10 00:58:34,480 root         INFO     Train Epoch: 109 [1024/8000 (13%)]\tTotal Loss: 0.452500\n",
      "Reconstruction: 0.386880, Regularization: 0.000210, Discriminator: 0.043438; Generator: 0.021972,\n",
      "D(x): 0.493, D(G(z)): 0.495\n",
      "2019-04-10 00:58:34,591 root         INFO     Train Epoch: 109 [1536/8000 (19%)]\tTotal Loss: 0.452422\n",
      "Reconstruction: 0.386750, Regularization: 0.000238, Discriminator: 0.043420; Generator: 0.022015,\n",
      "D(x): 0.493, D(G(z)): 0.494\n",
      "2019-04-10 00:58:34,703 root         INFO     Train Epoch: 109 [2048/8000 (26%)]\tTotal Loss: 0.453664\n",
      "Reconstruction: 0.387921, Regularization: 0.000208, Discriminator: 0.043478; Generator: 0.022056,\n",
      "D(x): 0.491, D(G(z)): 0.494\n",
      "2019-04-10 00:58:34,814 root         INFO     Train Epoch: 109 [2560/8000 (32%)]\tTotal Loss: 0.452697\n",
      "Reconstruction: 0.387028, Regularization: 0.000230, Discriminator: 0.043410; Generator: 0.022029,\n",
      "D(x): 0.493, D(G(z)): 0.494\n",
      "2019-04-10 00:58:34,924 root         INFO     Train Epoch: 109 [3072/8000 (38%)]\tTotal Loss: 0.454388\n",
      "Reconstruction: 0.388760, Regularization: 0.000203, Discriminator: 0.043346; Generator: 0.022080,\n",
      "D(x): 0.493, D(G(z)): 0.493\n",
      "2019-04-10 00:58:35,030 root         INFO     Train Epoch: 109 [3584/8000 (45%)]\tTotal Loss: 0.453587\n",
      "Reconstruction: 0.387845, Regularization: 0.000282, Discriminator: 0.043394; Generator: 0.022066,\n",
      "D(x): 0.493, D(G(z)): 0.494\n",
      "2019-04-10 00:58:35,136 root         INFO     Train Epoch: 109 [4096/8000 (51%)]\tTotal Loss: 0.453884\n",
      "Reconstruction: 0.388040, Regularization: 0.000246, Discriminator: 0.043483; Generator: 0.022115,\n",
      "D(x): 0.490, D(G(z)): 0.493\n",
      "2019-04-10 00:58:35,242 root         INFO     Train Epoch: 109 [4608/8000 (58%)]\tTotal Loss: 0.455053\n",
      "Reconstruction: 0.389329, Regularization: 0.000218, Discriminator: 0.043352; Generator: 0.022154,\n",
      "D(x): 0.492, D(G(z)): 0.492\n",
      "2019-04-10 00:58:35,350 root         INFO     Train Epoch: 109 [5120/8000 (64%)]\tTotal Loss: 0.455300\n",
      "Reconstruction: 0.389535, Regularization: 0.000222, Discriminator: 0.043399; Generator: 0.022143,\n",
      "D(x): 0.491, D(G(z)): 0.492\n",
      "2019-04-10 00:58:35,460 root         INFO     Train Epoch: 109 [5632/8000 (70%)]\tTotal Loss: 0.455257\n",
      "Reconstruction: 0.389375, Regularization: 0.000278, Discriminator: 0.043453; Generator: 0.022150,\n",
      "D(x): 0.490, D(G(z)): 0.492\n",
      "2019-04-10 00:58:35,570 root         INFO     Train Epoch: 109 [6144/8000 (77%)]\tTotal Loss: 0.455436\n",
      "Reconstruction: 0.389626, Regularization: 0.000270, Discriminator: 0.043359; Generator: 0.022180,\n",
      "D(x): 0.491, D(G(z)): 0.492\n",
      "2019-04-10 00:58:35,679 root         INFO     Train Epoch: 109 [6656/8000 (83%)]\tTotal Loss: 0.455920\n",
      "Reconstruction: 0.390145, Regularization: 0.000277, Discriminator: 0.043358; Generator: 0.022139,\n",
      "D(x): 0.492, D(G(z)): 0.492\n",
      "2019-04-10 00:58:35,788 root         INFO     Train Epoch: 109 [7168/8000 (90%)]\tTotal Loss: 0.457297\n",
      "Reconstruction: 0.391343, Regularization: 0.000233, Discriminator: 0.043407; Generator: 0.022314,\n",
      "D(x): 0.489, D(G(z)): 0.490\n",
      "2019-04-10 00:58:35,898 root         INFO     Train Epoch: 109 [7680/8000 (96%)]\tTotal Loss: 0.457386\n",
      "Reconstruction: 0.391487, Regularization: 0.000249, Discriminator: 0.043348; Generator: 0.022303,\n",
      "D(x): 0.490, D(G(z)): 0.490\n",
      "2019-04-10 00:58:35,977 root         INFO     ====> Epoch: 109 Average loss: 0.4544\n",
      "2019-04-10 00:58:36,004 root         INFO     Train Epoch: 110 [0/8000 (0%)]\tTotal Loss: 0.458102\n",
      "Reconstruction: 0.392215, Regularization: 0.000232, Discriminator: 0.043360; Generator: 0.022294,\n",
      "D(x): 0.490, D(G(z)): 0.490\n",
      "2019-04-10 00:58:36,112 root         INFO     Train Epoch: 110 [512/8000 (6%)]\tTotal Loss: 0.459118\n",
      "Reconstruction: 0.393180, Regularization: 0.000253, Discriminator: 0.043321; Generator: 0.022363,\n",
      "D(x): 0.489, D(G(z)): 0.489\n",
      "2019-04-10 00:58:36,219 root         INFO     Train Epoch: 110 [1024/8000 (13%)]\tTotal Loss: 0.458486\n",
      "Reconstruction: 0.392570, Regularization: 0.000238, Discriminator: 0.043282; Generator: 0.022397,\n",
      "D(x): 0.489, D(G(z)): 0.488\n",
      "2019-04-10 00:58:36,325 root         INFO     Train Epoch: 110 [1536/8000 (19%)]\tTotal Loss: 0.457479\n",
      "Reconstruction: 0.391627, Regularization: 0.000175, Discriminator: 0.043310; Generator: 0.022367,\n",
      "D(x): 0.489, D(G(z)): 0.489\n",
      "2019-04-10 00:58:36,437 root         INFO     Train Epoch: 110 [2048/8000 (26%)]\tTotal Loss: 0.456355\n",
      "Reconstruction: 0.390447, Regularization: 0.000246, Discriminator: 0.043319; Generator: 0.022343,\n",
      "D(x): 0.489, D(G(z)): 0.489\n",
      "2019-04-10 00:58:36,547 root         INFO     Train Epoch: 110 [2560/8000 (32%)]\tTotal Loss: 0.455934\n",
      "Reconstruction: 0.390008, Regularization: 0.000263, Discriminator: 0.043285; Generator: 0.022377,\n",
      "D(x): 0.490, D(G(z)): 0.489\n",
      "2019-04-10 00:58:36,658 root         INFO     Train Epoch: 110 [3072/8000 (38%)]\tTotal Loss: 0.454489\n",
      "Reconstruction: 0.388576, Regularization: 0.000246, Discriminator: 0.043208; Generator: 0.022459,\n",
      "D(x): 0.490, D(G(z)): 0.487\n",
      "2019-04-10 00:58:36,767 root         INFO     Train Epoch: 110 [3584/8000 (45%)]\tTotal Loss: 0.453886\n",
      "Reconstruction: 0.387916, Regularization: 0.000279, Discriminator: 0.043301; Generator: 0.022391,\n",
      "D(x): 0.489, D(G(z)): 0.488\n",
      "2019-04-10 00:58:36,878 root         INFO     Train Epoch: 110 [4096/8000 (51%)]\tTotal Loss: 0.453079\n",
      "Reconstruction: 0.387372, Regularization: 0.000279, Discriminator: 0.043137; Generator: 0.022291,\n",
      "D(x): 0.493, D(G(z)): 0.490\n",
      "2019-04-10 00:58:36,988 root         INFO     Train Epoch: 110 [4608/8000 (58%)]\tTotal Loss: 0.452536\n",
      "Reconstruction: 0.386726, Regularization: 0.000214, Discriminator: 0.043289; Generator: 0.022307,\n",
      "D(x): 0.491, D(G(z)): 0.490\n",
      "2019-04-10 00:58:37,098 root         INFO     Train Epoch: 110 [5120/8000 (64%)]\tTotal Loss: 0.453053\n",
      "Reconstruction: 0.387367, Regularization: 0.000188, Discriminator: 0.043321; Generator: 0.022176,\n",
      "D(x): 0.492, D(G(z)): 0.492\n",
      "2019-04-10 00:58:37,208 root         INFO     Train Epoch: 110 [5632/8000 (70%)]\tTotal Loss: 0.451924\n",
      "Reconstruction: 0.386156, Regularization: 0.000218, Discriminator: 0.043207; Generator: 0.022342,\n",
      "D(x): 0.491, D(G(z)): 0.489\n",
      "2019-04-10 00:58:37,316 root         INFO     Train Epoch: 110 [6144/8000 (77%)]\tTotal Loss: 0.451656\n",
      "Reconstruction: 0.386010, Regularization: 0.000281, Discriminator: 0.043181; Generator: 0.022183,\n",
      "D(x): 0.494, D(G(z)): 0.492\n",
      "2019-04-10 00:58:37,423 root         INFO     Train Epoch: 110 [6656/8000 (83%)]\tTotal Loss: 0.451984\n",
      "Reconstruction: 0.386355, Regularization: 0.000280, Discriminator: 0.043154; Generator: 0.022195,\n",
      "D(x): 0.494, D(G(z)): 0.492\n",
      "2019-04-10 00:58:37,531 root         INFO     Train Epoch: 110 [7168/8000 (90%)]\tTotal Loss: 0.450457\n",
      "Reconstruction: 0.384831, Regularization: 0.000253, Discriminator: 0.043159; Generator: 0.022214,\n",
      "D(x): 0.494, D(G(z)): 0.491\n",
      "2019-04-10 00:58:37,638 root         INFO     Train Epoch: 110 [7680/8000 (96%)]\tTotal Loss: 0.448806\n",
      "Reconstruction: 0.383403, Regularization: 0.000228, Discriminator: 0.043200; Generator: 0.021975,\n",
      "D(x): 0.497, D(G(z)): 0.495\n",
      "2019-04-10 00:58:37,716 root         INFO     ====> Epoch: 110 Average loss: 0.4542\n",
      "2019-04-10 00:58:37,744 root         INFO     Train Epoch: 111 [0/8000 (0%)]\tTotal Loss: 0.448390\n",
      "Reconstruction: 0.382884, Regularization: 0.000238, Discriminator: 0.043292; Generator: 0.021976,\n",
      "D(x): 0.496, D(G(z)): 0.495\n",
      "2019-04-10 00:58:37,854 root         INFO     Train Epoch: 111 [512/8000 (6%)]\tTotal Loss: 0.446745\n",
      "Reconstruction: 0.381016, Regularization: 0.000216, Discriminator: 0.043452; Generator: 0.022061,\n",
      "D(x): 0.492, D(G(z)): 0.494\n",
      "2019-04-10 00:58:37,964 root         INFO     Train Epoch: 111 [1024/8000 (13%)]\tTotal Loss: 0.444483\n",
      "Reconstruction: 0.378981, Regularization: 0.000222, Discriminator: 0.043423; Generator: 0.021857,\n",
      "D(x): 0.495, D(G(z)): 0.497\n",
      "2019-04-10 00:58:38,073 root         INFO     Train Epoch: 111 [1536/8000 (19%)]\tTotal Loss: 0.442237\n",
      "Reconstruction: 0.376896, Regularization: 0.000274, Discriminator: 0.043310; Generator: 0.021757,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:58:38,183 root         INFO     Train Epoch: 111 [2048/8000 (26%)]\tTotal Loss: 0.442163\n",
      "Reconstruction: 0.376909, Regularization: 0.000204, Discriminator: 0.043217; Generator: 0.021833,\n",
      "D(x): 0.499, D(G(z)): 0.497\n",
      "2019-04-10 00:58:38,293 root         INFO     Train Epoch: 111 [2560/8000 (32%)]\tTotal Loss: 0.441180\n",
      "Reconstruction: 0.375847, Regularization: 0.000231, Discriminator: 0.043424; Generator: 0.021677,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 00:58:38,403 root         INFO     Train Epoch: 111 [3072/8000 (38%)]\tTotal Loss: 0.438104\n",
      "Reconstruction: 0.373052, Regularization: 0.000193, Discriminator: 0.043287; Generator: 0.021572,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:58:38,513 root         INFO     Train Epoch: 111 [3584/8000 (45%)]\tTotal Loss: 0.438972\n",
      "Reconstruction: 0.373599, Regularization: 0.000231, Discriminator: 0.043344; Generator: 0.021798,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:38,623 root         INFO     Train Epoch: 111 [4096/8000 (51%)]\tTotal Loss: 0.437960\n",
      "Reconstruction: 0.372574, Regularization: 0.000229, Discriminator: 0.043443; Generator: 0.021714,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 00:58:38,733 root         INFO     Train Epoch: 111 [4608/8000 (58%)]\tTotal Loss: 0.436531\n",
      "Reconstruction: 0.371439, Regularization: 0.000225, Discriminator: 0.043323; Generator: 0.021545,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:38,844 root         INFO     Train Epoch: 111 [5120/8000 (64%)]\tTotal Loss: 0.436484\n",
      "Reconstruction: 0.371561, Regularization: 0.000224, Discriminator: 0.043330; Generator: 0.021369,\n",
      "D(x): 0.505, D(G(z)): 0.505\n",
      "2019-04-10 00:58:38,954 root         INFO     Train Epoch: 111 [5632/8000 (70%)]\tTotal Loss: 0.435837\n",
      "Reconstruction: 0.370475, Regularization: 0.000286, Discriminator: 0.043410; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:58:39,066 root         INFO     Train Epoch: 111 [6144/8000 (77%)]\tTotal Loss: 0.435534\n",
      "Reconstruction: 0.370512, Regularization: 0.000212, Discriminator: 0.043412; Generator: 0.021398,\n",
      "D(x): 0.503, D(G(z)): 0.504\n",
      "2019-04-10 00:58:39,177 root         INFO     Train Epoch: 111 [6656/8000 (83%)]\tTotal Loss: 0.433712\n",
      "Reconstruction: 0.368716, Regularization: 0.000253, Discriminator: 0.043402; Generator: 0.021341,\n",
      "D(x): 0.504, D(G(z)): 0.505\n",
      "2019-04-10 00:58:39,290 root         INFO     Train Epoch: 111 [7168/8000 (90%)]\tTotal Loss: 0.433729\n",
      "Reconstruction: 0.368695, Regularization: 0.000178, Discriminator: 0.043476; Generator: 0.021380,\n",
      "D(x): 0.502, D(G(z)): 0.505\n",
      "2019-04-10 00:58:39,403 root         INFO     Train Epoch: 111 [7680/8000 (96%)]\tTotal Loss: 0.430953\n",
      "Reconstruction: 0.366115, Regularization: 0.000239, Discriminator: 0.043334; Generator: 0.021265,\n",
      "D(x): 0.506, D(G(z)): 0.506\n",
      "2019-04-10 00:58:39,484 root         INFO     ====> Epoch: 111 Average loss: 0.4388\n",
      "2019-04-10 00:58:39,512 root         INFO     Train Epoch: 112 [0/8000 (0%)]\tTotal Loss: 0.432025\n",
      "Reconstruction: 0.367277, Regularization: 0.000207, Discriminator: 0.043281; Generator: 0.021260,\n",
      "D(x): 0.507, D(G(z)): 0.506\n",
      "2019-04-10 00:58:39,624 root         INFO     Train Epoch: 112 [512/8000 (6%)]\tTotal Loss: 0.430730\n",
      "Reconstruction: 0.365903, Regularization: 0.000216, Discriminator: 0.043288; Generator: 0.021323,\n",
      "D(x): 0.506, D(G(z)): 0.505\n",
      "2019-04-10 00:58:39,734 root         INFO     Train Epoch: 112 [1024/8000 (13%)]\tTotal Loss: 0.428918\n",
      "Reconstruction: 0.364249, Regularization: 0.000232, Discriminator: 0.043280; Generator: 0.021157,\n",
      "D(x): 0.509, D(G(z)): 0.508\n",
      "2019-04-10 00:58:39,844 root         INFO     Train Epoch: 112 [1536/8000 (19%)]\tTotal Loss: 0.427487\n",
      "Reconstruction: 0.362592, Regularization: 0.000226, Discriminator: 0.043482; Generator: 0.021187,\n",
      "D(x): 0.505, D(G(z)): 0.508\n",
      "2019-04-10 00:58:39,955 root         INFO     Train Epoch: 112 [2048/8000 (26%)]\tTotal Loss: 0.429246\n",
      "Reconstruction: 0.364364, Regularization: 0.000288, Discriminator: 0.043433; Generator: 0.021161,\n",
      "D(x): 0.506, D(G(z)): 0.508\n",
      "2019-04-10 00:58:40,066 root         INFO     Train Epoch: 112 [2560/8000 (32%)]\tTotal Loss: 0.426975\n",
      "Reconstruction: 0.362182, Regularization: 0.000222, Discriminator: 0.043367; Generator: 0.021204,\n",
      "D(x): 0.507, D(G(z)): 0.507\n",
      "2019-04-10 00:58:40,177 root         INFO     Train Epoch: 112 [3072/8000 (38%)]\tTotal Loss: 0.427133\n",
      "Reconstruction: 0.362418, Regularization: 0.000236, Discriminator: 0.043366; Generator: 0.021113,\n",
      "D(x): 0.508, D(G(z)): 0.509\n",
      "2019-04-10 00:58:40,288 root         INFO     Train Epoch: 112 [3584/8000 (45%)]\tTotal Loss: 0.426091\n",
      "Reconstruction: 0.361333, Regularization: 0.000230, Discriminator: 0.043342; Generator: 0.021185,\n",
      "D(x): 0.507, D(G(z)): 0.508\n",
      "2019-04-10 00:58:40,399 root         INFO     Train Epoch: 112 [4096/8000 (51%)]\tTotal Loss: 0.427450\n",
      "Reconstruction: 0.362644, Regularization: 0.000254, Discriminator: 0.043324; Generator: 0.021228,\n",
      "D(x): 0.507, D(G(z)): 0.507\n",
      "2019-04-10 00:58:40,506 root         INFO     Train Epoch: 112 [4608/8000 (58%)]\tTotal Loss: 0.426698\n",
      "Reconstruction: 0.361910, Regularization: 0.000206, Discriminator: 0.043315; Generator: 0.021266,\n",
      "D(x): 0.507, D(G(z)): 0.506\n",
      "2019-04-10 00:58:40,617 root         INFO     Train Epoch: 112 [5120/8000 (64%)]\tTotal Loss: 0.427974\n",
      "Reconstruction: 0.363160, Regularization: 0.000222, Discriminator: 0.043358; Generator: 0.021233,\n",
      "D(x): 0.506, D(G(z)): 0.507\n",
      "2019-04-10 00:58:40,727 root         INFO     Train Epoch: 112 [5632/8000 (70%)]\tTotal Loss: 0.427751\n",
      "Reconstruction: 0.362913, Regularization: 0.000218, Discriminator: 0.043300; Generator: 0.021320,\n",
      "D(x): 0.506, D(G(z)): 0.505\n",
      "2019-04-10 00:58:40,837 root         INFO     Train Epoch: 112 [6144/8000 (77%)]\tTotal Loss: 0.427762\n",
      "Reconstruction: 0.362925, Regularization: 0.000206, Discriminator: 0.043304; Generator: 0.021327,\n",
      "D(x): 0.506, D(G(z)): 0.505\n",
      "2019-04-10 00:58:40,948 root         INFO     Train Epoch: 112 [6656/8000 (83%)]\tTotal Loss: 0.428161\n",
      "Reconstruction: 0.363276, Regularization: 0.000231, Discriminator: 0.043332; Generator: 0.021322,\n",
      "D(x): 0.505, D(G(z)): 0.505\n",
      "2019-04-10 00:58:41,059 root         INFO     Train Epoch: 112 [7168/8000 (90%)]\tTotal Loss: 0.428293\n",
      "Reconstruction: 0.363385, Regularization: 0.000320, Discriminator: 0.043312; Generator: 0.021276,\n",
      "D(x): 0.506, D(G(z)): 0.506\n",
      "2019-04-10 00:58:41,169 root         INFO     Train Epoch: 112 [7680/8000 (96%)]\tTotal Loss: 0.427605\n",
      "Reconstruction: 0.362727, Regularization: 0.000248, Discriminator: 0.043265; Generator: 0.021365,\n",
      "D(x): 0.506, D(G(z)): 0.505\n",
      "2019-04-10 00:58:41,249 root         INFO     ====> Epoch: 112 Average loss: 0.4282\n",
      "2019-04-10 00:58:41,276 root         INFO     Train Epoch: 113 [0/8000 (0%)]\tTotal Loss: 0.428680\n",
      "Reconstruction: 0.363808, Regularization: 0.000211, Discriminator: 0.043262; Generator: 0.021399,\n",
      "D(x): 0.505, D(G(z)): 0.504\n",
      "2019-04-10 00:58:41,387 root         INFO     Train Epoch: 113 [512/8000 (6%)]\tTotal Loss: 0.428708\n",
      "Reconstruction: 0.363715, Regularization: 0.000238, Discriminator: 0.043290; Generator: 0.021465,\n",
      "D(x): 0.504, D(G(z)): 0.503\n",
      "2019-04-10 00:58:41,497 root         INFO     Train Epoch: 113 [1024/8000 (13%)]\tTotal Loss: 0.429520\n",
      "Reconstruction: 0.364544, Regularization: 0.000261, Discriminator: 0.043287; Generator: 0.021428,\n",
      "D(x): 0.504, D(G(z)): 0.504\n",
      "2019-04-10 00:58:41,607 root         INFO     Train Epoch: 113 [1536/8000 (19%)]\tTotal Loss: 0.429092\n",
      "Reconstruction: 0.364242, Regularization: 0.000234, Discriminator: 0.043258; Generator: 0.021358,\n",
      "D(x): 0.506, D(G(z)): 0.505\n",
      "2019-04-10 00:58:41,718 root         INFO     Train Epoch: 113 [2048/8000 (26%)]\tTotal Loss: 0.428295\n",
      "Reconstruction: 0.363411, Regularization: 0.000244, Discriminator: 0.043285; Generator: 0.021354,\n",
      "D(x): 0.506, D(G(z)): 0.505\n",
      "2019-04-10 00:58:41,828 root         INFO     Train Epoch: 113 [2560/8000 (32%)]\tTotal Loss: 0.429059\n",
      "Reconstruction: 0.364157, Regularization: 0.000218, Discriminator: 0.043292; Generator: 0.021391,\n",
      "D(x): 0.505, D(G(z)): 0.504\n",
      "2019-04-10 00:58:41,938 root         INFO     Train Epoch: 113 [3072/8000 (38%)]\tTotal Loss: 0.429169\n",
      "Reconstruction: 0.364274, Regularization: 0.000242, Discriminator: 0.043240; Generator: 0.021412,\n",
      "D(x): 0.505, D(G(z)): 0.504\n",
      "2019-04-10 00:58:42,048 root         INFO     Train Epoch: 113 [3584/8000 (45%)]\tTotal Loss: 0.430515\n",
      "Reconstruction: 0.365500, Regularization: 0.000230, Discriminator: 0.043434; Generator: 0.021352,\n",
      "D(x): 0.503, D(G(z)): 0.505\n",
      "2019-04-10 00:58:42,158 root         INFO     Train Epoch: 113 [4096/8000 (51%)]\tTotal Loss: 0.431251\n",
      "Reconstruction: 0.366257, Regularization: 0.000264, Discriminator: 0.043256; Generator: 0.021475,\n",
      "D(x): 0.504, D(G(z)): 0.503\n",
      "2019-04-10 00:58:42,269 root         INFO     Train Epoch: 113 [4608/8000 (58%)]\tTotal Loss: 0.430679\n",
      "Reconstruction: 0.365881, Regularization: 0.000215, Discriminator: 0.043119; Generator: 0.021464,\n",
      "D(x): 0.506, D(G(z)): 0.503\n",
      "2019-04-10 00:58:42,379 root         INFO     Train Epoch: 113 [5120/8000 (64%)]\tTotal Loss: 0.432941\n",
      "Reconstruction: 0.367902, Regularization: 0.000255, Discriminator: 0.043314; Generator: 0.021469,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:42,490 root         INFO     Train Epoch: 113 [5632/8000 (70%)]\tTotal Loss: 0.433941\n",
      "Reconstruction: 0.368854, Regularization: 0.000207, Discriminator: 0.043386; Generator: 0.021494,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-10 00:58:42,600 root         INFO     Train Epoch: 113 [6144/8000 (77%)]\tTotal Loss: 0.434832\n",
      "Reconstruction: 0.369840, Regularization: 0.000188, Discriminator: 0.043299; Generator: 0.021505,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:58:42,711 root         INFO     Train Epoch: 113 [6656/8000 (83%)]\tTotal Loss: 0.436022\n",
      "Reconstruction: 0.371028, Regularization: 0.000209, Discriminator: 0.043294; Generator: 0.021491,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:42,821 root         INFO     Train Epoch: 113 [7168/8000 (90%)]\tTotal Loss: 0.437346\n",
      "Reconstruction: 0.372347, Regularization: 0.000241, Discriminator: 0.043284; Generator: 0.021473,\n",
      "D(x): 0.504, D(G(z)): 0.503\n",
      "2019-04-10 00:58:42,934 root         INFO     Train Epoch: 113 [7680/8000 (96%)]\tTotal Loss: 0.438181\n",
      "Reconstruction: 0.372966, Regularization: 0.000191, Discriminator: 0.043412; Generator: 0.021612,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 00:58:43,015 root         INFO     ====> Epoch: 113 Average loss: 0.4318\n",
      "2019-04-10 00:58:43,042 root         INFO     Train Epoch: 114 [0/8000 (0%)]\tTotal Loss: 0.439279\n",
      "Reconstruction: 0.374016, Regularization: 0.000266, Discriminator: 0.043300; Generator: 0.021698,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:58:43,154 root         INFO     Train Epoch: 114 [512/8000 (6%)]\tTotal Loss: 0.439273\n",
      "Reconstruction: 0.374034, Regularization: 0.000245, Discriminator: 0.043327; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:58:43,264 root         INFO     Train Epoch: 114 [1024/8000 (13%)]\tTotal Loss: 0.441016\n",
      "Reconstruction: 0.375837, Regularization: 0.000219, Discriminator: 0.043278; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:58:43,374 root         INFO     Train Epoch: 114 [1536/8000 (19%)]\tTotal Loss: 0.441068\n",
      "Reconstruction: 0.375876, Regularization: 0.000208, Discriminator: 0.043293; Generator: 0.021692,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:58:43,484 root         INFO     Train Epoch: 114 [2048/8000 (26%)]\tTotal Loss: 0.440050\n",
      "Reconstruction: 0.374835, Regularization: 0.000230, Discriminator: 0.043259; Generator: 0.021725,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:58:43,595 root         INFO     Train Epoch: 114 [2560/8000 (32%)]\tTotal Loss: 0.441572\n",
      "Reconstruction: 0.376296, Regularization: 0.000232, Discriminator: 0.043347; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:43,705 root         INFO     Train Epoch: 114 [3072/8000 (38%)]\tTotal Loss: 0.441099\n",
      "Reconstruction: 0.375836, Regularization: 0.000243, Discriminator: 0.043349; Generator: 0.021670,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:58:43,815 root         INFO     Train Epoch: 114 [3584/8000 (45%)]\tTotal Loss: 0.442599\n",
      "Reconstruction: 0.377380, Regularization: 0.000191, Discriminator: 0.043385; Generator: 0.021643,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:58:43,925 root         INFO     Train Epoch: 114 [4096/8000 (51%)]\tTotal Loss: 0.442593\n",
      "Reconstruction: 0.377242, Regularization: 0.000212, Discriminator: 0.043340; Generator: 0.021799,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:44,035 root         INFO     Train Epoch: 114 [4608/8000 (58%)]\tTotal Loss: 0.442790\n",
      "Reconstruction: 0.377424, Regularization: 0.000206, Discriminator: 0.043360; Generator: 0.021801,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 00:58:44,146 root         INFO     Train Epoch: 114 [5120/8000 (64%)]\tTotal Loss: 0.443784\n",
      "Reconstruction: 0.378334, Regularization: 0.000213, Discriminator: 0.043458; Generator: 0.021779,\n",
      "D(x): 0.496, D(G(z)): 0.498\n",
      "2019-04-10 00:58:44,257 root         INFO     Train Epoch: 114 [5632/8000 (70%)]\tTotal Loss: 0.444126\n",
      "Reconstruction: 0.378732, Regularization: 0.000232, Discriminator: 0.043408; Generator: 0.021754,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 00:58:44,368 root         INFO     Train Epoch: 114 [6144/8000 (77%)]\tTotal Loss: 0.445324\n",
      "Reconstruction: 0.379902, Regularization: 0.000227, Discriminator: 0.043425; Generator: 0.021770,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 00:58:44,478 root         INFO     Train Epoch: 114 [6656/8000 (83%)]\tTotal Loss: 0.445059\n",
      "Reconstruction: 0.379704, Regularization: 0.000216, Discriminator: 0.043317; Generator: 0.021822,\n",
      "D(x): 0.498, D(G(z)): 0.497\n",
      "2019-04-10 00:58:44,589 root         INFO     Train Epoch: 114 [7168/8000 (90%)]\tTotal Loss: 0.447143\n",
      "Reconstruction: 0.381705, Regularization: 0.000214, Discriminator: 0.043373; Generator: 0.021852,\n",
      "D(x): 0.496, D(G(z)): 0.497\n",
      "2019-04-10 00:58:44,699 root         INFO     Train Epoch: 114 [7680/8000 (96%)]\tTotal Loss: 0.447675\n",
      "Reconstruction: 0.382133, Regularization: 0.000186, Discriminator: 0.043403; Generator: 0.021953,\n",
      "D(x): 0.494, D(G(z)): 0.495\n",
      "2019-04-10 00:58:44,780 root         INFO     ====> Epoch: 114 Average loss: 0.4429\n",
      "2019-04-10 00:58:44,807 root         INFO     Train Epoch: 115 [0/8000 (0%)]\tTotal Loss: 0.448344\n",
      "Reconstruction: 0.382749, Regularization: 0.000221, Discriminator: 0.043412; Generator: 0.021962,\n",
      "D(x): 0.494, D(G(z)): 0.495\n",
      "2019-04-10 00:58:44,915 root         INFO     Train Epoch: 115 [512/8000 (6%)]\tTotal Loss: 0.449165\n",
      "Reconstruction: 0.383607, Regularization: 0.000220, Discriminator: 0.043388; Generator: 0.021950,\n",
      "D(x): 0.494, D(G(z)): 0.495\n",
      "2019-04-10 00:58:45,022 root         INFO     Train Epoch: 115 [1024/8000 (13%)]\tTotal Loss: 0.450318\n",
      "Reconstruction: 0.384823, Regularization: 0.000222, Discriminator: 0.043362; Generator: 0.021911,\n",
      "D(x): 0.495, D(G(z)): 0.496\n",
      "2019-04-10 00:58:45,131 root         INFO     Train Epoch: 115 [1536/8000 (19%)]\tTotal Loss: 0.450579\n",
      "Reconstruction: 0.385155, Regularization: 0.000207, Discriminator: 0.043346; Generator: 0.021871,\n",
      "D(x): 0.496, D(G(z)): 0.497\n",
      "2019-04-10 00:58:45,238 root         INFO     Train Epoch: 115 [2048/8000 (26%)]\tTotal Loss: 0.450569\n",
      "Reconstruction: 0.384995, Regularization: 0.000195, Discriminator: 0.043342; Generator: 0.022037,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-10 00:58:45,346 root         INFO     Train Epoch: 115 [2560/8000 (32%)]\tTotal Loss: 0.451510\n",
      "Reconstruction: 0.386030, Regularization: 0.000174, Discriminator: 0.043326; Generator: 0.021980,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:45,454 root         INFO     Train Epoch: 115 [3072/8000 (38%)]\tTotal Loss: 0.451668\n",
      "Reconstruction: 0.386178, Regularization: 0.000180, Discriminator: 0.043310; Generator: 0.022000,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:45,564 root         INFO     Train Epoch: 115 [3584/8000 (45%)]\tTotal Loss: 0.450287\n",
      "Reconstruction: 0.384752, Regularization: 0.000218, Discriminator: 0.043300; Generator: 0.022017,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-10 00:58:45,676 root         INFO     Train Epoch: 115 [4096/8000 (51%)]\tTotal Loss: 0.450652\n",
      "Reconstruction: 0.385123, Regularization: 0.000214, Discriminator: 0.043315; Generator: 0.021999,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:45,789 root         INFO     Train Epoch: 115 [4608/8000 (58%)]\tTotal Loss: 0.448386\n",
      "Reconstruction: 0.382905, Regularization: 0.000194, Discriminator: 0.043313; Generator: 0.021975,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:45,901 root         INFO     Train Epoch: 115 [5120/8000 (64%)]\tTotal Loss: 0.448447\n",
      "Reconstruction: 0.382985, Regularization: 0.000189, Discriminator: 0.043295; Generator: 0.021978,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:46,013 root         INFO     Train Epoch: 115 [5632/8000 (70%)]\tTotal Loss: 0.447190\n",
      "Reconstruction: 0.381671, Regularization: 0.000201, Discriminator: 0.043336; Generator: 0.021983,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:46,125 root         INFO     Train Epoch: 115 [6144/8000 (77%)]\tTotal Loss: 0.446354\n",
      "Reconstruction: 0.381028, Regularization: 0.000203, Discriminator: 0.043260; Generator: 0.021863,\n",
      "D(x): 0.498, D(G(z)): 0.497\n",
      "2019-04-10 00:58:46,238 root         INFO     Train Epoch: 115 [6656/8000 (83%)]\tTotal Loss: 0.445085\n",
      "Reconstruction: 0.379723, Regularization: 0.000193, Discriminator: 0.043294; Generator: 0.021876,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:46,351 root         INFO     Train Epoch: 115 [7168/8000 (90%)]\tTotal Loss: 0.444511\n",
      "Reconstruction: 0.379205, Regularization: 0.000180, Discriminator: 0.043286; Generator: 0.021840,\n",
      "D(x): 0.498, D(G(z)): 0.497\n",
      "2019-04-10 00:58:46,464 root         INFO     Train Epoch: 115 [7680/8000 (96%)]\tTotal Loss: 0.444842\n",
      "Reconstruction: 0.379509, Regularization: 0.000224, Discriminator: 0.043204; Generator: 0.021905,\n",
      "D(x): 0.498, D(G(z)): 0.496\n",
      "2019-04-10 00:58:46,546 root         INFO     ====> Epoch: 115 Average loss: 0.4486\n",
      "2019-04-10 00:58:46,573 root         INFO     Train Epoch: 116 [0/8000 (0%)]\tTotal Loss: 0.445525\n",
      "Reconstruction: 0.379986, Regularization: 0.000227, Discriminator: 0.043327; Generator: 0.021985,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-10 00:58:46,686 root         INFO     Train Epoch: 116 [512/8000 (6%)]\tTotal Loss: 0.444101\n",
      "Reconstruction: 0.378612, Regularization: 0.000247, Discriminator: 0.043310; Generator: 0.021932,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-10 00:58:46,798 root         INFO     Train Epoch: 116 [1024/8000 (13%)]\tTotal Loss: 0.444402\n",
      "Reconstruction: 0.379129, Regularization: 0.000227, Discriminator: 0.043245; Generator: 0.021801,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:58:46,910 root         INFO     Train Epoch: 116 [1536/8000 (19%)]\tTotal Loss: 0.443620\n",
      "Reconstruction: 0.378335, Regularization: 0.000221, Discriminator: 0.043274; Generator: 0.021790,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:58:47,021 root         INFO     Train Epoch: 116 [2048/8000 (26%)]\tTotal Loss: 0.443256\n",
      "Reconstruction: 0.377851, Regularization: 0.000212, Discriminator: 0.043358; Generator: 0.021835,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:47,132 root         INFO     Train Epoch: 116 [2560/8000 (32%)]\tTotal Loss: 0.441645\n",
      "Reconstruction: 0.376278, Regularization: 0.000215, Discriminator: 0.043344; Generator: 0.021809,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 00:58:47,243 root         INFO     Train Epoch: 116 [3072/8000 (38%)]\tTotal Loss: 0.440330\n",
      "Reconstruction: 0.375156, Regularization: 0.000217, Discriminator: 0.043374; Generator: 0.021583,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:58:47,354 root         INFO     Train Epoch: 116 [3584/8000 (45%)]\tTotal Loss: 0.438218\n",
      "Reconstruction: 0.373308, Regularization: 0.000226, Discriminator: 0.043090; Generator: 0.021594,\n",
      "D(x): 0.505, D(G(z)): 0.501\n",
      "2019-04-10 00:58:47,466 root         INFO     Train Epoch: 116 [4096/8000 (51%)]\tTotal Loss: 0.437720\n",
      "Reconstruction: 0.372679, Regularization: 0.000200, Discriminator: 0.043277; Generator: 0.021564,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:47,578 root         INFO     Train Epoch: 116 [4608/8000 (58%)]\tTotal Loss: 0.437419\n",
      "Reconstruction: 0.372276, Regularization: 0.000194, Discriminator: 0.043378; Generator: 0.021571,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:47,689 root         INFO     Train Epoch: 116 [5120/8000 (64%)]\tTotal Loss: 0.435225\n",
      "Reconstruction: 0.370347, Regularization: 0.000199, Discriminator: 0.043347; Generator: 0.021332,\n",
      "D(x): 0.505, D(G(z)): 0.505\n",
      "2019-04-10 00:58:47,800 root         INFO     Train Epoch: 116 [5632/8000 (70%)]\tTotal Loss: 0.434023\n",
      "Reconstruction: 0.368918, Regularization: 0.000185, Discriminator: 0.043423; Generator: 0.021496,\n",
      "D(x): 0.501, D(G(z)): 0.503\n",
      "2019-04-10 00:58:47,911 root         INFO     Train Epoch: 116 [6144/8000 (77%)]\tTotal Loss: 0.433433\n",
      "Reconstruction: 0.368422, Regularization: 0.000179, Discriminator: 0.043284; Generator: 0.021548,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:48,020 root         INFO     Train Epoch: 116 [6656/8000 (83%)]\tTotal Loss: 0.432800\n",
      "Reconstruction: 0.367675, Regularization: 0.000230, Discriminator: 0.043409; Generator: 0.021485,\n",
      "D(x): 0.501, D(G(z)): 0.503\n",
      "2019-04-10 00:58:48,130 root         INFO     Train Epoch: 116 [7168/8000 (90%)]\tTotal Loss: 0.431537\n",
      "Reconstruction: 0.366536, Regularization: 0.000189, Discriminator: 0.043375; Generator: 0.021437,\n",
      "D(x): 0.503, D(G(z)): 0.504\n",
      "2019-04-10 00:58:48,242 root         INFO     Train Epoch: 116 [7680/8000 (96%)]\tTotal Loss: 0.431411\n",
      "Reconstruction: 0.366405, Regularization: 0.000221, Discriminator: 0.043338; Generator: 0.021447,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:48,322 root         INFO     ====> Epoch: 116 Average loss: 0.4383\n",
      "2019-04-10 00:58:48,350 root         INFO     Train Epoch: 117 [0/8000 (0%)]\tTotal Loss: 0.432025\n",
      "Reconstruction: 0.367088, Regularization: 0.000223, Discriminator: 0.043335; Generator: 0.021379,\n",
      "D(x): 0.504, D(G(z)): 0.505\n",
      "2019-04-10 00:58:48,464 root         INFO     Train Epoch: 117 [512/8000 (6%)]\tTotal Loss: 0.431150\n",
      "Reconstruction: 0.366146, Regularization: 0.000184, Discriminator: 0.043350; Generator: 0.021470,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:48,577 root         INFO     Train Epoch: 117 [1024/8000 (13%)]\tTotal Loss: 0.431871\n",
      "Reconstruction: 0.366974, Regularization: 0.000218, Discriminator: 0.043314; Generator: 0.021365,\n",
      "D(x): 0.505, D(G(z)): 0.505\n",
      "2019-04-10 00:58:48,689 root         INFO     Train Epoch: 117 [1536/8000 (19%)]\tTotal Loss: 0.432392\n",
      "Reconstruction: 0.367429, Regularization: 0.000270, Discriminator: 0.043318; Generator: 0.021374,\n",
      "D(x): 0.505, D(G(z)): 0.505\n",
      "2019-04-10 00:58:48,800 root         INFO     Train Epoch: 117 [2048/8000 (26%)]\tTotal Loss: 0.432595\n",
      "Reconstruction: 0.367671, Regularization: 0.000198, Discriminator: 0.043319; Generator: 0.021407,\n",
      "D(x): 0.504, D(G(z)): 0.504\n",
      "2019-04-10 00:58:48,911 root         INFO     Train Epoch: 117 [2560/8000 (32%)]\tTotal Loss: 0.432196\n",
      "Reconstruction: 0.367253, Regularization: 0.000220, Discriminator: 0.043335; Generator: 0.021389,\n",
      "D(x): 0.504, D(G(z)): 0.504\n",
      "2019-04-10 00:58:49,022 root         INFO     Train Epoch: 117 [3072/8000 (38%)]\tTotal Loss: 0.433213\n",
      "Reconstruction: 0.368200, Regularization: 0.000228, Discriminator: 0.043313; Generator: 0.021473,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:49,133 root         INFO     Train Epoch: 117 [3584/8000 (45%)]\tTotal Loss: 0.432619\n",
      "Reconstruction: 0.367569, Regularization: 0.000203, Discriminator: 0.043305; Generator: 0.021542,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:49,243 root         INFO     Train Epoch: 117 [4096/8000 (51%)]\tTotal Loss: 0.433879\n",
      "Reconstruction: 0.368853, Regularization: 0.000211, Discriminator: 0.043311; Generator: 0.021504,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:49,351 root         INFO     Train Epoch: 117 [4608/8000 (58%)]\tTotal Loss: 0.433232\n",
      "Reconstruction: 0.368197, Regularization: 0.000230, Discriminator: 0.043303; Generator: 0.021503,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:49,461 root         INFO     Train Epoch: 117 [5120/8000 (64%)]\tTotal Loss: 0.433953\n",
      "Reconstruction: 0.368930, Regularization: 0.000213, Discriminator: 0.043303; Generator: 0.021507,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:58:49,569 root         INFO     Train Epoch: 117 [5632/8000 (70%)]\tTotal Loss: 0.432942\n",
      "Reconstruction: 0.367905, Regularization: 0.000219, Discriminator: 0.043304; Generator: 0.021513,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:58:49,677 root         INFO     Train Epoch: 117 [6144/8000 (77%)]\tTotal Loss: 0.432327\n",
      "Reconstruction: 0.367254, Regularization: 0.000198, Discriminator: 0.043321; Generator: 0.021553,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:49,787 root         INFO     Train Epoch: 117 [6656/8000 (83%)]\tTotal Loss: 0.432819\n",
      "Reconstruction: 0.367852, Regularization: 0.000198, Discriminator: 0.043306; Generator: 0.021464,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:49,895 root         INFO     Train Epoch: 117 [7168/8000 (90%)]\tTotal Loss: 0.433159\n",
      "Reconstruction: 0.368163, Regularization: 0.000192, Discriminator: 0.043311; Generator: 0.021493,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:50,004 root         INFO     Train Epoch: 117 [7680/8000 (96%)]\tTotal Loss: 0.432710\n",
      "Reconstruction: 0.367745, Regularization: 0.000199, Discriminator: 0.043283; Generator: 0.021483,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:50,084 root         INFO     ====> Epoch: 117 Average loss: 0.4328\n",
      "2019-04-10 00:58:50,112 root         INFO     Train Epoch: 118 [0/8000 (0%)]\tTotal Loss: 0.434589\n",
      "Reconstruction: 0.369477, Regularization: 0.000217, Discriminator: 0.043261; Generator: 0.021634,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:58:50,224 root         INFO     Train Epoch: 118 [512/8000 (6%)]\tTotal Loss: 0.434675\n",
      "Reconstruction: 0.369539, Regularization: 0.000192, Discriminator: 0.043329; Generator: 0.021616,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:50,335 root         INFO     Train Epoch: 118 [1024/8000 (13%)]\tTotal Loss: 0.435197\n",
      "Reconstruction: 0.370153, Regularization: 0.000222, Discriminator: 0.043246; Generator: 0.021576,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 00:58:50,447 root         INFO     Train Epoch: 118 [1536/8000 (19%)]\tTotal Loss: 0.435913\n",
      "Reconstruction: 0.370910, Regularization: 0.000216, Discriminator: 0.043252; Generator: 0.021536,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:58:50,558 root         INFO     Train Epoch: 118 [2048/8000 (26%)]\tTotal Loss: 0.437647\n",
      "Reconstruction: 0.372408, Regularization: 0.000272, Discriminator: 0.043409; Generator: 0.021558,\n",
      "D(x): 0.500, D(G(z)): 0.502\n",
      "2019-04-10 00:58:50,668 root         INFO     Train Epoch: 118 [2560/8000 (32%)]\tTotal Loss: 0.438724\n",
      "Reconstruction: 0.373437, Regularization: 0.000233, Discriminator: 0.043379; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:58:50,780 root         INFO     Train Epoch: 118 [3072/8000 (38%)]\tTotal Loss: 0.440115\n",
      "Reconstruction: 0.374873, Regularization: 0.000202, Discriminator: 0.043272; Generator: 0.021769,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:58:50,891 root         INFO     Train Epoch: 118 [3584/8000 (45%)]\tTotal Loss: 0.441799\n",
      "Reconstruction: 0.376546, Regularization: 0.000190, Discriminator: 0.043360; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:51,002 root         INFO     Train Epoch: 118 [4096/8000 (51%)]\tTotal Loss: 0.442552\n",
      "Reconstruction: 0.377351, Regularization: 0.000167, Discriminator: 0.043308; Generator: 0.021725,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:51,112 root         INFO     Train Epoch: 118 [4608/8000 (58%)]\tTotal Loss: 0.443361\n",
      "Reconstruction: 0.377991, Regularization: 0.000230, Discriminator: 0.043353; Generator: 0.021786,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:51,222 root         INFO     Train Epoch: 118 [5120/8000 (64%)]\tTotal Loss: 0.443299\n",
      "Reconstruction: 0.377977, Regularization: 0.000219, Discriminator: 0.043343; Generator: 0.021761,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:51,333 root         INFO     Train Epoch: 118 [5632/8000 (70%)]\tTotal Loss: 0.442660\n",
      "Reconstruction: 0.377403, Regularization: 0.000186, Discriminator: 0.043322; Generator: 0.021749,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:51,444 root         INFO     Train Epoch: 118 [6144/8000 (77%)]\tTotal Loss: 0.444553\n",
      "Reconstruction: 0.379270, Regularization: 0.000180, Discriminator: 0.043339; Generator: 0.021765,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:51,555 root         INFO     Train Epoch: 118 [6656/8000 (83%)]\tTotal Loss: 0.443895\n",
      "Reconstruction: 0.378637, Regularization: 0.000157, Discriminator: 0.043331; Generator: 0.021771,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:51,665 root         INFO     Train Epoch: 118 [7168/8000 (90%)]\tTotal Loss: 0.444604\n",
      "Reconstruction: 0.379349, Regularization: 0.000162, Discriminator: 0.043318; Generator: 0.021775,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:51,776 root         INFO     Train Epoch: 118 [7680/8000 (96%)]\tTotal Loss: 0.444773\n",
      "Reconstruction: 0.379435, Regularization: 0.000169, Discriminator: 0.043328; Generator: 0.021842,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:51,856 root         INFO     ====> Epoch: 118 Average loss: 0.4410\n",
      "2019-04-10 00:58:51,883 root         INFO     Train Epoch: 119 [0/8000 (0%)]\tTotal Loss: 0.444220\n",
      "Reconstruction: 0.378915, Regularization: 0.000180, Discriminator: 0.043336; Generator: 0.021788,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:51,996 root         INFO     Train Epoch: 119 [512/8000 (6%)]\tTotal Loss: 0.445596\n",
      "Reconstruction: 0.380282, Regularization: 0.000190, Discriminator: 0.043337; Generator: 0.021787,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:52,106 root         INFO     Train Epoch: 119 [1024/8000 (13%)]\tTotal Loss: 0.444415\n",
      "Reconstruction: 0.379146, Regularization: 0.000164, Discriminator: 0.043336; Generator: 0.021770,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:52,216 root         INFO     Train Epoch: 119 [1536/8000 (19%)]\tTotal Loss: 0.445836\n",
      "Reconstruction: 0.380538, Regularization: 0.000137, Discriminator: 0.043326; Generator: 0.021834,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:52,325 root         INFO     Train Epoch: 119 [2048/8000 (26%)]\tTotal Loss: 0.445659\n",
      "Reconstruction: 0.380378, Regularization: 0.000140, Discriminator: 0.043326; Generator: 0.021815,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 00:58:52,435 root         INFO     Train Epoch: 119 [2560/8000 (32%)]\tTotal Loss: 0.444693\n",
      "Reconstruction: 0.379381, Regularization: 0.000155, Discriminator: 0.043320; Generator: 0.021838,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:52,545 root         INFO     Train Epoch: 119 [3072/8000 (38%)]\tTotal Loss: 0.445910\n",
      "Reconstruction: 0.380671, Regularization: 0.000140, Discriminator: 0.043323; Generator: 0.021775,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:52,655 root         INFO     Train Epoch: 119 [3584/8000 (45%)]\tTotal Loss: 0.445559\n",
      "Reconstruction: 0.380167, Regularization: 0.000198, Discriminator: 0.043304; Generator: 0.021890,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-10 00:58:52,765 root         INFO     Train Epoch: 119 [4096/8000 (51%)]\tTotal Loss: 0.445229\n",
      "Reconstruction: 0.379898, Regularization: 0.000166, Discriminator: 0.043299; Generator: 0.021866,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:52,875 root         INFO     Train Epoch: 119 [4608/8000 (58%)]\tTotal Loss: 0.444256\n",
      "Reconstruction: 0.378917, Regularization: 0.000152, Discriminator: 0.043300; Generator: 0.021887,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-10 00:58:52,986 root         INFO     Train Epoch: 119 [5120/8000 (64%)]\tTotal Loss: 0.444609\n",
      "Reconstruction: 0.379376, Regularization: 0.000163, Discriminator: 0.043277; Generator: 0.021792,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:58:53,095 root         INFO     Train Epoch: 119 [5632/8000 (70%)]\tTotal Loss: 0.443381\n",
      "Reconstruction: 0.377972, Regularization: 0.000187, Discriminator: 0.043321; Generator: 0.021902,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-10 00:58:53,205 root         INFO     Train Epoch: 119 [6144/8000 (77%)]\tTotal Loss: 0.443380\n",
      "Reconstruction: 0.378215, Regularization: 0.000153, Discriminator: 0.043261; Generator: 0.021751,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:58:53,314 root         INFO     Train Epoch: 119 [6656/8000 (83%)]\tTotal Loss: 0.443147\n",
      "Reconstruction: 0.377930, Regularization: 0.000201, Discriminator: 0.043299; Generator: 0.021718,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:53,425 root         INFO     Train Epoch: 119 [7168/8000 (90%)]\tTotal Loss: 0.441819\n",
      "Reconstruction: 0.376544, Regularization: 0.000151, Discriminator: 0.043345; Generator: 0.021779,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:53,535 root         INFO     Train Epoch: 119 [7680/8000 (96%)]\tTotal Loss: 0.441318\n",
      "Reconstruction: 0.376172, Regularization: 0.000166, Discriminator: 0.043252; Generator: 0.021727,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:58:53,615 root         INFO     ====> Epoch: 119 Average loss: 0.4443\n",
      "2019-04-10 00:58:53,642 root         INFO     Train Epoch: 120 [0/8000 (0%)]\tTotal Loss: 0.440945\n",
      "Reconstruction: 0.375734, Regularization: 0.000164, Discriminator: 0.043347; Generator: 0.021700,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:53,754 root         INFO     Train Epoch: 120 [512/8000 (6%)]\tTotal Loss: 0.438475\n",
      "Reconstruction: 0.373387, Regularization: 0.000149, Discriminator: 0.043243; Generator: 0.021696,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 00:58:53,865 root         INFO     Train Epoch: 120 [1024/8000 (13%)]\tTotal Loss: 0.437752\n",
      "Reconstruction: 0.372504, Regularization: 0.000162, Discriminator: 0.043388; Generator: 0.021699,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:58:53,976 root         INFO     Train Epoch: 120 [1536/8000 (19%)]\tTotal Loss: 0.436747\n",
      "Reconstruction: 0.371616, Regularization: 0.000151, Discriminator: 0.043386; Generator: 0.021594,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:58:54,086 root         INFO     Train Epoch: 120 [2048/8000 (26%)]\tTotal Loss: 0.436235\n",
      "Reconstruction: 0.371178, Regularization: 0.000160, Discriminator: 0.043342; Generator: 0.021554,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:58:54,196 root         INFO     Train Epoch: 120 [2560/8000 (32%)]\tTotal Loss: 0.435172\n",
      "Reconstruction: 0.370048, Regularization: 0.000180, Discriminator: 0.043344; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:54,306 root         INFO     Train Epoch: 120 [3072/8000 (38%)]\tTotal Loss: 0.435041\n",
      "Reconstruction: 0.369972, Regularization: 0.000168, Discriminator: 0.043355; Generator: 0.021546,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:58:54,416 root         INFO     Train Epoch: 120 [3584/8000 (45%)]\tTotal Loss: 0.434589\n",
      "Reconstruction: 0.369619, Regularization: 0.000173, Discriminator: 0.043348; Generator: 0.021449,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:54,525 root         INFO     Train Epoch: 120 [4096/8000 (51%)]\tTotal Loss: 0.433832\n",
      "Reconstruction: 0.368754, Regularization: 0.000199, Discriminator: 0.043347; Generator: 0.021533,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:54,634 root         INFO     Train Epoch: 120 [4608/8000 (58%)]\tTotal Loss: 0.434131\n",
      "Reconstruction: 0.369136, Regularization: 0.000161, Discriminator: 0.043329; Generator: 0.021505,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-10 00:58:54,743 root         INFO     Train Epoch: 120 [5120/8000 (64%)]\tTotal Loss: 0.433837\n",
      "Reconstruction: 0.368784, Regularization: 0.000156, Discriminator: 0.043324; Generator: 0.021573,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:54,851 root         INFO     Train Epoch: 120 [5632/8000 (70%)]\tTotal Loss: 0.434322\n",
      "Reconstruction: 0.369292, Regularization: 0.000193, Discriminator: 0.043327; Generator: 0.021511,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:54,959 root         INFO     Train Epoch: 120 [6144/8000 (77%)]\tTotal Loss: 0.433913\n",
      "Reconstruction: 0.368885, Regularization: 0.000171, Discriminator: 0.043321; Generator: 0.021535,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:55,066 root         INFO     Train Epoch: 120 [6656/8000 (83%)]\tTotal Loss: 0.434189\n",
      "Reconstruction: 0.369093, Regularization: 0.000203, Discriminator: 0.043322; Generator: 0.021571,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:55,175 root         INFO     Train Epoch: 120 [7168/8000 (90%)]\tTotal Loss: 0.434772\n",
      "Reconstruction: 0.369697, Regularization: 0.000177, Discriminator: 0.043302; Generator: 0.021596,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:55,283 root         INFO     Train Epoch: 120 [7680/8000 (96%)]\tTotal Loss: 0.434352\n",
      "Reconstruction: 0.369336, Regularization: 0.000178, Discriminator: 0.043314; Generator: 0.021523,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:55,362 root         INFO     ====> Epoch: 120 Average loss: 0.4354\n",
      "2019-04-10 00:58:55,391 root         INFO     Train Epoch: 121 [0/8000 (0%)]\tTotal Loss: 0.433981\n",
      "Reconstruction: 0.369000, Regularization: 0.000158, Discriminator: 0.043326; Generator: 0.021496,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:58:55,504 root         INFO     Train Epoch: 121 [512/8000 (6%)]\tTotal Loss: 0.434674\n",
      "Reconstruction: 0.369734, Regularization: 0.000180, Discriminator: 0.043279; Generator: 0.021480,\n",
      "D(x): 0.504, D(G(z)): 0.503\n",
      "2019-04-10 00:58:55,616 root         INFO     Train Epoch: 121 [1024/8000 (13%)]\tTotal Loss: 0.434979\n",
      "Reconstruction: 0.369978, Regularization: 0.000181, Discriminator: 0.043291; Generator: 0.021528,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:58:55,727 root         INFO     Train Epoch: 121 [1536/8000 (19%)]\tTotal Loss: 0.435590\n",
      "Reconstruction: 0.370560, Regularization: 0.000176, Discriminator: 0.043351; Generator: 0.021504,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-10 00:58:55,838 root         INFO     Train Epoch: 121 [2048/8000 (26%)]\tTotal Loss: 0.435807\n",
      "Reconstruction: 0.370662, Regularization: 0.000200, Discriminator: 0.043343; Generator: 0.021602,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:55,950 root         INFO     Train Epoch: 121 [2560/8000 (32%)]\tTotal Loss: 0.436700\n",
      "Reconstruction: 0.371497, Regularization: 0.000174, Discriminator: 0.043362; Generator: 0.021667,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:58:56,061 root         INFO     Train Epoch: 121 [3072/8000 (38%)]\tTotal Loss: 0.437212\n",
      "Reconstruction: 0.372210, Regularization: 0.000158, Discriminator: 0.043264; Generator: 0.021580,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:58:56,172 root         INFO     Train Epoch: 121 [3584/8000 (45%)]\tTotal Loss: 0.438764\n",
      "Reconstruction: 0.373644, Regularization: 0.000166, Discriminator: 0.043296; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:58:56,282 root         INFO     Train Epoch: 121 [4096/8000 (51%)]\tTotal Loss: 0.439774\n",
      "Reconstruction: 0.374672, Regularization: 0.000177, Discriminator: 0.043332; Generator: 0.021593,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:56,392 root         INFO     Train Epoch: 121 [4608/8000 (58%)]\tTotal Loss: 0.441044\n",
      "Reconstruction: 0.375900, Regularization: 0.000160, Discriminator: 0.043296; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:58:56,503 root         INFO     Train Epoch: 121 [5120/8000 (64%)]\tTotal Loss: 0.441911\n",
      "Reconstruction: 0.376668, Regularization: 0.000173, Discriminator: 0.043371; Generator: 0.021700,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:56,613 root         INFO     Train Epoch: 121 [5632/8000 (70%)]\tTotal Loss: 0.442894\n",
      "Reconstruction: 0.377615, Regularization: 0.000173, Discriminator: 0.043347; Generator: 0.021758,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:56,723 root         INFO     Train Epoch: 121 [6144/8000 (77%)]\tTotal Loss: 0.443531\n",
      "Reconstruction: 0.378217, Regularization: 0.000173, Discriminator: 0.043313; Generator: 0.021829,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:56,833 root         INFO     Train Epoch: 121 [6656/8000 (83%)]\tTotal Loss: 0.442652\n",
      "Reconstruction: 0.377298, Regularization: 0.000182, Discriminator: 0.043332; Generator: 0.021840,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-10 00:58:56,943 root         INFO     Train Epoch: 121 [7168/8000 (90%)]\tTotal Loss: 0.442897\n",
      "Reconstruction: 0.377687, Regularization: 0.000159, Discriminator: 0.043328; Generator: 0.021723,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:57,053 root         INFO     Train Epoch: 121 [7680/8000 (96%)]\tTotal Loss: 0.443143\n",
      "Reconstruction: 0.377957, Regularization: 0.000157, Discriminator: 0.043324; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:57,134 root         INFO     ====> Epoch: 121 Average loss: 0.4393\n",
      "2019-04-10 00:58:57,162 root         INFO     Train Epoch: 122 [0/8000 (0%)]\tTotal Loss: 0.443020\n",
      "Reconstruction: 0.377715, Regularization: 0.000169, Discriminator: 0.043331; Generator: 0.021805,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:57,271 root         INFO     Train Epoch: 122 [512/8000 (6%)]\tTotal Loss: 0.442849\n",
      "Reconstruction: 0.377591, Regularization: 0.000170, Discriminator: 0.043336; Generator: 0.021752,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:58:57,380 root         INFO     Train Epoch: 122 [1024/8000 (13%)]\tTotal Loss: 0.443553\n",
      "Reconstruction: 0.378341, Regularization: 0.000168, Discriminator: 0.043324; Generator: 0.021719,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:57,489 root         INFO     Train Epoch: 122 [1536/8000 (19%)]\tTotal Loss: 0.443489\n",
      "Reconstruction: 0.378200, Regularization: 0.000178, Discriminator: 0.043318; Generator: 0.021793,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:57,598 root         INFO     Train Epoch: 122 [2048/8000 (26%)]\tTotal Loss: 0.442893\n",
      "Reconstruction: 0.377604, Regularization: 0.000188, Discriminator: 0.043324; Generator: 0.021777,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:57,706 root         INFO     Train Epoch: 122 [2560/8000 (32%)]\tTotal Loss: 0.443263\n",
      "Reconstruction: 0.378014, Regularization: 0.000158, Discriminator: 0.043315; Generator: 0.021775,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:57,813 root         INFO     Train Epoch: 122 [3072/8000 (38%)]\tTotal Loss: 0.443567\n",
      "Reconstruction: 0.378349, Regularization: 0.000151, Discriminator: 0.043312; Generator: 0.021755,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:58:57,921 root         INFO     Train Epoch: 122 [3584/8000 (45%)]\tTotal Loss: 0.443966\n",
      "Reconstruction: 0.378676, Regularization: 0.000184, Discriminator: 0.043320; Generator: 0.021786,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:58,030 root         INFO     Train Epoch: 122 [4096/8000 (51%)]\tTotal Loss: 0.443288\n",
      "Reconstruction: 0.378053, Regularization: 0.000200, Discriminator: 0.043332; Generator: 0.021703,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:58,139 root         INFO     Train Epoch: 122 [4608/8000 (58%)]\tTotal Loss: 0.443463\n",
      "Reconstruction: 0.378224, Regularization: 0.000191, Discriminator: 0.043328; Generator: 0.021718,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:58,248 root         INFO     Train Epoch: 122 [5120/8000 (64%)]\tTotal Loss: 0.442610\n",
      "Reconstruction: 0.377368, Regularization: 0.000191, Discriminator: 0.043278; Generator: 0.021773,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:58:58,357 root         INFO     Train Epoch: 122 [5632/8000 (70%)]\tTotal Loss: 0.443025\n",
      "Reconstruction: 0.377729, Regularization: 0.000202, Discriminator: 0.043322; Generator: 0.021772,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:58:58,466 root         INFO     Train Epoch: 122 [6144/8000 (77%)]\tTotal Loss: 0.441617\n",
      "Reconstruction: 0.376416, Regularization: 0.000164, Discriminator: 0.043343; Generator: 0.021693,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:58:58,575 root         INFO     Train Epoch: 122 [6656/8000 (83%)]\tTotal Loss: 0.440100\n",
      "Reconstruction: 0.375003, Regularization: 0.000188, Discriminator: 0.043290; Generator: 0.021619,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:58,684 root         INFO     Train Epoch: 122 [7168/8000 (90%)]\tTotal Loss: 0.438218\n",
      "Reconstruction: 0.373138, Regularization: 0.000154, Discriminator: 0.043327; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:58,795 root         INFO     Train Epoch: 122 [7680/8000 (96%)]\tTotal Loss: 0.438696\n",
      "Reconstruction: 0.373565, Regularization: 0.000222, Discriminator: 0.043234; Generator: 0.021675,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:58:58,875 root         INFO     ====> Epoch: 122 Average loss: 0.4424\n",
      "2019-04-10 00:58:58,903 root         INFO     Train Epoch: 123 [0/8000 (0%)]\tTotal Loss: 0.436954\n",
      "Reconstruction: 0.371766, Regularization: 0.000182, Discriminator: 0.043340; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:58:59,016 root         INFO     Train Epoch: 123 [512/8000 (6%)]\tTotal Loss: 0.435585\n",
      "Reconstruction: 0.370478, Regularization: 0.000199, Discriminator: 0.043281; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:59,129 root         INFO     Train Epoch: 123 [1024/8000 (13%)]\tTotal Loss: 0.435570\n",
      "Reconstruction: 0.370549, Regularization: 0.000190, Discriminator: 0.043302; Generator: 0.021529,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:58:59,240 root         INFO     Train Epoch: 123 [1536/8000 (19%)]\tTotal Loss: 0.434710\n",
      "Reconstruction: 0.369626, Regularization: 0.000184, Discriminator: 0.043304; Generator: 0.021595,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:59,350 root         INFO     Train Epoch: 123 [2048/8000 (26%)]\tTotal Loss: 0.435887\n",
      "Reconstruction: 0.370852, Regularization: 0.000204, Discriminator: 0.043327; Generator: 0.021504,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-10 00:58:59,461 root         INFO     Train Epoch: 123 [2560/8000 (32%)]\tTotal Loss: 0.434978\n",
      "Reconstruction: 0.369893, Regularization: 0.000177, Discriminator: 0.043334; Generator: 0.021574,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:59,570 root         INFO     Train Epoch: 123 [3072/8000 (38%)]\tTotal Loss: 0.435383\n",
      "Reconstruction: 0.370296, Regularization: 0.000175, Discriminator: 0.043315; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:59,679 root         INFO     Train Epoch: 123 [3584/8000 (45%)]\tTotal Loss: 0.435655\n",
      "Reconstruction: 0.370538, Regularization: 0.000181, Discriminator: 0.043338; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:58:59,788 root         INFO     Train Epoch: 123 [4096/8000 (51%)]\tTotal Loss: 0.436316\n",
      "Reconstruction: 0.371245, Regularization: 0.000180, Discriminator: 0.043325; Generator: 0.021565,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:58:59,896 root         INFO     Train Epoch: 123 [4608/8000 (58%)]\tTotal Loss: 0.436332\n",
      "Reconstruction: 0.371256, Regularization: 0.000192, Discriminator: 0.043316; Generator: 0.021567,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:00,002 root         INFO     Train Epoch: 123 [5120/8000 (64%)]\tTotal Loss: 0.435610\n",
      "Reconstruction: 0.370555, Regularization: 0.000156, Discriminator: 0.043319; Generator: 0.021580,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:00,108 root         INFO     Train Epoch: 123 [5632/8000 (70%)]\tTotal Loss: 0.436410\n",
      "Reconstruction: 0.371303, Regularization: 0.000183, Discriminator: 0.043315; Generator: 0.021610,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:00,214 root         INFO     Train Epoch: 123 [6144/8000 (77%)]\tTotal Loss: 0.435970\n",
      "Reconstruction: 0.370880, Regularization: 0.000185, Discriminator: 0.043319; Generator: 0.021586,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:00,319 root         INFO     Train Epoch: 123 [6656/8000 (83%)]\tTotal Loss: 0.436574\n",
      "Reconstruction: 0.371481, Regularization: 0.000190, Discriminator: 0.043315; Generator: 0.021588,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:00,425 root         INFO     Train Epoch: 123 [7168/8000 (90%)]\tTotal Loss: 0.436574\n",
      "Reconstruction: 0.371407, Regularization: 0.000212, Discriminator: 0.043319; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:00,531 root         INFO     Train Epoch: 123 [7680/8000 (96%)]\tTotal Loss: 0.435284\n",
      "Reconstruction: 0.370134, Regularization: 0.000186, Discriminator: 0.043307; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:00,609 root         INFO     ====> Epoch: 123 Average loss: 0.4360\n",
      "2019-04-10 00:59:00,637 root         INFO     Train Epoch: 124 [0/8000 (0%)]\tTotal Loss: 0.435782\n",
      "Reconstruction: 0.370628, Regularization: 0.000176, Discriminator: 0.043340; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:00,748 root         INFO     Train Epoch: 124 [512/8000 (6%)]\tTotal Loss: 0.436953\n",
      "Reconstruction: 0.371920, Regularization: 0.000186, Discriminator: 0.043312; Generator: 0.021534,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:59:00,859 root         INFO     Train Epoch: 124 [1024/8000 (13%)]\tTotal Loss: 0.437791\n",
      "Reconstruction: 0.372604, Regularization: 0.000152, Discriminator: 0.043326; Generator: 0.021708,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:00,969 root         INFO     Train Epoch: 124 [1536/8000 (19%)]\tTotal Loss: 0.438995\n",
      "Reconstruction: 0.373833, Regularization: 0.000171, Discriminator: 0.043277; Generator: 0.021714,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:01,080 root         INFO     Train Epoch: 124 [2048/8000 (26%)]\tTotal Loss: 0.439318\n",
      "Reconstruction: 0.374185, Regularization: 0.000157, Discriminator: 0.043338; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:01,190 root         INFO     Train Epoch: 124 [2560/8000 (32%)]\tTotal Loss: 0.440161\n",
      "Reconstruction: 0.374939, Regularization: 0.000163, Discriminator: 0.043319; Generator: 0.021740,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:01,301 root         INFO     Train Epoch: 124 [3072/8000 (38%)]\tTotal Loss: 0.442621\n",
      "Reconstruction: 0.377406, Regularization: 0.000150, Discriminator: 0.043305; Generator: 0.021759,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:59:01,412 root         INFO     Train Epoch: 124 [3584/8000 (45%)]\tTotal Loss: 0.443035\n",
      "Reconstruction: 0.377845, Regularization: 0.000151, Discriminator: 0.043340; Generator: 0.021699,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:01,523 root         INFO     Train Epoch: 124 [4096/8000 (51%)]\tTotal Loss: 0.442464\n",
      "Reconstruction: 0.377283, Regularization: 0.000171, Discriminator: 0.043326; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:01,634 root         INFO     Train Epoch: 124 [4608/8000 (58%)]\tTotal Loss: 0.443147\n",
      "Reconstruction: 0.377987, Regularization: 0.000148, Discriminator: 0.043331; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:01,745 root         INFO     Train Epoch: 124 [5120/8000 (64%)]\tTotal Loss: 0.444060\n",
      "Reconstruction: 0.378860, Regularization: 0.000152, Discriminator: 0.043323; Generator: 0.021725,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:01,856 root         INFO     Train Epoch: 124 [5632/8000 (70%)]\tTotal Loss: 0.443788\n",
      "Reconstruction: 0.378524, Regularization: 0.000179, Discriminator: 0.043313; Generator: 0.021773,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:01,967 root         INFO     Train Epoch: 124 [6144/8000 (77%)]\tTotal Loss: 0.441508\n",
      "Reconstruction: 0.376298, Regularization: 0.000157, Discriminator: 0.043338; Generator: 0.021714,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:02,078 root         INFO     Train Epoch: 124 [6656/8000 (83%)]\tTotal Loss: 0.442607\n",
      "Reconstruction: 0.377365, Regularization: 0.000163, Discriminator: 0.043319; Generator: 0.021760,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:02,189 root         INFO     Train Epoch: 124 [7168/8000 (90%)]\tTotal Loss: 0.441750\n",
      "Reconstruction: 0.376594, Regularization: 0.000170, Discriminator: 0.043314; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:02,300 root         INFO     Train Epoch: 124 [7680/8000 (96%)]\tTotal Loss: 0.441889\n",
      "Reconstruction: 0.376698, Regularization: 0.000157, Discriminator: 0.043320; Generator: 0.021714,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:02,380 root         INFO     ====> Epoch: 124 Average loss: 0.4412\n",
      "2019-04-10 00:59:02,408 root         INFO     Train Epoch: 125 [0/8000 (0%)]\tTotal Loss: 0.440883\n",
      "Reconstruction: 0.375733, Regularization: 0.000159, Discriminator: 0.043326; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:02,520 root         INFO     Train Epoch: 125 [512/8000 (6%)]\tTotal Loss: 0.440548\n",
      "Reconstruction: 0.375499, Regularization: 0.000150, Discriminator: 0.043302; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:02,632 root         INFO     Train Epoch: 125 [1024/8000 (13%)]\tTotal Loss: 0.441923\n",
      "Reconstruction: 0.376756, Regularization: 0.000162, Discriminator: 0.043264; Generator: 0.021740,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:02,744 root         INFO     Train Epoch: 125 [1536/8000 (19%)]\tTotal Loss: 0.442354\n",
      "Reconstruction: 0.377130, Regularization: 0.000164, Discriminator: 0.043361; Generator: 0.021699,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:02,855 root         INFO     Train Epoch: 125 [2048/8000 (26%)]\tTotal Loss: 0.442841\n",
      "Reconstruction: 0.377623, Regularization: 0.000162, Discriminator: 0.043334; Generator: 0.021722,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:02,965 root         INFO     Train Epoch: 125 [2560/8000 (32%)]\tTotal Loss: 0.442088\n",
      "Reconstruction: 0.376908, Regularization: 0.000175, Discriminator: 0.043284; Generator: 0.021720,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:03,075 root         INFO     Train Epoch: 125 [3072/8000 (38%)]\tTotal Loss: 0.440991\n",
      "Reconstruction: 0.375900, Regularization: 0.000160, Discriminator: 0.043284; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:03,185 root         INFO     Train Epoch: 125 [3584/8000 (45%)]\tTotal Loss: 0.440616\n",
      "Reconstruction: 0.375524, Regularization: 0.000151, Discriminator: 0.043329; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:03,296 root         INFO     Train Epoch: 125 [4096/8000 (51%)]\tTotal Loss: 0.439507\n",
      "Reconstruction: 0.374451, Regularization: 0.000164, Discriminator: 0.043324; Generator: 0.021567,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:59:03,405 root         INFO     Train Epoch: 125 [4608/8000 (58%)]\tTotal Loss: 0.438329\n",
      "Reconstruction: 0.373343, Regularization: 0.000172, Discriminator: 0.043303; Generator: 0.021510,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:59:03,514 root         INFO     Train Epoch: 125 [5120/8000 (64%)]\tTotal Loss: 0.437001\n",
      "Reconstruction: 0.371906, Regularization: 0.000147, Discriminator: 0.043311; Generator: 0.021636,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:03,624 root         INFO     Train Epoch: 125 [5632/8000 (70%)]\tTotal Loss: 0.437769\n",
      "Reconstruction: 0.372670, Regularization: 0.000178, Discriminator: 0.043306; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:03,737 root         INFO     Train Epoch: 125 [6144/8000 (77%)]\tTotal Loss: 0.435847\n",
      "Reconstruction: 0.370701, Regularization: 0.000173, Discriminator: 0.043316; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:03,849 root         INFO     Train Epoch: 125 [6656/8000 (83%)]\tTotal Loss: 0.435532\n",
      "Reconstruction: 0.370402, Regularization: 0.000146, Discriminator: 0.043313; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:03,961 root         INFO     Train Epoch: 125 [7168/8000 (90%)]\tTotal Loss: 0.435268\n",
      "Reconstruction: 0.370176, Regularization: 0.000144, Discriminator: 0.043330; Generator: 0.021617,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:04,072 root         INFO     Train Epoch: 125 [7680/8000 (96%)]\tTotal Loss: 0.434849\n",
      "Reconstruction: 0.369724, Regularization: 0.000210, Discriminator: 0.043321; Generator: 0.021594,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:04,152 root         INFO     ====> Epoch: 125 Average loss: 0.4388\n",
      "2019-04-10 00:59:04,179 root         INFO     Train Epoch: 126 [0/8000 (0%)]\tTotal Loss: 0.435109\n",
      "Reconstruction: 0.370078, Regularization: 0.000180, Discriminator: 0.043315; Generator: 0.021537,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:59:04,291 root         INFO     Train Epoch: 126 [512/8000 (6%)]\tTotal Loss: 0.435771\n",
      "Reconstruction: 0.370680, Regularization: 0.000158, Discriminator: 0.043323; Generator: 0.021610,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:04,401 root         INFO     Train Epoch: 126 [1024/8000 (13%)]\tTotal Loss: 0.436742\n",
      "Reconstruction: 0.371621, Regularization: 0.000164, Discriminator: 0.043324; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:04,512 root         INFO     Train Epoch: 126 [1536/8000 (19%)]\tTotal Loss: 0.437151\n",
      "Reconstruction: 0.372017, Regularization: 0.000151, Discriminator: 0.043308; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:04,623 root         INFO     Train Epoch: 126 [2048/8000 (26%)]\tTotal Loss: 0.437155\n",
      "Reconstruction: 0.372022, Regularization: 0.000195, Discriminator: 0.043345; Generator: 0.021593,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:04,734 root         INFO     Train Epoch: 126 [2560/8000 (32%)]\tTotal Loss: 0.438181\n",
      "Reconstruction: 0.373073, Regularization: 0.000145, Discriminator: 0.043311; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:04,846 root         INFO     Train Epoch: 126 [3072/8000 (38%)]\tTotal Loss: 0.438563\n",
      "Reconstruction: 0.373406, Regularization: 0.000160, Discriminator: 0.043315; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:04,959 root         INFO     Train Epoch: 126 [3584/8000 (45%)]\tTotal Loss: 0.438263\n",
      "Reconstruction: 0.373215, Regularization: 0.000149, Discriminator: 0.043323; Generator: 0.021576,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:05,073 root         INFO     Train Epoch: 126 [4096/8000 (51%)]\tTotal Loss: 0.438517\n",
      "Reconstruction: 0.373408, Regularization: 0.000185, Discriminator: 0.043317; Generator: 0.021608,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:05,186 root         INFO     Train Epoch: 126 [4608/8000 (58%)]\tTotal Loss: 0.438321\n",
      "Reconstruction: 0.373139, Regularization: 0.000168, Discriminator: 0.043330; Generator: 0.021684,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:05,300 root         INFO     Train Epoch: 126 [5120/8000 (64%)]\tTotal Loss: 0.438997\n",
      "Reconstruction: 0.373816, Regularization: 0.000161, Discriminator: 0.043329; Generator: 0.021691,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:05,412 root         INFO     Train Epoch: 126 [5632/8000 (70%)]\tTotal Loss: 0.438713\n",
      "Reconstruction: 0.373619, Regularization: 0.000152, Discriminator: 0.043341; Generator: 0.021601,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:05,525 root         INFO     Train Epoch: 126 [6144/8000 (77%)]\tTotal Loss: 0.440587\n",
      "Reconstruction: 0.375433, Regularization: 0.000159, Discriminator: 0.043340; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:05,638 root         INFO     Train Epoch: 126 [6656/8000 (83%)]\tTotal Loss: 0.440525\n",
      "Reconstruction: 0.375304, Regularization: 0.000167, Discriminator: 0.043326; Generator: 0.021727,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:05,750 root         INFO     Train Epoch: 126 [7168/8000 (90%)]\tTotal Loss: 0.441681\n",
      "Reconstruction: 0.376450, Regularization: 0.000166, Discriminator: 0.043353; Generator: 0.021712,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:05,863 root         INFO     Train Epoch: 126 [7680/8000 (96%)]\tTotal Loss: 0.443033\n",
      "Reconstruction: 0.377786, Regularization: 0.000170, Discriminator: 0.043336; Generator: 0.021741,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:05,944 root         INFO     ====> Epoch: 126 Average loss: 0.4389\n",
      "2019-04-10 00:59:05,971 root         INFO     Train Epoch: 127 [0/8000 (0%)]\tTotal Loss: 0.443051\n",
      "Reconstruction: 0.377894, Regularization: 0.000177, Discriminator: 0.043322; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:06,086 root         INFO     Train Epoch: 127 [512/8000 (6%)]\tTotal Loss: 0.442886\n",
      "Reconstruction: 0.377743, Regularization: 0.000153, Discriminator: 0.043322; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:06,198 root         INFO     Train Epoch: 127 [1024/8000 (13%)]\tTotal Loss: 0.443883\n",
      "Reconstruction: 0.378735, Regularization: 0.000159, Discriminator: 0.043316; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:06,311 root         INFO     Train Epoch: 127 [1536/8000 (19%)]\tTotal Loss: 0.444116\n",
      "Reconstruction: 0.378968, Regularization: 0.000138, Discriminator: 0.043321; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:06,424 root         INFO     Train Epoch: 127 [2048/8000 (26%)]\tTotal Loss: 0.444562\n",
      "Reconstruction: 0.379340, Regularization: 0.000179, Discriminator: 0.043318; Generator: 0.021725,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:06,536 root         INFO     Train Epoch: 127 [2560/8000 (32%)]\tTotal Loss: 0.443858\n",
      "Reconstruction: 0.378707, Regularization: 0.000142, Discriminator: 0.043286; Generator: 0.021723,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:06,649 root         INFO     Train Epoch: 127 [3072/8000 (38%)]\tTotal Loss: 0.443964\n",
      "Reconstruction: 0.378771, Regularization: 0.000151, Discriminator: 0.043344; Generator: 0.021698,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:06,761 root         INFO     Train Epoch: 127 [3584/8000 (45%)]\tTotal Loss: 0.441962\n",
      "Reconstruction: 0.376784, Regularization: 0.000166, Discriminator: 0.043298; Generator: 0.021715,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:06,874 root         INFO     Train Epoch: 127 [4096/8000 (51%)]\tTotal Loss: 0.442030\n",
      "Reconstruction: 0.376878, Regularization: 0.000170, Discriminator: 0.043292; Generator: 0.021691,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:06,986 root         INFO     Train Epoch: 127 [4608/8000 (58%)]\tTotal Loss: 0.440047\n",
      "Reconstruction: 0.374898, Regularization: 0.000191, Discriminator: 0.043289; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:07,099 root         INFO     Train Epoch: 127 [5120/8000 (64%)]\tTotal Loss: 0.438733\n",
      "Reconstruction: 0.373552, Regularization: 0.000182, Discriminator: 0.043328; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:07,212 root         INFO     Train Epoch: 127 [5632/8000 (70%)]\tTotal Loss: 0.438013\n",
      "Reconstruction: 0.372831, Regularization: 0.000188, Discriminator: 0.043324; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:07,325 root         INFO     Train Epoch: 127 [6144/8000 (77%)]\tTotal Loss: 0.436933\n",
      "Reconstruction: 0.371787, Regularization: 0.000170, Discriminator: 0.043310; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:07,437 root         INFO     Train Epoch: 127 [6656/8000 (83%)]\tTotal Loss: 0.436951\n",
      "Reconstruction: 0.371772, Regularization: 0.000161, Discriminator: 0.043304; Generator: 0.021715,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:07,550 root         INFO     Train Epoch: 127 [7168/8000 (90%)]\tTotal Loss: 0.436081\n",
      "Reconstruction: 0.370911, Regularization: 0.000172, Discriminator: 0.043350; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:07,662 root         INFO     Train Epoch: 127 [7680/8000 (96%)]\tTotal Loss: 0.436440\n",
      "Reconstruction: 0.371398, Regularization: 0.000157, Discriminator: 0.043329; Generator: 0.021557,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:59:07,743 root         INFO     ====> Epoch: 127 Average loss: 0.4406\n",
      "2019-04-10 00:59:07,771 root         INFO     Train Epoch: 128 [0/8000 (0%)]\tTotal Loss: 0.436685\n",
      "Reconstruction: 0.371534, Regularization: 0.000164, Discriminator: 0.043327; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:07,883 root         INFO     Train Epoch: 128 [512/8000 (6%)]\tTotal Loss: 0.436119\n",
      "Reconstruction: 0.370986, Regularization: 0.000176, Discriminator: 0.043317; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:07,995 root         INFO     Train Epoch: 128 [1024/8000 (13%)]\tTotal Loss: 0.436401\n",
      "Reconstruction: 0.371272, Regularization: 0.000196, Discriminator: 0.043307; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:08,107 root         INFO     Train Epoch: 128 [1536/8000 (19%)]\tTotal Loss: 0.434933\n",
      "Reconstruction: 0.369873, Regularization: 0.000171, Discriminator: 0.043318; Generator: 0.021572,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:08,220 root         INFO     Train Epoch: 128 [2048/8000 (26%)]\tTotal Loss: 0.435447\n",
      "Reconstruction: 0.370361, Regularization: 0.000164, Discriminator: 0.043325; Generator: 0.021596,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:08,332 root         INFO     Train Epoch: 128 [2560/8000 (32%)]\tTotal Loss: 0.433802\n",
      "Reconstruction: 0.368772, Regularization: 0.000160, Discriminator: 0.043314; Generator: 0.021557,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:59:08,445 root         INFO     Train Epoch: 128 [3072/8000 (38%)]\tTotal Loss: 0.434080\n",
      "Reconstruction: 0.369075, Regularization: 0.000156, Discriminator: 0.043302; Generator: 0.021547,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:59:08,557 root         INFO     Train Epoch: 128 [3584/8000 (45%)]\tTotal Loss: 0.435327\n",
      "Reconstruction: 0.370233, Regularization: 0.000152, Discriminator: 0.043309; Generator: 0.021632,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:08,670 root         INFO     Train Epoch: 128 [4096/8000 (51%)]\tTotal Loss: 0.436824\n",
      "Reconstruction: 0.371745, Regularization: 0.000145, Discriminator: 0.043329; Generator: 0.021605,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:08,782 root         INFO     Train Epoch: 128 [4608/8000 (58%)]\tTotal Loss: 0.437540\n",
      "Reconstruction: 0.372474, Regularization: 0.000152, Discriminator: 0.043328; Generator: 0.021587,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:08,895 root         INFO     Train Epoch: 128 [5120/8000 (64%)]\tTotal Loss: 0.438763\n",
      "Reconstruction: 0.373589, Regularization: 0.000146, Discriminator: 0.043344; Generator: 0.021684,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:09,007 root         INFO     Train Epoch: 128 [5632/8000 (70%)]\tTotal Loss: 0.440495\n",
      "Reconstruction: 0.375317, Regularization: 0.000137, Discriminator: 0.043328; Generator: 0.021713,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:09,119 root         INFO     Train Epoch: 128 [6144/8000 (77%)]\tTotal Loss: 0.442003\n",
      "Reconstruction: 0.376874, Regularization: 0.000141, Discriminator: 0.043341; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:09,231 root         INFO     Train Epoch: 128 [6656/8000 (83%)]\tTotal Loss: 0.442775\n",
      "Reconstruction: 0.377607, Regularization: 0.000129, Discriminator: 0.043319; Generator: 0.021720,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:09,344 root         INFO     Train Epoch: 128 [7168/8000 (90%)]\tTotal Loss: 0.444573\n",
      "Reconstruction: 0.379361, Regularization: 0.000161, Discriminator: 0.043327; Generator: 0.021723,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:09,456 root         INFO     Train Epoch: 128 [7680/8000 (96%)]\tTotal Loss: 0.443763\n",
      "Reconstruction: 0.378507, Regularization: 0.000146, Discriminator: 0.043330; Generator: 0.021781,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:09,540 root         INFO     ====> Epoch: 128 Average loss: 0.4385\n",
      "2019-04-10 00:59:09,567 root         INFO     Train Epoch: 129 [0/8000 (0%)]\tTotal Loss: 0.444656\n",
      "Reconstruction: 0.379418, Regularization: 0.000142, Discriminator: 0.043319; Generator: 0.021778,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:09,680 root         INFO     Train Epoch: 129 [512/8000 (6%)]\tTotal Loss: 0.443497\n",
      "Reconstruction: 0.378276, Regularization: 0.000125, Discriminator: 0.043328; Generator: 0.021768,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:09,791 root         INFO     Train Epoch: 129 [1024/8000 (13%)]\tTotal Loss: 0.441391\n",
      "Reconstruction: 0.376253, Regularization: 0.000144, Discriminator: 0.043302; Generator: 0.021693,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:09,904 root         INFO     Train Epoch: 129 [1536/8000 (19%)]\tTotal Loss: 0.440863\n",
      "Reconstruction: 0.375670, Regularization: 0.000142, Discriminator: 0.043325; Generator: 0.021726,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:10,016 root         INFO     Train Epoch: 129 [2048/8000 (26%)]\tTotal Loss: 0.440265\n",
      "Reconstruction: 0.375143, Regularization: 0.000141, Discriminator: 0.043308; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:10,128 root         INFO     Train Epoch: 129 [2560/8000 (32%)]\tTotal Loss: 0.440737\n",
      "Reconstruction: 0.375668, Regularization: 0.000133, Discriminator: 0.043311; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:10,241 root         INFO     Train Epoch: 129 [3072/8000 (38%)]\tTotal Loss: 0.441121\n",
      "Reconstruction: 0.376155, Regularization: 0.000125, Discriminator: 0.043281; Generator: 0.021560,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:59:10,353 root         INFO     Train Epoch: 129 [3584/8000 (45%)]\tTotal Loss: 0.442453\n",
      "Reconstruction: 0.377229, Regularization: 0.000137, Discriminator: 0.043344; Generator: 0.021744,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:10,465 root         INFO     Train Epoch: 129 [4096/8000 (51%)]\tTotal Loss: 0.441466\n",
      "Reconstruction: 0.376277, Regularization: 0.000147, Discriminator: 0.043347; Generator: 0.021695,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:10,578 root         INFO     Train Epoch: 129 [4608/8000 (58%)]\tTotal Loss: 0.442118\n",
      "Reconstruction: 0.376956, Regularization: 0.000147, Discriminator: 0.043360; Generator: 0.021655,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:10,690 root         INFO     Train Epoch: 129 [5120/8000 (64%)]\tTotal Loss: 0.440629\n",
      "Reconstruction: 0.375508, Regularization: 0.000142, Discriminator: 0.043276; Generator: 0.021703,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:10,802 root         INFO     Train Epoch: 129 [5632/8000 (70%)]\tTotal Loss: 0.438273\n",
      "Reconstruction: 0.373193, Regularization: 0.000165, Discriminator: 0.043282; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:10,916 root         INFO     Train Epoch: 129 [6144/8000 (77%)]\tTotal Loss: 0.436807\n",
      "Reconstruction: 0.371665, Regularization: 0.000154, Discriminator: 0.043352; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:11,028 root         INFO     Train Epoch: 129 [6656/8000 (83%)]\tTotal Loss: 0.434775\n",
      "Reconstruction: 0.369467, Regularization: 0.000157, Discriminator: 0.043476; Generator: 0.021675,\n",
      "D(x): 0.497, D(G(z)): 0.500\n",
      "2019-04-10 00:59:11,138 root         INFO     Train Epoch: 129 [7168/8000 (90%)]\tTotal Loss: 0.432838\n",
      "Reconstruction: 0.367670, Regularization: 0.000156, Discriminator: 0.043371; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:11,249 root         INFO     Train Epoch: 129 [7680/8000 (96%)]\tTotal Loss: 0.431560\n",
      "Reconstruction: 0.366502, Regularization: 0.000152, Discriminator: 0.043327; Generator: 0.021579,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:11,330 root         INFO     ====> Epoch: 129 Average loss: 0.4394\n",
      "2019-04-10 00:59:11,357 root         INFO     Train Epoch: 130 [0/8000 (0%)]\tTotal Loss: 0.432450\n",
      "Reconstruction: 0.367366, Regularization: 0.000157, Discriminator: 0.043327; Generator: 0.021600,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:11,468 root         INFO     Train Epoch: 130 [512/8000 (6%)]\tTotal Loss: 0.433218\n",
      "Reconstruction: 0.368195, Regularization: 0.000139, Discriminator: 0.043317; Generator: 0.021567,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:11,577 root         INFO     Train Epoch: 130 [1024/8000 (13%)]\tTotal Loss: 0.433760\n",
      "Reconstruction: 0.368713, Regularization: 0.000151, Discriminator: 0.043303; Generator: 0.021593,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:11,685 root         INFO     Train Epoch: 130 [1536/8000 (19%)]\tTotal Loss: 0.435551\n",
      "Reconstruction: 0.370576, Regularization: 0.000128, Discriminator: 0.043248; Generator: 0.021599,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:11,793 root         INFO     Train Epoch: 130 [2048/8000 (26%)]\tTotal Loss: 0.437637\n",
      "Reconstruction: 0.372448, Regularization: 0.000125, Discriminator: 0.043354; Generator: 0.021710,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:11,901 root         INFO     Train Epoch: 130 [2560/8000 (32%)]\tTotal Loss: 0.438828\n",
      "Reconstruction: 0.373803, Regularization: 0.000126, Discriminator: 0.043301; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:12,010 root         INFO     Train Epoch: 130 [3072/8000 (38%)]\tTotal Loss: 0.442116\n",
      "Reconstruction: 0.377050, Regularization: 0.000121, Discriminator: 0.043281; Generator: 0.021664,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:12,118 root         INFO     Train Epoch: 130 [3584/8000 (45%)]\tTotal Loss: 0.443149\n",
      "Reconstruction: 0.377925, Regularization: 0.000134, Discriminator: 0.043363; Generator: 0.021727,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:12,227 root         INFO     Train Epoch: 130 [4096/8000 (51%)]\tTotal Loss: 0.444603\n",
      "Reconstruction: 0.379505, Regularization: 0.000098, Discriminator: 0.043296; Generator: 0.021705,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:12,335 root         INFO     Train Epoch: 130 [4608/8000 (58%)]\tTotal Loss: 0.443950\n",
      "Reconstruction: 0.378866, Regularization: 0.000095, Discriminator: 0.043315; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:12,443 root         INFO     Train Epoch: 130 [5120/8000 (64%)]\tTotal Loss: 0.444071\n",
      "Reconstruction: 0.378989, Regularization: 0.000111, Discriminator: 0.043333; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:12,552 root         INFO     Train Epoch: 130 [5632/8000 (70%)]\tTotal Loss: 0.442914\n",
      "Reconstruction: 0.377788, Regularization: 0.000095, Discriminator: 0.043286; Generator: 0.021745,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:12,660 root         INFO     Train Epoch: 130 [6144/8000 (77%)]\tTotal Loss: 0.441740\n",
      "Reconstruction: 0.376565, Regularization: 0.000109, Discriminator: 0.043338; Generator: 0.021729,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:12,769 root         INFO     Train Epoch: 130 [6656/8000 (83%)]\tTotal Loss: 0.439823\n",
      "Reconstruction: 0.374696, Regularization: 0.000087, Discriminator: 0.043315; Generator: 0.021725,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:12,878 root         INFO     Train Epoch: 130 [7168/8000 (90%)]\tTotal Loss: 0.438982\n",
      "Reconstruction: 0.373769, Regularization: 0.000108, Discriminator: 0.043391; Generator: 0.021714,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:12,986 root         INFO     Train Epoch: 130 [7680/8000 (96%)]\tTotal Loss: 0.438594\n",
      "Reconstruction: 0.373461, Regularization: 0.000136, Discriminator: 0.043331; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:13,065 root         INFO     ====> Epoch: 130 Average loss: 0.4396\n",
      "2019-04-10 00:59:13,092 root         INFO     Train Epoch: 131 [0/8000 (0%)]\tTotal Loss: 0.437252\n",
      "Reconstruction: 0.372200, Regularization: 0.000114, Discriminator: 0.043318; Generator: 0.021620,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:13,205 root         INFO     Train Epoch: 131 [512/8000 (6%)]\tTotal Loss: 0.437768\n",
      "Reconstruction: 0.372696, Regularization: 0.000113, Discriminator: 0.043309; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:13,318 root         INFO     Train Epoch: 131 [1024/8000 (13%)]\tTotal Loss: 0.437845\n",
      "Reconstruction: 0.372769, Regularization: 0.000083, Discriminator: 0.043295; Generator: 0.021698,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:13,430 root         INFO     Train Epoch: 131 [1536/8000 (19%)]\tTotal Loss: 0.440184\n",
      "Reconstruction: 0.375106, Regularization: 0.000093, Discriminator: 0.043323; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:13,543 root         INFO     Train Epoch: 131 [2048/8000 (26%)]\tTotal Loss: 0.440885\n",
      "Reconstruction: 0.375720, Regularization: 0.000093, Discriminator: 0.043356; Generator: 0.021716,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:13,655 root         INFO     Train Epoch: 131 [2560/8000 (32%)]\tTotal Loss: 0.440992\n",
      "Reconstruction: 0.375806, Regularization: 0.000111, Discriminator: 0.043368; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:13,768 root         INFO     Train Epoch: 131 [3072/8000 (38%)]\tTotal Loss: 0.441870\n",
      "Reconstruction: 0.376788, Regularization: 0.000103, Discriminator: 0.043335; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:13,880 root         INFO     Train Epoch: 131 [3584/8000 (45%)]\tTotal Loss: 0.441266\n",
      "Reconstruction: 0.376117, Regularization: 0.000097, Discriminator: 0.043348; Generator: 0.021705,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:13,990 root         INFO     Train Epoch: 131 [4096/8000 (51%)]\tTotal Loss: 0.440509\n",
      "Reconstruction: 0.375414, Regularization: 0.000093, Discriminator: 0.043304; Generator: 0.021698,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:14,101 root         INFO     Train Epoch: 131 [4608/8000 (58%)]\tTotal Loss: 0.439315\n",
      "Reconstruction: 0.374327, Regularization: 0.000090, Discriminator: 0.043271; Generator: 0.021628,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:14,210 root         INFO     Train Epoch: 131 [5120/8000 (64%)]\tTotal Loss: 0.436343\n",
      "Reconstruction: 0.371344, Regularization: 0.000100, Discriminator: 0.043333; Generator: 0.021565,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:59:14,318 root         INFO     Train Epoch: 131 [5632/8000 (70%)]\tTotal Loss: 0.435546\n",
      "Reconstruction: 0.370518, Regularization: 0.000109, Discriminator: 0.043316; Generator: 0.021604,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:14,426 root         INFO     Train Epoch: 131 [6144/8000 (77%)]\tTotal Loss: 0.433484\n",
      "Reconstruction: 0.368460, Regularization: 0.000111, Discriminator: 0.043340; Generator: 0.021572,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:14,535 root         INFO     Train Epoch: 131 [6656/8000 (83%)]\tTotal Loss: 0.432622\n",
      "Reconstruction: 0.367597, Regularization: 0.000121, Discriminator: 0.043315; Generator: 0.021589,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:14,644 root         INFO     Train Epoch: 131 [7168/8000 (90%)]\tTotal Loss: 0.432956\n",
      "Reconstruction: 0.367929, Regularization: 0.000131, Discriminator: 0.043319; Generator: 0.021577,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:14,752 root         INFO     Train Epoch: 131 [7680/8000 (96%)]\tTotal Loss: 0.433437\n",
      "Reconstruction: 0.368427, Regularization: 0.000106, Discriminator: 0.043282; Generator: 0.021622,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:14,831 root         INFO     ====> Epoch: 131 Average loss: 0.4377\n",
      "2019-04-10 00:59:14,858 root         INFO     Train Epoch: 132 [0/8000 (0%)]\tTotal Loss: 0.434024\n",
      "Reconstruction: 0.368971, Regularization: 0.000094, Discriminator: 0.043353; Generator: 0.021606,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:14,969 root         INFO     Train Epoch: 132 [512/8000 (6%)]\tTotal Loss: 0.435953\n",
      "Reconstruction: 0.370871, Regularization: 0.000097, Discriminator: 0.043295; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:15,079 root         INFO     Train Epoch: 132 [1024/8000 (13%)]\tTotal Loss: 0.437840\n",
      "Reconstruction: 0.372797, Regularization: 0.000121, Discriminator: 0.043276; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:15,188 root         INFO     Train Epoch: 132 [1536/8000 (19%)]\tTotal Loss: 0.439481\n",
      "Reconstruction: 0.374405, Regularization: 0.000092, Discriminator: 0.043305; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:15,297 root         INFO     Train Epoch: 132 [2048/8000 (26%)]\tTotal Loss: 0.440824\n",
      "Reconstruction: 0.375747, Regularization: 0.000092, Discriminator: 0.043328; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:15,406 root         INFO     Train Epoch: 132 [2560/8000 (32%)]\tTotal Loss: 0.443010\n",
      "Reconstruction: 0.377967, Regularization: 0.000095, Discriminator: 0.043318; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:15,516 root         INFO     Train Epoch: 132 [3072/8000 (38%)]\tTotal Loss: 0.444168\n",
      "Reconstruction: 0.378942, Regularization: 0.000120, Discriminator: 0.043323; Generator: 0.021782,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:15,625 root         INFO     Train Epoch: 132 [3584/8000 (45%)]\tTotal Loss: 0.445400\n",
      "Reconstruction: 0.380219, Regularization: 0.000109, Discriminator: 0.043326; Generator: 0.021746,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:15,733 root         INFO     Train Epoch: 132 [4096/8000 (51%)]\tTotal Loss: 0.445738\n",
      "Reconstruction: 0.380562, Regularization: 0.000096, Discriminator: 0.043326; Generator: 0.021755,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:15,840 root         INFO     Train Epoch: 132 [4608/8000 (58%)]\tTotal Loss: 0.445405\n",
      "Reconstruction: 0.380254, Regularization: 0.000106, Discriminator: 0.043318; Generator: 0.021727,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:15,947 root         INFO     Train Epoch: 132 [5120/8000 (64%)]\tTotal Loss: 0.444655\n",
      "Reconstruction: 0.379516, Regularization: 0.000119, Discriminator: 0.043347; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:16,055 root         INFO     Train Epoch: 132 [5632/8000 (70%)]\tTotal Loss: 0.443890\n",
      "Reconstruction: 0.378715, Regularization: 0.000101, Discriminator: 0.043297; Generator: 0.021777,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:59:16,162 root         INFO     Train Epoch: 132 [6144/8000 (77%)]\tTotal Loss: 0.441607\n",
      "Reconstruction: 0.376523, Regularization: 0.000112, Discriminator: 0.043258; Generator: 0.021714,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:16,269 root         INFO     Train Epoch: 132 [6656/8000 (83%)]\tTotal Loss: 0.440140\n",
      "Reconstruction: 0.375008, Regularization: 0.000082, Discriminator: 0.043368; Generator: 0.021682,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:16,377 root         INFO     Train Epoch: 132 [7168/8000 (90%)]\tTotal Loss: 0.438816\n",
      "Reconstruction: 0.373781, Regularization: 0.000091, Discriminator: 0.043290; Generator: 0.021653,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:16,484 root         INFO     Train Epoch: 132 [7680/8000 (96%)]\tTotal Loss: 0.437655\n",
      "Reconstruction: 0.372613, Regularization: 0.000098, Discriminator: 0.043312; Generator: 0.021632,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:16,564 root         INFO     ====> Epoch: 132 Average loss: 0.4414\n",
      "2019-04-10 00:59:16,592 root         INFO     Train Epoch: 133 [0/8000 (0%)]\tTotal Loss: 0.438032\n",
      "Reconstruction: 0.372991, Regularization: 0.000076, Discriminator: 0.043311; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:16,704 root         INFO     Train Epoch: 133 [512/8000 (6%)]\tTotal Loss: 0.437765\n",
      "Reconstruction: 0.372807, Regularization: 0.000073, Discriminator: 0.043290; Generator: 0.021596,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:16,815 root         INFO     Train Epoch: 133 [1024/8000 (13%)]\tTotal Loss: 0.438565\n",
      "Reconstruction: 0.373581, Regularization: 0.000103, Discriminator: 0.043303; Generator: 0.021577,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:16,926 root         INFO     Train Epoch: 133 [1536/8000 (19%)]\tTotal Loss: 0.440331\n",
      "Reconstruction: 0.375297, Regularization: 0.000077, Discriminator: 0.043318; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:17,037 root         INFO     Train Epoch: 133 [2048/8000 (26%)]\tTotal Loss: 0.441185\n",
      "Reconstruction: 0.376014, Regularization: 0.000089, Discriminator: 0.043339; Generator: 0.021743,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:17,148 root         INFO     Train Epoch: 133 [2560/8000 (32%)]\tTotal Loss: 0.441677\n",
      "Reconstruction: 0.376577, Regularization: 0.000088, Discriminator: 0.043349; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:17,259 root         INFO     Train Epoch: 133 [3072/8000 (38%)]\tTotal Loss: 0.442139\n",
      "Reconstruction: 0.376988, Regularization: 0.000097, Discriminator: 0.043346; Generator: 0.021708,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:17,371 root         INFO     Train Epoch: 133 [3584/8000 (45%)]\tTotal Loss: 0.442152\n",
      "Reconstruction: 0.377085, Regularization: 0.000086, Discriminator: 0.043315; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:17,483 root         INFO     Train Epoch: 133 [4096/8000 (51%)]\tTotal Loss: 0.441104\n",
      "Reconstruction: 0.376114, Regularization: 0.000091, Discriminator: 0.043319; Generator: 0.021580,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:17,593 root         INFO     Train Epoch: 133 [4608/8000 (58%)]\tTotal Loss: 0.439266\n",
      "Reconstruction: 0.374299, Regularization: 0.000098, Discriminator: 0.043228; Generator: 0.021640,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:17,703 root         INFO     Train Epoch: 133 [5120/8000 (64%)]\tTotal Loss: 0.436595\n",
      "Reconstruction: 0.371682, Regularization: 0.000085, Discriminator: 0.043202; Generator: 0.021625,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 00:59:17,813 root         INFO     Train Epoch: 133 [5632/8000 (70%)]\tTotal Loss: 0.434413\n",
      "Reconstruction: 0.369414, Regularization: 0.000107, Discriminator: 0.043284; Generator: 0.021608,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:17,923 root         INFO     Train Epoch: 133 [6144/8000 (77%)]\tTotal Loss: 0.432870\n",
      "Reconstruction: 0.367777, Regularization: 0.000092, Discriminator: 0.043362; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:18,032 root         INFO     Train Epoch: 133 [6656/8000 (83%)]\tTotal Loss: 0.431714\n",
      "Reconstruction: 0.366661, Regularization: 0.000098, Discriminator: 0.043356; Generator: 0.021599,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:18,142 root         INFO     Train Epoch: 133 [7168/8000 (90%)]\tTotal Loss: 0.431746\n",
      "Reconstruction: 0.366675, Regularization: 0.000114, Discriminator: 0.043337; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:18,251 root         INFO     Train Epoch: 133 [7680/8000 (96%)]\tTotal Loss: 0.432343\n",
      "Reconstruction: 0.367302, Regularization: 0.000103, Discriminator: 0.043307; Generator: 0.021631,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:18,331 root         INFO     ====> Epoch: 133 Average loss: 0.4376\n",
      "2019-04-10 00:59:18,359 root         INFO     Train Epoch: 134 [0/8000 (0%)]\tTotal Loss: 0.432223\n",
      "Reconstruction: 0.367238, Regularization: 0.000101, Discriminator: 0.043317; Generator: 0.021568,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:18,471 root         INFO     Train Epoch: 134 [512/8000 (6%)]\tTotal Loss: 0.433904\n",
      "Reconstruction: 0.368903, Regularization: 0.000108, Discriminator: 0.043292; Generator: 0.021601,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:18,582 root         INFO     Train Epoch: 134 [1024/8000 (13%)]\tTotal Loss: 0.436083\n",
      "Reconstruction: 0.371046, Regularization: 0.000102, Discriminator: 0.043351; Generator: 0.021584,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:18,694 root         INFO     Train Epoch: 134 [1536/8000 (19%)]\tTotal Loss: 0.438435\n",
      "Reconstruction: 0.373472, Regularization: 0.000107, Discriminator: 0.043233; Generator: 0.021624,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:18,806 root         INFO     Train Epoch: 134 [2048/8000 (26%)]\tTotal Loss: 0.441540\n",
      "Reconstruction: 0.376514, Regularization: 0.000097, Discriminator: 0.043280; Generator: 0.021649,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:18,917 root         INFO     Train Epoch: 134 [2560/8000 (32%)]\tTotal Loss: 0.443737\n",
      "Reconstruction: 0.378621, Regularization: 0.000095, Discriminator: 0.043290; Generator: 0.021732,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:19,029 root         INFO     Train Epoch: 134 [3072/8000 (38%)]\tTotal Loss: 0.446095\n",
      "Reconstruction: 0.380847, Regularization: 0.000088, Discriminator: 0.043364; Generator: 0.021796,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 00:59:19,140 root         INFO     Train Epoch: 134 [3584/8000 (45%)]\tTotal Loss: 0.447503\n",
      "Reconstruction: 0.382280, Regularization: 0.000074, Discriminator: 0.043372; Generator: 0.021777,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 00:59:19,250 root         INFO     Train Epoch: 134 [4096/8000 (51%)]\tTotal Loss: 0.447956\n",
      "Reconstruction: 0.382814, Regularization: 0.000096, Discriminator: 0.043321; Generator: 0.021724,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:19,360 root         INFO     Train Epoch: 134 [4608/8000 (58%)]\tTotal Loss: 0.447776\n",
      "Reconstruction: 0.382678, Regularization: 0.000088, Discriminator: 0.043331; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:19,470 root         INFO     Train Epoch: 134 [5120/8000 (64%)]\tTotal Loss: 0.446424\n",
      "Reconstruction: 0.381378, Regularization: 0.000096, Discriminator: 0.043243; Generator: 0.021707,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 00:59:19,579 root         INFO     Train Epoch: 134 [5632/8000 (70%)]\tTotal Loss: 0.444351\n",
      "Reconstruction: 0.379320, Regularization: 0.000107, Discriminator: 0.043247; Generator: 0.021676,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:19,688 root         INFO     Train Epoch: 134 [6144/8000 (77%)]\tTotal Loss: 0.441498\n",
      "Reconstruction: 0.376449, Regularization: 0.000081, Discriminator: 0.043300; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:19,798 root         INFO     Train Epoch: 134 [6656/8000 (83%)]\tTotal Loss: 0.439377\n",
      "Reconstruction: 0.374192, Regularization: 0.000087, Discriminator: 0.043377; Generator: 0.021720,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:19,907 root         INFO     Train Epoch: 134 [7168/8000 (90%)]\tTotal Loss: 0.436516\n",
      "Reconstruction: 0.371447, Regularization: 0.000085, Discriminator: 0.043329; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:20,017 root         INFO     Train Epoch: 134 [7680/8000 (96%)]\tTotal Loss: 0.434844\n",
      "Reconstruction: 0.369754, Regularization: 0.000086, Discriminator: 0.043378; Generator: 0.021625,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:20,098 root         INFO     ====> Epoch: 134 Average loss: 0.4415\n",
      "2019-04-10 00:59:20,125 root         INFO     Train Epoch: 135 [0/8000 (0%)]\tTotal Loss: 0.433490\n",
      "Reconstruction: 0.368422, Regularization: 0.000106, Discriminator: 0.043296; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:20,237 root         INFO     Train Epoch: 135 [512/8000 (6%)]\tTotal Loss: 0.433246\n",
      "Reconstruction: 0.368163, Regularization: 0.000088, Discriminator: 0.043342; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:20,348 root         INFO     Train Epoch: 135 [1024/8000 (13%)]\tTotal Loss: 0.432950\n",
      "Reconstruction: 0.367947, Regularization: 0.000098, Discriminator: 0.043294; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:20,459 root         INFO     Train Epoch: 135 [1536/8000 (19%)]\tTotal Loss: 0.433519\n",
      "Reconstruction: 0.368563, Regularization: 0.000084, Discriminator: 0.043256; Generator: 0.021617,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:20,570 root         INFO     Train Epoch: 135 [2048/8000 (26%)]\tTotal Loss: 0.434146\n",
      "Reconstruction: 0.369194, Regularization: 0.000080, Discriminator: 0.043325; Generator: 0.021547,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:59:20,679 root         INFO     Train Epoch: 135 [2560/8000 (32%)]\tTotal Loss: 0.436516\n",
      "Reconstruction: 0.371560, Regularization: 0.000089, Discriminator: 0.043238; Generator: 0.021630,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:20,790 root         INFO     Train Epoch: 135 [3072/8000 (38%)]\tTotal Loss: 0.437249\n",
      "Reconstruction: 0.372213, Regularization: 0.000085, Discriminator: 0.043360; Generator: 0.021592,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:20,901 root         INFO     Train Epoch: 135 [3584/8000 (45%)]\tTotal Loss: 0.439197\n",
      "Reconstruction: 0.374038, Regularization: 0.000086, Discriminator: 0.043450; Generator: 0.021622,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 00:59:21,012 root         INFO     Train Epoch: 135 [4096/8000 (51%)]\tTotal Loss: 0.441650\n",
      "Reconstruction: 0.376493, Regularization: 0.000093, Discriminator: 0.043393; Generator: 0.021671,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:21,123 root         INFO     Train Epoch: 135 [4608/8000 (58%)]\tTotal Loss: 0.442685\n",
      "Reconstruction: 0.377505, Regularization: 0.000080, Discriminator: 0.043358; Generator: 0.021743,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:21,235 root         INFO     Train Epoch: 135 [5120/8000 (64%)]\tTotal Loss: 0.444135\n",
      "Reconstruction: 0.379016, Regularization: 0.000073, Discriminator: 0.043341; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:21,346 root         INFO     Train Epoch: 135 [5632/8000 (70%)]\tTotal Loss: 0.444056\n",
      "Reconstruction: 0.378992, Regularization: 0.000084, Discriminator: 0.043315; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:21,457 root         INFO     Train Epoch: 135 [6144/8000 (77%)]\tTotal Loss: 0.443463\n",
      "Reconstruction: 0.378483, Regularization: 0.000081, Discriminator: 0.043284; Generator: 0.021615,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:21,568 root         INFO     Train Epoch: 135 [6656/8000 (83%)]\tTotal Loss: 0.442930\n",
      "Reconstruction: 0.377939, Regularization: 0.000089, Discriminator: 0.043267; Generator: 0.021635,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:21,680 root         INFO     Train Epoch: 135 [7168/8000 (90%)]\tTotal Loss: 0.442347\n",
      "Reconstruction: 0.377337, Regularization: 0.000093, Discriminator: 0.043287; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:21,791 root         INFO     Train Epoch: 135 [7680/8000 (96%)]\tTotal Loss: 0.441166\n",
      "Reconstruction: 0.376064, Regularization: 0.000110, Discriminator: 0.043324; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:21,872 root         INFO     ====> Epoch: 135 Average loss: 0.4390\n",
      "2019-04-10 00:59:21,900 root         INFO     Train Epoch: 136 [0/8000 (0%)]\tTotal Loss: 0.439670\n",
      "Reconstruction: 0.374630, Regularization: 0.000096, Discriminator: 0.043247; Generator: 0.021698,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 00:59:22,011 root         INFO     Train Epoch: 136 [512/8000 (6%)]\tTotal Loss: 0.438620\n",
      "Reconstruction: 0.373405, Regularization: 0.000109, Discriminator: 0.043395; Generator: 0.021711,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:22,124 root         INFO     Train Epoch: 136 [1024/8000 (13%)]\tTotal Loss: 0.436372\n",
      "Reconstruction: 0.371147, Regularization: 0.000076, Discriminator: 0.043473; Generator: 0.021676,\n",
      "D(x): 0.497, D(G(z)): 0.500\n",
      "2019-04-10 00:59:22,236 root         INFO     Train Epoch: 136 [1536/8000 (19%)]\tTotal Loss: 0.435800\n",
      "Reconstruction: 0.370642, Regularization: 0.000093, Discriminator: 0.043369; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:22,349 root         INFO     Train Epoch: 136 [2048/8000 (26%)]\tTotal Loss: 0.434191\n",
      "Reconstruction: 0.369064, Regularization: 0.000087, Discriminator: 0.043356; Generator: 0.021684,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:22,461 root         INFO     Train Epoch: 136 [2560/8000 (32%)]\tTotal Loss: 0.435505\n",
      "Reconstruction: 0.370375, Regularization: 0.000099, Discriminator: 0.043340; Generator: 0.021691,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:22,574 root         INFO     Train Epoch: 136 [3072/8000 (38%)]\tTotal Loss: 0.435441\n",
      "Reconstruction: 0.370250, Regularization: 0.000107, Discriminator: 0.043319; Generator: 0.021766,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:22,686 root         INFO     Train Epoch: 136 [3584/8000 (45%)]\tTotal Loss: 0.435375\n",
      "Reconstruction: 0.370300, Regularization: 0.000093, Discriminator: 0.043298; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:22,799 root         INFO     Train Epoch: 136 [4096/8000 (51%)]\tTotal Loss: 0.436509\n",
      "Reconstruction: 0.371494, Regularization: 0.000086, Discriminator: 0.043270; Generator: 0.021658,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:22,912 root         INFO     Train Epoch: 136 [4608/8000 (58%)]\tTotal Loss: 0.437118\n",
      "Reconstruction: 0.372018, Regularization: 0.000103, Discriminator: 0.043287; Generator: 0.021709,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:23,024 root         INFO     Train Epoch: 136 [5120/8000 (64%)]\tTotal Loss: 0.438348\n",
      "Reconstruction: 0.373282, Regularization: 0.000090, Discriminator: 0.043281; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:23,136 root         INFO     Train Epoch: 136 [5632/8000 (70%)]\tTotal Loss: 0.439371\n",
      "Reconstruction: 0.374304, Regularization: 0.000093, Discriminator: 0.043304; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:23,248 root         INFO     Train Epoch: 136 [6144/8000 (77%)]\tTotal Loss: 0.440311\n",
      "Reconstruction: 0.375244, Regularization: 0.000099, Discriminator: 0.043352; Generator: 0.021616,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:23,358 root         INFO     Train Epoch: 136 [6656/8000 (83%)]\tTotal Loss: 0.442059\n",
      "Reconstruction: 0.376923, Regularization: 0.000091, Discriminator: 0.043358; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:23,465 root         INFO     Train Epoch: 136 [7168/8000 (90%)]\tTotal Loss: 0.442798\n",
      "Reconstruction: 0.377675, Regularization: 0.000082, Discriminator: 0.043350; Generator: 0.021691,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:23,572 root         INFO     Train Epoch: 136 [7680/8000 (96%)]\tTotal Loss: 0.444707\n",
      "Reconstruction: 0.379502, Regularization: 0.000101, Discriminator: 0.043394; Generator: 0.021709,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:23,651 root         INFO     ====> Epoch: 136 Average loss: 0.4384\n",
      "2019-04-10 00:59:23,678 root         INFO     Train Epoch: 137 [0/8000 (0%)]\tTotal Loss: 0.444662\n",
      "Reconstruction: 0.379502, Regularization: 0.000091, Discriminator: 0.043401; Generator: 0.021668,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:23,790 root         INFO     Train Epoch: 137 [512/8000 (6%)]\tTotal Loss: 0.445440\n",
      "Reconstruction: 0.380307, Regularization: 0.000083, Discriminator: 0.043356; Generator: 0.021694,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:23,902 root         INFO     Train Epoch: 137 [1024/8000 (13%)]\tTotal Loss: 0.444802\n",
      "Reconstruction: 0.379722, Regularization: 0.000098, Discriminator: 0.043309; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:24,013 root         INFO     Train Epoch: 137 [1536/8000 (19%)]\tTotal Loss: 0.443592\n",
      "Reconstruction: 0.378688, Regularization: 0.000092, Discriminator: 0.043218; Generator: 0.021594,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 00:59:24,125 root         INFO     Train Epoch: 137 [2048/8000 (26%)]\tTotal Loss: 0.442803\n",
      "Reconstruction: 0.377826, Regularization: 0.000116, Discriminator: 0.043244; Generator: 0.021617,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:24,236 root         INFO     Train Epoch: 137 [2560/8000 (32%)]\tTotal Loss: 0.441440\n",
      "Reconstruction: 0.376411, Regularization: 0.000105, Discriminator: 0.043225; Generator: 0.021699,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 00:59:24,348 root         INFO     Train Epoch: 137 [3072/8000 (38%)]\tTotal Loss: 0.438709\n",
      "Reconstruction: 0.373595, Regularization: 0.000151, Discriminator: 0.043315; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:24,459 root         INFO     Train Epoch: 137 [3584/8000 (45%)]\tTotal Loss: 0.436336\n",
      "Reconstruction: 0.371375, Regularization: 0.000137, Discriminator: 0.043227; Generator: 0.021596,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 00:59:24,570 root         INFO     Train Epoch: 137 [4096/8000 (51%)]\tTotal Loss: 0.434162\n",
      "Reconstruction: 0.369030, Regularization: 0.000158, Discriminator: 0.043390; Generator: 0.021584,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:24,679 root         INFO     Train Epoch: 137 [4608/8000 (58%)]\tTotal Loss: 0.431336\n",
      "Reconstruction: 0.366209, Regularization: 0.000159, Discriminator: 0.043394; Generator: 0.021573,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:24,789 root         INFO     Train Epoch: 137 [5120/8000 (64%)]\tTotal Loss: 0.430920\n",
      "Reconstruction: 0.365845, Regularization: 0.000148, Discriminator: 0.043338; Generator: 0.021589,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:24,898 root         INFO     Train Epoch: 137 [5632/8000 (70%)]\tTotal Loss: 0.430480\n",
      "Reconstruction: 0.365393, Regularization: 0.000155, Discriminator: 0.043309; Generator: 0.021623,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:25,006 root         INFO     Train Epoch: 137 [6144/8000 (77%)]\tTotal Loss: 0.430017\n",
      "Reconstruction: 0.364903, Regularization: 0.000171, Discriminator: 0.043305; Generator: 0.021637,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:25,115 root         INFO     Train Epoch: 137 [6656/8000 (83%)]\tTotal Loss: 0.431122\n",
      "Reconstruction: 0.366027, Regularization: 0.000154, Discriminator: 0.043324; Generator: 0.021617,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:25,225 root         INFO     Train Epoch: 137 [7168/8000 (90%)]\tTotal Loss: 0.432090\n",
      "Reconstruction: 0.367026, Regularization: 0.000154, Discriminator: 0.043293; Generator: 0.021617,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:25,335 root         INFO     Train Epoch: 137 [7680/8000 (96%)]\tTotal Loss: 0.434127\n",
      "Reconstruction: 0.369140, Regularization: 0.000169, Discriminator: 0.043149; Generator: 0.021669,\n",
      "D(x): 0.503, D(G(z)): 0.500\n",
      "2019-04-10 00:59:25,415 root         INFO     ====> Epoch: 137 Average loss: 0.4367\n",
      "2019-04-10 00:59:25,442 root         INFO     Train Epoch: 138 [0/8000 (0%)]\tTotal Loss: 0.435249\n",
      "Reconstruction: 0.370144, Regularization: 0.000162, Discriminator: 0.043262; Generator: 0.021682,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:25,555 root         INFO     Train Epoch: 138 [512/8000 (6%)]\tTotal Loss: 0.437328\n",
      "Reconstruction: 0.372233, Regularization: 0.000161, Discriminator: 0.043292; Generator: 0.021642,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:25,667 root         INFO     Train Epoch: 138 [1024/8000 (13%)]\tTotal Loss: 0.440482\n",
      "Reconstruction: 0.375275, Regularization: 0.000195, Discriminator: 0.043211; Generator: 0.021802,\n",
      "D(x): 0.500, D(G(z)): 0.498\n",
      "2019-04-10 00:59:25,779 root         INFO     Train Epoch: 138 [1536/8000 (19%)]\tTotal Loss: 0.442347\n",
      "Reconstruction: 0.377060, Regularization: 0.000176, Discriminator: 0.043415; Generator: 0.021696,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:25,892 root         INFO     Train Epoch: 138 [2048/8000 (26%)]\tTotal Loss: 0.445211\n",
      "Reconstruction: 0.379919, Regularization: 0.000194, Discriminator: 0.043374; Generator: 0.021724,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:26,004 root         INFO     Train Epoch: 138 [2560/8000 (32%)]\tTotal Loss: 0.446314\n",
      "Reconstruction: 0.380903, Regularization: 0.000188, Discriminator: 0.043470; Generator: 0.021754,\n",
      "D(x): 0.496, D(G(z)): 0.499\n",
      "2019-04-10 00:59:26,116 root         INFO     Train Epoch: 138 [3072/8000 (38%)]\tTotal Loss: 0.447132\n",
      "Reconstruction: 0.381905, Regularization: 0.000178, Discriminator: 0.043360; Generator: 0.021688,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:26,228 root         INFO     Train Epoch: 138 [3584/8000 (45%)]\tTotal Loss: 0.449570\n",
      "Reconstruction: 0.384265, Regularization: 0.000167, Discriminator: 0.043271; Generator: 0.021867,\n",
      "D(x): 0.498, D(G(z)): 0.497\n",
      "2019-04-10 00:59:26,340 root         INFO     Train Epoch: 138 [4096/8000 (51%)]\tTotal Loss: 0.450104\n",
      "Reconstruction: 0.384767, Regularization: 0.000185, Discriminator: 0.043362; Generator: 0.021791,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 00:59:26,452 root         INFO     Train Epoch: 138 [4608/8000 (58%)]\tTotal Loss: 0.449962\n",
      "Reconstruction: 0.384708, Regularization: 0.000165, Discriminator: 0.043294; Generator: 0.021795,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:26,565 root         INFO     Train Epoch: 138 [5120/8000 (64%)]\tTotal Loss: 0.447771\n",
      "Reconstruction: 0.382632, Regularization: 0.000193, Discriminator: 0.043251; Generator: 0.021695,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 00:59:26,676 root         INFO     Train Epoch: 138 [5632/8000 (70%)]\tTotal Loss: 0.445935\n",
      "Reconstruction: 0.380765, Regularization: 0.000218, Discriminator: 0.043314; Generator: 0.021638,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:26,789 root         INFO     Train Epoch: 138 [6144/8000 (77%)]\tTotal Loss: 0.445936\n",
      "Reconstruction: 0.380831, Regularization: 0.000217, Discriminator: 0.043182; Generator: 0.021705,\n",
      "D(x): 0.502, D(G(z)): 0.499\n",
      "2019-04-10 00:59:26,901 root         INFO     Train Epoch: 138 [6656/8000 (83%)]\tTotal Loss: 0.442080\n",
      "Reconstruction: 0.376963, Regularization: 0.000225, Discriminator: 0.043259; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:27,011 root         INFO     Train Epoch: 138 [7168/8000 (90%)]\tTotal Loss: 0.439350\n",
      "Reconstruction: 0.374277, Regularization: 0.000220, Discriminator: 0.043245; Generator: 0.021607,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:27,122 root         INFO     Train Epoch: 138 [7680/8000 (96%)]\tTotal Loss: 0.439285\n",
      "Reconstruction: 0.373984, Regularization: 0.000220, Discriminator: 0.043452; Generator: 0.021630,\n",
      "D(x): 0.498, D(G(z)): 0.501\n",
      "2019-04-10 00:59:27,203 root         INFO     ====> Epoch: 138 Average loss: 0.4440\n",
      "2019-04-10 00:59:27,230 root         INFO     Train Epoch: 139 [0/8000 (0%)]\tTotal Loss: 0.437349\n",
      "Reconstruction: 0.372131, Regularization: 0.000218, Discriminator: 0.043353; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:27,343 root         INFO     Train Epoch: 139 [512/8000 (6%)]\tTotal Loss: 0.434781\n",
      "Reconstruction: 0.369566, Regularization: 0.000209, Discriminator: 0.043392; Generator: 0.021614,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:27,453 root         INFO     Train Epoch: 139 [1024/8000 (13%)]\tTotal Loss: 0.432919\n",
      "Reconstruction: 0.367780, Regularization: 0.000199, Discriminator: 0.043328; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:27,565 root         INFO     Train Epoch: 139 [1536/8000 (19%)]\tTotal Loss: 0.432010\n",
      "Reconstruction: 0.366782, Regularization: 0.000219, Discriminator: 0.043394; Generator: 0.021615,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:27,675 root         INFO     Train Epoch: 139 [2048/8000 (26%)]\tTotal Loss: 0.430302\n",
      "Reconstruction: 0.365148, Regularization: 0.000232, Discriminator: 0.043342; Generator: 0.021581,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:27,786 root         INFO     Train Epoch: 139 [2560/8000 (32%)]\tTotal Loss: 0.430790\n",
      "Reconstruction: 0.365639, Regularization: 0.000220, Discriminator: 0.043324; Generator: 0.021607,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:27,897 root         INFO     Train Epoch: 139 [3072/8000 (38%)]\tTotal Loss: 0.431121\n",
      "Reconstruction: 0.365930, Regularization: 0.000219, Discriminator: 0.043321; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:28,008 root         INFO     Train Epoch: 139 [3584/8000 (45%)]\tTotal Loss: 0.430923\n",
      "Reconstruction: 0.365848, Regularization: 0.000210, Discriminator: 0.043278; Generator: 0.021587,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:28,119 root         INFO     Train Epoch: 139 [4096/8000 (51%)]\tTotal Loss: 0.432541\n",
      "Reconstruction: 0.367461, Regularization: 0.000243, Discriminator: 0.043255; Generator: 0.021581,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:28,230 root         INFO     Train Epoch: 139 [4608/8000 (58%)]\tTotal Loss: 0.432769\n",
      "Reconstruction: 0.367680, Regularization: 0.000244, Discriminator: 0.043227; Generator: 0.021618,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:28,341 root         INFO     Train Epoch: 139 [5120/8000 (64%)]\tTotal Loss: 0.434543\n",
      "Reconstruction: 0.369377, Regularization: 0.000218, Discriminator: 0.043334; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:28,452 root         INFO     Train Epoch: 139 [5632/8000 (70%)]\tTotal Loss: 0.435386\n",
      "Reconstruction: 0.370224, Regularization: 0.000222, Discriminator: 0.043325; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:28,562 root         INFO     Train Epoch: 139 [6144/8000 (77%)]\tTotal Loss: 0.436461\n",
      "Reconstruction: 0.371341, Regularization: 0.000196, Discriminator: 0.043341; Generator: 0.021583,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:28,671 root         INFO     Train Epoch: 139 [6656/8000 (83%)]\tTotal Loss: 0.439752\n",
      "Reconstruction: 0.374581, Regularization: 0.000192, Discriminator: 0.043279; Generator: 0.021700,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:28,780 root         INFO     Train Epoch: 139 [7168/8000 (90%)]\tTotal Loss: 0.440825\n",
      "Reconstruction: 0.375516, Regularization: 0.000218, Discriminator: 0.043392; Generator: 0.021700,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:28,888 root         INFO     Train Epoch: 139 [7680/8000 (96%)]\tTotal Loss: 0.443356\n",
      "Reconstruction: 0.378115, Regularization: 0.000216, Discriminator: 0.043348; Generator: 0.021678,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:28,969 root         INFO     ====> Epoch: 139 Average loss: 0.4348\n",
      "2019-04-10 00:59:28,997 root         INFO     Train Epoch: 140 [0/8000 (0%)]\tTotal Loss: 0.443613\n",
      "Reconstruction: 0.378323, Regularization: 0.000194, Discriminator: 0.043385; Generator: 0.021710,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:29,110 root         INFO     Train Epoch: 140 [512/8000 (6%)]\tTotal Loss: 0.445612\n",
      "Reconstruction: 0.380287, Regularization: 0.000195, Discriminator: 0.043417; Generator: 0.021712,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:29,223 root         INFO     Train Epoch: 140 [1024/8000 (13%)]\tTotal Loss: 0.446477\n",
      "Reconstruction: 0.381281, Regularization: 0.000194, Discriminator: 0.043358; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:29,334 root         INFO     Train Epoch: 140 [1536/8000 (19%)]\tTotal Loss: 0.446748\n",
      "Reconstruction: 0.381498, Regularization: 0.000194, Discriminator: 0.043305; Generator: 0.021751,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:29,446 root         INFO     Train Epoch: 140 [2048/8000 (26%)]\tTotal Loss: 0.446277\n",
      "Reconstruction: 0.381077, Regularization: 0.000199, Discriminator: 0.043306; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:29,558 root         INFO     Train Epoch: 140 [2560/8000 (32%)]\tTotal Loss: 0.446478\n",
      "Reconstruction: 0.381235, Regularization: 0.000204, Discriminator: 0.043249; Generator: 0.021789,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:59:29,670 root         INFO     Train Epoch: 140 [3072/8000 (38%)]\tTotal Loss: 0.445481\n",
      "Reconstruction: 0.380286, Regularization: 0.000195, Discriminator: 0.043298; Generator: 0.021702,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:29,782 root         INFO     Train Epoch: 140 [3584/8000 (45%)]\tTotal Loss: 0.443804\n",
      "Reconstruction: 0.378523, Regularization: 0.000202, Discriminator: 0.043401; Generator: 0.021678,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 00:59:29,894 root         INFO     Train Epoch: 140 [4096/8000 (51%)]\tTotal Loss: 0.442383\n",
      "Reconstruction: 0.377109, Regularization: 0.000205, Discriminator: 0.043342; Generator: 0.021727,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:30,006 root         INFO     Train Epoch: 140 [4608/8000 (58%)]\tTotal Loss: 0.441305\n",
      "Reconstruction: 0.376011, Regularization: 0.000214, Discriminator: 0.043357; Generator: 0.021723,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:30,117 root         INFO     Train Epoch: 140 [5120/8000 (64%)]\tTotal Loss: 0.439392\n",
      "Reconstruction: 0.374216, Regularization: 0.000226, Discriminator: 0.043277; Generator: 0.021673,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:30,229 root         INFO     Train Epoch: 140 [5632/8000 (70%)]\tTotal Loss: 0.437339\n",
      "Reconstruction: 0.372064, Regularization: 0.000263, Discriminator: 0.043321; Generator: 0.021692,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:30,341 root         INFO     Train Epoch: 140 [6144/8000 (77%)]\tTotal Loss: 0.437238\n",
      "Reconstruction: 0.371940, Regularization: 0.000265, Discriminator: 0.043323; Generator: 0.021710,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:30,453 root         INFO     Train Epoch: 140 [6656/8000 (83%)]\tTotal Loss: 0.436684\n",
      "Reconstruction: 0.371445, Regularization: 0.000269, Discriminator: 0.043325; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:30,565 root         INFO     Train Epoch: 140 [7168/8000 (90%)]\tTotal Loss: 0.435385\n",
      "Reconstruction: 0.370137, Regularization: 0.000265, Discriminator: 0.043324; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:30,677 root         INFO     Train Epoch: 140 [7680/8000 (96%)]\tTotal Loss: 0.436072\n",
      "Reconstruction: 0.370829, Regularization: 0.000259, Discriminator: 0.043298; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:30,758 root         INFO     ====> Epoch: 140 Average loss: 0.4417\n",
      "2019-04-10 00:59:30,785 root         INFO     Train Epoch: 141 [0/8000 (0%)]\tTotal Loss: 0.436038\n",
      "Reconstruction: 0.370806, Regularization: 0.000265, Discriminator: 0.043301; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:30,899 root         INFO     Train Epoch: 141 [512/8000 (6%)]\tTotal Loss: 0.434349\n",
      "Reconstruction: 0.369169, Regularization: 0.000274, Discriminator: 0.043332; Generator: 0.021574,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:31,012 root         INFO     Train Epoch: 141 [1024/8000 (13%)]\tTotal Loss: 0.435693\n",
      "Reconstruction: 0.370510, Regularization: 0.000280, Discriminator: 0.043298; Generator: 0.021605,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:31,124 root         INFO     Train Epoch: 141 [1536/8000 (19%)]\tTotal Loss: 0.435679\n",
      "Reconstruction: 0.370475, Regularization: 0.000273, Discriminator: 0.043304; Generator: 0.021627,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:31,237 root         INFO     Train Epoch: 141 [2048/8000 (26%)]\tTotal Loss: 0.436872\n",
      "Reconstruction: 0.371596, Regularization: 0.000263, Discriminator: 0.043327; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:31,349 root         INFO     Train Epoch: 141 [2560/8000 (32%)]\tTotal Loss: 0.439476\n",
      "Reconstruction: 0.374178, Regularization: 0.000272, Discriminator: 0.043349; Generator: 0.021678,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:31,462 root         INFO     Train Epoch: 141 [3072/8000 (38%)]\tTotal Loss: 0.439467\n",
      "Reconstruction: 0.374247, Regularization: 0.000262, Discriminator: 0.043320; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:31,575 root         INFO     Train Epoch: 141 [3584/8000 (45%)]\tTotal Loss: 0.440048\n",
      "Reconstruction: 0.374750, Regularization: 0.000255, Discriminator: 0.043364; Generator: 0.021679,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:31,687 root         INFO     Train Epoch: 141 [4096/8000 (51%)]\tTotal Loss: 0.441564\n",
      "Reconstruction: 0.376295, Regularization: 0.000276, Discriminator: 0.043328; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:31,800 root         INFO     Train Epoch: 141 [4608/8000 (58%)]\tTotal Loss: 0.440528\n",
      "Reconstruction: 0.375265, Regularization: 0.000293, Discriminator: 0.043317; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:31,912 root         INFO     Train Epoch: 141 [5120/8000 (64%)]\tTotal Loss: 0.440455\n",
      "Reconstruction: 0.375209, Regularization: 0.000268, Discriminator: 0.043318; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:32,025 root         INFO     Train Epoch: 141 [5632/8000 (70%)]\tTotal Loss: 0.440876\n",
      "Reconstruction: 0.375665, Regularization: 0.000279, Discriminator: 0.043314; Generator: 0.021618,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:32,138 root         INFO     Train Epoch: 141 [6144/8000 (77%)]\tTotal Loss: 0.438674\n",
      "Reconstruction: 0.373525, Regularization: 0.000254, Discriminator: 0.043283; Generator: 0.021611,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:32,250 root         INFO     Train Epoch: 141 [6656/8000 (83%)]\tTotal Loss: 0.439140\n",
      "Reconstruction: 0.373944, Regularization: 0.000242, Discriminator: 0.043354; Generator: 0.021601,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:32,363 root         INFO     Train Epoch: 141 [7168/8000 (90%)]\tTotal Loss: 0.438157\n",
      "Reconstruction: 0.372937, Regularization: 0.000265, Discriminator: 0.043321; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:32,475 root         INFO     Train Epoch: 141 [7680/8000 (96%)]\tTotal Loss: 0.437735\n",
      "Reconstruction: 0.372488, Regularization: 0.000282, Discriminator: 0.043316; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:32,557 root         INFO     ====> Epoch: 141 Average loss: 0.4385\n",
      "2019-04-10 00:59:32,584 root         INFO     Train Epoch: 142 [0/8000 (0%)]\tTotal Loss: 0.437705\n",
      "Reconstruction: 0.372412, Regularization: 0.000266, Discriminator: 0.043358; Generator: 0.021669,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:32,696 root         INFO     Train Epoch: 142 [512/8000 (6%)]\tTotal Loss: 0.437340\n",
      "Reconstruction: 0.372128, Regularization: 0.000280, Discriminator: 0.043298; Generator: 0.021634,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:32,807 root         INFO     Train Epoch: 142 [1024/8000 (13%)]\tTotal Loss: 0.435985\n",
      "Reconstruction: 0.370718, Regularization: 0.000268, Discriminator: 0.043331; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:32,918 root         INFO     Train Epoch: 142 [1536/8000 (19%)]\tTotal Loss: 0.435782\n",
      "Reconstruction: 0.370498, Regularization: 0.000266, Discriminator: 0.043333; Generator: 0.021685,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:33,029 root         INFO     Train Epoch: 142 [2048/8000 (26%)]\tTotal Loss: 0.436648\n",
      "Reconstruction: 0.371310, Regularization: 0.000273, Discriminator: 0.043350; Generator: 0.021715,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:33,140 root         INFO     Train Epoch: 142 [2560/8000 (32%)]\tTotal Loss: 0.436487\n",
      "Reconstruction: 0.371248, Regularization: 0.000249, Discriminator: 0.043325; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:33,252 root         INFO     Train Epoch: 142 [3072/8000 (38%)]\tTotal Loss: 0.438438\n",
      "Reconstruction: 0.373223, Regularization: 0.000254, Discriminator: 0.043303; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:33,363 root         INFO     Train Epoch: 142 [3584/8000 (45%)]\tTotal Loss: 0.439471\n",
      "Reconstruction: 0.374136, Regularization: 0.000269, Discriminator: 0.043333; Generator: 0.021733,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:33,475 root         INFO     Train Epoch: 142 [4096/8000 (51%)]\tTotal Loss: 0.440682\n",
      "Reconstruction: 0.375450, Regularization: 0.000280, Discriminator: 0.043266; Generator: 0.021685,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:33,587 root         INFO     Train Epoch: 142 [4608/8000 (58%)]\tTotal Loss: 0.441263\n",
      "Reconstruction: 0.375993, Regularization: 0.000269, Discriminator: 0.043309; Generator: 0.021691,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:33,698 root         INFO     Train Epoch: 142 [5120/8000 (64%)]\tTotal Loss: 0.441415\n",
      "Reconstruction: 0.376114, Regularization: 0.000306, Discriminator: 0.043302; Generator: 0.021693,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:33,809 root         INFO     Train Epoch: 142 [5632/8000 (70%)]\tTotal Loss: 0.442790\n",
      "Reconstruction: 0.377485, Regularization: 0.000286, Discriminator: 0.043330; Generator: 0.021689,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:33,921 root         INFO     Train Epoch: 142 [6144/8000 (77%)]\tTotal Loss: 0.444207\n",
      "Reconstruction: 0.378914, Regularization: 0.000294, Discriminator: 0.043302; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:34,032 root         INFO     Train Epoch: 142 [6656/8000 (83%)]\tTotal Loss: 0.446030\n",
      "Reconstruction: 0.380621, Regularization: 0.000300, Discriminator: 0.043370; Generator: 0.021739,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:34,143 root         INFO     Train Epoch: 142 [7168/8000 (90%)]\tTotal Loss: 0.445050\n",
      "Reconstruction: 0.379663, Regularization: 0.000285, Discriminator: 0.043337; Generator: 0.021765,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:34,255 root         INFO     Train Epoch: 142 [7680/8000 (96%)]\tTotal Loss: 0.444904\n",
      "Reconstruction: 0.379512, Regularization: 0.000305, Discriminator: 0.043354; Generator: 0.021733,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:34,336 root         INFO     ====> Epoch: 142 Average loss: 0.4403\n",
      "2019-04-10 00:59:34,363 root         INFO     Train Epoch: 143 [0/8000 (0%)]\tTotal Loss: 0.443332\n",
      "Reconstruction: 0.377998, Regularization: 0.000287, Discriminator: 0.043330; Generator: 0.021718,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:34,477 root         INFO     Train Epoch: 143 [512/8000 (6%)]\tTotal Loss: 0.444017\n",
      "Reconstruction: 0.378761, Regularization: 0.000257, Discriminator: 0.043349; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:34,589 root         INFO     Train Epoch: 143 [1024/8000 (13%)]\tTotal Loss: 0.442004\n",
      "Reconstruction: 0.376742, Regularization: 0.000242, Discriminator: 0.043346; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:34,702 root         INFO     Train Epoch: 143 [1536/8000 (19%)]\tTotal Loss: 0.440764\n",
      "Reconstruction: 0.375529, Regularization: 0.000254, Discriminator: 0.043345; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:34,813 root         INFO     Train Epoch: 143 [2048/8000 (26%)]\tTotal Loss: 0.439344\n",
      "Reconstruction: 0.374096, Regularization: 0.000268, Discriminator: 0.043363; Generator: 0.021618,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:34,923 root         INFO     Train Epoch: 143 [2560/8000 (32%)]\tTotal Loss: 0.437991\n",
      "Reconstruction: 0.372740, Regularization: 0.000273, Discriminator: 0.043303; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:35,034 root         INFO     Train Epoch: 143 [3072/8000 (38%)]\tTotal Loss: 0.435749\n",
      "Reconstruction: 0.370482, Regularization: 0.000276, Discriminator: 0.043406; Generator: 0.021585,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:35,145 root         INFO     Train Epoch: 143 [3584/8000 (45%)]\tTotal Loss: 0.434510\n",
      "Reconstruction: 0.369403, Regularization: 0.000268, Discriminator: 0.043266; Generator: 0.021573,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:35,255 root         INFO     Train Epoch: 143 [4096/8000 (51%)]\tTotal Loss: 0.434087\n",
      "Reconstruction: 0.368844, Regularization: 0.000255, Discriminator: 0.043332; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:35,366 root         INFO     Train Epoch: 143 [4608/8000 (58%)]\tTotal Loss: 0.432510\n",
      "Reconstruction: 0.367313, Regularization: 0.000251, Discriminator: 0.043331; Generator: 0.021615,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:35,477 root         INFO     Train Epoch: 143 [5120/8000 (64%)]\tTotal Loss: 0.432708\n",
      "Reconstruction: 0.367528, Regularization: 0.000257, Discriminator: 0.043316; Generator: 0.021607,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:35,587 root         INFO     Train Epoch: 143 [5632/8000 (70%)]\tTotal Loss: 0.431772\n",
      "Reconstruction: 0.366563, Regularization: 0.000252, Discriminator: 0.043331; Generator: 0.021626,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:35,698 root         INFO     Train Epoch: 143 [6144/8000 (77%)]\tTotal Loss: 0.432815\n",
      "Reconstruction: 0.367601, Regularization: 0.000264, Discriminator: 0.043361; Generator: 0.021589,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:35,808 root         INFO     Train Epoch: 143 [6656/8000 (83%)]\tTotal Loss: 0.432552\n",
      "Reconstruction: 0.367279, Regularization: 0.000248, Discriminator: 0.043365; Generator: 0.021660,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:35,918 root         INFO     Train Epoch: 143 [7168/8000 (90%)]\tTotal Loss: 0.433263\n",
      "Reconstruction: 0.368117, Regularization: 0.000226, Discriminator: 0.043294; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:36,028 root         INFO     Train Epoch: 143 [7680/8000 (96%)]\tTotal Loss: 0.435147\n",
      "Reconstruction: 0.369881, Regularization: 0.000237, Discriminator: 0.043355; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:36,107 root         INFO     ====> Epoch: 143 Average loss: 0.4362\n",
      "2019-04-10 00:59:36,135 root         INFO     Train Epoch: 144 [0/8000 (0%)]\tTotal Loss: 0.436901\n",
      "Reconstruction: 0.371692, Regularization: 0.000223, Discriminator: 0.043279; Generator: 0.021708,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:36,247 root         INFO     Train Epoch: 144 [512/8000 (6%)]\tTotal Loss: 0.437159\n",
      "Reconstruction: 0.371967, Regularization: 0.000242, Discriminator: 0.043319; Generator: 0.021630,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:36,358 root         INFO     Train Epoch: 144 [1024/8000 (13%)]\tTotal Loss: 0.438622\n",
      "Reconstruction: 0.373408, Regularization: 0.000237, Discriminator: 0.043302; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:36,468 root         INFO     Train Epoch: 144 [1536/8000 (19%)]\tTotal Loss: 0.441323\n",
      "Reconstruction: 0.376050, Regularization: 0.000255, Discriminator: 0.043352; Generator: 0.021667,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:36,579 root         INFO     Train Epoch: 144 [2048/8000 (26%)]\tTotal Loss: 0.442188\n",
      "Reconstruction: 0.376911, Regularization: 0.000239, Discriminator: 0.043346; Generator: 0.021692,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:36,690 root         INFO     Train Epoch: 144 [2560/8000 (32%)]\tTotal Loss: 0.443937\n",
      "Reconstruction: 0.378775, Regularization: 0.000231, Discriminator: 0.043287; Generator: 0.021644,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:36,800 root         INFO     Train Epoch: 144 [3072/8000 (38%)]\tTotal Loss: 0.444792\n",
      "Reconstruction: 0.379522, Regularization: 0.000234, Discriminator: 0.043349; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:36,911 root         INFO     Train Epoch: 144 [3584/8000 (45%)]\tTotal Loss: 0.445706\n",
      "Reconstruction: 0.380457, Regularization: 0.000223, Discriminator: 0.043336; Generator: 0.021690,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:37,023 root         INFO     Train Epoch: 144 [4096/8000 (51%)]\tTotal Loss: 0.446736\n",
      "Reconstruction: 0.381387, Regularization: 0.000252, Discriminator: 0.043363; Generator: 0.021734,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:37,133 root         INFO     Train Epoch: 144 [4608/8000 (58%)]\tTotal Loss: 0.447881\n",
      "Reconstruction: 0.382621, Regularization: 0.000226, Discriminator: 0.043327; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:37,240 root         INFO     Train Epoch: 144 [5120/8000 (64%)]\tTotal Loss: 0.446781\n",
      "Reconstruction: 0.381535, Regularization: 0.000231, Discriminator: 0.043292; Generator: 0.021723,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:37,348 root         INFO     Train Epoch: 144 [5632/8000 (70%)]\tTotal Loss: 0.446144\n",
      "Reconstruction: 0.380939, Regularization: 0.000225, Discriminator: 0.043293; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:37,455 root         INFO     Train Epoch: 144 [6144/8000 (77%)]\tTotal Loss: 0.443337\n",
      "Reconstruction: 0.378093, Regularization: 0.000205, Discriminator: 0.043308; Generator: 0.021731,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:37,563 root         INFO     Train Epoch: 144 [6656/8000 (83%)]\tTotal Loss: 0.441576\n",
      "Reconstruction: 0.376446, Regularization: 0.000213, Discriminator: 0.043218; Generator: 0.021699,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 00:59:37,670 root         INFO     Train Epoch: 144 [7168/8000 (90%)]\tTotal Loss: 0.438459\n",
      "Reconstruction: 0.373397, Regularization: 0.000176, Discriminator: 0.043224; Generator: 0.021661,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:37,778 root         INFO     Train Epoch: 144 [7680/8000 (96%)]\tTotal Loss: 0.437679\n",
      "Reconstruction: 0.372368, Regularization: 0.000189, Discriminator: 0.043470; Generator: 0.021651,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 00:59:37,857 root         INFO     ====> Epoch: 144 Average loss: 0.4427\n",
      "2019-04-10 00:59:37,884 root         INFO     Train Epoch: 145 [0/8000 (0%)]\tTotal Loss: 0.435868\n",
      "Reconstruction: 0.370677, Regularization: 0.000201, Discriminator: 0.043340; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:37,995 root         INFO     Train Epoch: 145 [512/8000 (6%)]\tTotal Loss: 0.434429\n",
      "Reconstruction: 0.369237, Regularization: 0.000200, Discriminator: 0.043356; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:38,107 root         INFO     Train Epoch: 145 [1024/8000 (13%)]\tTotal Loss: 0.431739\n",
      "Reconstruction: 0.366600, Regularization: 0.000177, Discriminator: 0.043340; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:38,217 root         INFO     Train Epoch: 145 [1536/8000 (19%)]\tTotal Loss: 0.432946\n",
      "Reconstruction: 0.367841, Regularization: 0.000166, Discriminator: 0.043327; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:38,328 root         INFO     Train Epoch: 145 [2048/8000 (26%)]\tTotal Loss: 0.432142\n",
      "Reconstruction: 0.367002, Regularization: 0.000178, Discriminator: 0.043343; Generator: 0.021619,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:38,439 root         INFO     Train Epoch: 145 [2560/8000 (32%)]\tTotal Loss: 0.432672\n",
      "Reconstruction: 0.367563, Regularization: 0.000161, Discriminator: 0.043296; Generator: 0.021651,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:38,549 root         INFO     Train Epoch: 145 [3072/8000 (38%)]\tTotal Loss: 0.433030\n",
      "Reconstruction: 0.367968, Regularization: 0.000169, Discriminator: 0.043295; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:38,656 root         INFO     Train Epoch: 145 [3584/8000 (45%)]\tTotal Loss: 0.433414\n",
      "Reconstruction: 0.368242, Regularization: 0.000177, Discriminator: 0.043358; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:38,764 root         INFO     Train Epoch: 145 [4096/8000 (51%)]\tTotal Loss: 0.434694\n",
      "Reconstruction: 0.369631, Regularization: 0.000169, Discriminator: 0.043342; Generator: 0.021552,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:59:38,873 root         INFO     Train Epoch: 145 [4608/8000 (58%)]\tTotal Loss: 0.437166\n",
      "Reconstruction: 0.372059, Regularization: 0.000175, Discriminator: 0.043299; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:38,982 root         INFO     Train Epoch: 145 [5120/8000 (64%)]\tTotal Loss: 0.437068\n",
      "Reconstruction: 0.371895, Regularization: 0.000184, Discriminator: 0.043390; Generator: 0.021600,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:39,091 root         INFO     Train Epoch: 145 [5632/8000 (70%)]\tTotal Loss: 0.439144\n",
      "Reconstruction: 0.373920, Regularization: 0.000187, Discriminator: 0.043413; Generator: 0.021624,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 00:59:39,200 root         INFO     Train Epoch: 145 [6144/8000 (77%)]\tTotal Loss: 0.439625\n",
      "Reconstruction: 0.374511, Regularization: 0.000195, Discriminator: 0.043296; Generator: 0.021622,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:39,308 root         INFO     Train Epoch: 145 [6656/8000 (83%)]\tTotal Loss: 0.441010\n",
      "Reconstruction: 0.375875, Regularization: 0.000180, Discriminator: 0.043298; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:39,417 root         INFO     Train Epoch: 145 [7168/8000 (90%)]\tTotal Loss: 0.442387\n",
      "Reconstruction: 0.377206, Regularization: 0.000174, Discriminator: 0.043360; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:39,526 root         INFO     Train Epoch: 145 [7680/8000 (96%)]\tTotal Loss: 0.443773\n",
      "Reconstruction: 0.378590, Regularization: 0.000175, Discriminator: 0.043331; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:39,605 root         INFO     ====> Epoch: 145 Average loss: 0.4361\n",
      "2019-04-10 00:59:39,632 root         INFO     Train Epoch: 146 [0/8000 (0%)]\tTotal Loss: 0.443038\n",
      "Reconstruction: 0.377863, Regularization: 0.000173, Discriminator: 0.043314; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:39,746 root         INFO     Train Epoch: 146 [512/8000 (6%)]\tTotal Loss: 0.441842\n",
      "Reconstruction: 0.376681, Regularization: 0.000181, Discriminator: 0.043290; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:39,859 root         INFO     Train Epoch: 146 [1024/8000 (13%)]\tTotal Loss: 0.442731\n",
      "Reconstruction: 0.377523, Regularization: 0.000171, Discriminator: 0.043317; Generator: 0.021720,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:39,971 root         INFO     Train Epoch: 146 [1536/8000 (19%)]\tTotal Loss: 0.442264\n",
      "Reconstruction: 0.377087, Regularization: 0.000180, Discriminator: 0.043321; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:40,084 root         INFO     Train Epoch: 146 [2048/8000 (26%)]\tTotal Loss: 0.440940\n",
      "Reconstruction: 0.375720, Regularization: 0.000173, Discriminator: 0.043338; Generator: 0.021709,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:40,196 root         INFO     Train Epoch: 146 [2560/8000 (32%)]\tTotal Loss: 0.440039\n",
      "Reconstruction: 0.374816, Regularization: 0.000180, Discriminator: 0.043384; Generator: 0.021660,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:40,308 root         INFO     Train Epoch: 146 [3072/8000 (38%)]\tTotal Loss: 0.438855\n",
      "Reconstruction: 0.373648, Regularization: 0.000177, Discriminator: 0.043332; Generator: 0.021698,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:40,418 root         INFO     Train Epoch: 146 [3584/8000 (45%)]\tTotal Loss: 0.438308\n",
      "Reconstruction: 0.373057, Regularization: 0.000186, Discriminator: 0.043337; Generator: 0.021727,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:40,529 root         INFO     Train Epoch: 146 [4096/8000 (51%)]\tTotal Loss: 0.438897\n",
      "Reconstruction: 0.373717, Regularization: 0.000199, Discriminator: 0.043333; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:40,640 root         INFO     Train Epoch: 146 [4608/8000 (58%)]\tTotal Loss: 0.441032\n",
      "Reconstruction: 0.375896, Regularization: 0.000183, Discriminator: 0.043307; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:40,751 root         INFO     Train Epoch: 146 [5120/8000 (64%)]\tTotal Loss: 0.439727\n",
      "Reconstruction: 0.374532, Regularization: 0.000183, Discriminator: 0.043317; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:40,861 root         INFO     Train Epoch: 146 [5632/8000 (70%)]\tTotal Loss: 0.438700\n",
      "Reconstruction: 0.373547, Regularization: 0.000174, Discriminator: 0.043287; Generator: 0.021691,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:40,971 root         INFO     Train Epoch: 146 [6144/8000 (77%)]\tTotal Loss: 0.441286\n",
      "Reconstruction: 0.376156, Regularization: 0.000182, Discriminator: 0.043268; Generator: 0.021680,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:41,082 root         INFO     Train Epoch: 146 [6656/8000 (83%)]\tTotal Loss: 0.441630\n",
      "Reconstruction: 0.376498, Regularization: 0.000170, Discriminator: 0.043287; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:41,193 root         INFO     Train Epoch: 146 [7168/8000 (90%)]\tTotal Loss: 0.442586\n",
      "Reconstruction: 0.377464, Regularization: 0.000193, Discriminator: 0.043261; Generator: 0.021668,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:41,303 root         INFO     Train Epoch: 146 [7680/8000 (96%)]\tTotal Loss: 0.443619\n",
      "Reconstruction: 0.378452, Regularization: 0.000173, Discriminator: 0.043288; Generator: 0.021706,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:41,384 root         INFO     ====> Epoch: 146 Average loss: 0.4409\n",
      "2019-04-10 00:59:41,411 root         INFO     Train Epoch: 147 [0/8000 (0%)]\tTotal Loss: 0.441853\n",
      "Reconstruction: 0.376605, Regularization: 0.000180, Discriminator: 0.043314; Generator: 0.021754,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:41,523 root         INFO     Train Epoch: 147 [512/8000 (6%)]\tTotal Loss: 0.442174\n",
      "Reconstruction: 0.376968, Regularization: 0.000181, Discriminator: 0.043351; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:41,635 root         INFO     Train Epoch: 147 [1024/8000 (13%)]\tTotal Loss: 0.442254\n",
      "Reconstruction: 0.377099, Regularization: 0.000171, Discriminator: 0.043338; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:41,747 root         INFO     Train Epoch: 147 [1536/8000 (19%)]\tTotal Loss: 0.442339\n",
      "Reconstruction: 0.377172, Regularization: 0.000162, Discriminator: 0.043360; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:41,858 root         INFO     Train Epoch: 147 [2048/8000 (26%)]\tTotal Loss: 0.440174\n",
      "Reconstruction: 0.375020, Regularization: 0.000190, Discriminator: 0.043305; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:41,966 root         INFO     Train Epoch: 147 [2560/8000 (32%)]\tTotal Loss: 0.438862\n",
      "Reconstruction: 0.373742, Regularization: 0.000182, Discriminator: 0.043312; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:42,075 root         INFO     Train Epoch: 147 [3072/8000 (38%)]\tTotal Loss: 0.437797\n",
      "Reconstruction: 0.372688, Regularization: 0.000215, Discriminator: 0.043259; Generator: 0.021635,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:42,184 root         INFO     Train Epoch: 147 [3584/8000 (45%)]\tTotal Loss: 0.436736\n",
      "Reconstruction: 0.371589, Regularization: 0.000215, Discriminator: 0.043320; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:42,293 root         INFO     Train Epoch: 147 [4096/8000 (51%)]\tTotal Loss: 0.435090\n",
      "Reconstruction: 0.369864, Regularization: 0.000217, Discriminator: 0.043418; Generator: 0.021591,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:42,403 root         INFO     Train Epoch: 147 [4608/8000 (58%)]\tTotal Loss: 0.432833\n",
      "Reconstruction: 0.367608, Regularization: 0.000231, Discriminator: 0.043352; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:42,513 root         INFO     Train Epoch: 147 [5120/8000 (64%)]\tTotal Loss: 0.430390\n",
      "Reconstruction: 0.365207, Regularization: 0.000228, Discriminator: 0.043322; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:42,623 root         INFO     Train Epoch: 147 [5632/8000 (70%)]\tTotal Loss: 0.431735\n",
      "Reconstruction: 0.366495, Regularization: 0.000227, Discriminator: 0.043370; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:42,734 root         INFO     Train Epoch: 147 [6144/8000 (77%)]\tTotal Loss: 0.430690\n",
      "Reconstruction: 0.365570, Regularization: 0.000219, Discriminator: 0.043319; Generator: 0.021583,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:42,845 root         INFO     Train Epoch: 147 [6656/8000 (83%)]\tTotal Loss: 0.431296\n",
      "Reconstruction: 0.366153, Regularization: 0.000210, Discriminator: 0.043324; Generator: 0.021609,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:42,956 root         INFO     Train Epoch: 147 [7168/8000 (90%)]\tTotal Loss: 0.432020\n",
      "Reconstruction: 0.366941, Regularization: 0.000218, Discriminator: 0.043261; Generator: 0.021601,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:59:43,067 root         INFO     Train Epoch: 147 [7680/8000 (96%)]\tTotal Loss: 0.433852\n",
      "Reconstruction: 0.368586, Regularization: 0.000210, Discriminator: 0.043372; Generator: 0.021684,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:43,148 root         INFO     ====> Epoch: 147 Average loss: 0.4363\n",
      "2019-04-10 00:59:43,175 root         INFO     Train Epoch: 148 [0/8000 (0%)]\tTotal Loss: 0.434651\n",
      "Reconstruction: 0.369405, Regularization: 0.000224, Discriminator: 0.043345; Generator: 0.021676,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:43,289 root         INFO     Train Epoch: 148 [512/8000 (6%)]\tTotal Loss: 0.435406\n",
      "Reconstruction: 0.370263, Regularization: 0.000211, Discriminator: 0.043308; Generator: 0.021624,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:43,402 root         INFO     Train Epoch: 148 [1024/8000 (13%)]\tTotal Loss: 0.436404\n",
      "Reconstruction: 0.371250, Regularization: 0.000231, Discriminator: 0.043273; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:43,516 root         INFO     Train Epoch: 148 [1536/8000 (19%)]\tTotal Loss: 0.439179\n",
      "Reconstruction: 0.373993, Regularization: 0.000198, Discriminator: 0.043342; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:43,628 root         INFO     Train Epoch: 148 [2048/8000 (26%)]\tTotal Loss: 0.442078\n",
      "Reconstruction: 0.376756, Regularization: 0.000209, Discriminator: 0.043396; Generator: 0.021718,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:43,740 root         INFO     Train Epoch: 148 [2560/8000 (32%)]\tTotal Loss: 0.442483\n",
      "Reconstruction: 0.377198, Regularization: 0.000177, Discriminator: 0.043352; Generator: 0.021755,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:59:43,852 root         INFO     Train Epoch: 148 [3072/8000 (38%)]\tTotal Loss: 0.445353\n",
      "Reconstruction: 0.380127, Regularization: 0.000182, Discriminator: 0.043356; Generator: 0.021689,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:43,965 root         INFO     Train Epoch: 148 [3584/8000 (45%)]\tTotal Loss: 0.445258\n",
      "Reconstruction: 0.379967, Regularization: 0.000186, Discriminator: 0.043366; Generator: 0.021739,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:44,078 root         INFO     Train Epoch: 148 [4096/8000 (51%)]\tTotal Loss: 0.445546\n",
      "Reconstruction: 0.380332, Regularization: 0.000182, Discriminator: 0.043320; Generator: 0.021712,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:44,188 root         INFO     Train Epoch: 148 [4608/8000 (58%)]\tTotal Loss: 0.446016\n",
      "Reconstruction: 0.380823, Regularization: 0.000179, Discriminator: 0.043330; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:44,297 root         INFO     Train Epoch: 148 [5120/8000 (64%)]\tTotal Loss: 0.447020\n",
      "Reconstruction: 0.381833, Regularization: 0.000182, Discriminator: 0.043320; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:44,407 root         INFO     Train Epoch: 148 [5632/8000 (70%)]\tTotal Loss: 0.446846\n",
      "Reconstruction: 0.381605, Regularization: 0.000195, Discriminator: 0.043301; Generator: 0.021744,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:44,517 root         INFO     Train Epoch: 148 [6144/8000 (77%)]\tTotal Loss: 0.446879\n",
      "Reconstruction: 0.381670, Regularization: 0.000191, Discriminator: 0.043304; Generator: 0.021714,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:44,627 root         INFO     Train Epoch: 148 [6656/8000 (83%)]\tTotal Loss: 0.444369\n",
      "Reconstruction: 0.379168, Regularization: 0.000189, Discriminator: 0.043313; Generator: 0.021699,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:44,736 root         INFO     Train Epoch: 148 [7168/8000 (90%)]\tTotal Loss: 0.443176\n",
      "Reconstruction: 0.377864, Regularization: 0.000166, Discriminator: 0.043468; Generator: 0.021678,\n",
      "D(x): 0.497, D(G(z)): 0.500\n",
      "2019-04-10 00:59:44,845 root         INFO     Train Epoch: 148 [7680/8000 (96%)]\tTotal Loss: 0.441478\n",
      "Reconstruction: 0.376389, Regularization: 0.000159, Discriminator: 0.043269; Generator: 0.021661,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:44,926 root         INFO     ====> Epoch: 148 Average loss: 0.4427\n",
      "2019-04-10 00:59:44,953 root         INFO     Train Epoch: 149 [0/8000 (0%)]\tTotal Loss: 0.440653\n",
      "Reconstruction: 0.375492, Regularization: 0.000153, Discriminator: 0.043336; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:45,064 root         INFO     Train Epoch: 149 [512/8000 (6%)]\tTotal Loss: 0.438128\n",
      "Reconstruction: 0.373025, Regularization: 0.000143, Discriminator: 0.043289; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:45,174 root         INFO     Train Epoch: 149 [1024/8000 (13%)]\tTotal Loss: 0.437910\n",
      "Reconstruction: 0.372791, Regularization: 0.000139, Discriminator: 0.043321; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:45,284 root         INFO     Train Epoch: 149 [1536/8000 (19%)]\tTotal Loss: 0.438078\n",
      "Reconstruction: 0.372969, Regularization: 0.000149, Discriminator: 0.043310; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:45,395 root         INFO     Train Epoch: 149 [2048/8000 (26%)]\tTotal Loss: 0.436504\n",
      "Reconstruction: 0.371351, Regularization: 0.000146, Discriminator: 0.043354; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:45,505 root         INFO     Train Epoch: 149 [2560/8000 (32%)]\tTotal Loss: 0.435927\n",
      "Reconstruction: 0.370826, Regularization: 0.000142, Discriminator: 0.043331; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:45,615 root         INFO     Train Epoch: 149 [3072/8000 (38%)]\tTotal Loss: 0.435499\n",
      "Reconstruction: 0.370442, Regularization: 0.000133, Discriminator: 0.043310; Generator: 0.021613,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:45,726 root         INFO     Train Epoch: 149 [3584/8000 (45%)]\tTotal Loss: 0.434810\n",
      "Reconstruction: 0.369703, Regularization: 0.000144, Discriminator: 0.043336; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:45,838 root         INFO     Train Epoch: 149 [4096/8000 (51%)]\tTotal Loss: 0.436092\n",
      "Reconstruction: 0.370970, Regularization: 0.000136, Discriminator: 0.043309; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:45,948 root         INFO     Train Epoch: 149 [4608/8000 (58%)]\tTotal Loss: 0.436392\n",
      "Reconstruction: 0.371237, Regularization: 0.000142, Discriminator: 0.043326; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:46,059 root         INFO     Train Epoch: 149 [5120/8000 (64%)]\tTotal Loss: 0.437455\n",
      "Reconstruction: 0.372382, Regularization: 0.000137, Discriminator: 0.043334; Generator: 0.021601,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:46,170 root         INFO     Train Epoch: 149 [5632/8000 (70%)]\tTotal Loss: 0.436993\n",
      "Reconstruction: 0.371895, Regularization: 0.000137, Discriminator: 0.043330; Generator: 0.021631,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:46,281 root         INFO     Train Epoch: 149 [6144/8000 (77%)]\tTotal Loss: 0.437011\n",
      "Reconstruction: 0.371904, Regularization: 0.000143, Discriminator: 0.043320; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:46,392 root         INFO     Train Epoch: 149 [6656/8000 (83%)]\tTotal Loss: 0.437515\n",
      "Reconstruction: 0.372443, Regularization: 0.000144, Discriminator: 0.043321; Generator: 0.021607,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:46,503 root         INFO     Train Epoch: 149 [7168/8000 (90%)]\tTotal Loss: 0.437026\n",
      "Reconstruction: 0.371919, Regularization: 0.000142, Discriminator: 0.043303; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:46,614 root         INFO     Train Epoch: 149 [7680/8000 (96%)]\tTotal Loss: 0.437003\n",
      "Reconstruction: 0.371948, Regularization: 0.000140, Discriminator: 0.043285; Generator: 0.021630,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:46,695 root         INFO     ====> Epoch: 149 Average loss: 0.4371\n",
      "2019-04-10 00:59:46,723 root         INFO     Train Epoch: 150 [0/8000 (0%)]\tTotal Loss: 0.436769\n",
      "Reconstruction: 0.371630, Regularization: 0.000140, Discriminator: 0.043322; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:46,834 root         INFO     Train Epoch: 150 [512/8000 (6%)]\tTotal Loss: 0.437630\n",
      "Reconstruction: 0.372509, Regularization: 0.000130, Discriminator: 0.043338; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:46,945 root         INFO     Train Epoch: 150 [1024/8000 (13%)]\tTotal Loss: 0.436905\n",
      "Reconstruction: 0.371824, Regularization: 0.000137, Discriminator: 0.043325; Generator: 0.021619,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:47,056 root         INFO     Train Epoch: 150 [1536/8000 (19%)]\tTotal Loss: 0.437576\n",
      "Reconstruction: 0.372428, Regularization: 0.000141, Discriminator: 0.043336; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:47,166 root         INFO     Train Epoch: 150 [2048/8000 (26%)]\tTotal Loss: 0.437294\n",
      "Reconstruction: 0.372194, Regularization: 0.000138, Discriminator: 0.043330; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:47,277 root         INFO     Train Epoch: 150 [2560/8000 (32%)]\tTotal Loss: 0.437856\n",
      "Reconstruction: 0.372765, Regularization: 0.000143, Discriminator: 0.043324; Generator: 0.021624,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:47,389 root         INFO     Train Epoch: 150 [3072/8000 (38%)]\tTotal Loss: 0.437154\n",
      "Reconstruction: 0.372003, Regularization: 0.000139, Discriminator: 0.043345; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:47,499 root         INFO     Train Epoch: 150 [3584/8000 (45%)]\tTotal Loss: 0.437456\n",
      "Reconstruction: 0.372259, Regularization: 0.000145, Discriminator: 0.043317; Generator: 0.021735,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:47,610 root         INFO     Train Epoch: 150 [4096/8000 (51%)]\tTotal Loss: 0.438160\n",
      "Reconstruction: 0.372972, Regularization: 0.000143, Discriminator: 0.043356; Generator: 0.021689,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:47,720 root         INFO     Train Epoch: 150 [4608/8000 (58%)]\tTotal Loss: 0.439499\n",
      "Reconstruction: 0.374348, Regularization: 0.000137, Discriminator: 0.043318; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:47,832 root         INFO     Train Epoch: 150 [5120/8000 (64%)]\tTotal Loss: 0.440313\n",
      "Reconstruction: 0.375269, Regularization: 0.000132, Discriminator: 0.043254; Generator: 0.021657,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:47,940 root         INFO     Train Epoch: 150 [5632/8000 (70%)]\tTotal Loss: 0.441990\n",
      "Reconstruction: 0.376818, Regularization: 0.000145, Discriminator: 0.043331; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:48,049 root         INFO     Train Epoch: 150 [6144/8000 (77%)]\tTotal Loss: 0.442762\n",
      "Reconstruction: 0.377628, Regularization: 0.000137, Discriminator: 0.043294; Generator: 0.021703,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:48,158 root         INFO     Train Epoch: 150 [6656/8000 (83%)]\tTotal Loss: 0.445111\n",
      "Reconstruction: 0.379903, Regularization: 0.000139, Discriminator: 0.043345; Generator: 0.021724,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:48,268 root         INFO     Train Epoch: 150 [7168/8000 (90%)]\tTotal Loss: 0.445242\n",
      "Reconstruction: 0.380142, Regularization: 0.000134, Discriminator: 0.043321; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:48,377 root         INFO     Train Epoch: 150 [7680/8000 (96%)]\tTotal Loss: 0.445289\n",
      "Reconstruction: 0.380153, Regularization: 0.000132, Discriminator: 0.043326; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:48,458 root         INFO     ====> Epoch: 150 Average loss: 0.4399\n",
      "2019-04-10 00:59:48,485 root         INFO     Train Epoch: 151 [0/8000 (0%)]\tTotal Loss: 0.445511\n",
      "Reconstruction: 0.380371, Regularization: 0.000151, Discriminator: 0.043332; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:48,597 root         INFO     Train Epoch: 151 [512/8000 (6%)]\tTotal Loss: 0.445680\n",
      "Reconstruction: 0.380474, Regularization: 0.000136, Discriminator: 0.043348; Generator: 0.021722,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:48,708 root         INFO     Train Epoch: 151 [1024/8000 (13%)]\tTotal Loss: 0.443866\n",
      "Reconstruction: 0.378745, Regularization: 0.000139, Discriminator: 0.043320; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:48,820 root         INFO     Train Epoch: 151 [1536/8000 (19%)]\tTotal Loss: 0.443350\n",
      "Reconstruction: 0.378224, Regularization: 0.000143, Discriminator: 0.043314; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:48,931 root         INFO     Train Epoch: 151 [2048/8000 (26%)]\tTotal Loss: 0.442241\n",
      "Reconstruction: 0.377176, Regularization: 0.000149, Discriminator: 0.043262; Generator: 0.021655,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,041 root         INFO     Train Epoch: 151 [2560/8000 (32%)]\tTotal Loss: 0.439436\n",
      "Reconstruction: 0.374421, Regularization: 0.000155, Discriminator: 0.043223; Generator: 0.021637,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,152 root         INFO     Train Epoch: 151 [3072/8000 (38%)]\tTotal Loss: 0.437832\n",
      "Reconstruction: 0.372722, Regularization: 0.000136, Discriminator: 0.043329; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,263 root         INFO     Train Epoch: 151 [3584/8000 (45%)]\tTotal Loss: 0.436447\n",
      "Reconstruction: 0.371337, Regularization: 0.000121, Discriminator: 0.043349; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,373 root         INFO     Train Epoch: 151 [4096/8000 (51%)]\tTotal Loss: 0.434158\n",
      "Reconstruction: 0.368994, Regularization: 0.000162, Discriminator: 0.043319; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,483 root         INFO     Train Epoch: 151 [4608/8000 (58%)]\tTotal Loss: 0.433255\n",
      "Reconstruction: 0.368164, Regularization: 0.000151, Discriminator: 0.043344; Generator: 0.021597,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:49,592 root         INFO     Train Epoch: 151 [5120/8000 (64%)]\tTotal Loss: 0.433447\n",
      "Reconstruction: 0.368328, Regularization: 0.000141, Discriminator: 0.043324; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,704 root         INFO     Train Epoch: 151 [5632/8000 (70%)]\tTotal Loss: 0.431911\n",
      "Reconstruction: 0.366817, Regularization: 0.000133, Discriminator: 0.043322; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,814 root         INFO     Train Epoch: 151 [6144/8000 (77%)]\tTotal Loss: 0.432515\n",
      "Reconstruction: 0.367406, Regularization: 0.000133, Discriminator: 0.043306; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:49,924 root         INFO     Train Epoch: 151 [6656/8000 (83%)]\tTotal Loss: 0.433059\n",
      "Reconstruction: 0.367989, Regularization: 0.000144, Discriminator: 0.043297; Generator: 0.021630,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:50,034 root         INFO     Train Epoch: 151 [7168/8000 (90%)]\tTotal Loss: 0.434841\n",
      "Reconstruction: 0.369767, Regularization: 0.000132, Discriminator: 0.043277; Generator: 0.021665,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:50,144 root         INFO     Train Epoch: 151 [7680/8000 (96%)]\tTotal Loss: 0.435951\n",
      "Reconstruction: 0.370861, Regularization: 0.000147, Discriminator: 0.043346; Generator: 0.021597,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:50,225 root         INFO     ====> Epoch: 151 Average loss: 0.4373\n",
      "2019-04-10 00:59:50,252 root         INFO     Train Epoch: 152 [0/8000 (0%)]\tTotal Loss: 0.436982\n",
      "Reconstruction: 0.371827, Regularization: 0.000146, Discriminator: 0.043411; Generator: 0.021598,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:50,364 root         INFO     Train Epoch: 152 [512/8000 (6%)]\tTotal Loss: 0.438827\n",
      "Reconstruction: 0.373801, Regularization: 0.000130, Discriminator: 0.043284; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:50,478 root         INFO     Train Epoch: 152 [1024/8000 (13%)]\tTotal Loss: 0.440677\n",
      "Reconstruction: 0.375631, Regularization: 0.000131, Discriminator: 0.043334; Generator: 0.021582,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:50,591 root         INFO     Train Epoch: 152 [1536/8000 (19%)]\tTotal Loss: 0.442624\n",
      "Reconstruction: 0.377430, Regularization: 0.000135, Discriminator: 0.043361; Generator: 0.021698,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:50,704 root         INFO     Train Epoch: 152 [2048/8000 (26%)]\tTotal Loss: 0.444785\n",
      "Reconstruction: 0.379632, Regularization: 0.000131, Discriminator: 0.043347; Generator: 0.021676,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:50,816 root         INFO     Train Epoch: 152 [2560/8000 (32%)]\tTotal Loss: 0.445229\n",
      "Reconstruction: 0.380069, Regularization: 0.000118, Discriminator: 0.043356; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:50,928 root         INFO     Train Epoch: 152 [3072/8000 (38%)]\tTotal Loss: 0.446383\n",
      "Reconstruction: 0.381225, Regularization: 0.000123, Discriminator: 0.043313; Generator: 0.021723,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:51,040 root         INFO     Train Epoch: 152 [3584/8000 (45%)]\tTotal Loss: 0.445006\n",
      "Reconstruction: 0.379883, Regularization: 0.000118, Discriminator: 0.043301; Generator: 0.021704,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:51,153 root         INFO     Train Epoch: 152 [4096/8000 (51%)]\tTotal Loss: 0.443702\n",
      "Reconstruction: 0.378794, Regularization: 0.000118, Discriminator: 0.043139; Generator: 0.021651,\n",
      "D(x): 0.503, D(G(z)): 0.500\n",
      "2019-04-10 00:59:51,265 root         INFO     Train Epoch: 152 [4608/8000 (58%)]\tTotal Loss: 0.442916\n",
      "Reconstruction: 0.377911, Regularization: 0.000129, Discriminator: 0.043208; Generator: 0.021669,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:51,377 root         INFO     Train Epoch: 152 [5120/8000 (64%)]\tTotal Loss: 0.440461\n",
      "Reconstruction: 0.375315, Regularization: 0.000138, Discriminator: 0.043309; Generator: 0.021699,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:51,489 root         INFO     Train Epoch: 152 [5632/8000 (70%)]\tTotal Loss: 0.437357\n",
      "Reconstruction: 0.372152, Regularization: 0.000118, Discriminator: 0.043426; Generator: 0.021661,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 00:59:51,602 root         INFO     Train Epoch: 152 [6144/8000 (77%)]\tTotal Loss: 0.436167\n",
      "Reconstruction: 0.371132, Regularization: 0.000106, Discriminator: 0.043268; Generator: 0.021662,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:51,714 root         INFO     Train Epoch: 152 [6656/8000 (83%)]\tTotal Loss: 0.434557\n",
      "Reconstruction: 0.369399, Regularization: 0.000112, Discriminator: 0.043356; Generator: 0.021690,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:51,825 root         INFO     Train Epoch: 152 [7168/8000 (90%)]\tTotal Loss: 0.435003\n",
      "Reconstruction: 0.369895, Regularization: 0.000120, Discriminator: 0.043319; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:51,934 root         INFO     Train Epoch: 152 [7680/8000 (96%)]\tTotal Loss: 0.434896\n",
      "Reconstruction: 0.369814, Regularization: 0.000122, Discriminator: 0.043297; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:52,015 root         INFO     ====> Epoch: 152 Average loss: 0.4404\n",
      "2019-04-10 00:59:52,042 root         INFO     Train Epoch: 153 [0/8000 (0%)]\tTotal Loss: 0.434276\n",
      "Reconstruction: 0.369213, Regularization: 0.000112, Discriminator: 0.043317; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:52,154 root         INFO     Train Epoch: 153 [512/8000 (6%)]\tTotal Loss: 0.436709\n",
      "Reconstruction: 0.371678, Regularization: 0.000112, Discriminator: 0.043270; Generator: 0.021649,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:52,266 root         INFO     Train Epoch: 153 [1024/8000 (13%)]\tTotal Loss: 0.437549\n",
      "Reconstruction: 0.372464, Regularization: 0.000121, Discriminator: 0.043357; Generator: 0.021608,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:52,378 root         INFO     Train Epoch: 153 [1536/8000 (19%)]\tTotal Loss: 0.438787\n",
      "Reconstruction: 0.373692, Regularization: 0.000111, Discriminator: 0.043328; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:52,490 root         INFO     Train Epoch: 153 [2048/8000 (26%)]\tTotal Loss: 0.440111\n",
      "Reconstruction: 0.374934, Regularization: 0.000109, Discriminator: 0.043379; Generator: 0.021689,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:52,602 root         INFO     Train Epoch: 153 [2560/8000 (32%)]\tTotal Loss: 0.441941\n",
      "Reconstruction: 0.376818, Regularization: 0.000103, Discriminator: 0.043382; Generator: 0.021638,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:52,715 root         INFO     Train Epoch: 153 [3072/8000 (38%)]\tTotal Loss: 0.442552\n",
      "Reconstruction: 0.377400, Regularization: 0.000102, Discriminator: 0.043359; Generator: 0.021691,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:52,826 root         INFO     Train Epoch: 153 [3584/8000 (45%)]\tTotal Loss: 0.443206\n",
      "Reconstruction: 0.378058, Regularization: 0.000097, Discriminator: 0.043351; Generator: 0.021700,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:52,938 root         INFO     Train Epoch: 153 [4096/8000 (51%)]\tTotal Loss: 0.444286\n",
      "Reconstruction: 0.379198, Regularization: 0.000103, Discriminator: 0.043342; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:53,050 root         INFO     Train Epoch: 153 [4608/8000 (58%)]\tTotal Loss: 0.444006\n",
      "Reconstruction: 0.378924, Regularization: 0.000089, Discriminator: 0.043323; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:53,161 root         INFO     Train Epoch: 153 [5120/8000 (64%)]\tTotal Loss: 0.443919\n",
      "Reconstruction: 0.378802, Regularization: 0.000098, Discriminator: 0.043297; Generator: 0.021722,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:53,273 root         INFO     Train Epoch: 153 [5632/8000 (70%)]\tTotal Loss: 0.442090\n",
      "Reconstruction: 0.377103, Regularization: 0.000093, Discriminator: 0.043260; Generator: 0.021634,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:53,385 root         INFO     Train Epoch: 153 [6144/8000 (77%)]\tTotal Loss: 0.440812\n",
      "Reconstruction: 0.375665, Regularization: 0.000096, Discriminator: 0.043424; Generator: 0.021629,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 00:59:53,496 root         INFO     Train Epoch: 153 [6656/8000 (83%)]\tTotal Loss: 0.439567\n",
      "Reconstruction: 0.374579, Regularization: 0.000112, Discriminator: 0.043243; Generator: 0.021634,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:53,605 root         INFO     Train Epoch: 153 [7168/8000 (90%)]\tTotal Loss: 0.436588\n",
      "Reconstruction: 0.371495, Regularization: 0.000108, Discriminator: 0.043349; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:53,716 root         INFO     Train Epoch: 153 [7680/8000 (96%)]\tTotal Loss: 0.435993\n",
      "Reconstruction: 0.370871, Regularization: 0.000116, Discriminator: 0.043336; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:53,796 root         INFO     ====> Epoch: 153 Average loss: 0.4402\n",
      "2019-04-10 00:59:53,823 root         INFO     Train Epoch: 154 [0/8000 (0%)]\tTotal Loss: 0.433706\n",
      "Reconstruction: 0.368672, Regularization: 0.000115, Discriminator: 0.043310; Generator: 0.021609,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:53,935 root         INFO     Train Epoch: 154 [512/8000 (6%)]\tTotal Loss: 0.433750\n",
      "Reconstruction: 0.368585, Regularization: 0.000110, Discriminator: 0.043392; Generator: 0.021662,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:54,048 root         INFO     Train Epoch: 154 [1024/8000 (13%)]\tTotal Loss: 0.432997\n",
      "Reconstruction: 0.367932, Regularization: 0.000110, Discriminator: 0.043318; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:54,160 root         INFO     Train Epoch: 154 [1536/8000 (19%)]\tTotal Loss: 0.432421\n",
      "Reconstruction: 0.367340, Regularization: 0.000116, Discriminator: 0.043328; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:54,272 root         INFO     Train Epoch: 154 [2048/8000 (26%)]\tTotal Loss: 0.434088\n",
      "Reconstruction: 0.368940, Regularization: 0.000124, Discriminator: 0.043338; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:54,384 root         INFO     Train Epoch: 154 [2560/8000 (32%)]\tTotal Loss: 0.434938\n",
      "Reconstruction: 0.369824, Regularization: 0.000113, Discriminator: 0.043334; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:54,496 root         INFO     Train Epoch: 154 [3072/8000 (38%)]\tTotal Loss: 0.434588\n",
      "Reconstruction: 0.369542, Regularization: 0.000109, Discriminator: 0.043303; Generator: 0.021634,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:54,608 root         INFO     Train Epoch: 154 [3584/8000 (45%)]\tTotal Loss: 0.437264\n",
      "Reconstruction: 0.372286, Regularization: 0.000104, Discriminator: 0.043205; Generator: 0.021668,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:54,720 root         INFO     Train Epoch: 154 [4096/8000 (51%)]\tTotal Loss: 0.439829\n",
      "Reconstruction: 0.374733, Regularization: 0.000115, Discriminator: 0.043358; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:54,833 root         INFO     Train Epoch: 154 [4608/8000 (58%)]\tTotal Loss: 0.441532\n",
      "Reconstruction: 0.376358, Regularization: 0.000138, Discriminator: 0.043328; Generator: 0.021708,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:54,945 root         INFO     Train Epoch: 154 [5120/8000 (64%)]\tTotal Loss: 0.443070\n",
      "Reconstruction: 0.377966, Regularization: 0.000127, Discriminator: 0.043343; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:55,057 root         INFO     Train Epoch: 154 [5632/8000 (70%)]\tTotal Loss: 0.445433\n",
      "Reconstruction: 0.380297, Regularization: 0.000132, Discriminator: 0.043300; Generator: 0.021705,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:55,169 root         INFO     Train Epoch: 154 [6144/8000 (77%)]\tTotal Loss: 0.446334\n",
      "Reconstruction: 0.381107, Regularization: 0.000139, Discriminator: 0.043371; Generator: 0.021717,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:59:55,281 root         INFO     Train Epoch: 154 [6656/8000 (83%)]\tTotal Loss: 0.447154\n",
      "Reconstruction: 0.381958, Regularization: 0.000131, Discriminator: 0.043373; Generator: 0.021693,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:55,393 root         INFO     Train Epoch: 154 [7168/8000 (90%)]\tTotal Loss: 0.447034\n",
      "Reconstruction: 0.381898, Regularization: 0.000133, Discriminator: 0.043317; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:55,505 root         INFO     Train Epoch: 154 [7680/8000 (96%)]\tTotal Loss: 0.447819\n",
      "Reconstruction: 0.382681, Regularization: 0.000145, Discriminator: 0.043301; Generator: 0.021692,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:55,586 root         INFO     ====> Epoch: 154 Average loss: 0.4398\n",
      "2019-04-10 00:59:55,613 root         INFO     Train Epoch: 155 [0/8000 (0%)]\tTotal Loss: 0.446044\n",
      "Reconstruction: 0.380951, Regularization: 0.000138, Discriminator: 0.043244; Generator: 0.021710,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:55,726 root         INFO     Train Epoch: 155 [512/8000 (6%)]\tTotal Loss: 0.444392\n",
      "Reconstruction: 0.379416, Regularization: 0.000135, Discriminator: 0.043173; Generator: 0.021668,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 00:59:55,839 root         INFO     Train Epoch: 155 [1024/8000 (13%)]\tTotal Loss: 0.442793\n",
      "Reconstruction: 0.377723, Regularization: 0.000124, Discriminator: 0.043279; Generator: 0.021667,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:55,951 root         INFO     Train Epoch: 155 [1536/8000 (19%)]\tTotal Loss: 0.439701\n",
      "Reconstruction: 0.374663, Regularization: 0.000120, Discriminator: 0.043280; Generator: 0.021638,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:56,063 root         INFO     Train Epoch: 155 [2048/8000 (26%)]\tTotal Loss: 0.437679\n",
      "Reconstruction: 0.372607, Regularization: 0.000139, Discriminator: 0.043275; Generator: 0.021658,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:56,175 root         INFO     Train Epoch: 155 [2560/8000 (32%)]\tTotal Loss: 0.435519\n",
      "Reconstruction: 0.370391, Regularization: 0.000144, Discriminator: 0.043340; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:56,287 root         INFO     Train Epoch: 155 [3072/8000 (38%)]\tTotal Loss: 0.434252\n",
      "Reconstruction: 0.369081, Regularization: 0.000141, Discriminator: 0.043371; Generator: 0.021660,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:56,398 root         INFO     Train Epoch: 155 [3584/8000 (45%)]\tTotal Loss: 0.432626\n",
      "Reconstruction: 0.367549, Regularization: 0.000144, Discriminator: 0.043335; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:56,511 root         INFO     Train Epoch: 155 [4096/8000 (51%)]\tTotal Loss: 0.431888\n",
      "Reconstruction: 0.366777, Regularization: 0.000154, Discriminator: 0.043323; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:56,623 root         INFO     Train Epoch: 155 [4608/8000 (58%)]\tTotal Loss: 0.431686\n",
      "Reconstruction: 0.366606, Regularization: 0.000139, Discriminator: 0.043325; Generator: 0.021616,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:56,733 root         INFO     Train Epoch: 155 [5120/8000 (64%)]\tTotal Loss: 0.432128\n",
      "Reconstruction: 0.367055, Regularization: 0.000140, Discriminator: 0.043319; Generator: 0.021613,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:56,845 root         INFO     Train Epoch: 155 [5632/8000 (70%)]\tTotal Loss: 0.431760\n",
      "Reconstruction: 0.366705, Regularization: 0.000128, Discriminator: 0.043307; Generator: 0.021620,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:56,956 root         INFO     Train Epoch: 155 [6144/8000 (77%)]\tTotal Loss: 0.435125\n",
      "Reconstruction: 0.370091, Regularization: 0.000134, Discriminator: 0.043293; Generator: 0.021607,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:59:57,066 root         INFO     Train Epoch: 155 [6656/8000 (83%)]\tTotal Loss: 0.435828\n",
      "Reconstruction: 0.370657, Regularization: 0.000157, Discriminator: 0.043398; Generator: 0.021615,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:57,177 root         INFO     Train Epoch: 155 [7168/8000 (90%)]\tTotal Loss: 0.437207\n",
      "Reconstruction: 0.372044, Regularization: 0.000149, Discriminator: 0.043320; Generator: 0.021694,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:57,287 root         INFO     Train Epoch: 155 [7680/8000 (96%)]\tTotal Loss: 0.438595\n",
      "Reconstruction: 0.373477, Regularization: 0.000133, Discriminator: 0.043314; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:57,368 root         INFO     ====> Epoch: 155 Average loss: 0.4366\n",
      "2019-04-10 00:59:57,395 root         INFO     Train Epoch: 156 [0/8000 (0%)]\tTotal Loss: 0.438799\n",
      "Reconstruction: 0.373623, Regularization: 0.000127, Discriminator: 0.043374; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:57,508 root         INFO     Train Epoch: 156 [512/8000 (6%)]\tTotal Loss: 0.440550\n",
      "Reconstruction: 0.375493, Regularization: 0.000132, Discriminator: 0.043282; Generator: 0.021643,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:57,620 root         INFO     Train Epoch: 156 [1024/8000 (13%)]\tTotal Loss: 0.442814\n",
      "Reconstruction: 0.377729, Regularization: 0.000124, Discriminator: 0.043284; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:57,731 root         INFO     Train Epoch: 156 [1536/8000 (19%)]\tTotal Loss: 0.443468\n",
      "Reconstruction: 0.378302, Regularization: 0.000108, Discriminator: 0.043353; Generator: 0.021705,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:57,841 root         INFO     Train Epoch: 156 [2048/8000 (26%)]\tTotal Loss: 0.444500\n",
      "Reconstruction: 0.379382, Regularization: 0.000112, Discriminator: 0.043318; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:57,952 root         INFO     Train Epoch: 156 [2560/8000 (32%)]\tTotal Loss: 0.444904\n",
      "Reconstruction: 0.379790, Regularization: 0.000110, Discriminator: 0.043316; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:58,063 root         INFO     Train Epoch: 156 [3072/8000 (38%)]\tTotal Loss: 0.445178\n",
      "Reconstruction: 0.380040, Regularization: 0.000111, Discriminator: 0.043327; Generator: 0.021700,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:59:58,175 root         INFO     Train Epoch: 156 [3584/8000 (45%)]\tTotal Loss: 0.444462\n",
      "Reconstruction: 0.379396, Regularization: 0.000105, Discriminator: 0.043277; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:58,288 root         INFO     Train Epoch: 156 [4096/8000 (51%)]\tTotal Loss: 0.443494\n",
      "Reconstruction: 0.378413, Regularization: 0.000100, Discriminator: 0.043288; Generator: 0.021693,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:58,401 root         INFO     Train Epoch: 156 [4608/8000 (58%)]\tTotal Loss: 0.441508\n",
      "Reconstruction: 0.376417, Regularization: 0.000098, Discriminator: 0.043285; Generator: 0.021707,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:58,514 root         INFO     Train Epoch: 156 [5120/8000 (64%)]\tTotal Loss: 0.439679\n",
      "Reconstruction: 0.374568, Regularization: 0.000101, Discriminator: 0.043338; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:58,627 root         INFO     Train Epoch: 156 [5632/8000 (70%)]\tTotal Loss: 0.438725\n",
      "Reconstruction: 0.373658, Regularization: 0.000110, Discriminator: 0.043289; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:58,740 root         INFO     Train Epoch: 156 [6144/8000 (77%)]\tTotal Loss: 0.436442\n",
      "Reconstruction: 0.371295, Regularization: 0.000117, Discriminator: 0.043350; Generator: 0.021680,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:58,852 root         INFO     Train Epoch: 156 [6656/8000 (83%)]\tTotal Loss: 0.435810\n",
      "Reconstruction: 0.370671, Regularization: 0.000119, Discriminator: 0.043345; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:59:58,965 root         INFO     Train Epoch: 156 [7168/8000 (90%)]\tTotal Loss: 0.435969\n",
      "Reconstruction: 0.370893, Regularization: 0.000118, Discriminator: 0.043326; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:59,079 root         INFO     Train Epoch: 156 [7680/8000 (96%)]\tTotal Loss: 0.435613\n",
      "Reconstruction: 0.370531, Regularization: 0.000110, Discriminator: 0.043318; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:59,160 root         INFO     ====> Epoch: 156 Average loss: 0.4409\n",
      "2019-04-10 00:59:59,188 root         INFO     Train Epoch: 157 [0/8000 (0%)]\tTotal Loss: 0.435352\n",
      "Reconstruction: 0.370281, Regularization: 0.000115, Discriminator: 0.043318; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:59,299 root         INFO     Train Epoch: 157 [512/8000 (6%)]\tTotal Loss: 0.436263\n",
      "Reconstruction: 0.371154, Regularization: 0.000114, Discriminator: 0.043299; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:59:59,409 root         INFO     Train Epoch: 157 [1024/8000 (13%)]\tTotal Loss: 0.436866\n",
      "Reconstruction: 0.371758, Regularization: 0.000121, Discriminator: 0.043335; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:59,519 root         INFO     Train Epoch: 157 [1536/8000 (19%)]\tTotal Loss: 0.435193\n",
      "Reconstruction: 0.370146, Regularization: 0.000113, Discriminator: 0.043296; Generator: 0.021637,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:59:59,628 root         INFO     Train Epoch: 157 [2048/8000 (26%)]\tTotal Loss: 0.437297\n",
      "Reconstruction: 0.372217, Regularization: 0.000118, Discriminator: 0.043297; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:59,738 root         INFO     Train Epoch: 157 [2560/8000 (32%)]\tTotal Loss: 0.437127\n",
      "Reconstruction: 0.371996, Regularization: 0.000126, Discriminator: 0.043365; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:59:59,847 root         INFO     Train Epoch: 157 [3072/8000 (38%)]\tTotal Loss: 0.438296\n",
      "Reconstruction: 0.373197, Regularization: 0.000115, Discriminator: 0.043363; Generator: 0.021622,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:59:59,958 root         INFO     Train Epoch: 157 [3584/8000 (45%)]\tTotal Loss: 0.438943\n",
      "Reconstruction: 0.373831, Regularization: 0.000120, Discriminator: 0.043357; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:00,069 root         INFO     Train Epoch: 157 [4096/8000 (51%)]\tTotal Loss: 0.439388\n",
      "Reconstruction: 0.374347, Regularization: 0.000112, Discriminator: 0.043299; Generator: 0.021630,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:00,180 root         INFO     Train Epoch: 157 [4608/8000 (58%)]\tTotal Loss: 0.439992\n",
      "Reconstruction: 0.374894, Regularization: 0.000117, Discriminator: 0.043348; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:00,291 root         INFO     Train Epoch: 157 [5120/8000 (64%)]\tTotal Loss: 0.439746\n",
      "Reconstruction: 0.374677, Regularization: 0.000124, Discriminator: 0.043308; Generator: 0.021637,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:00,402 root         INFO     Train Epoch: 157 [5632/8000 (70%)]\tTotal Loss: 0.440713\n",
      "Reconstruction: 0.375646, Regularization: 0.000118, Discriminator: 0.043299; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:00,514 root         INFO     Train Epoch: 157 [6144/8000 (77%)]\tTotal Loss: 0.440481\n",
      "Reconstruction: 0.375441, Regularization: 0.000124, Discriminator: 0.043307; Generator: 0.021609,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:00,625 root         INFO     Train Epoch: 157 [6656/8000 (83%)]\tTotal Loss: 0.440204\n",
      "Reconstruction: 0.375101, Regularization: 0.000126, Discriminator: 0.043321; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:00,736 root         INFO     Train Epoch: 157 [7168/8000 (90%)]\tTotal Loss: 0.442062\n",
      "Reconstruction: 0.376976, Regularization: 0.000123, Discriminator: 0.043308; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:00,847 root         INFO     Train Epoch: 157 [7680/8000 (96%)]\tTotal Loss: 0.442414\n",
      "Reconstruction: 0.377288, Regularization: 0.000127, Discriminator: 0.043295; Generator: 0.021705,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:00,927 root         INFO     ====> Epoch: 157 Average loss: 0.4388\n",
      "2019-04-10 01:00:00,955 root         INFO     Train Epoch: 158 [0/8000 (0%)]\tTotal Loss: 0.441688\n",
      "Reconstruction: 0.376571, Regularization: 0.000123, Discriminator: 0.043325; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:01,066 root         INFO     Train Epoch: 158 [512/8000 (6%)]\tTotal Loss: 0.440921\n",
      "Reconstruction: 0.375812, Regularization: 0.000135, Discriminator: 0.043335; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:01,177 root         INFO     Train Epoch: 158 [1024/8000 (13%)]\tTotal Loss: 0.441112\n",
      "Reconstruction: 0.375977, Regularization: 0.000126, Discriminator: 0.043335; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:01,288 root         INFO     Train Epoch: 158 [1536/8000 (19%)]\tTotal Loss: 0.440551\n",
      "Reconstruction: 0.375370, Regularization: 0.000133, Discriminator: 0.043342; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:01,400 root         INFO     Train Epoch: 158 [2048/8000 (26%)]\tTotal Loss: 0.441087\n",
      "Reconstruction: 0.375927, Regularization: 0.000122, Discriminator: 0.043333; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:01,512 root         INFO     Train Epoch: 158 [2560/8000 (32%)]\tTotal Loss: 0.440331\n",
      "Reconstruction: 0.375160, Regularization: 0.000127, Discriminator: 0.043330; Generator: 0.021713,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:01,623 root         INFO     Train Epoch: 158 [3072/8000 (38%)]\tTotal Loss: 0.439160\n",
      "Reconstruction: 0.374025, Regularization: 0.000115, Discriminator: 0.043363; Generator: 0.021657,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:01,734 root         INFO     Train Epoch: 158 [3584/8000 (45%)]\tTotal Loss: 0.439183\n",
      "Reconstruction: 0.374028, Regularization: 0.000120, Discriminator: 0.043309; Generator: 0.021726,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:01,846 root         INFO     Train Epoch: 158 [4096/8000 (51%)]\tTotal Loss: 0.439368\n",
      "Reconstruction: 0.374275, Regularization: 0.000117, Discriminator: 0.043313; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:01,961 root         INFO     Train Epoch: 158 [4608/8000 (58%)]\tTotal Loss: 0.438322\n",
      "Reconstruction: 0.373223, Regularization: 0.000117, Discriminator: 0.043311; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:02,076 root         INFO     Train Epoch: 158 [5120/8000 (64%)]\tTotal Loss: 0.439585\n",
      "Reconstruction: 0.374507, Regularization: 0.000111, Discriminator: 0.043276; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:02,187 root         INFO     Train Epoch: 158 [5632/8000 (70%)]\tTotal Loss: 0.440537\n",
      "Reconstruction: 0.375486, Regularization: 0.000121, Discriminator: 0.043289; Generator: 0.021641,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:02,302 root         INFO     Train Epoch: 158 [6144/8000 (77%)]\tTotal Loss: 0.441493\n",
      "Reconstruction: 0.376369, Regularization: 0.000114, Discriminator: 0.043301; Generator: 0.021709,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:02,415 root         INFO     Train Epoch: 158 [6656/8000 (83%)]\tTotal Loss: 0.440852\n",
      "Reconstruction: 0.375732, Regularization: 0.000113, Discriminator: 0.043340; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:02,525 root         INFO     Train Epoch: 158 [7168/8000 (90%)]\tTotal Loss: 0.440893\n",
      "Reconstruction: 0.375760, Regularization: 0.000125, Discriminator: 0.043338; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:02,635 root         INFO     Train Epoch: 158 [7680/8000 (96%)]\tTotal Loss: 0.440643\n",
      "Reconstruction: 0.375559, Regularization: 0.000124, Discriminator: 0.043341; Generator: 0.021618,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:02,715 root         INFO     ====> Epoch: 158 Average loss: 0.4403\n",
      "2019-04-10 01:00:02,743 root         INFO     Train Epoch: 159 [0/8000 (0%)]\tTotal Loss: 0.440763\n",
      "Reconstruction: 0.375622, Regularization: 0.000120, Discriminator: 0.043335; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:02,856 root         INFO     Train Epoch: 159 [512/8000 (6%)]\tTotal Loss: 0.440467\n",
      "Reconstruction: 0.375395, Regularization: 0.000119, Discriminator: 0.043315; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:02,969 root         INFO     Train Epoch: 159 [1024/8000 (13%)]\tTotal Loss: 0.439771\n",
      "Reconstruction: 0.374711, Regularization: 0.000115, Discriminator: 0.043318; Generator: 0.021628,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:03,082 root         INFO     Train Epoch: 159 [1536/8000 (19%)]\tTotal Loss: 0.439190\n",
      "Reconstruction: 0.374155, Regularization: 0.000130, Discriminator: 0.043304; Generator: 0.021601,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:03,195 root         INFO     Train Epoch: 159 [2048/8000 (26%)]\tTotal Loss: 0.437800\n",
      "Reconstruction: 0.372755, Regularization: 0.000132, Discriminator: 0.043307; Generator: 0.021606,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:03,306 root         INFO     Train Epoch: 159 [2560/8000 (32%)]\tTotal Loss: 0.436930\n",
      "Reconstruction: 0.371904, Regularization: 0.000122, Discriminator: 0.043278; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:03,419 root         INFO     Train Epoch: 159 [3072/8000 (38%)]\tTotal Loss: 0.435628\n",
      "Reconstruction: 0.370555, Regularization: 0.000115, Discriminator: 0.043329; Generator: 0.021629,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:03,534 root         INFO     Train Epoch: 159 [3584/8000 (45%)]\tTotal Loss: 0.435139\n",
      "Reconstruction: 0.370078, Regularization: 0.000104, Discriminator: 0.043296; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:03,646 root         INFO     Train Epoch: 159 [4096/8000 (51%)]\tTotal Loss: 0.433857\n",
      "Reconstruction: 0.368744, Regularization: 0.000110, Discriminator: 0.043379; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:03,762 root         INFO     Train Epoch: 159 [4608/8000 (58%)]\tTotal Loss: 0.432178\n",
      "Reconstruction: 0.367109, Regularization: 0.000110, Discriminator: 0.043332; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:03,874 root         INFO     Train Epoch: 159 [5120/8000 (64%)]\tTotal Loss: 0.432872\n",
      "Reconstruction: 0.367738, Regularization: 0.000116, Discriminator: 0.043339; Generator: 0.021679,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:03,989 root         INFO     Train Epoch: 159 [5632/8000 (70%)]\tTotal Loss: 0.432840\n",
      "Reconstruction: 0.367805, Regularization: 0.000112, Discriminator: 0.043307; Generator: 0.021616,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:04,106 root         INFO     Train Epoch: 159 [6144/8000 (77%)]\tTotal Loss: 0.433759\n",
      "Reconstruction: 0.368726, Regularization: 0.000108, Discriminator: 0.043279; Generator: 0.021646,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:04,220 root         INFO     Train Epoch: 159 [6656/8000 (83%)]\tTotal Loss: 0.435257\n",
      "Reconstruction: 0.370123, Regularization: 0.000129, Discriminator: 0.043351; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:04,333 root         INFO     Train Epoch: 159 [7168/8000 (90%)]\tTotal Loss: 0.437412\n",
      "Reconstruction: 0.372321, Regularization: 0.000143, Discriminator: 0.043267; Generator: 0.021681,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:04,445 root         INFO     Train Epoch: 159 [7680/8000 (96%)]\tTotal Loss: 0.438851\n",
      "Reconstruction: 0.373856, Regularization: 0.000112, Discriminator: 0.043213; Generator: 0.021669,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 01:00:04,527 root         INFO     ====> Epoch: 159 Average loss: 0.4362\n",
      "2019-04-10 01:00:04,553 root         INFO     Train Epoch: 160 [0/8000 (0%)]\tTotal Loss: 0.441364\n",
      "Reconstruction: 0.376129, Regularization: 0.000131, Discriminator: 0.043421; Generator: 0.021683,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:04,667 root         INFO     Train Epoch: 160 [512/8000 (6%)]\tTotal Loss: 0.443045\n",
      "Reconstruction: 0.377890, Regularization: 0.000088, Discriminator: 0.043362; Generator: 0.021705,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:04,780 root         INFO     Train Epoch: 160 [1024/8000 (13%)]\tTotal Loss: 0.444727\n",
      "Reconstruction: 0.379575, Regularization: 0.000068, Discriminator: 0.043361; Generator: 0.021722,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:04,893 root         INFO     Train Epoch: 160 [1536/8000 (19%)]\tTotal Loss: 0.447157\n",
      "Reconstruction: 0.382008, Regularization: 0.000077, Discriminator: 0.043330; Generator: 0.021743,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:05,006 root         INFO     Train Epoch: 160 [2048/8000 (26%)]\tTotal Loss: 0.447811\n",
      "Reconstruction: 0.382727, Regularization: 0.000066, Discriminator: 0.043344; Generator: 0.021674,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:05,120 root         INFO     Train Epoch: 160 [2560/8000 (32%)]\tTotal Loss: 0.448897\n",
      "Reconstruction: 0.383766, Regularization: 0.000060, Discriminator: 0.043344; Generator: 0.021727,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:05,235 root         INFO     Train Epoch: 160 [3072/8000 (38%)]\tTotal Loss: 0.449313\n",
      "Reconstruction: 0.384236, Regularization: 0.000062, Discriminator: 0.043314; Generator: 0.021701,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:05,349 root         INFO     Train Epoch: 160 [3584/8000 (45%)]\tTotal Loss: 0.448201\n",
      "Reconstruction: 0.383134, Regularization: 0.000057, Discriminator: 0.043306; Generator: 0.021703,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:05,463 root         INFO     Train Epoch: 160 [4096/8000 (51%)]\tTotal Loss: 0.447699\n",
      "Reconstruction: 0.382661, Regularization: 0.000058, Discriminator: 0.043284; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:05,578 root         INFO     Train Epoch: 160 [4608/8000 (58%)]\tTotal Loss: 0.445560\n",
      "Reconstruction: 0.380479, Regularization: 0.000059, Discriminator: 0.043341; Generator: 0.021681,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:05,692 root         INFO     Train Epoch: 160 [5120/8000 (64%)]\tTotal Loss: 0.444096\n",
      "Reconstruction: 0.378988, Regularization: 0.000045, Discriminator: 0.043387; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:05,811 root         INFO     Train Epoch: 160 [5632/8000 (70%)]\tTotal Loss: 0.441211\n",
      "Reconstruction: 0.376193, Regularization: 0.000051, Discriminator: 0.043316; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:05,935 root         INFO     Train Epoch: 160 [6144/8000 (77%)]\tTotal Loss: 0.439012\n",
      "Reconstruction: 0.373934, Regularization: 0.000033, Discriminator: 0.043378; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:06,050 root         INFO     Train Epoch: 160 [6656/8000 (83%)]\tTotal Loss: 0.436312\n",
      "Reconstruction: 0.371227, Regularization: 0.000024, Discriminator: 0.043398; Generator: 0.021664,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:06,164 root         INFO     Train Epoch: 160 [7168/8000 (90%)]\tTotal Loss: 0.433916\n",
      "Reconstruction: 0.368760, Regularization: 0.000019, Discriminator: 0.043451; Generator: 0.021686,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:06,278 root         INFO     Train Epoch: 160 [7680/8000 (96%)]\tTotal Loss: 0.431991\n",
      "Reconstruction: 0.366930, Regularization: 0.000024, Discriminator: 0.043398; Generator: 0.021639,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:06,360 root         INFO     ====> Epoch: 160 Average loss: 0.4432\n",
      "2019-04-10 01:00:06,388 root         INFO     Train Epoch: 161 [0/8000 (0%)]\tTotal Loss: 0.431396\n",
      "Reconstruction: 0.366329, Regularization: 0.000019, Discriminator: 0.043388; Generator: 0.021659,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:06,506 root         INFO     Train Epoch: 161 [512/8000 (6%)]\tTotal Loss: 0.430493\n",
      "Reconstruction: 0.365530, Regularization: 0.000028, Discriminator: 0.043311; Generator: 0.021623,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:06,623 root         INFO     Train Epoch: 161 [1024/8000 (13%)]\tTotal Loss: 0.430146\n",
      "Reconstruction: 0.365131, Regularization: 0.000022, Discriminator: 0.043318; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:06,741 root         INFO     Train Epoch: 161 [1536/8000 (19%)]\tTotal Loss: 0.430521\n",
      "Reconstruction: 0.365590, Regularization: 0.000020, Discriminator: 0.043303; Generator: 0.021608,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:06,859 root         INFO     Train Epoch: 161 [2048/8000 (26%)]\tTotal Loss: 0.431591\n",
      "Reconstruction: 0.366653, Regularization: 0.000015, Discriminator: 0.043286; Generator: 0.021637,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:06,973 root         INFO     Train Epoch: 161 [2560/8000 (32%)]\tTotal Loss: 0.432723\n",
      "Reconstruction: 0.367746, Regularization: 0.000021, Discriminator: 0.043343; Generator: 0.021613,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:07,087 root         INFO     Train Epoch: 161 [3072/8000 (38%)]\tTotal Loss: 0.434620\n",
      "Reconstruction: 0.369704, Regularization: 0.000018, Discriminator: 0.043291; Generator: 0.021607,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:07,202 root         INFO     Train Epoch: 161 [3584/8000 (45%)]\tTotal Loss: 0.436348\n",
      "Reconstruction: 0.371470, Regularization: 0.000014, Discriminator: 0.043282; Generator: 0.021581,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:07,313 root         INFO     Train Epoch: 161 [4096/8000 (51%)]\tTotal Loss: 0.438336\n",
      "Reconstruction: 0.373405, Regularization: 0.000017, Discriminator: 0.043268; Generator: 0.021645,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:07,422 root         INFO     Train Epoch: 161 [4608/8000 (58%)]\tTotal Loss: 0.440334\n",
      "Reconstruction: 0.375260, Regularization: 0.000012, Discriminator: 0.043405; Generator: 0.021656,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:07,531 root         INFO     Train Epoch: 161 [5120/8000 (64%)]\tTotal Loss: 0.442153\n",
      "Reconstruction: 0.377123, Regularization: 0.000012, Discriminator: 0.043390; Generator: 0.021629,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 01:00:07,640 root         INFO     Train Epoch: 161 [5632/8000 (70%)]\tTotal Loss: 0.443494\n",
      "Reconstruction: 0.378487, Regularization: 0.000014, Discriminator: 0.043353; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:07,748 root         INFO     Train Epoch: 161 [6144/8000 (77%)]\tTotal Loss: 0.444616\n",
      "Reconstruction: 0.379588, Regularization: 0.000012, Discriminator: 0.043355; Generator: 0.021660,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:07,857 root         INFO     Train Epoch: 161 [6656/8000 (83%)]\tTotal Loss: 0.445186\n",
      "Reconstruction: 0.380209, Regularization: 0.000015, Discriminator: 0.043326; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:07,966 root         INFO     Train Epoch: 161 [7168/8000 (90%)]\tTotal Loss: 0.445674\n",
      "Reconstruction: 0.380715, Regularization: 0.000015, Discriminator: 0.043275; Generator: 0.021669,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:08,075 root         INFO     Train Epoch: 161 [7680/8000 (96%)]\tTotal Loss: 0.445036\n",
      "Reconstruction: 0.380046, Regularization: 0.000013, Discriminator: 0.043317; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:08,155 root         INFO     ====> Epoch: 161 Average loss: 0.4379\n",
      "2019-04-10 01:00:08,183 root         INFO     Train Epoch: 162 [0/8000 (0%)]\tTotal Loss: 0.444665\n",
      "Reconstruction: 0.379612, Regularization: 0.000012, Discriminator: 0.043356; Generator: 0.021684,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:08,295 root         INFO     Train Epoch: 162 [512/8000 (6%)]\tTotal Loss: 0.443532\n",
      "Reconstruction: 0.378350, Regularization: 0.000020, Discriminator: 0.043448; Generator: 0.021714,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:00:08,405 root         INFO     Train Epoch: 162 [1024/8000 (13%)]\tTotal Loss: 0.442136\n",
      "Reconstruction: 0.377046, Regularization: 0.000016, Discriminator: 0.043355; Generator: 0.021719,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:08,514 root         INFO     Train Epoch: 162 [1536/8000 (19%)]\tTotal Loss: 0.440820\n",
      "Reconstruction: 0.375668, Regularization: 0.000016, Discriminator: 0.043401; Generator: 0.021735,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:08,621 root         INFO     Train Epoch: 162 [2048/8000 (26%)]\tTotal Loss: 0.439494\n",
      "Reconstruction: 0.374432, Regularization: 0.000023, Discriminator: 0.043305; Generator: 0.021734,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:08,728 root         INFO     Train Epoch: 162 [2560/8000 (32%)]\tTotal Loss: 0.438412\n",
      "Reconstruction: 0.373245, Regularization: 0.000022, Discriminator: 0.043420; Generator: 0.021724,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:00:08,836 root         INFO     Train Epoch: 162 [3072/8000 (38%)]\tTotal Loss: 0.437016\n",
      "Reconstruction: 0.371984, Regularization: 0.000019, Discriminator: 0.043317; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:08,945 root         INFO     Train Epoch: 162 [3584/8000 (45%)]\tTotal Loss: 0.436584\n",
      "Reconstruction: 0.371502, Regularization: 0.000013, Discriminator: 0.043323; Generator: 0.021747,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:09,052 root         INFO     Train Epoch: 162 [4096/8000 (51%)]\tTotal Loss: 0.436391\n",
      "Reconstruction: 0.371426, Regularization: 0.000015, Discriminator: 0.043278; Generator: 0.021671,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:09,160 root         INFO     Train Epoch: 162 [4608/8000 (58%)]\tTotal Loss: 0.436188\n",
      "Reconstruction: 0.371331, Regularization: 0.000019, Discriminator: 0.043216; Generator: 0.021622,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:09,268 root         INFO     Train Epoch: 162 [5120/8000 (64%)]\tTotal Loss: 0.436784\n",
      "Reconstruction: 0.371860, Regularization: 0.000015, Discriminator: 0.043297; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:09,376 root         INFO     Train Epoch: 162 [5632/8000 (70%)]\tTotal Loss: 0.437708\n",
      "Reconstruction: 0.372827, Regularization: 0.000014, Discriminator: 0.043166; Generator: 0.021701,\n",
      "D(x): 0.502, D(G(z)): 0.499\n",
      "2019-04-10 01:00:09,484 root         INFO     Train Epoch: 162 [6144/8000 (77%)]\tTotal Loss: 0.438955\n",
      "Reconstruction: 0.373961, Regularization: 0.000014, Discriminator: 0.043330; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:09,592 root         INFO     Train Epoch: 162 [6656/8000 (83%)]\tTotal Loss: 0.440419\n",
      "Reconstruction: 0.375389, Regularization: 0.000017, Discriminator: 0.043316; Generator: 0.021697,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:09,701 root         INFO     Train Epoch: 162 [7168/8000 (90%)]\tTotal Loss: 0.441459\n",
      "Reconstruction: 0.376324, Regularization: 0.000013, Discriminator: 0.043442; Generator: 0.021680,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:09,809 root         INFO     Train Epoch: 162 [7680/8000 (96%)]\tTotal Loss: 0.442684\n",
      "Reconstruction: 0.377645, Regularization: 0.000011, Discriminator: 0.043334; Generator: 0.021694,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:09,889 root         INFO     ====> Epoch: 162 Average loss: 0.4394\n",
      "2019-04-10 01:00:09,916 root         INFO     Train Epoch: 163 [0/8000 (0%)]\tTotal Loss: 0.443178\n",
      "Reconstruction: 0.378038, Regularization: 0.000013, Discriminator: 0.043408; Generator: 0.021720,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:10,028 root         INFO     Train Epoch: 163 [512/8000 (6%)]\tTotal Loss: 0.443582\n",
      "Reconstruction: 0.378469, Regularization: 0.000014, Discriminator: 0.043381; Generator: 0.021718,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:10,139 root         INFO     Train Epoch: 163 [1024/8000 (13%)]\tTotal Loss: 0.443566\n",
      "Reconstruction: 0.378551, Regularization: 0.000010, Discriminator: 0.043345; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:10,250 root         INFO     Train Epoch: 163 [1536/8000 (19%)]\tTotal Loss: 0.443222\n",
      "Reconstruction: 0.378199, Regularization: 0.000011, Discriminator: 0.043338; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:10,360 root         INFO     Train Epoch: 163 [2048/8000 (26%)]\tTotal Loss: 0.441924\n",
      "Reconstruction: 0.377000, Regularization: 0.000017, Discriminator: 0.043267; Generator: 0.021640,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:10,471 root         INFO     Train Epoch: 163 [2560/8000 (32%)]\tTotal Loss: 0.440305\n",
      "Reconstruction: 0.375437, Regularization: 0.000017, Discriminator: 0.043259; Generator: 0.021593,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:10,581 root         INFO     Train Epoch: 163 [3072/8000 (38%)]\tTotal Loss: 0.438354\n",
      "Reconstruction: 0.373446, Regularization: 0.000016, Discriminator: 0.043268; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:10,691 root         INFO     Train Epoch: 163 [3584/8000 (45%)]\tTotal Loss: 0.436273\n",
      "Reconstruction: 0.371236, Regularization: 0.000027, Discriminator: 0.043396; Generator: 0.021615,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:10,802 root         INFO     Train Epoch: 163 [4096/8000 (51%)]\tTotal Loss: 0.434480\n",
      "Reconstruction: 0.369576, Regularization: 0.000026, Discriminator: 0.043310; Generator: 0.021567,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:10,912 root         INFO     Train Epoch: 163 [4608/8000 (58%)]\tTotal Loss: 0.432725\n",
      "Reconstruction: 0.367713, Regularization: 0.000027, Discriminator: 0.043365; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:11,024 root         INFO     Train Epoch: 163 [5120/8000 (64%)]\tTotal Loss: 0.431899\n",
      "Reconstruction: 0.366867, Regularization: 0.000023, Discriminator: 0.043374; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:11,136 root         INFO     Train Epoch: 163 [5632/8000 (70%)]\tTotal Loss: 0.431515\n",
      "Reconstruction: 0.366542, Regularization: 0.000026, Discriminator: 0.043316; Generator: 0.021632,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:11,248 root         INFO     Train Epoch: 163 [6144/8000 (77%)]\tTotal Loss: 0.431780\n",
      "Reconstruction: 0.366772, Regularization: 0.000035, Discriminator: 0.043365; Generator: 0.021609,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:11,359 root         INFO     Train Epoch: 163 [6656/8000 (83%)]\tTotal Loss: 0.432196\n",
      "Reconstruction: 0.367250, Regularization: 0.000029, Discriminator: 0.043284; Generator: 0.021632,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:11,471 root         INFO     Train Epoch: 163 [7168/8000 (90%)]\tTotal Loss: 0.433122\n",
      "Reconstruction: 0.368100, Regularization: 0.000022, Discriminator: 0.043314; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:11,583 root         INFO     Train Epoch: 163 [7680/8000 (96%)]\tTotal Loss: 0.434417\n",
      "Reconstruction: 0.369397, Regularization: 0.000028, Discriminator: 0.043337; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:11,664 root         INFO     ====> Epoch: 163 Average loss: 0.4368\n",
      "2019-04-10 01:00:11,691 root         INFO     Train Epoch: 164 [0/8000 (0%)]\tTotal Loss: 0.435181\n",
      "Reconstruction: 0.370193, Regularization: 0.000029, Discriminator: 0.043294; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:11,803 root         INFO     Train Epoch: 164 [512/8000 (6%)]\tTotal Loss: 0.436878\n",
      "Reconstruction: 0.371921, Regularization: 0.000033, Discriminator: 0.043277; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:11,914 root         INFO     Train Epoch: 164 [1024/8000 (13%)]\tTotal Loss: 0.438727\n",
      "Reconstruction: 0.373674, Regularization: 0.000053, Discriminator: 0.043346; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:12,027 root         INFO     Train Epoch: 164 [1536/8000 (19%)]\tTotal Loss: 0.440945\n",
      "Reconstruction: 0.375825, Regularization: 0.000055, Discriminator: 0.043350; Generator: 0.021715,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:12,137 root         INFO     Train Epoch: 164 [2048/8000 (26%)]\tTotal Loss: 0.442738\n",
      "Reconstruction: 0.377659, Regularization: 0.000061, Discriminator: 0.043353; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:12,248 root         INFO     Train Epoch: 164 [2560/8000 (32%)]\tTotal Loss: 0.444275\n",
      "Reconstruction: 0.379165, Regularization: 0.000049, Discriminator: 0.043322; Generator: 0.021738,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:12,359 root         INFO     Train Epoch: 164 [3072/8000 (38%)]\tTotal Loss: 0.446438\n",
      "Reconstruction: 0.381318, Regularization: 0.000048, Discriminator: 0.043361; Generator: 0.021710,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:12,469 root         INFO     Train Epoch: 164 [3584/8000 (45%)]\tTotal Loss: 0.448696\n",
      "Reconstruction: 0.383489, Regularization: 0.000034, Discriminator: 0.043441; Generator: 0.021732,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:00:12,580 root         INFO     Train Epoch: 164 [4096/8000 (51%)]\tTotal Loss: 0.449315\n",
      "Reconstruction: 0.384199, Regularization: 0.000031, Discriminator: 0.043315; Generator: 0.021771,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:12,691 root         INFO     Train Epoch: 164 [4608/8000 (58%)]\tTotal Loss: 0.450207\n",
      "Reconstruction: 0.385069, Regularization: 0.000033, Discriminator: 0.043342; Generator: 0.021762,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:12,801 root         INFO     Train Epoch: 164 [5120/8000 (64%)]\tTotal Loss: 0.450442\n",
      "Reconstruction: 0.385368, Regularization: 0.000031, Discriminator: 0.043312; Generator: 0.021731,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:12,911 root         INFO     Train Epoch: 164 [5632/8000 (70%)]\tTotal Loss: 0.450349\n",
      "Reconstruction: 0.385298, Regularization: 0.000033, Discriminator: 0.043267; Generator: 0.021751,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:13,022 root         INFO     Train Epoch: 164 [6144/8000 (77%)]\tTotal Loss: 0.449033\n",
      "Reconstruction: 0.384018, Regularization: 0.000029, Discriminator: 0.043263; Generator: 0.021723,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:13,131 root         INFO     Train Epoch: 164 [6656/8000 (83%)]\tTotal Loss: 0.447091\n",
      "Reconstruction: 0.382037, Regularization: 0.000020, Discriminator: 0.043322; Generator: 0.021711,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:13,241 root         INFO     Train Epoch: 164 [7168/8000 (90%)]\tTotal Loss: 0.445418\n",
      "Reconstruction: 0.380382, Regularization: 0.000031, Discriminator: 0.043343; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:13,351 root         INFO     Train Epoch: 164 [7680/8000 (96%)]\tTotal Loss: 0.443134\n",
      "Reconstruction: 0.378121, Regularization: 0.000041, Discriminator: 0.043269; Generator: 0.021702,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:13,432 root         INFO     ====> Epoch: 164 Average loss: 0.4452\n",
      "2019-04-10 01:00:13,459 root         INFO     Train Epoch: 165 [0/8000 (0%)]\tTotal Loss: 0.442005\n",
      "Reconstruction: 0.376989, Regularization: 0.000037, Discriminator: 0.043310; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:13,570 root         INFO     Train Epoch: 165 [512/8000 (6%)]\tTotal Loss: 0.439241\n",
      "Reconstruction: 0.374332, Regularization: 0.000028, Discriminator: 0.043261; Generator: 0.021620,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:13,681 root         INFO     Train Epoch: 165 [1024/8000 (13%)]\tTotal Loss: 0.436595\n",
      "Reconstruction: 0.371652, Regularization: 0.000023, Discriminator: 0.043249; Generator: 0.021670,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:13,791 root         INFO     Train Epoch: 165 [1536/8000 (19%)]\tTotal Loss: 0.434190\n",
      "Reconstruction: 0.369144, Regularization: 0.000020, Discriminator: 0.043395; Generator: 0.021632,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:13,901 root         INFO     Train Epoch: 165 [2048/8000 (26%)]\tTotal Loss: 0.432115\n",
      "Reconstruction: 0.367108, Regularization: 0.000030, Discriminator: 0.043355; Generator: 0.021622,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,011 root         INFO     Train Epoch: 165 [2560/8000 (32%)]\tTotal Loss: 0.430663\n",
      "Reconstruction: 0.365585, Regularization: 0.000026, Discriminator: 0.043422; Generator: 0.021630,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:14,122 root         INFO     Train Epoch: 165 [3072/8000 (38%)]\tTotal Loss: 0.429405\n",
      "Reconstruction: 0.364512, Regularization: 0.000031, Discriminator: 0.043277; Generator: 0.021584,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,232 root         INFO     Train Epoch: 165 [3584/8000 (45%)]\tTotal Loss: 0.428798\n",
      "Reconstruction: 0.363813, Regularization: 0.000024, Discriminator: 0.043354; Generator: 0.021606,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,343 root         INFO     Train Epoch: 165 [4096/8000 (51%)]\tTotal Loss: 0.428274\n",
      "Reconstruction: 0.363331, Regularization: 0.000026, Discriminator: 0.043319; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,452 root         INFO     Train Epoch: 165 [4608/8000 (58%)]\tTotal Loss: 0.427995\n",
      "Reconstruction: 0.363089, Regularization: 0.000027, Discriminator: 0.043309; Generator: 0.021570,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,562 root         INFO     Train Epoch: 165 [5120/8000 (64%)]\tTotal Loss: 0.429496\n",
      "Reconstruction: 0.364558, Regularization: 0.000026, Discriminator: 0.043334; Generator: 0.021578,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,671 root         INFO     Train Epoch: 165 [5632/8000 (70%)]\tTotal Loss: 0.430453\n",
      "Reconstruction: 0.365578, Regularization: 0.000023, Discriminator: 0.043263; Generator: 0.021589,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,779 root         INFO     Train Epoch: 165 [6144/8000 (77%)]\tTotal Loss: 0.431998\n",
      "Reconstruction: 0.367009, Regularization: 0.000020, Discriminator: 0.043354; Generator: 0.021615,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,887 root         INFO     Train Epoch: 165 [6656/8000 (83%)]\tTotal Loss: 0.433087\n",
      "Reconstruction: 0.368212, Regularization: 0.000022, Discriminator: 0.043278; Generator: 0.021575,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:14,995 root         INFO     Train Epoch: 165 [7168/8000 (90%)]\tTotal Loss: 0.434914\n",
      "Reconstruction: 0.369990, Regularization: 0.000024, Discriminator: 0.043310; Generator: 0.021591,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:15,105 root         INFO     Train Epoch: 165 [7680/8000 (96%)]\tTotal Loss: 0.436713\n",
      "Reconstruction: 0.371763, Regularization: 0.000026, Discriminator: 0.043335; Generator: 0.021588,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:15,186 root         INFO     ====> Epoch: 165 Average loss: 0.4326\n",
      "2019-04-10 01:00:15,213 root         INFO     Train Epoch: 166 [0/8000 (0%)]\tTotal Loss: 0.437888\n",
      "Reconstruction: 0.372920, Regularization: 0.000023, Discriminator: 0.043357; Generator: 0.021589,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:15,324 root         INFO     Train Epoch: 166 [512/8000 (6%)]\tTotal Loss: 0.439590\n",
      "Reconstruction: 0.374601, Regularization: 0.000021, Discriminator: 0.043337; Generator: 0.021631,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:15,436 root         INFO     Train Epoch: 166 [1024/8000 (13%)]\tTotal Loss: 0.441258\n",
      "Reconstruction: 0.376352, Regularization: 0.000012, Discriminator: 0.043255; Generator: 0.021639,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:15,546 root         INFO     Train Epoch: 166 [1536/8000 (19%)]\tTotal Loss: 0.443259\n",
      "Reconstruction: 0.378077, Regularization: 0.000014, Discriminator: 0.043450; Generator: 0.021719,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:00:15,658 root         INFO     Train Epoch: 166 [2048/8000 (26%)]\tTotal Loss: 0.444043\n",
      "Reconstruction: 0.379106, Regularization: 0.000017, Discriminator: 0.043310; Generator: 0.021611,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:15,769 root         INFO     Train Epoch: 166 [2560/8000 (32%)]\tTotal Loss: 0.445187\n",
      "Reconstruction: 0.380197, Regularization: 0.000010, Discriminator: 0.043298; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:15,879 root         INFO     Train Epoch: 166 [3072/8000 (38%)]\tTotal Loss: 0.445811\n",
      "Reconstruction: 0.380820, Regularization: 0.000017, Discriminator: 0.043293; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:15,991 root         INFO     Train Epoch: 166 [3584/8000 (45%)]\tTotal Loss: 0.446090\n",
      "Reconstruction: 0.381082, Regularization: 0.000010, Discriminator: 0.043307; Generator: 0.021692,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:16,102 root         INFO     Train Epoch: 166 [4096/8000 (51%)]\tTotal Loss: 0.446074\n",
      "Reconstruction: 0.381043, Regularization: 0.000011, Discriminator: 0.043303; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:16,213 root         INFO     Train Epoch: 166 [4608/8000 (58%)]\tTotal Loss: 0.445772\n",
      "Reconstruction: 0.380705, Regularization: 0.000008, Discriminator: 0.043340; Generator: 0.021719,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:16,324 root         INFO     Train Epoch: 166 [5120/8000 (64%)]\tTotal Loss: 0.445174\n",
      "Reconstruction: 0.380087, Regularization: 0.000011, Discriminator: 0.043347; Generator: 0.021729,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:16,436 root         INFO     Train Epoch: 166 [5632/8000 (70%)]\tTotal Loss: 0.444656\n",
      "Reconstruction: 0.379631, Regularization: 0.000014, Discriminator: 0.043281; Generator: 0.021730,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:16,547 root         INFO     Train Epoch: 166 [6144/8000 (77%)]\tTotal Loss: 0.443688\n",
      "Reconstruction: 0.378619, Regularization: 0.000011, Discriminator: 0.043361; Generator: 0.021698,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:16,659 root         INFO     Train Epoch: 166 [6656/8000 (83%)]\tTotal Loss: 0.442713\n",
      "Reconstruction: 0.377646, Regularization: 0.000018, Discriminator: 0.043319; Generator: 0.021731,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:16,771 root         INFO     Train Epoch: 166 [7168/8000 (90%)]\tTotal Loss: 0.441965\n",
      "Reconstruction: 0.376908, Regularization: 0.000015, Discriminator: 0.043341; Generator: 0.021701,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:16,882 root         INFO     Train Epoch: 166 [7680/8000 (96%)]\tTotal Loss: 0.441200\n",
      "Reconstruction: 0.376083, Regularization: 0.000012, Discriminator: 0.043344; Generator: 0.021762,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:16,961 root         INFO     ====> Epoch: 166 Average loss: 0.4436\n",
      "2019-04-10 01:00:16,989 root         INFO     Train Epoch: 167 [0/8000 (0%)]\tTotal Loss: 0.440803\n",
      "Reconstruction: 0.375771, Regularization: 0.000016, Discriminator: 0.043304; Generator: 0.021712,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:17,099 root         INFO     Train Epoch: 167 [512/8000 (6%)]\tTotal Loss: 0.440156\n",
      "Reconstruction: 0.375101, Regularization: 0.000011, Discriminator: 0.043308; Generator: 0.021737,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:17,208 root         INFO     Train Epoch: 167 [1024/8000 (13%)]\tTotal Loss: 0.439791\n",
      "Reconstruction: 0.374754, Regularization: 0.000010, Discriminator: 0.043307; Generator: 0.021721,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:17,319 root         INFO     Train Epoch: 167 [1536/8000 (19%)]\tTotal Loss: 0.439647\n",
      "Reconstruction: 0.374717, Regularization: 0.000014, Discriminator: 0.043269; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:17,427 root         INFO     Train Epoch: 167 [2048/8000 (26%)]\tTotal Loss: 0.439725\n",
      "Reconstruction: 0.374766, Regularization: 0.000010, Discriminator: 0.043282; Generator: 0.021667,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:17,535 root         INFO     Train Epoch: 167 [2560/8000 (32%)]\tTotal Loss: 0.440100\n",
      "Reconstruction: 0.375205, Regularization: 0.000008, Discriminator: 0.043237; Generator: 0.021650,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 01:00:17,643 root         INFO     Train Epoch: 167 [3072/8000 (38%)]\tTotal Loss: 0.440522\n",
      "Reconstruction: 0.375474, Regularization: 0.000012, Discriminator: 0.043336; Generator: 0.021699,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:17,751 root         INFO     Train Epoch: 167 [3584/8000 (45%)]\tTotal Loss: 0.440667\n",
      "Reconstruction: 0.375611, Regularization: 0.000007, Discriminator: 0.043338; Generator: 0.021710,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:17,859 root         INFO     Train Epoch: 167 [4096/8000 (51%)]\tTotal Loss: 0.440852\n",
      "Reconstruction: 0.375810, Regularization: 0.000011, Discriminator: 0.043335; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:17,967 root         INFO     Train Epoch: 167 [4608/8000 (58%)]\tTotal Loss: 0.440476\n",
      "Reconstruction: 0.375474, Regularization: 0.000011, Discriminator: 0.043325; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:18,075 root         INFO     Train Epoch: 167 [5120/8000 (64%)]\tTotal Loss: 0.440285\n",
      "Reconstruction: 0.375274, Regularization: 0.000011, Discriminator: 0.043353; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:18,182 root         INFO     Train Epoch: 167 [5632/8000 (70%)]\tTotal Loss: 0.439685\n",
      "Reconstruction: 0.374651, Regularization: 0.000012, Discriminator: 0.043359; Generator: 0.021663,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:18,290 root         INFO     Train Epoch: 167 [6144/8000 (77%)]\tTotal Loss: 0.438422\n",
      "Reconstruction: 0.373441, Regularization: 0.000008, Discriminator: 0.043341; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:18,397 root         INFO     Train Epoch: 167 [6656/8000 (83%)]\tTotal Loss: 0.437011\n",
      "Reconstruction: 0.372084, Regularization: 0.000010, Discriminator: 0.043319; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:18,506 root         INFO     Train Epoch: 167 [7168/8000 (90%)]\tTotal Loss: 0.435858\n",
      "Reconstruction: 0.370954, Regularization: 0.000005, Discriminator: 0.043303; Generator: 0.021596,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:18,612 root         INFO     Train Epoch: 167 [7680/8000 (96%)]\tTotal Loss: 0.434697\n",
      "Reconstruction: 0.369719, Regularization: 0.000008, Discriminator: 0.043383; Generator: 0.021586,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:18,691 root         INFO     ====> Epoch: 167 Average loss: 0.4392\n",
      "2019-04-10 01:00:18,718 root         INFO     Train Epoch: 168 [0/8000 (0%)]\tTotal Loss: 0.433946\n",
      "Reconstruction: 0.369048, Regularization: 0.000013, Discriminator: 0.043299; Generator: 0.021586,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:18,827 root         INFO     Train Epoch: 168 [512/8000 (6%)]\tTotal Loss: 0.433321\n",
      "Reconstruction: 0.368409, Regularization: 0.000012, Discriminator: 0.043357; Generator: 0.021543,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 01:00:18,935 root         INFO     Train Epoch: 168 [1024/8000 (13%)]\tTotal Loss: 0.433124\n",
      "Reconstruction: 0.368216, Regularization: 0.000013, Discriminator: 0.043280; Generator: 0.021615,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:19,043 root         INFO     Train Epoch: 168 [1536/8000 (19%)]\tTotal Loss: 0.432535\n",
      "Reconstruction: 0.367588, Regularization: 0.000008, Discriminator: 0.043310; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:19,151 root         INFO     Train Epoch: 168 [2048/8000 (26%)]\tTotal Loss: 0.432475\n",
      "Reconstruction: 0.367578, Regularization: 0.000012, Discriminator: 0.043306; Generator: 0.021579,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:19,259 root         INFO     Train Epoch: 168 [2560/8000 (32%)]\tTotal Loss: 0.432703\n",
      "Reconstruction: 0.367789, Regularization: 0.000010, Discriminator: 0.043323; Generator: 0.021581,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:19,366 root         INFO     Train Epoch: 168 [3072/8000 (38%)]\tTotal Loss: 0.432999\n",
      "Reconstruction: 0.368061, Regularization: 0.000011, Discriminator: 0.043314; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:19,475 root         INFO     Train Epoch: 168 [3584/8000 (45%)]\tTotal Loss: 0.433856\n",
      "Reconstruction: 0.368815, Regularization: 0.000010, Discriminator: 0.043351; Generator: 0.021680,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:19,583 root         INFO     Train Epoch: 168 [4096/8000 (51%)]\tTotal Loss: 0.434433\n",
      "Reconstruction: 0.369550, Regularization: 0.000010, Discriminator: 0.043275; Generator: 0.021598,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:19,691 root         INFO     Train Epoch: 168 [4608/8000 (58%)]\tTotal Loss: 0.435542\n",
      "Reconstruction: 0.370522, Regularization: 0.000008, Discriminator: 0.043357; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:19,799 root         INFO     Train Epoch: 168 [5120/8000 (64%)]\tTotal Loss: 0.436790\n",
      "Reconstruction: 0.371780, Regularization: 0.000009, Discriminator: 0.043322; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:19,908 root         INFO     Train Epoch: 168 [5632/8000 (70%)]\tTotal Loss: 0.438121\n",
      "Reconstruction: 0.373022, Regularization: 0.000008, Discriminator: 0.043393; Generator: 0.021698,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:20,016 root         INFO     Train Epoch: 168 [6144/8000 (77%)]\tTotal Loss: 0.439360\n",
      "Reconstruction: 0.374338, Regularization: 0.000009, Discriminator: 0.043344; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:20,124 root         INFO     Train Epoch: 168 [6656/8000 (83%)]\tTotal Loss: 0.440953\n",
      "Reconstruction: 0.375917, Regularization: 0.000011, Discriminator: 0.043331; Generator: 0.021694,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:20,232 root         INFO     Train Epoch: 168 [7168/8000 (90%)]\tTotal Loss: 0.442164\n",
      "Reconstruction: 0.377133, Regularization: 0.000009, Discriminator: 0.043271; Generator: 0.021751,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:20,338 root         INFO     Train Epoch: 168 [7680/8000 (96%)]\tTotal Loss: 0.443267\n",
      "Reconstruction: 0.378237, Regularization: 0.000008, Discriminator: 0.043358; Generator: 0.021663,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:20,418 root         INFO     ====> Epoch: 168 Average loss: 0.4361\n",
      "2019-04-10 01:00:20,445 root         INFO     Train Epoch: 169 [0/8000 (0%)]\tTotal Loss: 0.444159\n",
      "Reconstruction: 0.379065, Regularization: 0.000010, Discriminator: 0.043329; Generator: 0.021754,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:20,553 root         INFO     Train Epoch: 169 [512/8000 (6%)]\tTotal Loss: 0.445337\n",
      "Reconstruction: 0.380256, Regularization: 0.000009, Discriminator: 0.043342; Generator: 0.021730,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:20,659 root         INFO     Train Epoch: 169 [1024/8000 (13%)]\tTotal Loss: 0.446480\n",
      "Reconstruction: 0.381357, Regularization: 0.000005, Discriminator: 0.043349; Generator: 0.021769,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:20,766 root         INFO     Train Epoch: 169 [1536/8000 (19%)]\tTotal Loss: 0.447104\n",
      "Reconstruction: 0.382045, Regularization: 0.000006, Discriminator: 0.043338; Generator: 0.021715,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:20,873 root         INFO     Train Epoch: 169 [2048/8000 (26%)]\tTotal Loss: 0.447595\n",
      "Reconstruction: 0.382532, Regularization: 0.000007, Discriminator: 0.043319; Generator: 0.021736,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:20,980 root         INFO     Train Epoch: 169 [2560/8000 (32%)]\tTotal Loss: 0.447717\n",
      "Reconstruction: 0.382635, Regularization: 0.000009, Discriminator: 0.043322; Generator: 0.021751,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:21,086 root         INFO     Train Epoch: 169 [3072/8000 (38%)]\tTotal Loss: 0.447535\n",
      "Reconstruction: 0.382511, Regularization: 0.000007, Discriminator: 0.043319; Generator: 0.021697,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:21,193 root         INFO     Train Epoch: 169 [3584/8000 (45%)]\tTotal Loss: 0.446997\n",
      "Reconstruction: 0.381998, Regularization: 0.000006, Discriminator: 0.043313; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:21,299 root         INFO     Train Epoch: 169 [4096/8000 (51%)]\tTotal Loss: 0.446399\n",
      "Reconstruction: 0.381352, Regularization: 0.000010, Discriminator: 0.043343; Generator: 0.021694,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:21,406 root         INFO     Train Epoch: 169 [4608/8000 (58%)]\tTotal Loss: 0.445394\n",
      "Reconstruction: 0.380349, Regularization: 0.000008, Discriminator: 0.043296; Generator: 0.021742,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:21,513 root         INFO     Train Epoch: 169 [5120/8000 (64%)]\tTotal Loss: 0.444095\n",
      "Reconstruction: 0.379073, Regularization: 0.000009, Discriminator: 0.043304; Generator: 0.021710,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:21,619 root         INFO     Train Epoch: 169 [5632/8000 (70%)]\tTotal Loss: 0.442323\n",
      "Reconstruction: 0.377277, Regularization: 0.000011, Discriminator: 0.043313; Generator: 0.021722,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:21,726 root         INFO     Train Epoch: 169 [6144/8000 (77%)]\tTotal Loss: 0.440172\n",
      "Reconstruction: 0.375135, Regularization: 0.000006, Discriminator: 0.043385; Generator: 0.021645,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:21,832 root         INFO     Train Epoch: 169 [6656/8000 (83%)]\tTotal Loss: 0.437937\n",
      "Reconstruction: 0.372998, Regularization: 0.000012, Discriminator: 0.043271; Generator: 0.021656,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:21,939 root         INFO     Train Epoch: 169 [7168/8000 (90%)]\tTotal Loss: 0.436173\n",
      "Reconstruction: 0.371120, Regularization: 0.000007, Discriminator: 0.043374; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:22,047 root         INFO     Train Epoch: 169 [7680/8000 (96%)]\tTotal Loss: 0.434146\n",
      "Reconstruction: 0.369237, Regularization: 0.000004, Discriminator: 0.043311; Generator: 0.021593,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:22,127 root         INFO     ====> Epoch: 169 Average loss: 0.4436\n",
      "2019-04-10 01:00:22,154 root         INFO     Train Epoch: 170 [0/8000 (0%)]\tTotal Loss: 0.433445\n",
      "Reconstruction: 0.368477, Regularization: 0.000007, Discriminator: 0.043353; Generator: 0.021609,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:22,264 root         INFO     Train Epoch: 170 [512/8000 (6%)]\tTotal Loss: 0.432291\n",
      "Reconstruction: 0.367337, Regularization: 0.000006, Discriminator: 0.043332; Generator: 0.021616,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:22,373 root         INFO     Train Epoch: 170 [1024/8000 (13%)]\tTotal Loss: 0.431579\n",
      "Reconstruction: 0.366630, Regularization: 0.000008, Discriminator: 0.043331; Generator: 0.021610,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:22,483 root         INFO     Train Epoch: 170 [1536/8000 (19%)]\tTotal Loss: 0.431461\n",
      "Reconstruction: 0.366501, Regularization: 0.000005, Discriminator: 0.043320; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:22,593 root         INFO     Train Epoch: 170 [2048/8000 (26%)]\tTotal Loss: 0.431522\n",
      "Reconstruction: 0.366577, Regularization: 0.000009, Discriminator: 0.043323; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:22,703 root         INFO     Train Epoch: 170 [2560/8000 (32%)]\tTotal Loss: 0.431462\n",
      "Reconstruction: 0.366513, Regularization: 0.000008, Discriminator: 0.043313; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:22,812 root         INFO     Train Epoch: 170 [3072/8000 (38%)]\tTotal Loss: 0.432203\n",
      "Reconstruction: 0.367271, Regularization: 0.000008, Discriminator: 0.043323; Generator: 0.021600,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:22,920 root         INFO     Train Epoch: 170 [3584/8000 (45%)]\tTotal Loss: 0.432623\n",
      "Reconstruction: 0.367700, Regularization: 0.000009, Discriminator: 0.043316; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:23,028 root         INFO     Train Epoch: 170 [4096/8000 (51%)]\tTotal Loss: 0.433761\n",
      "Reconstruction: 0.368896, Regularization: 0.000007, Discriminator: 0.043260; Generator: 0.021599,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:23,136 root         INFO     Train Epoch: 170 [4608/8000 (58%)]\tTotal Loss: 0.434978\n",
      "Reconstruction: 0.370042, Regularization: 0.000007, Discriminator: 0.043344; Generator: 0.021585,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:23,244 root         INFO     Train Epoch: 170 [5120/8000 (64%)]\tTotal Loss: 0.436425\n",
      "Reconstruction: 0.371509, Regularization: 0.000007, Discriminator: 0.043276; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:23,352 root         INFO     Train Epoch: 170 [5632/8000 (70%)]\tTotal Loss: 0.437848\n",
      "Reconstruction: 0.372894, Regularization: 0.000010, Discriminator: 0.043283; Generator: 0.021661,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:23,460 root         INFO     Train Epoch: 170 [6144/8000 (77%)]\tTotal Loss: 0.439061\n",
      "Reconstruction: 0.374151, Regularization: 0.000015, Discriminator: 0.043266; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:23,569 root         INFO     Train Epoch: 170 [6656/8000 (83%)]\tTotal Loss: 0.440473\n",
      "Reconstruction: 0.375503, Regularization: 0.000016, Discriminator: 0.043319; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:23,677 root         INFO     Train Epoch: 170 [7168/8000 (90%)]\tTotal Loss: 0.441316\n",
      "Reconstruction: 0.376295, Regularization: 0.000013, Discriminator: 0.043327; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:23,785 root         INFO     Train Epoch: 170 [7680/8000 (96%)]\tTotal Loss: 0.441854\n",
      "Reconstruction: 0.376841, Regularization: 0.000013, Discriminator: 0.043320; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:23,865 root         INFO     ====> Epoch: 170 Average loss: 0.4353\n",
      "2019-04-10 01:00:23,892 root         INFO     Train Epoch: 171 [0/8000 (0%)]\tTotal Loss: 0.442069\n",
      "Reconstruction: 0.377115, Regularization: 0.000015, Discriminator: 0.043260; Generator: 0.021678,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:24,001 root         INFO     Train Epoch: 171 [512/8000 (6%)]\tTotal Loss: 0.442469\n",
      "Reconstruction: 0.377415, Regularization: 0.000013, Discriminator: 0.043299; Generator: 0.021742,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:24,110 root         INFO     Train Epoch: 171 [1024/8000 (13%)]\tTotal Loss: 0.441639\n",
      "Reconstruction: 0.376686, Regularization: 0.000012, Discriminator: 0.043287; Generator: 0.021654,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:24,219 root         INFO     Train Epoch: 171 [1536/8000 (19%)]\tTotal Loss: 0.441435\n",
      "Reconstruction: 0.376448, Regularization: 0.000012, Discriminator: 0.043314; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:24,328 root         INFO     Train Epoch: 171 [2048/8000 (26%)]\tTotal Loss: 0.440867\n",
      "Reconstruction: 0.375734, Regularization: 0.000013, Discriminator: 0.043390; Generator: 0.021729,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:24,437 root         INFO     Train Epoch: 171 [2560/8000 (32%)]\tTotal Loss: 0.440636\n",
      "Reconstruction: 0.375569, Regularization: 0.000009, Discriminator: 0.043357; Generator: 0.021701,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:24,546 root         INFO     Train Epoch: 171 [3072/8000 (38%)]\tTotal Loss: 0.440384\n",
      "Reconstruction: 0.375313, Regularization: 0.000012, Discriminator: 0.043333; Generator: 0.021726,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:24,656 root         INFO     Train Epoch: 171 [3584/8000 (45%)]\tTotal Loss: 0.440273\n",
      "Reconstruction: 0.375203, Regularization: 0.000011, Discriminator: 0.043329; Generator: 0.021730,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:24,765 root         INFO     Train Epoch: 171 [4096/8000 (51%)]\tTotal Loss: 0.440155\n",
      "Reconstruction: 0.375103, Regularization: 0.000015, Discriminator: 0.043326; Generator: 0.021710,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:24,874 root         INFO     Train Epoch: 171 [4608/8000 (58%)]\tTotal Loss: 0.440048\n",
      "Reconstruction: 0.375043, Regularization: 0.000014, Discriminator: 0.043306; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:24,983 root         INFO     Train Epoch: 171 [5120/8000 (64%)]\tTotal Loss: 0.440658\n",
      "Reconstruction: 0.375611, Regularization: 0.000010, Discriminator: 0.043312; Generator: 0.021725,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:25,091 root         INFO     Train Epoch: 171 [5632/8000 (70%)]\tTotal Loss: 0.440947\n",
      "Reconstruction: 0.375904, Regularization: 0.000012, Discriminator: 0.043328; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:25,200 root         INFO     Train Epoch: 171 [6144/8000 (77%)]\tTotal Loss: 0.441577\n",
      "Reconstruction: 0.376624, Regularization: 0.000011, Discriminator: 0.043277; Generator: 0.021665,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:25,309 root         INFO     Train Epoch: 171 [6656/8000 (83%)]\tTotal Loss: 0.442219\n",
      "Reconstruction: 0.377205, Regularization: 0.000015, Discriminator: 0.043309; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:25,418 root         INFO     Train Epoch: 171 [7168/8000 (90%)]\tTotal Loss: 0.442678\n",
      "Reconstruction: 0.377673, Regularization: 0.000014, Discriminator: 0.043315; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:25,527 root         INFO     Train Epoch: 171 [7680/8000 (96%)]\tTotal Loss: 0.443151\n",
      "Reconstruction: 0.378147, Regularization: 0.000015, Discriminator: 0.043305; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:25,608 root         INFO     ====> Epoch: 171 Average loss: 0.4413\n",
      "2019-04-10 01:00:25,635 root         INFO     Train Epoch: 172 [0/8000 (0%)]\tTotal Loss: 0.443259\n",
      "Reconstruction: 0.378279, Regularization: 0.000015, Discriminator: 0.043344; Generator: 0.021621,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:25,746 root         INFO     Train Epoch: 172 [512/8000 (6%)]\tTotal Loss: 0.444098\n",
      "Reconstruction: 0.379051, Regularization: 0.000012, Discriminator: 0.043350; Generator: 0.021685,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:25,856 root         INFO     Train Epoch: 172 [1024/8000 (13%)]\tTotal Loss: 0.444154\n",
      "Reconstruction: 0.379128, Regularization: 0.000015, Discriminator: 0.043326; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:25,966 root         INFO     Train Epoch: 172 [1536/8000 (19%)]\tTotal Loss: 0.443653\n",
      "Reconstruction: 0.378579, Regularization: 0.000016, Discriminator: 0.043371; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:26,077 root         INFO     Train Epoch: 172 [2048/8000 (26%)]\tTotal Loss: 0.442739\n",
      "Reconstruction: 0.377773, Regularization: 0.000014, Discriminator: 0.043337; Generator: 0.021616,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:26,187 root         INFO     Train Epoch: 172 [2560/8000 (32%)]\tTotal Loss: 0.441645\n",
      "Reconstruction: 0.376635, Regularization: 0.000014, Discriminator: 0.043327; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:26,297 root         INFO     Train Epoch: 172 [3072/8000 (38%)]\tTotal Loss: 0.440454\n",
      "Reconstruction: 0.375514, Regularization: 0.000022, Discriminator: 0.043313; Generator: 0.021605,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:26,408 root         INFO     Train Epoch: 172 [3584/8000 (45%)]\tTotal Loss: 0.438509\n",
      "Reconstruction: 0.373592, Regularization: 0.000017, Discriminator: 0.043288; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:26,518 root         INFO     Train Epoch: 172 [4096/8000 (51%)]\tTotal Loss: 0.436926\n",
      "Reconstruction: 0.371895, Regularization: 0.000014, Discriminator: 0.043399; Generator: 0.021617,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 01:00:26,629 root         INFO     Train Epoch: 172 [4608/8000 (58%)]\tTotal Loss: 0.434956\n",
      "Reconstruction: 0.370047, Regularization: 0.000018, Discriminator: 0.043285; Generator: 0.021606,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:26,739 root         INFO     Train Epoch: 172 [5120/8000 (64%)]\tTotal Loss: 0.433448\n",
      "Reconstruction: 0.368422, Regularization: 0.000017, Discriminator: 0.043371; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:26,849 root         INFO     Train Epoch: 172 [5632/8000 (70%)]\tTotal Loss: 0.432323\n",
      "Reconstruction: 0.367232, Regularization: 0.000017, Discriminator: 0.043415; Generator: 0.021659,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:26,959 root         INFO     Train Epoch: 172 [6144/8000 (77%)]\tTotal Loss: 0.431074\n",
      "Reconstruction: 0.366078, Regularization: 0.000013, Discriminator: 0.043331; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:27,069 root         INFO     Train Epoch: 172 [6656/8000 (83%)]\tTotal Loss: 0.430928\n",
      "Reconstruction: 0.365904, Regularization: 0.000016, Discriminator: 0.043326; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:27,179 root         INFO     Train Epoch: 172 [7168/8000 (90%)]\tTotal Loss: 0.431174\n",
      "Reconstruction: 0.366197, Regularization: 0.000017, Discriminator: 0.043301; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:27,290 root         INFO     Train Epoch: 172 [7680/8000 (96%)]\tTotal Loss: 0.431709\n",
      "Reconstruction: 0.366752, Regularization: 0.000014, Discriminator: 0.043316; Generator: 0.021628,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:27,370 root         INFO     ====> Epoch: 172 Average loss: 0.4373\n",
      "2019-04-10 01:00:27,397 root         INFO     Train Epoch: 173 [0/8000 (0%)]\tTotal Loss: 0.432608\n",
      "Reconstruction: 0.367540, Regularization: 0.000012, Discriminator: 0.043403; Generator: 0.021653,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:27,510 root         INFO     Train Epoch: 173 [512/8000 (6%)]\tTotal Loss: 0.433746\n",
      "Reconstruction: 0.368883, Regularization: 0.000022, Discriminator: 0.043242; Generator: 0.021599,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:27,622 root         INFO     Train Epoch: 173 [1024/8000 (13%)]\tTotal Loss: 0.435183\n",
      "Reconstruction: 0.370319, Regularization: 0.000020, Discriminator: 0.043196; Generator: 0.021647,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 01:00:27,734 root         INFO     Train Epoch: 173 [1536/8000 (19%)]\tTotal Loss: 0.437758\n",
      "Reconstruction: 0.372734, Regularization: 0.000019, Discriminator: 0.043357; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:27,845 root         INFO     Train Epoch: 173 [2048/8000 (26%)]\tTotal Loss: 0.439932\n",
      "Reconstruction: 0.374834, Regularization: 0.000017, Discriminator: 0.043369; Generator: 0.021713,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:27,957 root         INFO     Train Epoch: 173 [2560/8000 (32%)]\tTotal Loss: 0.441549\n",
      "Reconstruction: 0.376548, Regularization: 0.000016, Discriminator: 0.043279; Generator: 0.021706,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:28,069 root         INFO     Train Epoch: 173 [3072/8000 (38%)]\tTotal Loss: 0.443455\n",
      "Reconstruction: 0.378382, Regularization: 0.000017, Discriminator: 0.043367; Generator: 0.021688,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:28,180 root         INFO     Train Epoch: 173 [3584/8000 (45%)]\tTotal Loss: 0.444597\n",
      "Reconstruction: 0.379589, Regularization: 0.000014, Discriminator: 0.043325; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:28,292 root         INFO     Train Epoch: 173 [4096/8000 (51%)]\tTotal Loss: 0.445808\n",
      "Reconstruction: 0.380816, Regularization: 0.000015, Discriminator: 0.043306; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:28,404 root         INFO     Train Epoch: 173 [4608/8000 (58%)]\tTotal Loss: 0.446771\n",
      "Reconstruction: 0.381712, Regularization: 0.000016, Discriminator: 0.043349; Generator: 0.021694,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:28,516 root         INFO     Train Epoch: 173 [5120/8000 (64%)]\tTotal Loss: 0.447106\n",
      "Reconstruction: 0.382082, Regularization: 0.000018, Discriminator: 0.043320; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:28,627 root         INFO     Train Epoch: 173 [5632/8000 (70%)]\tTotal Loss: 0.446385\n",
      "Reconstruction: 0.381431, Regularization: 0.000017, Discriminator: 0.043292; Generator: 0.021645,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:28,739 root         INFO     Train Epoch: 173 [6144/8000 (77%)]\tTotal Loss: 0.445719\n",
      "Reconstruction: 0.380751, Regularization: 0.000016, Discriminator: 0.043285; Generator: 0.021667,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:28,851 root         INFO     Train Epoch: 173 [6656/8000 (83%)]\tTotal Loss: 0.444523\n",
      "Reconstruction: 0.379552, Regularization: 0.000017, Discriminator: 0.043276; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:28,963 root         INFO     Train Epoch: 173 [7168/8000 (90%)]\tTotal Loss: 0.443005\n",
      "Reconstruction: 0.377905, Regularization: 0.000018, Discriminator: 0.043372; Generator: 0.021711,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:29,074 root         INFO     Train Epoch: 173 [7680/8000 (96%)]\tTotal Loss: 0.441250\n",
      "Reconstruction: 0.376235, Regularization: 0.000029, Discriminator: 0.043269; Generator: 0.021718,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:29,154 root         INFO     ====> Epoch: 173 Average loss: 0.4421\n",
      "2019-04-10 01:00:29,182 root         INFO     Train Epoch: 174 [0/8000 (0%)]\tTotal Loss: 0.439703\n",
      "Reconstruction: 0.374689, Regularization: 0.000025, Discriminator: 0.043316; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:29,294 root         INFO     Train Epoch: 174 [512/8000 (6%)]\tTotal Loss: 0.438318\n",
      "Reconstruction: 0.373213, Regularization: 0.000025, Discriminator: 0.043331; Generator: 0.021750,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:29,405 root         INFO     Train Epoch: 174 [1024/8000 (13%)]\tTotal Loss: 0.436598\n",
      "Reconstruction: 0.371580, Regularization: 0.000020, Discriminator: 0.043293; Generator: 0.021704,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:29,516 root         INFO     Train Epoch: 174 [1536/8000 (19%)]\tTotal Loss: 0.435584\n",
      "Reconstruction: 0.370396, Regularization: 0.000023, Discriminator: 0.043459; Generator: 0.021706,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:00:29,627 root         INFO     Train Epoch: 174 [2048/8000 (26%)]\tTotal Loss: 0.434510\n",
      "Reconstruction: 0.369527, Regularization: 0.000017, Discriminator: 0.043289; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:29,738 root         INFO     Train Epoch: 174 [2560/8000 (32%)]\tTotal Loss: 0.434480\n",
      "Reconstruction: 0.369519, Regularization: 0.000018, Discriminator: 0.043329; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:29,849 root         INFO     Train Epoch: 174 [3072/8000 (38%)]\tTotal Loss: 0.434521\n",
      "Reconstruction: 0.369555, Regularization: 0.000018, Discriminator: 0.043333; Generator: 0.021615,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:29,960 root         INFO     Train Epoch: 174 [3584/8000 (45%)]\tTotal Loss: 0.435059\n",
      "Reconstruction: 0.370055, Regularization: 0.000016, Discriminator: 0.043320; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:30,071 root         INFO     Train Epoch: 174 [4096/8000 (51%)]\tTotal Loss: 0.435865\n",
      "Reconstruction: 0.370901, Regularization: 0.000021, Discriminator: 0.043311; Generator: 0.021632,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:30,181 root         INFO     Train Epoch: 174 [4608/8000 (58%)]\tTotal Loss: 0.436976\n",
      "Reconstruction: 0.371992, Regularization: 0.000019, Discriminator: 0.043321; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:30,292 root         INFO     Train Epoch: 174 [5120/8000 (64%)]\tTotal Loss: 0.438178\n",
      "Reconstruction: 0.373193, Regularization: 0.000020, Discriminator: 0.043388; Generator: 0.021576,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:30,403 root         INFO     Train Epoch: 174 [5632/8000 (70%)]\tTotal Loss: 0.440009\n",
      "Reconstruction: 0.375013, Regularization: 0.000021, Discriminator: 0.043325; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:30,513 root         INFO     Train Epoch: 174 [6144/8000 (77%)]\tTotal Loss: 0.441475\n",
      "Reconstruction: 0.376362, Regularization: 0.000025, Discriminator: 0.043406; Generator: 0.021683,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:30,623 root         INFO     Train Epoch: 174 [6656/8000 (83%)]\tTotal Loss: 0.442383\n",
      "Reconstruction: 0.377341, Regularization: 0.000032, Discriminator: 0.043342; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:30,732 root         INFO     Train Epoch: 174 [7168/8000 (90%)]\tTotal Loss: 0.443452\n",
      "Reconstruction: 0.378387, Regularization: 0.000033, Discriminator: 0.043346; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:30,842 root         INFO     Train Epoch: 174 [7680/8000 (96%)]\tTotal Loss: 0.443644\n",
      "Reconstruction: 0.378669, Regularization: 0.000033, Discriminator: 0.043321; Generator: 0.021621,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:30,922 root         INFO     ====> Epoch: 174 Average loss: 0.4380\n",
      "2019-04-10 01:00:30,949 root         INFO     Train Epoch: 175 [0/8000 (0%)]\tTotal Loss: 0.443463\n",
      "Reconstruction: 0.378448, Regularization: 0.000034, Discriminator: 0.043281; Generator: 0.021701,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:31,061 root         INFO     Train Epoch: 175 [512/8000 (6%)]\tTotal Loss: 0.442592\n",
      "Reconstruction: 0.377666, Regularization: 0.000034, Discriminator: 0.043293; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:31,173 root         INFO     Train Epoch: 175 [1024/8000 (13%)]\tTotal Loss: 0.441410\n",
      "Reconstruction: 0.376460, Regularization: 0.000028, Discriminator: 0.043309; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:31,284 root         INFO     Train Epoch: 175 [1536/8000 (19%)]\tTotal Loss: 0.438927\n",
      "Reconstruction: 0.374034, Regularization: 0.000033, Discriminator: 0.043260; Generator: 0.021600,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:31,395 root         INFO     Train Epoch: 175 [2048/8000 (26%)]\tTotal Loss: 0.438056\n",
      "Reconstruction: 0.373106, Regularization: 0.000030, Discriminator: 0.043279; Generator: 0.021642,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:31,506 root         INFO     Train Epoch: 175 [2560/8000 (32%)]\tTotal Loss: 0.435864\n",
      "Reconstruction: 0.370920, Regularization: 0.000029, Discriminator: 0.043284; Generator: 0.021631,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:31,617 root         INFO     Train Epoch: 175 [3072/8000 (38%)]\tTotal Loss: 0.434348\n",
      "Reconstruction: 0.369312, Regularization: 0.000026, Discriminator: 0.043322; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:31,726 root         INFO     Train Epoch: 175 [3584/8000 (45%)]\tTotal Loss: 0.433750\n",
      "Reconstruction: 0.368612, Regularization: 0.000031, Discriminator: 0.043419; Generator: 0.021688,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:31,836 root         INFO     Train Epoch: 175 [4096/8000 (51%)]\tTotal Loss: 0.432860\n",
      "Reconstruction: 0.367795, Regularization: 0.000028, Discriminator: 0.043364; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:31,946 root         INFO     Train Epoch: 175 [4608/8000 (58%)]\tTotal Loss: 0.432471\n",
      "Reconstruction: 0.367394, Regularization: 0.000027, Discriminator: 0.043349; Generator: 0.021702,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:32,056 root         INFO     Train Epoch: 175 [5120/8000 (64%)]\tTotal Loss: 0.432319\n",
      "Reconstruction: 0.367329, Regularization: 0.000029, Discriminator: 0.043291; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:32,165 root         INFO     Train Epoch: 175 [5632/8000 (70%)]\tTotal Loss: 0.433881\n",
      "Reconstruction: 0.368895, Regularization: 0.000026, Discriminator: 0.043280; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:32,275 root         INFO     Train Epoch: 175 [6144/8000 (77%)]\tTotal Loss: 0.435250\n",
      "Reconstruction: 0.370383, Regularization: 0.000032, Discriminator: 0.043211; Generator: 0.021624,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:32,385 root         INFO     Train Epoch: 175 [6656/8000 (83%)]\tTotal Loss: 0.436991\n",
      "Reconstruction: 0.371957, Regularization: 0.000035, Discriminator: 0.043333; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:32,494 root         INFO     Train Epoch: 175 [7168/8000 (90%)]\tTotal Loss: 0.439130\n",
      "Reconstruction: 0.374077, Regularization: 0.000031, Discriminator: 0.043371; Generator: 0.021652,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:32,604 root         INFO     Train Epoch: 175 [7680/8000 (96%)]\tTotal Loss: 0.441155\n",
      "Reconstruction: 0.376059, Regularization: 0.000030, Discriminator: 0.043368; Generator: 0.021697,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:32,684 root         INFO     ====> Epoch: 175 Average loss: 0.4367\n",
      "2019-04-10 01:00:32,712 root         INFO     Train Epoch: 176 [0/8000 (0%)]\tTotal Loss: 0.442268\n",
      "Reconstruction: 0.377168, Regularization: 0.000029, Discriminator: 0.043367; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:32,823 root         INFO     Train Epoch: 176 [512/8000 (6%)]\tTotal Loss: 0.444187\n",
      "Reconstruction: 0.379175, Regularization: 0.000022, Discriminator: 0.043355; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:32,934 root         INFO     Train Epoch: 176 [1024/8000 (13%)]\tTotal Loss: 0.445662\n",
      "Reconstruction: 0.380556, Regularization: 0.000018, Discriminator: 0.043334; Generator: 0.021754,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:33,043 root         INFO     Train Epoch: 176 [1536/8000 (19%)]\tTotal Loss: 0.446862\n",
      "Reconstruction: 0.381701, Regularization: 0.000020, Discriminator: 0.043421; Generator: 0.021720,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:00:33,153 root         INFO     Train Epoch: 176 [2048/8000 (26%)]\tTotal Loss: 0.448060\n",
      "Reconstruction: 0.382952, Regularization: 0.000020, Discriminator: 0.043384; Generator: 0.021705,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:33,263 root         INFO     Train Epoch: 176 [2560/8000 (32%)]\tTotal Loss: 0.448379\n",
      "Reconstruction: 0.383298, Regularization: 0.000021, Discriminator: 0.043349; Generator: 0.021711,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:33,373 root         INFO     Train Epoch: 176 [3072/8000 (38%)]\tTotal Loss: 0.448139\n",
      "Reconstruction: 0.383116, Regularization: 0.000021, Discriminator: 0.043330; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:33,483 root         INFO     Train Epoch: 176 [3584/8000 (45%)]\tTotal Loss: 0.447176\n",
      "Reconstruction: 0.382197, Regularization: 0.000021, Discriminator: 0.043272; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:33,593 root         INFO     Train Epoch: 176 [4096/8000 (51%)]\tTotal Loss: 0.445743\n",
      "Reconstruction: 0.380799, Regularization: 0.000018, Discriminator: 0.043233; Generator: 0.021693,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 01:00:33,703 root         INFO     Train Epoch: 176 [4608/8000 (58%)]\tTotal Loss: 0.443614\n",
      "Reconstruction: 0.378646, Regularization: 0.000014, Discriminator: 0.043277; Generator: 0.021677,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:33,813 root         INFO     Train Epoch: 176 [5120/8000 (64%)]\tTotal Loss: 0.440688\n",
      "Reconstruction: 0.375870, Regularization: 0.000014, Discriminator: 0.043198; Generator: 0.021606,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 01:00:33,923 root         INFO     Train Epoch: 176 [5632/8000 (70%)]\tTotal Loss: 0.438212\n",
      "Reconstruction: 0.373050, Regularization: 0.000028, Discriminator: 0.043494; Generator: 0.021640,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:34,033 root         INFO     Train Epoch: 176 [6144/8000 (77%)]\tTotal Loss: 0.435305\n",
      "Reconstruction: 0.370407, Regularization: 0.000019, Discriminator: 0.043217; Generator: 0.021662,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 01:00:34,144 root         INFO     Train Epoch: 176 [6656/8000 (83%)]\tTotal Loss: 0.433660\n",
      "Reconstruction: 0.368535, Regularization: 0.000020, Discriminator: 0.043450; Generator: 0.021654,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:34,255 root         INFO     Train Epoch: 176 [7168/8000 (90%)]\tTotal Loss: 0.432126\n",
      "Reconstruction: 0.366993, Regularization: 0.000024, Discriminator: 0.043474; Generator: 0.021635,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:34,366 root         INFO     Train Epoch: 176 [7680/8000 (96%)]\tTotal Loss: 0.431343\n",
      "Reconstruction: 0.366266, Regularization: 0.000027, Discriminator: 0.043391; Generator: 0.021658,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:34,447 root         INFO     ====> Epoch: 176 Average loss: 0.4419\n",
      "2019-04-10 01:00:34,474 root         INFO     Train Epoch: 177 [0/8000 (0%)]\tTotal Loss: 0.430699\n",
      "Reconstruction: 0.365674, Regularization: 0.000026, Discriminator: 0.043356; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:34,587 root         INFO     Train Epoch: 177 [512/8000 (6%)]\tTotal Loss: 0.430361\n",
      "Reconstruction: 0.365393, Regularization: 0.000024, Discriminator: 0.043319; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:34,698 root         INFO     Train Epoch: 177 [1024/8000 (13%)]\tTotal Loss: 0.430634\n",
      "Reconstruction: 0.365710, Regularization: 0.000024, Discriminator: 0.043282; Generator: 0.021618,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:34,809 root         INFO     Train Epoch: 177 [1536/8000 (19%)]\tTotal Loss: 0.431522\n",
      "Reconstruction: 0.366575, Regularization: 0.000028, Discriminator: 0.043273; Generator: 0.021645,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:34,919 root         INFO     Train Epoch: 177 [2048/8000 (26%)]\tTotal Loss: 0.432366\n",
      "Reconstruction: 0.367440, Regularization: 0.000025, Discriminator: 0.043282; Generator: 0.021620,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:35,029 root         INFO     Train Epoch: 177 [2560/8000 (32%)]\tTotal Loss: 0.433662\n",
      "Reconstruction: 0.368645, Regularization: 0.000019, Discriminator: 0.043371; Generator: 0.021627,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:35,139 root         INFO     Train Epoch: 177 [3072/8000 (38%)]\tTotal Loss: 0.435321\n",
      "Reconstruction: 0.370343, Regularization: 0.000016, Discriminator: 0.043311; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:35,250 root         INFO     Train Epoch: 177 [3584/8000 (45%)]\tTotal Loss: 0.436864\n",
      "Reconstruction: 0.371921, Regularization: 0.000017, Discriminator: 0.043265; Generator: 0.021661,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:35,358 root         INFO     Train Epoch: 177 [4096/8000 (51%)]\tTotal Loss: 0.438606\n",
      "Reconstruction: 0.373654, Regularization: 0.000012, Discriminator: 0.043253; Generator: 0.021687,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:35,466 root         INFO     Train Epoch: 177 [4608/8000 (58%)]\tTotal Loss: 0.440166\n",
      "Reconstruction: 0.375269, Regularization: 0.000010, Discriminator: 0.043292; Generator: 0.021595,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:35,575 root         INFO     Train Epoch: 177 [5120/8000 (64%)]\tTotal Loss: 0.441668\n",
      "Reconstruction: 0.376692, Regularization: 0.000009, Discriminator: 0.043282; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:35,685 root         INFO     Train Epoch: 177 [5632/8000 (70%)]\tTotal Loss: 0.443394\n",
      "Reconstruction: 0.378339, Regularization: 0.000007, Discriminator: 0.043326; Generator: 0.021723,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:35,795 root         INFO     Train Epoch: 177 [6144/8000 (77%)]\tTotal Loss: 0.444774\n",
      "Reconstruction: 0.379682, Regularization: 0.000012, Discriminator: 0.043358; Generator: 0.021722,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:35,904 root         INFO     Train Epoch: 177 [6656/8000 (83%)]\tTotal Loss: 0.445507\n",
      "Reconstruction: 0.380471, Regularization: 0.000011, Discriminator: 0.043360; Generator: 0.021665,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:36,013 root         INFO     Train Epoch: 177 [7168/8000 (90%)]\tTotal Loss: 0.446481\n",
      "Reconstruction: 0.381432, Regularization: 0.000012, Discriminator: 0.043322; Generator: 0.021716,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:36,123 root         INFO     Train Epoch: 177 [7680/8000 (96%)]\tTotal Loss: 0.446726\n",
      "Reconstruction: 0.381714, Regularization: 0.000011, Discriminator: 0.043323; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:36,204 root         INFO     ====> Epoch: 177 Average loss: 0.4383\n",
      "2019-04-10 01:00:36,231 root         INFO     Train Epoch: 178 [0/8000 (0%)]\tTotal Loss: 0.446886\n",
      "Reconstruction: 0.381883, Regularization: 0.000010, Discriminator: 0.043303; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:36,345 root         INFO     Train Epoch: 178 [512/8000 (6%)]\tTotal Loss: 0.446320\n",
      "Reconstruction: 0.381375, Regularization: 0.000010, Discriminator: 0.043291; Generator: 0.021644,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:36,457 root         INFO     Train Epoch: 178 [1024/8000 (13%)]\tTotal Loss: 0.445586\n",
      "Reconstruction: 0.380604, Regularization: 0.000012, Discriminator: 0.043327; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:36,570 root         INFO     Train Epoch: 178 [1536/8000 (19%)]\tTotal Loss: 0.444799\n",
      "Reconstruction: 0.379763, Regularization: 0.000013, Discriminator: 0.043349; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:36,683 root         INFO     Train Epoch: 178 [2048/8000 (26%)]\tTotal Loss: 0.443416\n",
      "Reconstruction: 0.378428, Regularization: 0.000010, Discriminator: 0.043256; Generator: 0.021723,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:36,795 root         INFO     Train Epoch: 178 [2560/8000 (32%)]\tTotal Loss: 0.441868\n",
      "Reconstruction: 0.376761, Regularization: 0.000008, Discriminator: 0.043412; Generator: 0.021687,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:36,908 root         INFO     Train Epoch: 178 [3072/8000 (38%)]\tTotal Loss: 0.440164\n",
      "Reconstruction: 0.375129, Regularization: 0.000011, Discriminator: 0.043373; Generator: 0.021651,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,021 root         INFO     Train Epoch: 178 [3584/8000 (45%)]\tTotal Loss: 0.438998\n",
      "Reconstruction: 0.373881, Regularization: 0.000009, Discriminator: 0.043400; Generator: 0.021708,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:37,133 root         INFO     Train Epoch: 178 [4096/8000 (51%)]\tTotal Loss: 0.437455\n",
      "Reconstruction: 0.372525, Regularization: 0.000008, Discriminator: 0.043261; Generator: 0.021660,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,244 root         INFO     Train Epoch: 178 [4608/8000 (58%)]\tTotal Loss: 0.436548\n",
      "Reconstruction: 0.371560, Regularization: 0.000007, Discriminator: 0.043317; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,356 root         INFO     Train Epoch: 178 [5120/8000 (64%)]\tTotal Loss: 0.435905\n",
      "Reconstruction: 0.370913, Regularization: 0.000009, Discriminator: 0.043301; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,467 root         INFO     Train Epoch: 178 [5632/8000 (70%)]\tTotal Loss: 0.435414\n",
      "Reconstruction: 0.370400, Regularization: 0.000009, Discriminator: 0.043319; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,578 root         INFO     Train Epoch: 178 [6144/8000 (77%)]\tTotal Loss: 0.434809\n",
      "Reconstruction: 0.369853, Regularization: 0.000007, Discriminator: 0.043304; Generator: 0.021645,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,690 root         INFO     Train Epoch: 178 [6656/8000 (83%)]\tTotal Loss: 0.434927\n",
      "Reconstruction: 0.369958, Regularization: 0.000010, Discriminator: 0.043283; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,801 root         INFO     Train Epoch: 178 [7168/8000 (90%)]\tTotal Loss: 0.435188\n",
      "Reconstruction: 0.370172, Regularization: 0.000007, Discriminator: 0.043345; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,912 root         INFO     Train Epoch: 178 [7680/8000 (96%)]\tTotal Loss: 0.435676\n",
      "Reconstruction: 0.370651, Regularization: 0.000010, Discriminator: 0.043336; Generator: 0.021679,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:37,993 root         INFO     ====> Epoch: 178 Average loss: 0.4394\n",
      "2019-04-10 01:00:38,020 root         INFO     Train Epoch: 179 [0/8000 (0%)]\tTotal Loss: 0.435704\n",
      "Reconstruction: 0.370710, Regularization: 0.000007, Discriminator: 0.043313; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:38,131 root         INFO     Train Epoch: 179 [512/8000 (6%)]\tTotal Loss: 0.436004\n",
      "Reconstruction: 0.371061, Regularization: 0.000008, Discriminator: 0.043335; Generator: 0.021600,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:38,241 root         INFO     Train Epoch: 179 [1024/8000 (13%)]\tTotal Loss: 0.436669\n",
      "Reconstruction: 0.371668, Regularization: 0.000008, Discriminator: 0.043354; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:38,352 root         INFO     Train Epoch: 179 [1536/8000 (19%)]\tTotal Loss: 0.437607\n",
      "Reconstruction: 0.372623, Regularization: 0.000007, Discriminator: 0.043353; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:38,462 root         INFO     Train Epoch: 179 [2048/8000 (26%)]\tTotal Loss: 0.438459\n",
      "Reconstruction: 0.373455, Regularization: 0.000008, Discriminator: 0.043342; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:38,573 root         INFO     Train Epoch: 179 [2560/8000 (32%)]\tTotal Loss: 0.439180\n",
      "Reconstruction: 0.374136, Regularization: 0.000007, Discriminator: 0.043386; Generator: 0.021652,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:38,683 root         INFO     Train Epoch: 179 [3072/8000 (38%)]\tTotal Loss: 0.440096\n",
      "Reconstruction: 0.375115, Regularization: 0.000009, Discriminator: 0.043363; Generator: 0.021609,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:38,793 root         INFO     Train Epoch: 179 [3584/8000 (45%)]\tTotal Loss: 0.440634\n",
      "Reconstruction: 0.375620, Regularization: 0.000010, Discriminator: 0.043341; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:38,903 root         INFO     Train Epoch: 179 [4096/8000 (51%)]\tTotal Loss: 0.441232\n",
      "Reconstruction: 0.376230, Regularization: 0.000011, Discriminator: 0.043333; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:39,014 root         INFO     Train Epoch: 179 [4608/8000 (58%)]\tTotal Loss: 0.441458\n",
      "Reconstruction: 0.376462, Regularization: 0.000012, Discriminator: 0.043315; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:39,124 root         INFO     Train Epoch: 179 [5120/8000 (64%)]\tTotal Loss: 0.441609\n",
      "Reconstruction: 0.376659, Regularization: 0.000013, Discriminator: 0.043300; Generator: 0.021637,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:39,234 root         INFO     Train Epoch: 179 [5632/8000 (70%)]\tTotal Loss: 0.441218\n",
      "Reconstruction: 0.376278, Regularization: 0.000011, Discriminator: 0.043322; Generator: 0.021606,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:39,344 root         INFO     Train Epoch: 179 [6144/8000 (77%)]\tTotal Loss: 0.440910\n",
      "Reconstruction: 0.375936, Regularization: 0.000011, Discriminator: 0.043340; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:39,454 root         INFO     Train Epoch: 179 [6656/8000 (83%)]\tTotal Loss: 0.440124\n",
      "Reconstruction: 0.375174, Regularization: 0.000013, Discriminator: 0.043303; Generator: 0.021635,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:39,564 root         INFO     Train Epoch: 179 [7168/8000 (90%)]\tTotal Loss: 0.439623\n",
      "Reconstruction: 0.374637, Regularization: 0.000010, Discriminator: 0.043312; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:39,673 root         INFO     Train Epoch: 179 [7680/8000 (96%)]\tTotal Loss: 0.438816\n",
      "Reconstruction: 0.373780, Regularization: 0.000010, Discriminator: 0.043348; Generator: 0.021678,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:39,754 root         INFO     ====> Epoch: 179 Average loss: 0.4394\n",
      "2019-04-10 01:00:39,781 root         INFO     Train Epoch: 180 [0/8000 (0%)]\tTotal Loss: 0.438080\n",
      "Reconstruction: 0.373103, Regularization: 0.000011, Discriminator: 0.043325; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:39,894 root         INFO     Train Epoch: 180 [512/8000 (6%)]\tTotal Loss: 0.436883\n",
      "Reconstruction: 0.371885, Regularization: 0.000010, Discriminator: 0.043318; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:40,006 root         INFO     Train Epoch: 180 [1024/8000 (13%)]\tTotal Loss: 0.437020\n",
      "Reconstruction: 0.371939, Regularization: 0.000011, Discriminator: 0.043362; Generator: 0.021709,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:40,118 root         INFO     Train Epoch: 180 [1536/8000 (19%)]\tTotal Loss: 0.436927\n",
      "Reconstruction: 0.371898, Regularization: 0.000011, Discriminator: 0.043333; Generator: 0.021684,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:40,229 root         INFO     Train Epoch: 180 [2048/8000 (26%)]\tTotal Loss: 0.436527\n",
      "Reconstruction: 0.371464, Regularization: 0.000011, Discriminator: 0.043365; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:40,340 root         INFO     Train Epoch: 180 [2560/8000 (32%)]\tTotal Loss: 0.437572\n",
      "Reconstruction: 0.372557, Regularization: 0.000012, Discriminator: 0.043329; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:40,451 root         INFO     Train Epoch: 180 [3072/8000 (38%)]\tTotal Loss: 0.437593\n",
      "Reconstruction: 0.372536, Regularization: 0.000012, Discriminator: 0.043321; Generator: 0.021724,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:40,561 root         INFO     Train Epoch: 180 [3584/8000 (45%)]\tTotal Loss: 0.438245\n",
      "Reconstruction: 0.373238, Regularization: 0.000009, Discriminator: 0.043308; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:40,670 root         INFO     Train Epoch: 180 [4096/8000 (51%)]\tTotal Loss: 0.439023\n",
      "Reconstruction: 0.374040, Regularization: 0.000010, Discriminator: 0.043285; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:40,778 root         INFO     Train Epoch: 180 [4608/8000 (58%)]\tTotal Loss: 0.439992\n",
      "Reconstruction: 0.375000, Regularization: 0.000011, Discriminator: 0.043280; Generator: 0.021700,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:40,886 root         INFO     Train Epoch: 180 [5120/8000 (64%)]\tTotal Loss: 0.440677\n",
      "Reconstruction: 0.375686, Regularization: 0.000011, Discriminator: 0.043315; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:40,993 root         INFO     Train Epoch: 180 [5632/8000 (70%)]\tTotal Loss: 0.441447\n",
      "Reconstruction: 0.376405, Regularization: 0.000009, Discriminator: 0.043303; Generator: 0.021731,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:41,100 root         INFO     Train Epoch: 180 [6144/8000 (77%)]\tTotal Loss: 0.442085\n",
      "Reconstruction: 0.377074, Regularization: 0.000012, Discriminator: 0.043301; Generator: 0.021697,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:41,207 root         INFO     Train Epoch: 180 [6656/8000 (83%)]\tTotal Loss: 0.442694\n",
      "Reconstruction: 0.377676, Regularization: 0.000011, Discriminator: 0.043306; Generator: 0.021701,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:41,314 root         INFO     Train Epoch: 180 [7168/8000 (90%)]\tTotal Loss: 0.443167\n",
      "Reconstruction: 0.378124, Regularization: 0.000010, Discriminator: 0.043356; Generator: 0.021677,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:41,421 root         INFO     Train Epoch: 180 [7680/8000 (96%)]\tTotal Loss: 0.443455\n",
      "Reconstruction: 0.378436, Regularization: 0.000011, Discriminator: 0.043339; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:41,501 root         INFO     ====> Epoch: 180 Average loss: 0.4395\n",
      "2019-04-10 01:00:41,528 root         INFO     Train Epoch: 181 [0/8000 (0%)]\tTotal Loss: 0.443153\n",
      "Reconstruction: 0.378129, Regularization: 0.000010, Discriminator: 0.043342; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:41,639 root         INFO     Train Epoch: 181 [512/8000 (6%)]\tTotal Loss: 0.442745\n",
      "Reconstruction: 0.377733, Regularization: 0.000009, Discriminator: 0.043323; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:41,748 root         INFO     Train Epoch: 181 [1024/8000 (13%)]\tTotal Loss: 0.441858\n",
      "Reconstruction: 0.376839, Regularization: 0.000011, Discriminator: 0.043370; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:41,857 root         INFO     Train Epoch: 181 [1536/8000 (19%)]\tTotal Loss: 0.440381\n",
      "Reconstruction: 0.375376, Regularization: 0.000014, Discriminator: 0.043355; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:41,966 root         INFO     Train Epoch: 181 [2048/8000 (26%)]\tTotal Loss: 0.438939\n",
      "Reconstruction: 0.373937, Regularization: 0.000017, Discriminator: 0.043332; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:42,075 root         INFO     Train Epoch: 181 [2560/8000 (32%)]\tTotal Loss: 0.437786\n",
      "Reconstruction: 0.372813, Regularization: 0.000012, Discriminator: 0.043344; Generator: 0.021617,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:42,184 root         INFO     Train Epoch: 181 [3072/8000 (38%)]\tTotal Loss: 0.436122\n",
      "Reconstruction: 0.371203, Regularization: 0.000008, Discriminator: 0.043313; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:42,291 root         INFO     Train Epoch: 181 [3584/8000 (45%)]\tTotal Loss: 0.434968\n",
      "Reconstruction: 0.370002, Regularization: 0.000011, Discriminator: 0.043359; Generator: 0.021596,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:42,398 root         INFO     Train Epoch: 181 [4096/8000 (51%)]\tTotal Loss: 0.433967\n",
      "Reconstruction: 0.369050, Regularization: 0.000012, Discriminator: 0.043306; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:42,507 root         INFO     Train Epoch: 181 [4608/8000 (58%)]\tTotal Loss: 0.433032\n",
      "Reconstruction: 0.367991, Regularization: 0.000012, Discriminator: 0.043380; Generator: 0.021649,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:42,616 root         INFO     Train Epoch: 181 [5120/8000 (64%)]\tTotal Loss: 0.432610\n",
      "Reconstruction: 0.367666, Regularization: 0.000012, Discriminator: 0.043314; Generator: 0.021619,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:42,723 root         INFO     Train Epoch: 181 [5632/8000 (70%)]\tTotal Loss: 0.432408\n",
      "Reconstruction: 0.367424, Regularization: 0.000011, Discriminator: 0.043314; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:42,831 root         INFO     Train Epoch: 181 [6144/8000 (77%)]\tTotal Loss: 0.432641\n",
      "Reconstruction: 0.367724, Regularization: 0.000009, Discriminator: 0.043325; Generator: 0.021583,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:42,940 root         INFO     Train Epoch: 181 [6656/8000 (83%)]\tTotal Loss: 0.433374\n",
      "Reconstruction: 0.368384, Regularization: 0.000009, Discriminator: 0.043328; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:43,047 root         INFO     Train Epoch: 181 [7168/8000 (90%)]\tTotal Loss: 0.434363\n",
      "Reconstruction: 0.369409, Regularization: 0.000007, Discriminator: 0.043302; Generator: 0.021645,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:43,153 root         INFO     Train Epoch: 181 [7680/8000 (96%)]\tTotal Loss: 0.435312\n",
      "Reconstruction: 0.370350, Regularization: 0.000008, Discriminator: 0.043317; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:43,233 root         INFO     ====> Epoch: 181 Average loss: 0.4363\n",
      "2019-04-10 01:00:43,261 root         INFO     Train Epoch: 182 [0/8000 (0%)]\tTotal Loss: 0.436137\n",
      "Reconstruction: 0.371124, Regularization: 0.000005, Discriminator: 0.043367; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:43,373 root         INFO     Train Epoch: 182 [512/8000 (6%)]\tTotal Loss: 0.437691\n",
      "Reconstruction: 0.372740, Regularization: 0.000004, Discriminator: 0.043297; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:43,484 root         INFO     Train Epoch: 182 [1024/8000 (13%)]\tTotal Loss: 0.439514\n",
      "Reconstruction: 0.374496, Regularization: 0.000005, Discriminator: 0.043371; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:43,594 root         INFO     Train Epoch: 182 [1536/8000 (19%)]\tTotal Loss: 0.441276\n",
      "Reconstruction: 0.376306, Regularization: 0.000004, Discriminator: 0.043283; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:43,705 root         INFO     Train Epoch: 182 [2048/8000 (26%)]\tTotal Loss: 0.443136\n",
      "Reconstruction: 0.378063, Regularization: 0.000003, Discriminator: 0.043332; Generator: 0.021738,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:43,815 root         INFO     Train Epoch: 182 [2560/8000 (32%)]\tTotal Loss: 0.444638\n",
      "Reconstruction: 0.379570, Regularization: 0.000006, Discriminator: 0.043307; Generator: 0.021756,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 01:00:43,926 root         INFO     Train Epoch: 182 [3072/8000 (38%)]\tTotal Loss: 0.445799\n",
      "Reconstruction: 0.380807, Regularization: 0.000008, Discriminator: 0.043298; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:44,037 root         INFO     Train Epoch: 182 [3584/8000 (45%)]\tTotal Loss: 0.446869\n",
      "Reconstruction: 0.381774, Regularization: 0.000009, Discriminator: 0.043344; Generator: 0.021741,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:44,148 root         INFO     Train Epoch: 182 [4096/8000 (51%)]\tTotal Loss: 0.447171\n",
      "Reconstruction: 0.382114, Regularization: 0.000009, Discriminator: 0.043322; Generator: 0.021725,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:44,259 root         INFO     Train Epoch: 182 [4608/8000 (58%)]\tTotal Loss: 0.447089\n",
      "Reconstruction: 0.382081, Regularization: 0.000012, Discriminator: 0.043327; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:44,370 root         INFO     Train Epoch: 182 [5120/8000 (64%)]\tTotal Loss: 0.446565\n",
      "Reconstruction: 0.381570, Regularization: 0.000010, Discriminator: 0.043307; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:44,480 root         INFO     Train Epoch: 182 [5632/8000 (70%)]\tTotal Loss: 0.446330\n",
      "Reconstruction: 0.381302, Regularization: 0.000012, Discriminator: 0.043305; Generator: 0.021711,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:44,591 root         INFO     Train Epoch: 182 [6144/8000 (77%)]\tTotal Loss: 0.444519\n",
      "Reconstruction: 0.379591, Regularization: 0.000006, Discriminator: 0.043259; Generator: 0.021663,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:44,701 root         INFO     Train Epoch: 182 [6656/8000 (83%)]\tTotal Loss: 0.443199\n",
      "Reconstruction: 0.378186, Regularization: 0.000004, Discriminator: 0.043316; Generator: 0.021693,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:44,811 root         INFO     Train Epoch: 182 [7168/8000 (90%)]\tTotal Loss: 0.441489\n",
      "Reconstruction: 0.376379, Regularization: 0.000004, Discriminator: 0.043404; Generator: 0.021702,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:44,922 root         INFO     Train Epoch: 182 [7680/8000 (96%)]\tTotal Loss: 0.439342\n",
      "Reconstruction: 0.374424, Regularization: 0.000003, Discriminator: 0.043267; Generator: 0.021648,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:45,002 root         INFO     ====> Epoch: 182 Average loss: 0.4433\n",
      "2019-04-10 01:00:45,029 root         INFO     Train Epoch: 183 [0/8000 (0%)]\tTotal Loss: 0.438283\n",
      "Reconstruction: 0.373317, Regularization: 0.000003, Discriminator: 0.043308; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:45,141 root         INFO     Train Epoch: 183 [512/8000 (6%)]\tTotal Loss: 0.436726\n",
      "Reconstruction: 0.371790, Regularization: 0.000002, Discriminator: 0.043278; Generator: 0.021655,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:45,253 root         INFO     Train Epoch: 183 [1024/8000 (13%)]\tTotal Loss: 0.435363\n",
      "Reconstruction: 0.370305, Regularization: 0.000003, Discriminator: 0.043384; Generator: 0.021671,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:45,364 root         INFO     Train Epoch: 183 [1536/8000 (19%)]\tTotal Loss: 0.433672\n",
      "Reconstruction: 0.368725, Regularization: 0.000005, Discriminator: 0.043318; Generator: 0.021624,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:45,475 root         INFO     Train Epoch: 183 [2048/8000 (26%)]\tTotal Loss: 0.432708\n",
      "Reconstruction: 0.367745, Regularization: 0.000006, Discriminator: 0.043308; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:45,586 root         INFO     Train Epoch: 183 [2560/8000 (32%)]\tTotal Loss: 0.431945\n",
      "Reconstruction: 0.366962, Regularization: 0.000006, Discriminator: 0.043330; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:45,697 root         INFO     Train Epoch: 183 [3072/8000 (38%)]\tTotal Loss: 0.431547\n",
      "Reconstruction: 0.366595, Regularization: 0.000007, Discriminator: 0.043332; Generator: 0.021614,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:45,807 root         INFO     Train Epoch: 183 [3584/8000 (45%)]\tTotal Loss: 0.431904\n",
      "Reconstruction: 0.366923, Regularization: 0.000005, Discriminator: 0.043352; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:45,918 root         INFO     Train Epoch: 183 [4096/8000 (51%)]\tTotal Loss: 0.432637\n",
      "Reconstruction: 0.367667, Regularization: 0.000008, Discriminator: 0.043321; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:46,029 root         INFO     Train Epoch: 183 [4608/8000 (58%)]\tTotal Loss: 0.433362\n",
      "Reconstruction: 0.368437, Regularization: 0.000009, Discriminator: 0.043312; Generator: 0.021604,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:46,140 root         INFO     Train Epoch: 183 [5120/8000 (64%)]\tTotal Loss: 0.434360\n",
      "Reconstruction: 0.369508, Regularization: 0.000010, Discriminator: 0.043279; Generator: 0.021563,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 01:00:46,251 root         INFO     Train Epoch: 183 [5632/8000 (70%)]\tTotal Loss: 0.435714\n",
      "Reconstruction: 0.370733, Regularization: 0.000012, Discriminator: 0.043312; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:46,362 root         INFO     Train Epoch: 183 [6144/8000 (77%)]\tTotal Loss: 0.437010\n",
      "Reconstruction: 0.372096, Regularization: 0.000012, Discriminator: 0.043306; Generator: 0.021596,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:46,473 root         INFO     Train Epoch: 183 [6656/8000 (83%)]\tTotal Loss: 0.438676\n",
      "Reconstruction: 0.373735, Regularization: 0.000013, Discriminator: 0.043288; Generator: 0.021641,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:46,585 root         INFO     Train Epoch: 183 [7168/8000 (90%)]\tTotal Loss: 0.439690\n",
      "Reconstruction: 0.374718, Regularization: 0.000013, Discriminator: 0.043309; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:46,695 root         INFO     Train Epoch: 183 [7680/8000 (96%)]\tTotal Loss: 0.440759\n",
      "Reconstruction: 0.375746, Regularization: 0.000017, Discriminator: 0.043343; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:46,776 root         INFO     ====> Epoch: 183 Average loss: 0.4352\n",
      "2019-04-10 01:00:46,803 root         INFO     Train Epoch: 184 [0/8000 (0%)]\tTotal Loss: 0.441090\n",
      "Reconstruction: 0.376155, Regularization: 0.000020, Discriminator: 0.043330; Generator: 0.021584,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:46,914 root         INFO     Train Epoch: 184 [512/8000 (6%)]\tTotal Loss: 0.441874\n",
      "Reconstruction: 0.376888, Regularization: 0.000024, Discriminator: 0.043312; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:47,025 root         INFO     Train Epoch: 184 [1024/8000 (13%)]\tTotal Loss: 0.442377\n",
      "Reconstruction: 0.377434, Regularization: 0.000023, Discriminator: 0.043300; Generator: 0.021619,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:47,136 root         INFO     Train Epoch: 184 [1536/8000 (19%)]\tTotal Loss: 0.443406\n",
      "Reconstruction: 0.378382, Regularization: 0.000022, Discriminator: 0.043314; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:47,247 root         INFO     Train Epoch: 184 [2048/8000 (26%)]\tTotal Loss: 0.443540\n",
      "Reconstruction: 0.378533, Regularization: 0.000024, Discriminator: 0.043320; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:47,357 root         INFO     Train Epoch: 184 [2560/8000 (32%)]\tTotal Loss: 0.443645\n",
      "Reconstruction: 0.378601, Regularization: 0.000027, Discriminator: 0.043327; Generator: 0.021690,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:47,468 root         INFO     Train Epoch: 184 [3072/8000 (38%)]\tTotal Loss: 0.444351\n",
      "Reconstruction: 0.379245, Regularization: 0.000029, Discriminator: 0.043338; Generator: 0.021739,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:47,580 root         INFO     Train Epoch: 184 [3584/8000 (45%)]\tTotal Loss: 0.443503\n",
      "Reconstruction: 0.378458, Regularization: 0.000030, Discriminator: 0.043308; Generator: 0.021707,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:47,691 root         INFO     Train Epoch: 184 [4096/8000 (51%)]\tTotal Loss: 0.443258\n",
      "Reconstruction: 0.378157, Regularization: 0.000030, Discriminator: 0.043342; Generator: 0.021729,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:47,802 root         INFO     Train Epoch: 184 [4608/8000 (58%)]\tTotal Loss: 0.442962\n",
      "Reconstruction: 0.377873, Regularization: 0.000030, Discriminator: 0.043340; Generator: 0.021718,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:47,913 root         INFO     Train Epoch: 184 [5120/8000 (64%)]\tTotal Loss: 0.443586\n",
      "Reconstruction: 0.378512, Regularization: 0.000029, Discriminator: 0.043321; Generator: 0.021724,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:48,024 root         INFO     Train Epoch: 184 [5632/8000 (70%)]\tTotal Loss: 0.442758\n",
      "Reconstruction: 0.377688, Regularization: 0.000031, Discriminator: 0.043310; Generator: 0.021728,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:48,136 root         INFO     Train Epoch: 184 [6144/8000 (77%)]\tTotal Loss: 0.442626\n",
      "Reconstruction: 0.377570, Regularization: 0.000031, Discriminator: 0.043308; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:48,247 root         INFO     Train Epoch: 184 [6656/8000 (83%)]\tTotal Loss: 0.442269\n",
      "Reconstruction: 0.377252, Regularization: 0.000028, Discriminator: 0.043278; Generator: 0.021711,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:48,358 root         INFO     Train Epoch: 184 [7168/8000 (90%)]\tTotal Loss: 0.441483\n",
      "Reconstruction: 0.376484, Regularization: 0.000033, Discriminator: 0.043288; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:48,469 root         INFO     Train Epoch: 184 [7680/8000 (96%)]\tTotal Loss: 0.441976\n",
      "Reconstruction: 0.376962, Regularization: 0.000037, Discriminator: 0.043310; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:48,549 root         INFO     ====> Epoch: 184 Average loss: 0.4428\n",
      "2019-04-10 01:00:48,577 root         INFO     Train Epoch: 185 [0/8000 (0%)]\tTotal Loss: 0.441024\n",
      "Reconstruction: 0.375964, Regularization: 0.000038, Discriminator: 0.043308; Generator: 0.021713,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:48,689 root         INFO     Train Epoch: 185 [512/8000 (6%)]\tTotal Loss: 0.440882\n",
      "Reconstruction: 0.375881, Regularization: 0.000042, Discriminator: 0.043284; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:48,801 root         INFO     Train Epoch: 185 [1024/8000 (13%)]\tTotal Loss: 0.439180\n",
      "Reconstruction: 0.374119, Regularization: 0.000042, Discriminator: 0.043323; Generator: 0.021695,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:48,913 root         INFO     Train Epoch: 185 [1536/8000 (19%)]\tTotal Loss: 0.438849\n",
      "Reconstruction: 0.373816, Regularization: 0.000038, Discriminator: 0.043364; Generator: 0.021631,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:49,025 root         INFO     Train Epoch: 185 [2048/8000 (26%)]\tTotal Loss: 0.437364\n",
      "Reconstruction: 0.372361, Regularization: 0.000041, Discriminator: 0.043320; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:49,137 root         INFO     Train Epoch: 185 [2560/8000 (32%)]\tTotal Loss: 0.436521\n",
      "Reconstruction: 0.371498, Regularization: 0.000036, Discriminator: 0.043310; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:49,249 root         INFO     Train Epoch: 185 [3072/8000 (38%)]\tTotal Loss: 0.435082\n",
      "Reconstruction: 0.370128, Regularization: 0.000034, Discriminator: 0.043261; Generator: 0.021659,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:49,361 root         INFO     Train Epoch: 185 [3584/8000 (45%)]\tTotal Loss: 0.434046\n",
      "Reconstruction: 0.369099, Regularization: 0.000030, Discriminator: 0.043312; Generator: 0.021605,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:49,473 root         INFO     Train Epoch: 185 [4096/8000 (51%)]\tTotal Loss: 0.432292\n",
      "Reconstruction: 0.367373, Regularization: 0.000026, Discriminator: 0.043298; Generator: 0.021595,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:49,585 root         INFO     Train Epoch: 185 [4608/8000 (58%)]\tTotal Loss: 0.431268\n",
      "Reconstruction: 0.366365, Regularization: 0.000023, Discriminator: 0.043308; Generator: 0.021572,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:49,697 root         INFO     Train Epoch: 185 [5120/8000 (64%)]\tTotal Loss: 0.429829\n",
      "Reconstruction: 0.364873, Regularization: 0.000022, Discriminator: 0.043347; Generator: 0.021587,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:49,808 root         INFO     Train Epoch: 185 [5632/8000 (70%)]\tTotal Loss: 0.429250\n",
      "Reconstruction: 0.364349, Regularization: 0.000022, Discriminator: 0.043300; Generator: 0.021579,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:49,921 root         INFO     Train Epoch: 185 [6144/8000 (77%)]\tTotal Loss: 0.429750\n",
      "Reconstruction: 0.364861, Regularization: 0.000021, Discriminator: 0.043316; Generator: 0.021552,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 01:00:50,031 root         INFO     Train Epoch: 185 [6656/8000 (83%)]\tTotal Loss: 0.430361\n",
      "Reconstruction: 0.365468, Regularization: 0.000028, Discriminator: 0.043318; Generator: 0.021546,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 01:00:50,141 root         INFO     Train Epoch: 185 [7168/8000 (90%)]\tTotal Loss: 0.431389\n",
      "Reconstruction: 0.366479, Regularization: 0.000032, Discriminator: 0.043257; Generator: 0.021620,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:50,251 root         INFO     Train Epoch: 185 [7680/8000 (96%)]\tTotal Loss: 0.432511\n",
      "Reconstruction: 0.367558, Regularization: 0.000039, Discriminator: 0.043295; Generator: 0.021620,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:50,331 root         INFO     ====> Epoch: 185 Average loss: 0.4341\n",
      "2019-04-10 01:00:50,358 root         INFO     Train Epoch: 186 [0/8000 (0%)]\tTotal Loss: 0.433966\n",
      "Reconstruction: 0.369026, Regularization: 0.000039, Discriminator: 0.043247; Generator: 0.021653,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:50,470 root         INFO     Train Epoch: 186 [512/8000 (6%)]\tTotal Loss: 0.434713\n",
      "Reconstruction: 0.369659, Regularization: 0.000039, Discriminator: 0.043358; Generator: 0.021657,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:50,582 root         INFO     Train Epoch: 186 [1024/8000 (13%)]\tTotal Loss: 0.436289\n",
      "Reconstruction: 0.371273, Regularization: 0.000038, Discriminator: 0.043318; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:50,692 root         INFO     Train Epoch: 186 [1536/8000 (19%)]\tTotal Loss: 0.438349\n",
      "Reconstruction: 0.373343, Regularization: 0.000046, Discriminator: 0.043352; Generator: 0.021609,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:50,802 root         INFO     Train Epoch: 186 [2048/8000 (26%)]\tTotal Loss: 0.440331\n",
      "Reconstruction: 0.375388, Regularization: 0.000049, Discriminator: 0.043220; Generator: 0.021674,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:50,912 root         INFO     Train Epoch: 186 [2560/8000 (32%)]\tTotal Loss: 0.442191\n",
      "Reconstruction: 0.377111, Regularization: 0.000037, Discriminator: 0.043329; Generator: 0.021714,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:51,023 root         INFO     Train Epoch: 186 [3072/8000 (38%)]\tTotal Loss: 0.443381\n",
      "Reconstruction: 0.378404, Regularization: 0.000032, Discriminator: 0.043242; Generator: 0.021703,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 01:00:51,134 root         INFO     Train Epoch: 186 [3584/8000 (45%)]\tTotal Loss: 0.445485\n",
      "Reconstruction: 0.380356, Regularization: 0.000027, Discriminator: 0.043325; Generator: 0.021776,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:51,246 root         INFO     Train Epoch: 186 [4096/8000 (51%)]\tTotal Loss: 0.447453\n",
      "Reconstruction: 0.382375, Regularization: 0.000032, Discriminator: 0.043325; Generator: 0.021721,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:51,358 root         INFO     Train Epoch: 186 [4608/8000 (58%)]\tTotal Loss: 0.449143\n",
      "Reconstruction: 0.383951, Regularization: 0.000038, Discriminator: 0.043399; Generator: 0.021755,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 01:00:51,470 root         INFO     Train Epoch: 186 [5120/8000 (64%)]\tTotal Loss: 0.450048\n",
      "Reconstruction: 0.384907, Regularization: 0.000041, Discriminator: 0.043357; Generator: 0.021744,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:51,582 root         INFO     Train Epoch: 186 [5632/8000 (70%)]\tTotal Loss: 0.450737\n",
      "Reconstruction: 0.385640, Regularization: 0.000036, Discriminator: 0.043342; Generator: 0.021719,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:51,694 root         INFO     Train Epoch: 186 [6144/8000 (77%)]\tTotal Loss: 0.451849\n",
      "Reconstruction: 0.386694, Regularization: 0.000036, Discriminator: 0.043353; Generator: 0.021766,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:51,807 root         INFO     Train Epoch: 186 [6656/8000 (83%)]\tTotal Loss: 0.451772\n",
      "Reconstruction: 0.386688, Regularization: 0.000036, Discriminator: 0.043308; Generator: 0.021739,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:51,919 root         INFO     Train Epoch: 186 [7168/8000 (90%)]\tTotal Loss: 0.450885\n",
      "Reconstruction: 0.385764, Regularization: 0.000039, Discriminator: 0.043308; Generator: 0.021774,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:52,032 root         INFO     Train Epoch: 186 [7680/8000 (96%)]\tTotal Loss: 0.449223\n",
      "Reconstruction: 0.384156, Regularization: 0.000041, Discriminator: 0.043257; Generator: 0.021769,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 01:00:52,113 root         INFO     ====> Epoch: 186 Average loss: 0.4450\n",
      "2019-04-10 01:00:52,140 root         INFO     Train Epoch: 187 [0/8000 (0%)]\tTotal Loss: 0.447538\n",
      "Reconstruction: 0.382322, Regularization: 0.000036, Discriminator: 0.043399; Generator: 0.021782,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 01:00:52,252 root         INFO     Train Epoch: 187 [512/8000 (6%)]\tTotal Loss: 0.444511\n",
      "Reconstruction: 0.379500, Regularization: 0.000029, Discriminator: 0.043269; Generator: 0.021712,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:52,363 root         INFO     Train Epoch: 187 [1024/8000 (13%)]\tTotal Loss: 0.441724\n",
      "Reconstruction: 0.376578, Regularization: 0.000027, Discriminator: 0.043384; Generator: 0.021735,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:52,475 root         INFO     Train Epoch: 187 [1536/8000 (19%)]\tTotal Loss: 0.439021\n",
      "Reconstruction: 0.373950, Regularization: 0.000026, Discriminator: 0.043337; Generator: 0.021708,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:52,585 root         INFO     Train Epoch: 187 [2048/8000 (26%)]\tTotal Loss: 0.436638\n",
      "Reconstruction: 0.371519, Regularization: 0.000019, Discriminator: 0.043394; Generator: 0.021705,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:52,694 root         INFO     Train Epoch: 187 [2560/8000 (32%)]\tTotal Loss: 0.435024\n",
      "Reconstruction: 0.370039, Regularization: 0.000018, Discriminator: 0.043331; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:52,802 root         INFO     Train Epoch: 187 [3072/8000 (38%)]\tTotal Loss: 0.434093\n",
      "Reconstruction: 0.369237, Regularization: 0.000015, Discriminator: 0.043289; Generator: 0.021551,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 01:00:52,910 root         INFO     Train Epoch: 187 [3584/8000 (45%)]\tTotal Loss: 0.433722\n",
      "Reconstruction: 0.368785, Regularization: 0.000013, Discriminator: 0.043343; Generator: 0.021581,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:53,018 root         INFO     Train Epoch: 187 [4096/8000 (51%)]\tTotal Loss: 0.434081\n",
      "Reconstruction: 0.368994, Regularization: 0.000014, Discriminator: 0.043436; Generator: 0.021637,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:53,126 root         INFO     Train Epoch: 187 [4608/8000 (58%)]\tTotal Loss: 0.434596\n",
      "Reconstruction: 0.369537, Regularization: 0.000015, Discriminator: 0.043401; Generator: 0.021643,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:53,234 root         INFO     Train Epoch: 187 [5120/8000 (64%)]\tTotal Loss: 0.434939\n",
      "Reconstruction: 0.369941, Regularization: 0.000013, Discriminator: 0.043368; Generator: 0.021618,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:53,341 root         INFO     Train Epoch: 187 [5632/8000 (70%)]\tTotal Loss: 0.435398\n",
      "Reconstruction: 0.370355, Regularization: 0.000015, Discriminator: 0.043379; Generator: 0.021649,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:53,449 root         INFO     Train Epoch: 187 [6144/8000 (77%)]\tTotal Loss: 0.435220\n",
      "Reconstruction: 0.370213, Regularization: 0.000014, Discriminator: 0.043341; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:53,557 root         INFO     Train Epoch: 187 [6656/8000 (83%)]\tTotal Loss: 0.435809\n",
      "Reconstruction: 0.370845, Regularization: 0.000014, Discriminator: 0.043327; Generator: 0.021623,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:53,665 root         INFO     Train Epoch: 187 [7168/8000 (90%)]\tTotal Loss: 0.434761\n",
      "Reconstruction: 0.369844, Regularization: 0.000014, Discriminator: 0.043286; Generator: 0.021617,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:53,774 root         INFO     Train Epoch: 187 [7680/8000 (96%)]\tTotal Loss: 0.434306\n",
      "Reconstruction: 0.369406, Regularization: 0.000012, Discriminator: 0.043305; Generator: 0.021583,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:53,855 root         INFO     ====> Epoch: 187 Average loss: 0.4366\n",
      "2019-04-10 01:00:53,882 root         INFO     Train Epoch: 188 [0/8000 (0%)]\tTotal Loss: 0.433885\n",
      "Reconstruction: 0.368975, Regularization: 0.000013, Discriminator: 0.043311; Generator: 0.021587,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:53,994 root         INFO     Train Epoch: 188 [512/8000 (6%)]\tTotal Loss: 0.432964\n",
      "Reconstruction: 0.368084, Regularization: 0.000012, Discriminator: 0.043319; Generator: 0.021549,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 01:00:54,105 root         INFO     Train Epoch: 188 [1024/8000 (13%)]\tTotal Loss: 0.431972\n",
      "Reconstruction: 0.367142, Regularization: 0.000011, Discriminator: 0.043237; Generator: 0.021583,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 01:00:54,216 root         INFO     Train Epoch: 188 [1536/8000 (19%)]\tTotal Loss: 0.431299\n",
      "Reconstruction: 0.366427, Regularization: 0.000013, Discriminator: 0.043300; Generator: 0.021559,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 01:00:54,327 root         INFO     Train Epoch: 188 [2048/8000 (26%)]\tTotal Loss: 0.430323\n",
      "Reconstruction: 0.365345, Regularization: 0.000012, Discriminator: 0.043358; Generator: 0.021608,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:54,438 root         INFO     Train Epoch: 188 [2560/8000 (32%)]\tTotal Loss: 0.429402\n",
      "Reconstruction: 0.364407, Regularization: 0.000010, Discriminator: 0.043316; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:54,549 root         INFO     Train Epoch: 188 [3072/8000 (38%)]\tTotal Loss: 0.429387\n",
      "Reconstruction: 0.364408, Regularization: 0.000010, Discriminator: 0.043350; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:54,657 root         INFO     Train Epoch: 188 [3584/8000 (45%)]\tTotal Loss: 0.429483\n",
      "Reconstruction: 0.364505, Regularization: 0.000008, Discriminator: 0.043363; Generator: 0.021607,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:54,764 root         INFO     Train Epoch: 188 [4096/8000 (51%)]\tTotal Loss: 0.430487\n",
      "Reconstruction: 0.365529, Regularization: 0.000008, Discriminator: 0.043328; Generator: 0.021621,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:54,870 root         INFO     Train Epoch: 188 [4608/8000 (58%)]\tTotal Loss: 0.431511\n",
      "Reconstruction: 0.366559, Regularization: 0.000008, Discriminator: 0.043356; Generator: 0.021588,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:54,977 root         INFO     Train Epoch: 188 [5120/8000 (64%)]\tTotal Loss: 0.433301\n",
      "Reconstruction: 0.368428, Regularization: 0.000007, Discriminator: 0.043260; Generator: 0.021606,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:55,083 root         INFO     Train Epoch: 188 [5632/8000 (70%)]\tTotal Loss: 0.435282\n",
      "Reconstruction: 0.370282, Regularization: 0.000007, Discriminator: 0.043326; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:55,189 root         INFO     Train Epoch: 188 [6144/8000 (77%)]\tTotal Loss: 0.437689\n",
      "Reconstruction: 0.372662, Regularization: 0.000007, Discriminator: 0.043311; Generator: 0.021709,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:55,296 root         INFO     Train Epoch: 188 [6656/8000 (83%)]\tTotal Loss: 0.440194\n",
      "Reconstruction: 0.375272, Regularization: 0.000008, Discriminator: 0.043288; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:55,402 root         INFO     Train Epoch: 188 [7168/8000 (90%)]\tTotal Loss: 0.443095\n",
      "Reconstruction: 0.378152, Regularization: 0.000010, Discriminator: 0.043245; Generator: 0.021689,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:55,508 root         INFO     Train Epoch: 188 [7680/8000 (96%)]\tTotal Loss: 0.445421\n",
      "Reconstruction: 0.380308, Regularization: 0.000011, Discriminator: 0.043305; Generator: 0.021797,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:55,586 root         INFO     ====> Epoch: 188 Average loss: 0.4342\n",
      "2019-04-10 01:00:55,613 root         INFO     Train Epoch: 189 [0/8000 (0%)]\tTotal Loss: 0.447137\n",
      "Reconstruction: 0.381954, Regularization: 0.000009, Discriminator: 0.043435; Generator: 0.021739,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:00:55,720 root         INFO     Train Epoch: 189 [512/8000 (6%)]\tTotal Loss: 0.449428\n",
      "Reconstruction: 0.384339, Regularization: 0.000009, Discriminator: 0.043292; Generator: 0.021788,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:55,828 root         INFO     Train Epoch: 189 [1024/8000 (13%)]\tTotal Loss: 0.451148\n",
      "Reconstruction: 0.385961, Regularization: 0.000011, Discriminator: 0.043407; Generator: 0.021769,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 01:00:55,935 root         INFO     Train Epoch: 189 [1536/8000 (19%)]\tTotal Loss: 0.452444\n",
      "Reconstruction: 0.387332, Regularization: 0.000014, Discriminator: 0.043334; Generator: 0.021763,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:56,042 root         INFO     Train Epoch: 189 [2048/8000 (26%)]\tTotal Loss: 0.453795\n",
      "Reconstruction: 0.388697, Regularization: 0.000014, Discriminator: 0.043317; Generator: 0.021767,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:56,149 root         INFO     Train Epoch: 189 [2560/8000 (32%)]\tTotal Loss: 0.454665\n",
      "Reconstruction: 0.389577, Regularization: 0.000013, Discriminator: 0.043304; Generator: 0.021771,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 01:00:56,256 root         INFO     Train Epoch: 189 [3072/8000 (38%)]\tTotal Loss: 0.454666\n",
      "Reconstruction: 0.389579, Regularization: 0.000014, Discriminator: 0.043318; Generator: 0.021755,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 01:00:56,362 root         INFO     Train Epoch: 189 [3584/8000 (45%)]\tTotal Loss: 0.454844\n",
      "Reconstruction: 0.389773, Regularization: 0.000013, Discriminator: 0.043278; Generator: 0.021780,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 01:00:56,469 root         INFO     Train Epoch: 189 [4096/8000 (51%)]\tTotal Loss: 0.453079\n",
      "Reconstruction: 0.387989, Regularization: 0.000011, Discriminator: 0.043317; Generator: 0.021761,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:56,576 root         INFO     Train Epoch: 189 [4608/8000 (58%)]\tTotal Loss: 0.451032\n",
      "Reconstruction: 0.386034, Regularization: 0.000010, Discriminator: 0.043251; Generator: 0.021736,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:00:56,682 root         INFO     Train Epoch: 189 [5120/8000 (64%)]\tTotal Loss: 0.448710\n",
      "Reconstruction: 0.383592, Regularization: 0.000006, Discriminator: 0.043377; Generator: 0.021735,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:56,789 root         INFO     Train Epoch: 189 [5632/8000 (70%)]\tTotal Loss: 0.445921\n",
      "Reconstruction: 0.380892, Regularization: 0.000004, Discriminator: 0.043304; Generator: 0.021721,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:00:56,896 root         INFO     Train Epoch: 189 [6144/8000 (77%)]\tTotal Loss: 0.443070\n",
      "Reconstruction: 0.377956, Regularization: 0.000003, Discriminator: 0.043351; Generator: 0.021760,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:00:57,003 root         INFO     Train Epoch: 189 [6656/8000 (83%)]\tTotal Loss: 0.440115\n",
      "Reconstruction: 0.375005, Regularization: 0.000002, Discriminator: 0.043370; Generator: 0.021738,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:00:57,110 root         INFO     Train Epoch: 189 [7168/8000 (90%)]\tTotal Loss: 0.437540\n",
      "Reconstruction: 0.372438, Regularization: 0.000004, Discriminator: 0.043406; Generator: 0.021691,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:00:57,219 root         INFO     Train Epoch: 189 [7680/8000 (96%)]\tTotal Loss: 0.435077\n",
      "Reconstruction: 0.370220, Regularization: 0.000002, Discriminator: 0.043222; Generator: 0.021634,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-10 01:00:57,298 root         INFO     ====> Epoch: 189 Average loss: 0.4482\n",
      "2019-04-10 01:00:57,325 root         INFO     Train Epoch: 190 [0/8000 (0%)]\tTotal Loss: 0.434054\n",
      "Reconstruction: 0.369105, Regularization: 0.000002, Discriminator: 0.043323; Generator: 0.021624,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:57,434 root         INFO     Train Epoch: 190 [512/8000 (6%)]\tTotal Loss: 0.432883\n",
      "Reconstruction: 0.367993, Regularization: 0.000002, Discriminator: 0.043314; Generator: 0.021573,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:57,542 root         INFO     Train Epoch: 190 [1024/8000 (13%)]\tTotal Loss: 0.432314\n",
      "Reconstruction: 0.367348, Regularization: 0.000002, Discriminator: 0.043342; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:57,651 root         INFO     Train Epoch: 190 [1536/8000 (19%)]\tTotal Loss: 0.432437\n",
      "Reconstruction: 0.367475, Regularization: 0.000005, Discriminator: 0.043355; Generator: 0.021603,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:57,759 root         INFO     Train Epoch: 190 [2048/8000 (26%)]\tTotal Loss: 0.432177\n",
      "Reconstruction: 0.367201, Regularization: 0.000004, Discriminator: 0.043366; Generator: 0.021606,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:57,867 root         INFO     Train Epoch: 190 [2560/8000 (32%)]\tTotal Loss: 0.432412\n",
      "Reconstruction: 0.367426, Regularization: 0.000003, Discriminator: 0.043379; Generator: 0.021603,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:57,976 root         INFO     Train Epoch: 190 [3072/8000 (38%)]\tTotal Loss: 0.432829\n",
      "Reconstruction: 0.367892, Regularization: 0.000003, Discriminator: 0.043349; Generator: 0.021585,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:58,083 root         INFO     Train Epoch: 190 [3584/8000 (45%)]\tTotal Loss: 0.433180\n",
      "Reconstruction: 0.368236, Regularization: 0.000003, Discriminator: 0.043356; Generator: 0.021586,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:58,190 root         INFO     Train Epoch: 190 [4096/8000 (51%)]\tTotal Loss: 0.434112\n",
      "Reconstruction: 0.369163, Regularization: 0.000003, Discriminator: 0.043314; Generator: 0.021632,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:00:58,299 root         INFO     Train Epoch: 190 [4608/8000 (58%)]\tTotal Loss: 0.434818\n",
      "Reconstruction: 0.369789, Regularization: 0.000003, Discriminator: 0.043335; Generator: 0.021692,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:58,409 root         INFO     Train Epoch: 190 [5120/8000 (64%)]\tTotal Loss: 0.435229\n",
      "Reconstruction: 0.370183, Regularization: 0.000003, Discriminator: 0.043367; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:58,519 root         INFO     Train Epoch: 190 [5632/8000 (70%)]\tTotal Loss: 0.435517\n",
      "Reconstruction: 0.370556, Regularization: 0.000004, Discriminator: 0.043329; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:58,629 root         INFO     Train Epoch: 190 [6144/8000 (77%)]\tTotal Loss: 0.435304\n",
      "Reconstruction: 0.370423, Regularization: 0.000003, Discriminator: 0.043301; Generator: 0.021578,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:58,738 root         INFO     Train Epoch: 190 [6656/8000 (83%)]\tTotal Loss: 0.435205\n",
      "Reconstruction: 0.370325, Regularization: 0.000003, Discriminator: 0.043274; Generator: 0.021603,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:58,848 root         INFO     Train Epoch: 190 [7168/8000 (90%)]\tTotal Loss: 0.434987\n",
      "Reconstruction: 0.370151, Regularization: 0.000003, Discriminator: 0.043261; Generator: 0.021573,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:58,957 root         INFO     Train Epoch: 190 [7680/8000 (96%)]\tTotal Loss: 0.434840\n",
      "Reconstruction: 0.370023, Regularization: 0.000002, Discriminator: 0.043279; Generator: 0.021535,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 01:00:59,036 root         INFO     ====> Epoch: 190 Average loss: 0.4339\n",
      "2019-04-10 01:00:59,064 root         INFO     Train Epoch: 191 [0/8000 (0%)]\tTotal Loss: 0.434555\n",
      "Reconstruction: 0.369740, Regularization: 0.000003, Discriminator: 0.043256; Generator: 0.021556,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 01:00:59,173 root         INFO     Train Epoch: 191 [512/8000 (6%)]\tTotal Loss: 0.434613\n",
      "Reconstruction: 0.369738, Regularization: 0.000003, Discriminator: 0.043303; Generator: 0.021569,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:00:59,280 root         INFO     Train Epoch: 191 [1024/8000 (13%)]\tTotal Loss: 0.434771\n",
      "Reconstruction: 0.369855, Regularization: 0.000003, Discriminator: 0.043320; Generator: 0.021593,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:59,388 root         INFO     Train Epoch: 191 [1536/8000 (19%)]\tTotal Loss: 0.434752\n",
      "Reconstruction: 0.369813, Regularization: 0.000003, Discriminator: 0.043342; Generator: 0.021594,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:00:59,496 root         INFO     Train Epoch: 191 [2048/8000 (26%)]\tTotal Loss: 0.434996\n",
      "Reconstruction: 0.370004, Regularization: 0.000003, Discriminator: 0.043365; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:00:59,602 root         INFO     Train Epoch: 191 [2560/8000 (32%)]\tTotal Loss: 0.434871\n",
      "Reconstruction: 0.369836, Regularization: 0.000003, Discriminator: 0.043376; Generator: 0.021656,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:59,710 root         INFO     Train Epoch: 191 [3072/8000 (38%)]\tTotal Loss: 0.435078\n",
      "Reconstruction: 0.370103, Regularization: 0.000006, Discriminator: 0.043304; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:00:59,817 root         INFO     Train Epoch: 191 [3584/8000 (45%)]\tTotal Loss: 0.435737\n",
      "Reconstruction: 0.370696, Regularization: 0.000005, Discriminator: 0.043365; Generator: 0.021671,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:00:59,924 root         INFO     Train Epoch: 191 [4096/8000 (51%)]\tTotal Loss: 0.436233\n",
      "Reconstruction: 0.371241, Regularization: 0.000004, Discriminator: 0.043340; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:00,032 root         INFO     Train Epoch: 191 [4608/8000 (58%)]\tTotal Loss: 0.437394\n",
      "Reconstruction: 0.372457, Regularization: 0.000003, Discriminator: 0.043293; Generator: 0.021640,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:00,139 root         INFO     Train Epoch: 191 [5120/8000 (64%)]\tTotal Loss: 0.438951\n",
      "Reconstruction: 0.373955, Regularization: 0.000007, Discriminator: 0.043310; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:00,246 root         INFO     Train Epoch: 191 [5632/8000 (70%)]\tTotal Loss: 0.440566\n",
      "Reconstruction: 0.375470, Regularization: 0.000006, Discriminator: 0.043415; Generator: 0.021675,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:01:00,354 root         INFO     Train Epoch: 191 [6144/8000 (77%)]\tTotal Loss: 0.442133\n",
      "Reconstruction: 0.377051, Regularization: 0.000003, Discriminator: 0.043351; Generator: 0.021727,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:00,464 root         INFO     Train Epoch: 191 [6656/8000 (83%)]\tTotal Loss: 0.443667\n",
      "Reconstruction: 0.378537, Regularization: 0.000004, Discriminator: 0.043362; Generator: 0.021765,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:01:00,574 root         INFO     Train Epoch: 191 [7168/8000 (90%)]\tTotal Loss: 0.445291\n",
      "Reconstruction: 0.380284, Regularization: 0.000003, Discriminator: 0.043316; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:00,683 root         INFO     Train Epoch: 191 [7680/8000 (96%)]\tTotal Loss: 0.446908\n",
      "Reconstruction: 0.381800, Regularization: 0.000002, Discriminator: 0.043399; Generator: 0.021706,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:00,763 root         INFO     ====> Epoch: 191 Average loss: 0.4383\n",
      "2019-04-10 01:01:00,790 root         INFO     Train Epoch: 192 [0/8000 (0%)]\tTotal Loss: 0.447842\n",
      "Reconstruction: 0.382801, Regularization: 0.000002, Discriminator: 0.043281; Generator: 0.021758,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 01:01:00,896 root         INFO     Train Epoch: 192 [512/8000 (6%)]\tTotal Loss: 0.449407\n",
      "Reconstruction: 0.384261, Regularization: 0.000002, Discriminator: 0.043352; Generator: 0.021791,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 01:01:01,002 root         INFO     Train Epoch: 192 [1024/8000 (13%)]\tTotal Loss: 0.450385\n",
      "Reconstruction: 0.385299, Regularization: 0.000003, Discriminator: 0.043335; Generator: 0.021748,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:01,109 root         INFO     Train Epoch: 192 [1536/8000 (19%)]\tTotal Loss: 0.451535\n",
      "Reconstruction: 0.386441, Regularization: 0.000002, Discriminator: 0.043339; Generator: 0.021753,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:01,216 root         INFO     Train Epoch: 192 [2048/8000 (26%)]\tTotal Loss: 0.452335\n",
      "Reconstruction: 0.387189, Regularization: 0.000002, Discriminator: 0.043343; Generator: 0.021801,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 01:01:01,322 root         INFO     Train Epoch: 192 [2560/8000 (32%)]\tTotal Loss: 0.452229\n",
      "Reconstruction: 0.387143, Regularization: 0.000003, Discriminator: 0.043320; Generator: 0.021764,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 01:01:01,429 root         INFO     Train Epoch: 192 [3072/8000 (38%)]\tTotal Loss: 0.451992\n",
      "Reconstruction: 0.386843, Regularization: 0.000002, Discriminator: 0.043335; Generator: 0.021811,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-10 01:01:01,537 root         INFO     Train Epoch: 192 [3584/8000 (45%)]\tTotal Loss: 0.451073\n",
      "Reconstruction: 0.386066, Regularization: 0.000003, Discriminator: 0.043271; Generator: 0.021733,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:01:01,644 root         INFO     Train Epoch: 192 [4096/8000 (51%)]\tTotal Loss: 0.449904\n",
      "Reconstruction: 0.384828, Regularization: 0.000004, Discriminator: 0.043340; Generator: 0.021732,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:01,751 root         INFO     Train Epoch: 192 [4608/8000 (58%)]\tTotal Loss: 0.448123\n",
      "Reconstruction: 0.383186, Regularization: 0.000003, Discriminator: 0.043195; Generator: 0.021738,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 01:01:01,857 root         INFO     Train Epoch: 192 [5120/8000 (64%)]\tTotal Loss: 0.446019\n",
      "Reconstruction: 0.381074, Regularization: 0.000002, Discriminator: 0.043206; Generator: 0.021737,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 01:01:01,963 root         INFO     Train Epoch: 192 [5632/8000 (70%)]\tTotal Loss: 0.443630\n",
      "Reconstruction: 0.378618, Regularization: 0.000001, Discriminator: 0.043259; Generator: 0.021752,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:01:02,070 root         INFO     Train Epoch: 192 [6144/8000 (77%)]\tTotal Loss: 0.440826\n",
      "Reconstruction: 0.375820, Regularization: 0.000001, Discriminator: 0.043362; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:02,177 root         INFO     Train Epoch: 192 [6656/8000 (83%)]\tTotal Loss: 0.438443\n",
      "Reconstruction: 0.373402, Regularization: 0.000001, Discriminator: 0.043406; Generator: 0.021634,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:02,284 root         INFO     Train Epoch: 192 [7168/8000 (90%)]\tTotal Loss: 0.436296\n",
      "Reconstruction: 0.371425, Regularization: 0.000001, Discriminator: 0.043255; Generator: 0.021615,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:01:02,390 root         INFO     Train Epoch: 192 [7680/8000 (96%)]\tTotal Loss: 0.434752\n",
      "Reconstruction: 0.369822, Regularization: 0.000001, Discriminator: 0.043304; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:02,469 root         INFO     ====> Epoch: 192 Average loss: 0.4465\n",
      "2019-04-10 01:01:02,496 root         INFO     Train Epoch: 193 [0/8000 (0%)]\tTotal Loss: 0.434086\n",
      "Reconstruction: 0.369153, Regularization: 0.000001, Discriminator: 0.043325; Generator: 0.021608,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:02,605 root         INFO     Train Epoch: 193 [512/8000 (6%)]\tTotal Loss: 0.432979\n",
      "Reconstruction: 0.368047, Regularization: 0.000001, Discriminator: 0.043305; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:02,714 root         INFO     Train Epoch: 193 [1024/8000 (13%)]\tTotal Loss: 0.432413\n",
      "Reconstruction: 0.367469, Regularization: 0.000001, Discriminator: 0.043293; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:02,822 root         INFO     Train Epoch: 193 [1536/8000 (19%)]\tTotal Loss: 0.431703\n",
      "Reconstruction: 0.366793, Regularization: 0.000001, Discriminator: 0.043335; Generator: 0.021573,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:02,931 root         INFO     Train Epoch: 193 [2048/8000 (26%)]\tTotal Loss: 0.431421\n",
      "Reconstruction: 0.366465, Regularization: 0.000001, Discriminator: 0.043345; Generator: 0.021610,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:01:03,040 root         INFO     Train Epoch: 193 [2560/8000 (32%)]\tTotal Loss: 0.431369\n",
      "Reconstruction: 0.366482, Regularization: 0.000001, Discriminator: 0.043305; Generator: 0.021581,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:01:03,149 root         INFO     Train Epoch: 193 [3072/8000 (38%)]\tTotal Loss: 0.431771\n",
      "Reconstruction: 0.366860, Regularization: 0.000001, Discriminator: 0.043344; Generator: 0.021567,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 01:01:03,258 root         INFO     Train Epoch: 193 [3584/8000 (45%)]\tTotal Loss: 0.432209\n",
      "Reconstruction: 0.367251, Regularization: 0.000001, Discriminator: 0.043341; Generator: 0.021617,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:01:03,367 root         INFO     Train Epoch: 193 [4096/8000 (51%)]\tTotal Loss: 0.432736\n",
      "Reconstruction: 0.367850, Regularization: 0.000000, Discriminator: 0.043316; Generator: 0.021569,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:01:03,476 root         INFO     Train Epoch: 193 [4608/8000 (58%)]\tTotal Loss: 0.433619\n",
      "Reconstruction: 0.368661, Regularization: 0.000001, Discriminator: 0.043364; Generator: 0.021593,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:01:03,585 root         INFO     Train Epoch: 193 [5120/8000 (64%)]\tTotal Loss: 0.434129\n",
      "Reconstruction: 0.369224, Regularization: 0.000001, Discriminator: 0.043314; Generator: 0.021591,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:03,694 root         INFO     Train Epoch: 193 [5632/8000 (70%)]\tTotal Loss: 0.434999\n",
      "Reconstruction: 0.370022, Regularization: 0.000000, Discriminator: 0.043335; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:03,803 root         INFO     Train Epoch: 193 [6144/8000 (77%)]\tTotal Loss: 0.435633\n",
      "Reconstruction: 0.370660, Regularization: 0.000001, Discriminator: 0.043317; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:03,912 root         INFO     Train Epoch: 193 [6656/8000 (83%)]\tTotal Loss: 0.436193\n",
      "Reconstruction: 0.371286, Regularization: 0.000001, Discriminator: 0.043304; Generator: 0.021602,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:04,021 root         INFO     Train Epoch: 193 [7168/8000 (90%)]\tTotal Loss: 0.436530\n",
      "Reconstruction: 0.371600, Regularization: 0.000001, Discriminator: 0.043303; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:04,130 root         INFO     Train Epoch: 193 [7680/8000 (96%)]\tTotal Loss: 0.436732\n",
      "Reconstruction: 0.371842, Regularization: 0.000001, Discriminator: 0.043292; Generator: 0.021598,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:04,210 root         INFO     ====> Epoch: 193 Average loss: 0.4336\n",
      "2019-04-10 01:01:04,237 root         INFO     Train Epoch: 194 [0/8000 (0%)]\tTotal Loss: 0.436976\n",
      "Reconstruction: 0.372041, Regularization: 0.000001, Discriminator: 0.043323; Generator: 0.021611,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:04,347 root         INFO     Train Epoch: 194 [512/8000 (6%)]\tTotal Loss: 0.437202\n",
      "Reconstruction: 0.372342, Regularization: 0.000001, Discriminator: 0.043280; Generator: 0.021579,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:01:04,456 root         INFO     Train Epoch: 194 [1024/8000 (13%)]\tTotal Loss: 0.437484\n",
      "Reconstruction: 0.372528, Regularization: 0.000001, Discriminator: 0.043308; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:04,566 root         INFO     Train Epoch: 194 [1536/8000 (19%)]\tTotal Loss: 0.437921\n",
      "Reconstruction: 0.372968, Regularization: 0.000002, Discriminator: 0.043281; Generator: 0.021670,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:04,676 root         INFO     Train Epoch: 194 [2048/8000 (26%)]\tTotal Loss: 0.438217\n",
      "Reconstruction: 0.373220, Regularization: 0.000002, Discriminator: 0.043362; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:04,785 root         INFO     Train Epoch: 194 [2560/8000 (32%)]\tTotal Loss: 0.438808\n",
      "Reconstruction: 0.373853, Regularization: 0.000003, Discriminator: 0.043304; Generator: 0.021646,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:04,897 root         INFO     Train Epoch: 194 [3072/8000 (38%)]\tTotal Loss: 0.439287\n",
      "Reconstruction: 0.374294, Regularization: 0.000002, Discriminator: 0.043349; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:05,008 root         INFO     Train Epoch: 194 [3584/8000 (45%)]\tTotal Loss: 0.440024\n",
      "Reconstruction: 0.375005, Regularization: 0.000004, Discriminator: 0.043328; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:05,119 root         INFO     Train Epoch: 194 [4096/8000 (51%)]\tTotal Loss: 0.441079\n",
      "Reconstruction: 0.375997, Regularization: 0.000003, Discriminator: 0.043330; Generator: 0.021748,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:05,231 root         INFO     Train Epoch: 194 [4608/8000 (58%)]\tTotal Loss: 0.441842\n",
      "Reconstruction: 0.376753, Regularization: 0.000003, Discriminator: 0.043397; Generator: 0.021688,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:01:05,341 root         INFO     Train Epoch: 194 [5120/8000 (64%)]\tTotal Loss: 0.442678\n",
      "Reconstruction: 0.377638, Regularization: 0.000002, Discriminator: 0.043329; Generator: 0.021709,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:05,452 root         INFO     Train Epoch: 194 [5632/8000 (70%)]\tTotal Loss: 0.443755\n",
      "Reconstruction: 0.378663, Regularization: 0.000002, Discriminator: 0.043340; Generator: 0.021750,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:05,562 root         INFO     Train Epoch: 194 [6144/8000 (77%)]\tTotal Loss: 0.444727\n",
      "Reconstruction: 0.379643, Regularization: 0.000001, Discriminator: 0.043370; Generator: 0.021713,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:05,673 root         INFO     Train Epoch: 194 [6656/8000 (83%)]\tTotal Loss: 0.445482\n",
      "Reconstruction: 0.380443, Regularization: 0.000002, Discriminator: 0.043342; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:05,783 root         INFO     Train Epoch: 194 [7168/8000 (90%)]\tTotal Loss: 0.446475\n",
      "Reconstruction: 0.381411, Regularization: 0.000002, Discriminator: 0.043317; Generator: 0.021745,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:05,893 root         INFO     Train Epoch: 194 [7680/8000 (96%)]\tTotal Loss: 0.447275\n",
      "Reconstruction: 0.382283, Regularization: 0.000003, Discriminator: 0.043321; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:05,973 root         INFO     ====> Epoch: 194 Average loss: 0.4414\n",
      "2019-04-10 01:01:06,001 root         INFO     Train Epoch: 195 [0/8000 (0%)]\tTotal Loss: 0.447920\n",
      "Reconstruction: 0.382884, Regularization: 0.000004, Discriminator: 0.043346; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:06,114 root         INFO     Train Epoch: 195 [512/8000 (6%)]\tTotal Loss: 0.448516\n",
      "Reconstruction: 0.383467, Regularization: 0.000003, Discriminator: 0.043342; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:06,227 root         INFO     Train Epoch: 195 [1024/8000 (13%)]\tTotal Loss: 0.448970\n",
      "Reconstruction: 0.383925, Regularization: 0.000004, Discriminator: 0.043297; Generator: 0.021745,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:06,340 root         INFO     Train Epoch: 195 [1536/8000 (19%)]\tTotal Loss: 0.449336\n",
      "Reconstruction: 0.384316, Regularization: 0.000003, Discriminator: 0.043303; Generator: 0.021714,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:06,452 root         INFO     Train Epoch: 195 [2048/8000 (26%)]\tTotal Loss: 0.449282\n",
      "Reconstruction: 0.384247, Regularization: 0.000004, Discriminator: 0.043314; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:06,564 root         INFO     Train Epoch: 195 [2560/8000 (32%)]\tTotal Loss: 0.448932\n",
      "Reconstruction: 0.383947, Regularization: 0.000004, Discriminator: 0.043309; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:06,677 root         INFO     Train Epoch: 195 [3072/8000 (38%)]\tTotal Loss: 0.448396\n",
      "Reconstruction: 0.383359, Regularization: 0.000004, Discriminator: 0.043332; Generator: 0.021700,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:06,789 root         INFO     Train Epoch: 195 [3584/8000 (45%)]\tTotal Loss: 0.447724\n",
      "Reconstruction: 0.382684, Regularization: 0.000004, Discriminator: 0.043317; Generator: 0.021720,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:06,902 root         INFO     Train Epoch: 195 [4096/8000 (51%)]\tTotal Loss: 0.446453\n",
      "Reconstruction: 0.381393, Regularization: 0.000002, Discriminator: 0.043328; Generator: 0.021730,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:07,013 root         INFO     Train Epoch: 195 [4608/8000 (58%)]\tTotal Loss: 0.444440\n",
      "Reconstruction: 0.379498, Regularization: 0.000002, Discriminator: 0.043253; Generator: 0.021687,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:07,124 root         INFO     Train Epoch: 195 [5120/8000 (64%)]\tTotal Loss: 0.442216\n",
      "Reconstruction: 0.377260, Regularization: 0.000001, Discriminator: 0.043282; Generator: 0.021673,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:07,235 root         INFO     Train Epoch: 195 [5632/8000 (70%)]\tTotal Loss: 0.439746\n",
      "Reconstruction: 0.374799, Regularization: 0.000000, Discriminator: 0.043287; Generator: 0.021659,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:07,347 root         INFO     Train Epoch: 195 [6144/8000 (77%)]\tTotal Loss: 0.437712\n",
      "Reconstruction: 0.372683, Regularization: 0.000001, Discriminator: 0.043390; Generator: 0.021639,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:07,458 root         INFO     Train Epoch: 195 [6656/8000 (83%)]\tTotal Loss: 0.435880\n",
      "Reconstruction: 0.370984, Regularization: 0.000000, Discriminator: 0.043228; Generator: 0.021667,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:07,570 root         INFO     Train Epoch: 195 [7168/8000 (90%)]\tTotal Loss: 0.434269\n",
      "Reconstruction: 0.369281, Regularization: 0.000000, Discriminator: 0.043301; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:07,681 root         INFO     Train Epoch: 195 [7680/8000 (96%)]\tTotal Loss: 0.432721\n",
      "Reconstruction: 0.367643, Regularization: 0.000003, Discriminator: 0.043413; Generator: 0.021663,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:07,763 root         INFO     ====> Epoch: 195 Average loss: 0.4437\n",
      "2019-04-10 01:01:07,790 root         INFO     Train Epoch: 196 [0/8000 (0%)]\tTotal Loss: 0.431738\n",
      "Reconstruction: 0.366772, Regularization: 0.000003, Discriminator: 0.043354; Generator: 0.021609,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:01:07,901 root         INFO     Train Epoch: 196 [512/8000 (6%)]\tTotal Loss: 0.430930\n",
      "Reconstruction: 0.365987, Regularization: 0.000003, Discriminator: 0.043369; Generator: 0.021571,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:08,011 root         INFO     Train Epoch: 196 [1024/8000 (13%)]\tTotal Loss: 0.430432\n",
      "Reconstruction: 0.365504, Regularization: 0.000003, Discriminator: 0.043333; Generator: 0.021592,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:08,122 root         INFO     Train Epoch: 196 [1536/8000 (19%)]\tTotal Loss: 0.430158\n",
      "Reconstruction: 0.365268, Regularization: 0.000002, Discriminator: 0.043316; Generator: 0.021572,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:01:08,232 root         INFO     Train Epoch: 196 [2048/8000 (26%)]\tTotal Loss: 0.430674\n",
      "Reconstruction: 0.365722, Regularization: 0.000003, Discriminator: 0.043346; Generator: 0.021603,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:08,342 root         INFO     Train Epoch: 196 [2560/8000 (32%)]\tTotal Loss: 0.431333\n",
      "Reconstruction: 0.366342, Regularization: 0.000002, Discriminator: 0.043332; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:08,452 root         INFO     Train Epoch: 196 [3072/8000 (38%)]\tTotal Loss: 0.431877\n",
      "Reconstruction: 0.366913, Regularization: 0.000002, Discriminator: 0.043320; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:08,563 root         INFO     Train Epoch: 196 [3584/8000 (45%)]\tTotal Loss: 0.432800\n",
      "Reconstruction: 0.367926, Regularization: 0.000002, Discriminator: 0.043304; Generator: 0.021568,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:01:08,673 root         INFO     Train Epoch: 196 [4096/8000 (51%)]\tTotal Loss: 0.433695\n",
      "Reconstruction: 0.368777, Regularization: 0.000002, Discriminator: 0.043317; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:08,781 root         INFO     Train Epoch: 196 [4608/8000 (58%)]\tTotal Loss: 0.434533\n",
      "Reconstruction: 0.369615, Regularization: 0.000003, Discriminator: 0.043344; Generator: 0.021571,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:08,889 root         INFO     Train Epoch: 196 [5120/8000 (64%)]\tTotal Loss: 0.435563\n",
      "Reconstruction: 0.370609, Regularization: 0.000005, Discriminator: 0.043292; Generator: 0.021658,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:08,998 root         INFO     Train Epoch: 196 [5632/8000 (70%)]\tTotal Loss: 0.436582\n",
      "Reconstruction: 0.371642, Regularization: 0.000005, Discriminator: 0.043331; Generator: 0.021603,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:09,106 root         INFO     Train Epoch: 196 [6144/8000 (77%)]\tTotal Loss: 0.437460\n",
      "Reconstruction: 0.372548, Regularization: 0.000004, Discriminator: 0.043286; Generator: 0.021622,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:09,214 root         INFO     Train Epoch: 196 [6656/8000 (83%)]\tTotal Loss: 0.437836\n",
      "Reconstruction: 0.372944, Regularization: 0.000003, Discriminator: 0.043309; Generator: 0.021580,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 01:01:09,323 root         INFO     Train Epoch: 196 [7168/8000 (90%)]\tTotal Loss: 0.438407\n",
      "Reconstruction: 0.373472, Regularization: 0.000003, Discriminator: 0.043282; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:09,431 root         INFO     Train Epoch: 196 [7680/8000 (96%)]\tTotal Loss: 0.438825\n",
      "Reconstruction: 0.373878, Regularization: 0.000003, Discriminator: 0.043327; Generator: 0.021618,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:09,511 root         INFO     ====> Epoch: 196 Average loss: 0.4340\n",
      "2019-04-10 01:01:09,539 root         INFO     Train Epoch: 197 [0/8000 (0%)]\tTotal Loss: 0.439032\n",
      "Reconstruction: 0.374107, Regularization: 0.000003, Discriminator: 0.043275; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:09,647 root         INFO     Train Epoch: 197 [512/8000 (6%)]\tTotal Loss: 0.439702\n",
      "Reconstruction: 0.374705, Regularization: 0.000004, Discriminator: 0.043330; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:09,755 root         INFO     Train Epoch: 197 [1024/8000 (13%)]\tTotal Loss: 0.439960\n",
      "Reconstruction: 0.374962, Regularization: 0.000004, Discriminator: 0.043283; Generator: 0.021711,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:01:09,862 root         INFO     Train Epoch: 197 [1536/8000 (19%)]\tTotal Loss: 0.440286\n",
      "Reconstruction: 0.375264, Regularization: 0.000004, Discriminator: 0.043328; Generator: 0.021690,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:09,970 root         INFO     Train Epoch: 197 [2048/8000 (26%)]\tTotal Loss: 0.440411\n",
      "Reconstruction: 0.375403, Regularization: 0.000003, Discriminator: 0.043325; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:10,078 root         INFO     Train Epoch: 197 [2560/8000 (32%)]\tTotal Loss: 0.440730\n",
      "Reconstruction: 0.375705, Regularization: 0.000003, Discriminator: 0.043342; Generator: 0.021680,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:10,186 root         INFO     Train Epoch: 197 [3072/8000 (38%)]\tTotal Loss: 0.440882\n",
      "Reconstruction: 0.375823, Regularization: 0.000003, Discriminator: 0.043342; Generator: 0.021714,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:10,294 root         INFO     Train Epoch: 197 [3584/8000 (45%)]\tTotal Loss: 0.441399\n",
      "Reconstruction: 0.376355, Regularization: 0.000003, Discriminator: 0.043368; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:10,402 root         INFO     Train Epoch: 197 [4096/8000 (51%)]\tTotal Loss: 0.442279\n",
      "Reconstruction: 0.377189, Regularization: 0.000003, Discriminator: 0.043371; Generator: 0.021715,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:10,510 root         INFO     Train Epoch: 197 [4608/8000 (58%)]\tTotal Loss: 0.443314\n",
      "Reconstruction: 0.378236, Regularization: 0.000003, Discriminator: 0.043346; Generator: 0.021730,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:10,618 root         INFO     Train Epoch: 197 [5120/8000 (64%)]\tTotal Loss: 0.444383\n",
      "Reconstruction: 0.379307, Regularization: 0.000003, Discriminator: 0.043350; Generator: 0.021722,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:10,727 root         INFO     Train Epoch: 197 [5632/8000 (70%)]\tTotal Loss: 0.445294\n",
      "Reconstruction: 0.380308, Regularization: 0.000002, Discriminator: 0.043338; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:10,835 root         INFO     Train Epoch: 197 [6144/8000 (77%)]\tTotal Loss: 0.446723\n",
      "Reconstruction: 0.381709, Regularization: 0.000003, Discriminator: 0.043268; Generator: 0.021743,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:01:10,943 root         INFO     Train Epoch: 197 [6656/8000 (83%)]\tTotal Loss: 0.448045\n",
      "Reconstruction: 0.382890, Regularization: 0.000003, Discriminator: 0.043404; Generator: 0.021748,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,051 root         INFO     Train Epoch: 197 [7168/8000 (90%)]\tTotal Loss: 0.448896\n",
      "Reconstruction: 0.383798, Regularization: 0.000004, Discriminator: 0.043354; Generator: 0.021740,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,158 root         INFO     Train Epoch: 197 [7680/8000 (96%)]\tTotal Loss: 0.449274\n",
      "Reconstruction: 0.384262, Regularization: 0.000004, Discriminator: 0.043324; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:11,237 root         INFO     ====> Epoch: 197 Average loss: 0.4434\n",
      "2019-04-10 01:01:11,265 root         INFO     Train Epoch: 198 [0/8000 (0%)]\tTotal Loss: 0.449607\n",
      "Reconstruction: 0.384559, Regularization: 0.000005, Discriminator: 0.043319; Generator: 0.021725,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,375 root         INFO     Train Epoch: 198 [512/8000 (6%)]\tTotal Loss: 0.449600\n",
      "Reconstruction: 0.384544, Regularization: 0.000005, Discriminator: 0.043315; Generator: 0.021736,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,486 root         INFO     Train Epoch: 198 [1024/8000 (13%)]\tTotal Loss: 0.449344\n",
      "Reconstruction: 0.384298, Regularization: 0.000004, Discriminator: 0.043328; Generator: 0.021713,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,596 root         INFO     Train Epoch: 198 [1536/8000 (19%)]\tTotal Loss: 0.448452\n",
      "Reconstruction: 0.383438, Regularization: 0.000003, Discriminator: 0.043314; Generator: 0.021697,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,706 root         INFO     Train Epoch: 198 [2048/8000 (26%)]\tTotal Loss: 0.447096\n",
      "Reconstruction: 0.382198, Regularization: 0.000002, Discriminator: 0.043201; Generator: 0.021695,\n",
      "D(x): 0.501, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,817 root         INFO     Train Epoch: 198 [2560/8000 (32%)]\tTotal Loss: 0.445211\n",
      "Reconstruction: 0.380230, Regularization: 0.000001, Discriminator: 0.043271; Generator: 0.021709,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 01:01:11,927 root         INFO     Train Epoch: 198 [3072/8000 (38%)]\tTotal Loss: 0.442597\n",
      "Reconstruction: 0.377679, Regularization: 0.000001, Discriminator: 0.043237; Generator: 0.021681,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 01:01:12,037 root         INFO     Train Epoch: 198 [3584/8000 (45%)]\tTotal Loss: 0.439952\n",
      "Reconstruction: 0.374870, Regularization: 0.000002, Discriminator: 0.043421; Generator: 0.021660,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:01:12,148 root         INFO     Train Epoch: 198 [4096/8000 (51%)]\tTotal Loss: 0.436953\n",
      "Reconstruction: 0.372131, Regularization: 0.000001, Discriminator: 0.043198; Generator: 0.021623,\n",
      "D(x): 0.503, D(G(z)): 0.501\n",
      "2019-04-10 01:01:12,258 root         INFO     Train Epoch: 198 [4608/8000 (58%)]\tTotal Loss: 0.435075\n",
      "Reconstruction: 0.369934, Regularization: 0.000001, Discriminator: 0.043503; Generator: 0.021637,\n",
      "D(x): 0.498, D(G(z)): 0.500\n",
      "2019-04-10 01:01:12,369 root         INFO     Train Epoch: 198 [5120/8000 (64%)]\tTotal Loss: 0.433052\n",
      "Reconstruction: 0.368093, Regularization: 0.000000, Discriminator: 0.043335; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:01:12,479 root         INFO     Train Epoch: 198 [5632/8000 (70%)]\tTotal Loss: 0.431863\n",
      "Reconstruction: 0.366837, Regularization: 0.000001, Discriminator: 0.043408; Generator: 0.021617,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-10 01:01:12,589 root         INFO     Train Epoch: 198 [6144/8000 (77%)]\tTotal Loss: 0.430648\n",
      "Reconstruction: 0.365710, Regularization: 0.000000, Discriminator: 0.043354; Generator: 0.021584,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:12,699 root         INFO     Train Epoch: 198 [6656/8000 (83%)]\tTotal Loss: 0.430455\n",
      "Reconstruction: 0.365486, Regularization: 0.000001, Discriminator: 0.043332; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:12,809 root         INFO     Train Epoch: 198 [7168/8000 (90%)]\tTotal Loss: 0.430420\n",
      "Reconstruction: 0.365486, Regularization: 0.000001, Discriminator: 0.043346; Generator: 0.021588,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:12,919 root         INFO     Train Epoch: 198 [7680/8000 (96%)]\tTotal Loss: 0.430942\n",
      "Reconstruction: 0.365988, Regularization: 0.000000, Discriminator: 0.043328; Generator: 0.021626,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:01:12,999 root         INFO     ====> Epoch: 198 Average loss: 0.4391\n",
      "2019-04-10 01:01:13,026 root         INFO     Train Epoch: 199 [0/8000 (0%)]\tTotal Loss: 0.431222\n",
      "Reconstruction: 0.366319, Regularization: 0.000000, Discriminator: 0.043315; Generator: 0.021588,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:13,137 root         INFO     Train Epoch: 199 [512/8000 (6%)]\tTotal Loss: 0.431944\n",
      "Reconstruction: 0.367015, Regularization: 0.000000, Discriminator: 0.043330; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:13,248 root         INFO     Train Epoch: 199 [1024/8000 (13%)]\tTotal Loss: 0.432971\n",
      "Reconstruction: 0.368017, Regularization: 0.000000, Discriminator: 0.043324; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 01:01:13,358 root         INFO     Train Epoch: 199 [1536/8000 (19%)]\tTotal Loss: 0.434140\n",
      "Reconstruction: 0.369118, Regularization: 0.000000, Discriminator: 0.043379; Generator: 0.021643,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:13,468 root         INFO     Train Epoch: 199 [2048/8000 (26%)]\tTotal Loss: 0.434846\n",
      "Reconstruction: 0.370025, Regularization: 0.000001, Discriminator: 0.043258; Generator: 0.021563,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 01:01:13,579 root         INFO     Train Epoch: 199 [2560/8000 (32%)]\tTotal Loss: 0.436167\n",
      "Reconstruction: 0.371258, Regularization: 0.000000, Discriminator: 0.043317; Generator: 0.021592,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:13,689 root         INFO     Train Epoch: 199 [3072/8000 (38%)]\tTotal Loss: 0.437095\n",
      "Reconstruction: 0.372114, Regularization: 0.000001, Discriminator: 0.043347; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:13,800 root         INFO     Train Epoch: 199 [3584/8000 (45%)]\tTotal Loss: 0.437812\n",
      "Reconstruction: 0.372840, Regularization: 0.000000, Discriminator: 0.043324; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:13,911 root         INFO     Train Epoch: 199 [4096/8000 (51%)]\tTotal Loss: 0.438313\n",
      "Reconstruction: 0.373418, Regularization: 0.000002, Discriminator: 0.043297; Generator: 0.021596,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:14,021 root         INFO     Train Epoch: 199 [4608/8000 (58%)]\tTotal Loss: 0.438847\n",
      "Reconstruction: 0.373950, Regularization: 0.000001, Discriminator: 0.043265; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 01:01:14,133 root         INFO     Train Epoch: 199 [5120/8000 (64%)]\tTotal Loss: 0.439491\n",
      "Reconstruction: 0.374537, Regularization: 0.000002, Discriminator: 0.043315; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:14,244 root         INFO     Train Epoch: 199 [5632/8000 (70%)]\tTotal Loss: 0.439884\n",
      "Reconstruction: 0.374929, Regularization: 0.000003, Discriminator: 0.043287; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:14,354 root         INFO     Train Epoch: 199 [6144/8000 (77%)]\tTotal Loss: 0.440328\n",
      "Reconstruction: 0.375333, Regularization: 0.000003, Discriminator: 0.043344; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:14,464 root         INFO     Train Epoch: 199 [6656/8000 (83%)]\tTotal Loss: 0.440367\n",
      "Reconstruction: 0.375361, Regularization: 0.000002, Discriminator: 0.043333; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:14,574 root         INFO     Train Epoch: 199 [7168/8000 (90%)]\tTotal Loss: 0.440543\n",
      "Reconstruction: 0.375567, Regularization: 0.000002, Discriminator: 0.043292; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 01:01:14,685 root         INFO     Train Epoch: 199 [7680/8000 (96%)]\tTotal Loss: 0.441097\n",
      "Reconstruction: 0.376066, Regularization: 0.000004, Discriminator: 0.043353; Generator: 0.021674,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 01:01:14,764 root         INFO     ====> Epoch: 199 Average loss: 0.4374\n",
      "2019-04-10 01:01:14,778 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) done      TrainVEM()\n",
      "2019-04-10 01:01:14,778 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 01:01:14,778 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   DONE\n",
      "2019-04-10 01:01:14,779 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:01:14,779 luigi-interface DEBUG    Pending tasks: 2\n",
      "2019-04-10 01:01:14,779 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) running   TrainVAE()\n",
      "2019-04-10 01:01:14,780 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-10 01:01:14,780 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-10 01:01:14,781 root         INFO     Values of VAE's decoder parameters before training:\n",
      "2019-04-10 01:01:14,781 root         INFO     layers.0.weight\n",
      "2019-04-10 01:01:14,781 root         INFO     tensor([[-0.6497],\n",
      "        [ 0.9626]], device='cuda:0')\n",
      "2019-04-10 01:01:14,782 root         INFO     layers.0.bias\n",
      "2019-04-10 01:01:14,782 root         INFO     tensor([-0.9750, -0.2332], device='cuda:0')\n",
      "2019-04-10 01:01:14,783 root         INFO     layers.1.weight\n",
      "2019-04-10 01:01:14,784 root         INFO     tensor([[-0.1379, -0.5922],\n",
      "        [-0.2357,  0.5636]], device='cuda:0')\n",
      "2019-04-10 01:01:14,785 root         INFO     layers.1.bias\n",
      "2019-04-10 01:01:14,785 root         INFO     tensor([-0.1943,  0.0769], device='cuda:0')\n",
      "2019-04-10 01:01:14,786 root         INFO     layers.2.weight\n",
      "2019-04-10 01:01:14,786 root         INFO     tensor([[ 0.6644, -0.5124],\n",
      "        [-0.4031, -0.4051]], device='cuda:0')\n",
      "2019-04-10 01:01:14,787 root         INFO     layers.2.bias\n",
      "2019-04-10 01:01:14,787 root         INFO     tensor([-0.5565,  0.1725], device='cuda:0')\n",
      "2019-04-10 01:01:14,788 root         INFO     layers.3.weight\n",
      "2019-04-10 01:01:14,788 root         INFO     tensor([[-0.3523, -0.6266],\n",
      "        [-0.3515, -0.2904]], device='cuda:0')\n",
      "2019-04-10 01:01:14,789 root         INFO     layers.3.bias\n",
      "2019-04-10 01:01:14,789 root         INFO     tensor([-0.3550,  0.0500], device='cuda:0')\n",
      "2019-04-10 01:01:14,790 root         INFO     layers.4.weight\n",
      "2019-04-10 01:01:14,790 root         INFO     tensor([[ 0.2561, -0.3836],\n",
      "        [ 0.3839,  0.2321]], device='cuda:0')\n",
      "2019-04-10 01:01:14,791 root         INFO     layers.4.bias\n",
      "2019-04-10 01:01:14,791 root         INFO     tensor([-0.6679, -0.5838], device='cuda:0')\n",
      "2019-04-10 01:01:14,792 root         INFO     layers.5.weight\n",
      "2019-04-10 01:01:14,792 root         INFO     tensor([[-0.1573,  0.5253],\n",
      "        [ 0.6630,  0.1170]], device='cuda:0')\n",
      "2019-04-10 01:01:14,793 root         INFO     layers.5.bias\n",
      "2019-04-10 01:01:14,793 root         INFO     tensor([ 0.4975, -0.3834], device='cuda:0')\n",
      "2019-04-10 01:01:14,818 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 7.846128\n",
      "Reconstruction: 5.822622, Regularization: 2.023506\n",
      "2019-04-10 01:01:14,883 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: 8.413287\n",
      "Reconstruction: 6.277318, Regularization: 2.135969\n",
      "2019-04-10 01:01:14,946 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: 9.838552\n",
      "Reconstruction: 7.612849, Regularization: 2.225703\n",
      "2019-04-10 01:01:15,009 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: 8.005777\n",
      "Reconstruction: 5.963937, Regularization: 2.041841\n",
      "2019-04-10 01:01:15,072 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: 8.195824\n",
      "Reconstruction: 5.900893, Regularization: 2.294930\n",
      "2019-04-10 01:01:15,135 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: 6.514953\n",
      "Reconstruction: 4.443342, Regularization: 2.071611\n",
      "2019-04-10 01:01:15,198 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: 7.841286\n",
      "Reconstruction: 5.773916, Regularization: 2.067370\n",
      "2019-04-10 01:01:15,261 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: 5.635639\n",
      "Reconstruction: 3.713039, Regularization: 1.922601\n",
      "2019-04-10 01:01:15,323 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: 5.959963\n",
      "Reconstruction: 3.839532, Regularization: 2.120431\n",
      "2019-04-10 01:01:15,385 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: 6.695278\n",
      "Reconstruction: 4.656024, Regularization: 2.039254\n",
      "2019-04-10 01:01:15,447 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: 5.305213\n",
      "Reconstruction: 3.289725, Regularization: 2.015488\n",
      "2019-04-10 01:01:15,511 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: 4.243719\n",
      "Reconstruction: 2.559018, Regularization: 1.684701\n",
      "2019-04-10 01:01:15,575 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: 4.396250\n",
      "Reconstruction: 2.560688, Regularization: 1.835562\n",
      "2019-04-10 01:01:15,638 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: 3.436491\n",
      "Reconstruction: 1.844266, Regularization: 1.592225\n",
      "2019-04-10 01:01:15,702 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: 4.057766\n",
      "Reconstruction: 2.367594, Regularization: 1.690172\n",
      "2019-04-10 01:01:15,765 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: 4.591332\n",
      "Reconstruction: 2.648365, Regularization: 1.942967\n",
      "2019-04-10 01:01:15,820 root         INFO     ====> Epoch: 0 Average loss: 5.9170\n",
      "2019-04-10 01:01:15,843 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: 4.516862\n",
      "Reconstruction: 2.647501, Regularization: 1.869361\n",
      "2019-04-10 01:01:15,907 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: 4.078029\n",
      "Reconstruction: 2.211078, Regularization: 1.866951\n",
      "2019-04-10 01:01:15,971 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: 4.302875\n",
      "Reconstruction: 2.348512, Regularization: 1.954363\n",
      "2019-04-10 01:01:16,035 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: 4.783562\n",
      "Reconstruction: 2.696194, Regularization: 2.087368\n",
      "2019-04-10 01:01:16,098 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: 3.796474\n",
      "Reconstruction: 2.057569, Regularization: 1.738905\n",
      "2019-04-10 01:01:16,162 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: 4.193924\n",
      "Reconstruction: 2.346587, Regularization: 1.847337\n",
      "2019-04-10 01:01:16,226 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: 3.769621\n",
      "Reconstruction: 1.991831, Regularization: 1.777790\n",
      "2019-04-10 01:01:16,290 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: 4.118927\n",
      "Reconstruction: 2.314456, Regularization: 1.804471\n",
      "2019-04-10 01:01:16,354 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: 3.339290\n",
      "Reconstruction: 1.657040, Regularization: 1.682250\n",
      "2019-04-10 01:01:16,418 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: 3.663706\n",
      "Reconstruction: 1.797652, Regularization: 1.866054\n",
      "2019-04-10 01:01:16,482 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: 3.613206\n",
      "Reconstruction: 1.778797, Regularization: 1.834409\n",
      "2019-04-10 01:01:16,546 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: 3.790715\n",
      "Reconstruction: 1.860723, Regularization: 1.929992\n",
      "2019-04-10 01:01:16,610 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: 3.275140\n",
      "Reconstruction: 1.521680, Regularization: 1.753460\n",
      "2019-04-10 01:01:16,674 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: 2.949405\n",
      "Reconstruction: 1.331964, Regularization: 1.617441\n",
      "2019-04-10 01:01:16,737 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: 3.229370\n",
      "Reconstruction: 1.440584, Regularization: 1.788786\n",
      "2019-04-10 01:01:16,801 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: 3.297959\n",
      "Reconstruction: 1.549875, Regularization: 1.748084\n",
      "2019-04-10 01:01:16,855 root         INFO     ====> Epoch: 1 Average loss: 3.6930\n",
      "2019-04-10 01:01:16,879 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: 3.226807\n",
      "Reconstruction: 1.474194, Regularization: 1.752613\n",
      "2019-04-10 01:01:16,943 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: 2.941768\n",
      "Reconstruction: 1.319247, Regularization: 1.622521\n",
      "2019-04-10 01:01:17,007 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: 2.984887\n",
      "Reconstruction: 1.211312, Regularization: 1.773575\n",
      "2019-04-10 01:01:17,071 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: 2.971921\n",
      "Reconstruction: 1.231864, Regularization: 1.740058\n",
      "2019-04-10 01:01:17,134 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: 3.007573\n",
      "Reconstruction: 1.354121, Regularization: 1.653452\n",
      "2019-04-10 01:01:17,198 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: 2.706620\n",
      "Reconstruction: 1.090011, Regularization: 1.616609\n",
      "2019-04-10 01:01:17,261 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: 3.123540\n",
      "Reconstruction: 1.304878, Regularization: 1.818663\n",
      "2019-04-10 01:01:17,322 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: 2.989657\n",
      "Reconstruction: 1.305538, Regularization: 1.684119\n",
      "2019-04-10 01:01:17,383 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: 2.922275\n",
      "Reconstruction: 1.213032, Regularization: 1.709242\n",
      "2019-04-10 01:01:17,445 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: 2.581858\n",
      "Reconstruction: 1.026107, Regularization: 1.555750\n",
      "2019-04-10 01:01:17,506 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: 2.807855\n",
      "Reconstruction: 1.098133, Regularization: 1.709722\n",
      "2019-04-10 01:01:17,569 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: 2.739797\n",
      "Reconstruction: 1.066094, Regularization: 1.673703\n",
      "2019-04-10 01:01:17,632 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: 2.576900\n",
      "Reconstruction: 1.015538, Regularization: 1.561362\n",
      "2019-04-10 01:01:17,695 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: 2.576222\n",
      "Reconstruction: 0.904626, Regularization: 1.671596\n",
      "2019-04-10 01:01:17,759 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: 2.720789\n",
      "Reconstruction: 1.121733, Regularization: 1.599056\n",
      "2019-04-10 01:01:17,822 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: 2.186319\n",
      "Reconstruction: 0.825734, Regularization: 1.360585\n",
      "2019-04-10 01:01:17,876 root         INFO     ====> Epoch: 2 Average loss: 2.8377\n",
      "2019-04-10 01:01:17,900 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: 2.821987\n",
      "Reconstruction: 1.082294, Regularization: 1.739693\n",
      "2019-04-10 01:01:17,964 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: 2.776740\n",
      "Reconstruction: 1.042758, Regularization: 1.733982\n",
      "2019-04-10 01:01:18,028 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: 2.432557\n",
      "Reconstruction: 0.905513, Regularization: 1.527044\n",
      "2019-04-10 01:01:18,091 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: 2.565416\n",
      "Reconstruction: 0.985581, Regularization: 1.579835\n",
      "2019-04-10 01:01:18,154 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: 2.613748\n",
      "Reconstruction: 0.944351, Regularization: 1.669397\n",
      "2019-04-10 01:01:18,218 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: 2.127670\n",
      "Reconstruction: 0.735927, Regularization: 1.391743\n",
      "2019-04-10 01:01:18,281 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: 2.555143\n",
      "Reconstruction: 0.943851, Regularization: 1.611292\n",
      "2019-04-10 01:01:18,344 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: 2.521966\n",
      "Reconstruction: 0.903550, Regularization: 1.618416\n",
      "2019-04-10 01:01:18,407 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: 2.340497\n",
      "Reconstruction: 0.832395, Regularization: 1.508101\n",
      "2019-04-10 01:01:18,470 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: 2.351327\n",
      "Reconstruction: 0.799344, Regularization: 1.551983\n",
      "2019-04-10 01:01:18,533 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: 2.513913\n",
      "Reconstruction: 0.877846, Regularization: 1.636068\n",
      "2019-04-10 01:01:18,596 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: 2.224570\n",
      "Reconstruction: 0.722848, Regularization: 1.501722\n",
      "2019-04-10 01:01:18,659 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: 2.081425\n",
      "Reconstruction: 0.685854, Regularization: 1.395571\n",
      "2019-04-10 01:01:18,722 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: 2.457030\n",
      "Reconstruction: 0.860777, Regularization: 1.596253\n",
      "2019-04-10 01:01:18,785 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: 2.238989\n",
      "Reconstruction: 0.756335, Regularization: 1.482655\n",
      "2019-04-10 01:01:18,848 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: 2.201826\n",
      "Reconstruction: 0.725753, Regularization: 1.476073\n",
      "2019-04-10 01:01:18,903 root         INFO     ====> Epoch: 3 Average loss: 2.3662\n",
      "2019-04-10 01:01:18,926 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: 2.476554\n",
      "Reconstruction: 0.881592, Regularization: 1.594962\n",
      "2019-04-10 01:01:18,990 root         INFO     Train Epoch: 4 [512/8000 (6%)]\tTotal Loss: 1.919015\n",
      "Reconstruction: 0.609200, Regularization: 1.309815\n",
      "2019-04-10 01:01:19,053 root         INFO     Train Epoch: 4 [1024/8000 (13%)]\tTotal Loss: 2.129502\n",
      "Reconstruction: 0.707603, Regularization: 1.421899\n",
      "2019-04-10 01:01:19,116 root         INFO     Train Epoch: 4 [1536/8000 (19%)]\tTotal Loss: 2.099548\n",
      "Reconstruction: 0.688904, Regularization: 1.410644\n",
      "2019-04-10 01:01:19,180 root         INFO     Train Epoch: 4 [2048/8000 (26%)]\tTotal Loss: 2.221757\n",
      "Reconstruction: 0.730815, Regularization: 1.490942\n",
      "2019-04-10 01:01:19,243 root         INFO     Train Epoch: 4 [2560/8000 (32%)]\tTotal Loss: 2.043515\n",
      "Reconstruction: 0.654012, Regularization: 1.389503\n",
      "2019-04-10 01:01:19,306 root         INFO     Train Epoch: 4 [3072/8000 (38%)]\tTotal Loss: 1.934004\n",
      "Reconstruction: 0.615341, Regularization: 1.318663\n",
      "2019-04-10 01:01:19,369 root         INFO     Train Epoch: 4 [3584/8000 (45%)]\tTotal Loss: 1.959178\n",
      "Reconstruction: 0.621513, Regularization: 1.337665\n",
      "2019-04-10 01:01:19,433 root         INFO     Train Epoch: 4 [4096/8000 (51%)]\tTotal Loss: 2.066052\n",
      "Reconstruction: 0.646848, Regularization: 1.419205\n",
      "2019-04-10 01:01:19,496 root         INFO     Train Epoch: 4 [4608/8000 (58%)]\tTotal Loss: 1.918634\n",
      "Reconstruction: 0.632365, Regularization: 1.286269\n",
      "2019-04-10 01:01:19,560 root         INFO     Train Epoch: 4 [5120/8000 (64%)]\tTotal Loss: 1.816303\n",
      "Reconstruction: 0.552644, Regularization: 1.263659\n",
      "2019-04-10 01:01:19,624 root         INFO     Train Epoch: 4 [5632/8000 (70%)]\tTotal Loss: 2.074056\n",
      "Reconstruction: 0.653444, Regularization: 1.420612\n",
      "2019-04-10 01:01:19,687 root         INFO     Train Epoch: 4 [6144/8000 (77%)]\tTotal Loss: 1.964914\n",
      "Reconstruction: 0.615631, Regularization: 1.349283\n",
      "2019-04-10 01:01:19,751 root         INFO     Train Epoch: 4 [6656/8000 (83%)]\tTotal Loss: 1.863555\n",
      "Reconstruction: 0.574770, Regularization: 1.288785\n",
      "2019-04-10 01:01:19,815 root         INFO     Train Epoch: 4 [7168/8000 (90%)]\tTotal Loss: 2.010751\n",
      "Reconstruction: 0.605424, Regularization: 1.405326\n",
      "2019-04-10 01:01:19,879 root         INFO     Train Epoch: 4 [7680/8000 (96%)]\tTotal Loss: 1.956338\n",
      "Reconstruction: 0.632066, Regularization: 1.324272\n",
      "2019-04-10 01:01:19,933 root         INFO     ====> Epoch: 4 Average loss: 2.0464\n",
      "2019-04-10 01:01:19,957 root         INFO     Train Epoch: 5 [0/8000 (0%)]\tTotal Loss: 1.922698\n",
      "Reconstruction: 0.595462, Regularization: 1.327236\n",
      "2019-04-10 01:01:20,021 root         INFO     Train Epoch: 5 [512/8000 (6%)]\tTotal Loss: 1.835512\n",
      "Reconstruction: 0.546823, Regularization: 1.288689\n",
      "2019-04-10 01:01:20,084 root         INFO     Train Epoch: 5 [1024/8000 (13%)]\tTotal Loss: 1.851112\n",
      "Reconstruction: 0.543433, Regularization: 1.307678\n",
      "2019-04-10 01:01:20,147 root         INFO     Train Epoch: 5 [1536/8000 (19%)]\tTotal Loss: 1.848418\n",
      "Reconstruction: 0.550852, Regularization: 1.297566\n",
      "2019-04-10 01:01:20,210 root         INFO     Train Epoch: 5 [2048/8000 (26%)]\tTotal Loss: 1.910086\n",
      "Reconstruction: 0.569079, Regularization: 1.341006\n",
      "2019-04-10 01:01:20,273 root         INFO     Train Epoch: 5 [2560/8000 (32%)]\tTotal Loss: 1.813897\n",
      "Reconstruction: 0.557353, Regularization: 1.256544\n",
      "2019-04-10 01:01:20,335 root         INFO     Train Epoch: 5 [3072/8000 (38%)]\tTotal Loss: 1.854205\n",
      "Reconstruction: 0.565644, Regularization: 1.288562\n",
      "2019-04-10 01:01:20,397 root         INFO     Train Epoch: 5 [3584/8000 (45%)]\tTotal Loss: 1.828524\n",
      "Reconstruction: 0.559240, Regularization: 1.269285\n",
      "2019-04-10 01:01:20,459 root         INFO     Train Epoch: 5 [4096/8000 (51%)]\tTotal Loss: 1.849199\n",
      "Reconstruction: 0.555731, Regularization: 1.293468\n",
      "2019-04-10 01:01:20,522 root         INFO     Train Epoch: 5 [4608/8000 (58%)]\tTotal Loss: 1.680243\n",
      "Reconstruction: 0.492978, Regularization: 1.187265\n",
      "2019-04-10 01:01:20,584 root         INFO     Train Epoch: 5 [5120/8000 (64%)]\tTotal Loss: 1.767912\n",
      "Reconstruction: 0.540259, Regularization: 1.227652\n",
      "2019-04-10 01:01:20,647 root         INFO     Train Epoch: 5 [5632/8000 (70%)]\tTotal Loss: 1.692923\n",
      "Reconstruction: 0.511615, Regularization: 1.181308\n",
      "2019-04-10 01:01:20,709 root         INFO     Train Epoch: 5 [6144/8000 (77%)]\tTotal Loss: 1.575253\n",
      "Reconstruction: 0.492866, Regularization: 1.082387\n",
      "2019-04-10 01:01:20,771 root         INFO     Train Epoch: 5 [6656/8000 (83%)]\tTotal Loss: 1.696414\n",
      "Reconstruction: 0.502907, Regularization: 1.193507\n",
      "2019-04-10 01:01:20,833 root         INFO     Train Epoch: 5 [7168/8000 (90%)]\tTotal Loss: 1.636989\n",
      "Reconstruction: 0.517247, Regularization: 1.119742\n",
      "2019-04-10 01:01:20,896 root         INFO     Train Epoch: 5 [7680/8000 (96%)]\tTotal Loss: 1.840420\n",
      "Reconstruction: 0.526426, Regularization: 1.313994\n",
      "2019-04-10 01:01:20,949 root         INFO     ====> Epoch: 5 Average loss: 1.8028\n",
      "2019-04-10 01:01:20,973 root         INFO     Train Epoch: 6 [0/8000 (0%)]\tTotal Loss: 1.600116\n",
      "Reconstruction: 0.458291, Regularization: 1.141826\n",
      "2019-04-10 01:01:21,036 root         INFO     Train Epoch: 6 [512/8000 (6%)]\tTotal Loss: 1.696499\n",
      "Reconstruction: 0.506814, Regularization: 1.189685\n",
      "2019-04-10 01:01:21,098 root         INFO     Train Epoch: 6 [1024/8000 (13%)]\tTotal Loss: 1.788874\n",
      "Reconstruction: 0.514238, Regularization: 1.274635\n",
      "2019-04-10 01:01:21,160 root         INFO     Train Epoch: 6 [1536/8000 (19%)]\tTotal Loss: 1.611800\n",
      "Reconstruction: 0.509035, Regularization: 1.102765\n",
      "2019-04-10 01:01:21,223 root         INFO     Train Epoch: 6 [2048/8000 (26%)]\tTotal Loss: 1.778802\n",
      "Reconstruction: 0.528565, Regularization: 1.250237\n",
      "2019-04-10 01:01:21,286 root         INFO     Train Epoch: 6 [2560/8000 (32%)]\tTotal Loss: 1.586181\n",
      "Reconstruction: 0.468215, Regularization: 1.117966\n",
      "2019-04-10 01:01:21,348 root         INFO     Train Epoch: 6 [3072/8000 (38%)]\tTotal Loss: 1.635080\n",
      "Reconstruction: 0.482514, Regularization: 1.152565\n",
      "2019-04-10 01:01:21,411 root         INFO     Train Epoch: 6 [3584/8000 (45%)]\tTotal Loss: 1.639012\n",
      "Reconstruction: 0.459979, Regularization: 1.179033\n",
      "2019-04-10 01:01:21,474 root         INFO     Train Epoch: 6 [4096/8000 (51%)]\tTotal Loss: 1.540697\n",
      "Reconstruction: 0.457471, Regularization: 1.083225\n",
      "2019-04-10 01:01:21,536 root         INFO     Train Epoch: 6 [4608/8000 (58%)]\tTotal Loss: 1.566013\n",
      "Reconstruction: 0.474497, Regularization: 1.091516\n",
      "2019-04-10 01:01:21,599 root         INFO     Train Epoch: 6 [5120/8000 (64%)]\tTotal Loss: 1.671371\n",
      "Reconstruction: 0.506879, Regularization: 1.164493\n",
      "2019-04-10 01:01:21,662 root         INFO     Train Epoch: 6 [5632/8000 (70%)]\tTotal Loss: 1.571970\n",
      "Reconstruction: 0.456164, Regularization: 1.115806\n",
      "2019-04-10 01:01:21,724 root         INFO     Train Epoch: 6 [6144/8000 (77%)]\tTotal Loss: 1.542897\n",
      "Reconstruction: 0.468690, Regularization: 1.074206\n",
      "2019-04-10 01:01:21,786 root         INFO     Train Epoch: 6 [6656/8000 (83%)]\tTotal Loss: 1.420987\n",
      "Reconstruction: 0.447331, Regularization: 0.973656\n",
      "2019-04-10 01:01:21,848 root         INFO     Train Epoch: 6 [7168/8000 (90%)]\tTotal Loss: 1.612988\n",
      "Reconstruction: 0.480425, Regularization: 1.132563\n",
      "2019-04-10 01:01:21,910 root         INFO     Train Epoch: 6 [7680/8000 (96%)]\tTotal Loss: 1.457762\n",
      "Reconstruction: 0.419363, Regularization: 1.038399\n",
      "2019-04-10 01:01:21,964 root         INFO     ====> Epoch: 6 Average loss: 1.6013\n",
      "2019-04-10 01:01:21,988 root         INFO     Train Epoch: 7 [0/8000 (0%)]\tTotal Loss: 1.462578\n",
      "Reconstruction: 0.465566, Regularization: 0.997012\n",
      "2019-04-10 01:01:22,052 root         INFO     Train Epoch: 7 [512/8000 (6%)]\tTotal Loss: 1.487853\n",
      "Reconstruction: 0.423421, Regularization: 1.064432\n",
      "2019-04-10 01:01:22,115 root         INFO     Train Epoch: 7 [1024/8000 (13%)]\tTotal Loss: 1.534561\n",
      "Reconstruction: 0.457309, Regularization: 1.077252\n",
      "2019-04-10 01:01:22,179 root         INFO     Train Epoch: 7 [1536/8000 (19%)]\tTotal Loss: 1.423517\n",
      "Reconstruction: 0.414401, Regularization: 1.009116\n",
      "2019-04-10 01:01:22,243 root         INFO     Train Epoch: 7 [2048/8000 (26%)]\tTotal Loss: 1.358875\n",
      "Reconstruction: 0.388098, Regularization: 0.970777\n",
      "2019-04-10 01:01:22,307 root         INFO     Train Epoch: 7 [2560/8000 (32%)]\tTotal Loss: 1.519001\n",
      "Reconstruction: 0.472933, Regularization: 1.046068\n",
      "2019-04-10 01:01:22,371 root         INFO     Train Epoch: 7 [3072/8000 (38%)]\tTotal Loss: 1.404087\n",
      "Reconstruction: 0.400501, Regularization: 1.003586\n",
      "2019-04-10 01:01:22,433 root         INFO     Train Epoch: 7 [3584/8000 (45%)]\tTotal Loss: 1.414639\n",
      "Reconstruction: 0.440700, Regularization: 0.973940\n",
      "2019-04-10 01:01:22,496 root         INFO     Train Epoch: 7 [4096/8000 (51%)]\tTotal Loss: 1.363746\n",
      "Reconstruction: 0.480237, Regularization: 0.883508\n",
      "2019-04-10 01:01:22,559 root         INFO     Train Epoch: 7 [4608/8000 (58%)]\tTotal Loss: 1.340477\n",
      "Reconstruction: 0.424058, Regularization: 0.916419\n",
      "2019-04-10 01:01:22,622 root         INFO     Train Epoch: 7 [5120/8000 (64%)]\tTotal Loss: 1.491851\n",
      "Reconstruction: 0.428463, Regularization: 1.063389\n",
      "2019-04-10 01:01:22,685 root         INFO     Train Epoch: 7 [5632/8000 (70%)]\tTotal Loss: 1.344443\n",
      "Reconstruction: 0.389289, Regularization: 0.955154\n",
      "2019-04-10 01:01:22,748 root         INFO     Train Epoch: 7 [6144/8000 (77%)]\tTotal Loss: 1.399423\n",
      "Reconstruction: 0.411240, Regularization: 0.988183\n",
      "2019-04-10 01:01:22,811 root         INFO     Train Epoch: 7 [6656/8000 (83%)]\tTotal Loss: 1.434729\n",
      "Reconstruction: 0.451657, Regularization: 0.983071\n",
      "2019-04-10 01:01:22,874 root         INFO     Train Epoch: 7 [7168/8000 (90%)]\tTotal Loss: 1.377908\n",
      "Reconstruction: 0.423335, Regularization: 0.954573\n",
      "2019-04-10 01:01:22,936 root         INFO     Train Epoch: 7 [7680/8000 (96%)]\tTotal Loss: 1.329967\n",
      "Reconstruction: 0.422410, Regularization: 0.907557\n",
      "2019-04-10 01:01:22,990 root         INFO     ====> Epoch: 7 Average loss: 1.4279\n",
      "2019-04-10 01:01:23,014 root         INFO     Train Epoch: 8 [0/8000 (0%)]\tTotal Loss: 1.390210\n",
      "Reconstruction: 0.426824, Regularization: 0.963386\n",
      "2019-04-10 01:01:23,077 root         INFO     Train Epoch: 8 [512/8000 (6%)]\tTotal Loss: 1.284202\n",
      "Reconstruction: 0.415125, Regularization: 0.869077\n",
      "2019-04-10 01:01:23,140 root         INFO     Train Epoch: 8 [1024/8000 (13%)]\tTotal Loss: 1.473703\n",
      "Reconstruction: 0.455692, Regularization: 1.018012\n",
      "2019-04-10 01:01:23,202 root         INFO     Train Epoch: 8 [1536/8000 (19%)]\tTotal Loss: 1.351769\n",
      "Reconstruction: 0.413499, Regularization: 0.938271\n",
      "2019-04-10 01:01:23,265 root         INFO     Train Epoch: 8 [2048/8000 (26%)]\tTotal Loss: 1.342056\n",
      "Reconstruction: 0.398231, Regularization: 0.943825\n",
      "2019-04-10 01:01:23,327 root         INFO     Train Epoch: 8 [2560/8000 (32%)]\tTotal Loss: 1.362267\n",
      "Reconstruction: 0.416649, Regularization: 0.945618\n",
      "2019-04-10 01:01:23,391 root         INFO     Train Epoch: 8 [3072/8000 (38%)]\tTotal Loss: 1.241802\n",
      "Reconstruction: 0.382589, Regularization: 0.859212\n",
      "2019-04-10 01:01:23,454 root         INFO     Train Epoch: 8 [3584/8000 (45%)]\tTotal Loss: 1.264105\n",
      "Reconstruction: 0.357906, Regularization: 0.906199\n",
      "2019-04-10 01:01:23,516 root         INFO     Train Epoch: 8 [4096/8000 (51%)]\tTotal Loss: 1.233891\n",
      "Reconstruction: 0.378202, Regularization: 0.855689\n",
      "2019-04-10 01:01:23,579 root         INFO     Train Epoch: 8 [4608/8000 (58%)]\tTotal Loss: 1.348638\n",
      "Reconstruction: 0.411751, Regularization: 0.936887\n",
      "2019-04-10 01:01:23,642 root         INFO     Train Epoch: 8 [5120/8000 (64%)]\tTotal Loss: 1.210118\n",
      "Reconstruction: 0.397266, Regularization: 0.812853\n",
      "2019-04-10 01:01:23,705 root         INFO     Train Epoch: 8 [5632/8000 (70%)]\tTotal Loss: 1.237819\n",
      "Reconstruction: 0.392562, Regularization: 0.845257\n",
      "2019-04-10 01:01:23,768 root         INFO     Train Epoch: 8 [6144/8000 (77%)]\tTotal Loss: 1.246199\n",
      "Reconstruction: 0.435637, Regularization: 0.810562\n",
      "2019-04-10 01:01:23,830 root         INFO     Train Epoch: 8 [6656/8000 (83%)]\tTotal Loss: 1.241125\n",
      "Reconstruction: 0.395424, Regularization: 0.845701\n",
      "2019-04-10 01:01:23,893 root         INFO     Train Epoch: 8 [7168/8000 (90%)]\tTotal Loss: 1.208348\n",
      "Reconstruction: 0.401624, Regularization: 0.806724\n",
      "2019-04-10 01:01:23,955 root         INFO     Train Epoch: 8 [7680/8000 (96%)]\tTotal Loss: 1.201014\n",
      "Reconstruction: 0.384724, Regularization: 0.816289\n",
      "2019-04-10 01:01:24,010 root         INFO     ====> Epoch: 8 Average loss: 1.2773\n",
      "2019-04-10 01:01:24,034 root         INFO     Train Epoch: 9 [0/8000 (0%)]\tTotal Loss: 1.253175\n",
      "Reconstruction: 0.374871, Regularization: 0.878304\n",
      "2019-04-10 01:01:24,097 root         INFO     Train Epoch: 9 [512/8000 (6%)]\tTotal Loss: 1.173441\n",
      "Reconstruction: 0.382797, Regularization: 0.790645\n",
      "2019-04-10 01:01:24,161 root         INFO     Train Epoch: 9 [1024/8000 (13%)]\tTotal Loss: 1.215706\n",
      "Reconstruction: 0.393697, Regularization: 0.822009\n",
      "2019-04-10 01:01:24,224 root         INFO     Train Epoch: 9 [1536/8000 (19%)]\tTotal Loss: 1.231090\n",
      "Reconstruction: 0.386680, Regularization: 0.844410\n",
      "2019-04-10 01:01:24,287 root         INFO     Train Epoch: 9 [2048/8000 (26%)]\tTotal Loss: 1.162501\n",
      "Reconstruction: 0.358910, Regularization: 0.803591\n",
      "2019-04-10 01:01:24,350 root         INFO     Train Epoch: 9 [2560/8000 (32%)]\tTotal Loss: 1.171539\n",
      "Reconstruction: 0.366260, Regularization: 0.805279\n",
      "2019-04-10 01:01:24,412 root         INFO     Train Epoch: 9 [3072/8000 (38%)]\tTotal Loss: 1.198310\n",
      "Reconstruction: 0.385332, Regularization: 0.812978\n",
      "2019-04-10 01:01:24,474 root         INFO     Train Epoch: 9 [3584/8000 (45%)]\tTotal Loss: 1.091739\n",
      "Reconstruction: 0.378981, Regularization: 0.712758\n",
      "2019-04-10 01:01:24,537 root         INFO     Train Epoch: 9 [4096/8000 (51%)]\tTotal Loss: 1.107637\n",
      "Reconstruction: 0.381163, Regularization: 0.726474\n",
      "2019-04-10 01:01:24,599 root         INFO     Train Epoch: 9 [4608/8000 (58%)]\tTotal Loss: 1.024752\n",
      "Reconstruction: 0.377354, Regularization: 0.647398\n",
      "2019-04-10 01:01:24,661 root         INFO     Train Epoch: 9 [5120/8000 (64%)]\tTotal Loss: 1.153732\n",
      "Reconstruction: 0.378296, Regularization: 0.775436\n",
      "2019-04-10 01:01:24,723 root         INFO     Train Epoch: 9 [5632/8000 (70%)]\tTotal Loss: 1.155998\n",
      "Reconstruction: 0.365784, Regularization: 0.790214\n",
      "2019-04-10 01:01:24,786 root         INFO     Train Epoch: 9 [6144/8000 (77%)]\tTotal Loss: 1.111475\n",
      "Reconstruction: 0.352466, Regularization: 0.759008\n",
      "2019-04-10 01:01:24,847 root         INFO     Train Epoch: 9 [6656/8000 (83%)]\tTotal Loss: 1.088873\n",
      "Reconstruction: 0.372441, Regularization: 0.716432\n",
      "2019-04-10 01:01:24,909 root         INFO     Train Epoch: 9 [7168/8000 (90%)]\tTotal Loss: 1.075254\n",
      "Reconstruction: 0.336297, Regularization: 0.738957\n",
      "2019-04-10 01:01:24,971 root         INFO     Train Epoch: 9 [7680/8000 (96%)]\tTotal Loss: 1.001275\n",
      "Reconstruction: 0.314167, Regularization: 0.687107\n",
      "2019-04-10 01:01:25,025 root         INFO     ====> Epoch: 9 Average loss: 1.1428\n",
      "2019-04-10 01:01:25,049 root         INFO     Train Epoch: 10 [0/8000 (0%)]\tTotal Loss: 1.193306\n",
      "Reconstruction: 0.396505, Regularization: 0.796801\n",
      "2019-04-10 01:01:25,114 root         INFO     Train Epoch: 10 [512/8000 (6%)]\tTotal Loss: 1.082314\n",
      "Reconstruction: 0.383829, Regularization: 0.698485\n",
      "2019-04-10 01:01:25,179 root         INFO     Train Epoch: 10 [1024/8000 (13%)]\tTotal Loss: 1.018243\n",
      "Reconstruction: 0.334177, Regularization: 0.684065\n",
      "2019-04-10 01:01:25,243 root         INFO     Train Epoch: 10 [1536/8000 (19%)]\tTotal Loss: 1.047440\n",
      "Reconstruction: 0.354980, Regularization: 0.692460\n",
      "2019-04-10 01:01:25,307 root         INFO     Train Epoch: 10 [2048/8000 (26%)]\tTotal Loss: 1.005459\n",
      "Reconstruction: 0.342817, Regularization: 0.662642\n",
      "2019-04-10 01:01:25,371 root         INFO     Train Epoch: 10 [2560/8000 (32%)]\tTotal Loss: 1.087358\n",
      "Reconstruction: 0.387364, Regularization: 0.699994\n",
      "2019-04-10 01:01:25,435 root         INFO     Train Epoch: 10 [3072/8000 (38%)]\tTotal Loss: 1.012845\n",
      "Reconstruction: 0.346551, Regularization: 0.666294\n",
      "2019-04-10 01:01:25,499 root         INFO     Train Epoch: 10 [3584/8000 (45%)]\tTotal Loss: 1.041589\n",
      "Reconstruction: 0.360636, Regularization: 0.680953\n",
      "2019-04-10 01:01:25,563 root         INFO     Train Epoch: 10 [4096/8000 (51%)]\tTotal Loss: 1.034604\n",
      "Reconstruction: 0.344720, Regularization: 0.689884\n",
      "2019-04-10 01:01:25,628 root         INFO     Train Epoch: 10 [4608/8000 (58%)]\tTotal Loss: 0.978929\n",
      "Reconstruction: 0.361295, Regularization: 0.617634\n",
      "2019-04-10 01:01:25,691 root         INFO     Train Epoch: 10 [5120/8000 (64%)]\tTotal Loss: 0.964647\n",
      "Reconstruction: 0.342595, Regularization: 0.622052\n",
      "2019-04-10 01:01:25,755 root         INFO     Train Epoch: 10 [5632/8000 (70%)]\tTotal Loss: 0.948693\n",
      "Reconstruction: 0.316404, Regularization: 0.632288\n",
      "2019-04-10 01:01:25,819 root         INFO     Train Epoch: 10 [6144/8000 (77%)]\tTotal Loss: 1.063827\n",
      "Reconstruction: 0.385688, Regularization: 0.678139\n",
      "2019-04-10 01:01:25,883 root         INFO     Train Epoch: 10 [6656/8000 (83%)]\tTotal Loss: 0.977522\n",
      "Reconstruction: 0.347652, Regularization: 0.629870\n",
      "2019-04-10 01:01:25,947 root         INFO     Train Epoch: 10 [7168/8000 (90%)]\tTotal Loss: 0.905294\n",
      "Reconstruction: 0.331843, Regularization: 0.573452\n",
      "2019-04-10 01:01:26,011 root         INFO     Train Epoch: 10 [7680/8000 (96%)]\tTotal Loss: 0.984313\n",
      "Reconstruction: 0.356294, Regularization: 0.628019\n",
      "2019-04-10 01:01:26,066 root         INFO     ====> Epoch: 10 Average loss: 1.0246\n",
      "2019-04-10 01:01:26,090 root         INFO     Train Epoch: 11 [0/8000 (0%)]\tTotal Loss: 0.953933\n",
      "Reconstruction: 0.356507, Regularization: 0.597427\n",
      "2019-04-10 01:01:26,154 root         INFO     Train Epoch: 11 [512/8000 (6%)]\tTotal Loss: 0.884042\n",
      "Reconstruction: 0.325655, Regularization: 0.558387\n",
      "2019-04-10 01:01:26,219 root         INFO     Train Epoch: 11 [1024/8000 (13%)]\tTotal Loss: 0.949521\n",
      "Reconstruction: 0.357082, Regularization: 0.592438\n",
      "2019-04-10 01:01:26,283 root         INFO     Train Epoch: 11 [1536/8000 (19%)]\tTotal Loss: 0.903901\n",
      "Reconstruction: 0.329432, Regularization: 0.574468\n",
      "2019-04-10 01:01:26,347 root         INFO     Train Epoch: 11 [2048/8000 (26%)]\tTotal Loss: 0.970224\n",
      "Reconstruction: 0.362396, Regularization: 0.607829\n",
      "2019-04-10 01:01:26,412 root         INFO     Train Epoch: 11 [2560/8000 (32%)]\tTotal Loss: 0.901441\n",
      "Reconstruction: 0.334455, Regularization: 0.566986\n",
      "2019-04-10 01:01:26,476 root         INFO     Train Epoch: 11 [3072/8000 (38%)]\tTotal Loss: 0.900118\n",
      "Reconstruction: 0.347469, Regularization: 0.552648\n",
      "2019-04-10 01:01:26,539 root         INFO     Train Epoch: 11 [3584/8000 (45%)]\tTotal Loss: 0.925671\n",
      "Reconstruction: 0.347361, Regularization: 0.578310\n",
      "2019-04-10 01:01:26,604 root         INFO     Train Epoch: 11 [4096/8000 (51%)]\tTotal Loss: 0.949275\n",
      "Reconstruction: 0.343785, Regularization: 0.605490\n",
      "2019-04-10 01:01:26,667 root         INFO     Train Epoch: 11 [4608/8000 (58%)]\tTotal Loss: 0.835256\n",
      "Reconstruction: 0.343310, Regularization: 0.491947\n",
      "2019-04-10 01:01:26,731 root         INFO     Train Epoch: 11 [5120/8000 (64%)]\tTotal Loss: 0.970853\n",
      "Reconstruction: 0.342213, Regularization: 0.628640\n",
      "2019-04-10 01:01:26,795 root         INFO     Train Epoch: 11 [5632/8000 (70%)]\tTotal Loss: 0.815772\n",
      "Reconstruction: 0.309439, Regularization: 0.506334\n",
      "2019-04-10 01:01:26,859 root         INFO     Train Epoch: 11 [6144/8000 (77%)]\tTotal Loss: 0.943139\n",
      "Reconstruction: 0.348140, Regularization: 0.594999\n",
      "2019-04-10 01:01:26,923 root         INFO     Train Epoch: 11 [6656/8000 (83%)]\tTotal Loss: 0.870215\n",
      "Reconstruction: 0.347480, Regularization: 0.522735\n",
      "2019-04-10 01:01:26,987 root         INFO     Train Epoch: 11 [7168/8000 (90%)]\tTotal Loss: 0.854424\n",
      "Reconstruction: 0.335960, Regularization: 0.518464\n",
      "2019-04-10 01:01:27,053 root         INFO     Train Epoch: 11 [7680/8000 (96%)]\tTotal Loss: 0.931558\n",
      "Reconstruction: 0.348217, Regularization: 0.583341\n",
      "2019-04-10 01:01:27,109 root         INFO     ====> Epoch: 11 Average loss: 0.9191\n",
      "2019-04-10 01:01:27,133 root         INFO     Train Epoch: 12 [0/8000 (0%)]\tTotal Loss: 0.855540\n",
      "Reconstruction: 0.324071, Regularization: 0.531468\n",
      "2019-04-10 01:01:27,196 root         INFO     Train Epoch: 12 [512/8000 (6%)]\tTotal Loss: 0.830416\n",
      "Reconstruction: 0.337765, Regularization: 0.492651\n",
      "2019-04-10 01:01:27,260 root         INFO     Train Epoch: 12 [1024/8000 (13%)]\tTotal Loss: 0.832666\n",
      "Reconstruction: 0.334676, Regularization: 0.497989\n",
      "2019-04-10 01:01:27,323 root         INFO     Train Epoch: 12 [1536/8000 (19%)]\tTotal Loss: 0.812345\n",
      "Reconstruction: 0.323432, Regularization: 0.488913\n",
      "2019-04-10 01:01:27,387 root         INFO     Train Epoch: 12 [2048/8000 (26%)]\tTotal Loss: 0.839184\n",
      "Reconstruction: 0.325348, Regularization: 0.513836\n",
      "2019-04-10 01:01:27,451 root         INFO     Train Epoch: 12 [2560/8000 (32%)]\tTotal Loss: 0.886643\n",
      "Reconstruction: 0.318255, Regularization: 0.568388\n",
      "2019-04-10 01:01:27,515 root         INFO     Train Epoch: 12 [3072/8000 (38%)]\tTotal Loss: 0.871581\n",
      "Reconstruction: 0.323704, Regularization: 0.547877\n",
      "2019-04-10 01:01:27,579 root         INFO     Train Epoch: 12 [3584/8000 (45%)]\tTotal Loss: 0.809503\n",
      "Reconstruction: 0.309660, Regularization: 0.499843\n",
      "2019-04-10 01:01:27,642 root         INFO     Train Epoch: 12 [4096/8000 (51%)]\tTotal Loss: 0.813009\n",
      "Reconstruction: 0.343569, Regularization: 0.469440\n",
      "2019-04-10 01:01:27,705 root         INFO     Train Epoch: 12 [4608/8000 (58%)]\tTotal Loss: 0.831166\n",
      "Reconstruction: 0.328733, Regularization: 0.502433\n",
      "2019-04-10 01:01:27,769 root         INFO     Train Epoch: 12 [5120/8000 (64%)]\tTotal Loss: 0.823875\n",
      "Reconstruction: 0.336847, Regularization: 0.487028\n",
      "2019-04-10 01:01:27,832 root         INFO     Train Epoch: 12 [5632/8000 (70%)]\tTotal Loss: 0.799680\n",
      "Reconstruction: 0.304179, Regularization: 0.495501\n",
      "2019-04-10 01:01:27,895 root         INFO     Train Epoch: 12 [6144/8000 (77%)]\tTotal Loss: 0.747780\n",
      "Reconstruction: 0.323358, Regularization: 0.424422\n",
      "2019-04-10 01:01:27,958 root         INFO     Train Epoch: 12 [6656/8000 (83%)]\tTotal Loss: 0.756641\n",
      "Reconstruction: 0.322772, Regularization: 0.433869\n",
      "2019-04-10 01:01:28,022 root         INFO     Train Epoch: 12 [7168/8000 (90%)]\tTotal Loss: 0.757275\n",
      "Reconstruction: 0.323442, Regularization: 0.433833\n",
      "2019-04-10 01:01:28,085 root         INFO     Train Epoch: 12 [7680/8000 (96%)]\tTotal Loss: 0.777470\n",
      "Reconstruction: 0.321634, Regularization: 0.455836\n",
      "2019-04-10 01:01:28,139 root         INFO     ====> Epoch: 12 Average loss: 0.8283\n",
      "2019-04-10 01:01:28,162 root         INFO     Train Epoch: 13 [0/8000 (0%)]\tTotal Loss: 0.778964\n",
      "Reconstruction: 0.335823, Regularization: 0.443141\n",
      "2019-04-10 01:01:28,226 root         INFO     Train Epoch: 13 [512/8000 (6%)]\tTotal Loss: 0.773460\n",
      "Reconstruction: 0.316567, Regularization: 0.456893\n",
      "2019-04-10 01:01:28,290 root         INFO     Train Epoch: 13 [1024/8000 (13%)]\tTotal Loss: 0.758471\n",
      "Reconstruction: 0.315427, Regularization: 0.443044\n",
      "2019-04-10 01:01:28,353 root         INFO     Train Epoch: 13 [1536/8000 (19%)]\tTotal Loss: 0.766109\n",
      "Reconstruction: 0.321076, Regularization: 0.445033\n",
      "2019-04-10 01:01:28,417 root         INFO     Train Epoch: 13 [2048/8000 (26%)]\tTotal Loss: 0.796266\n",
      "Reconstruction: 0.333214, Regularization: 0.463052\n",
      "2019-04-10 01:01:28,481 root         INFO     Train Epoch: 13 [2560/8000 (32%)]\tTotal Loss: 0.778770\n",
      "Reconstruction: 0.337321, Regularization: 0.441449\n",
      "2019-04-10 01:01:28,545 root         INFO     Train Epoch: 13 [3072/8000 (38%)]\tTotal Loss: 0.768412\n",
      "Reconstruction: 0.344734, Regularization: 0.423678\n",
      "2019-04-10 01:01:28,609 root         INFO     Train Epoch: 13 [3584/8000 (45%)]\tTotal Loss: 0.728568\n",
      "Reconstruction: 0.322433, Regularization: 0.406135\n",
      "2019-04-10 01:01:28,672 root         INFO     Train Epoch: 13 [4096/8000 (51%)]\tTotal Loss: 0.749715\n",
      "Reconstruction: 0.322460, Regularization: 0.427255\n",
      "2019-04-10 01:01:28,735 root         INFO     Train Epoch: 13 [4608/8000 (58%)]\tTotal Loss: 0.741719\n",
      "Reconstruction: 0.317185, Regularization: 0.424534\n",
      "2019-04-10 01:01:28,798 root         INFO     Train Epoch: 13 [5120/8000 (64%)]\tTotal Loss: 0.738126\n",
      "Reconstruction: 0.325393, Regularization: 0.412733\n",
      "2019-04-10 01:01:28,860 root         INFO     Train Epoch: 13 [5632/8000 (70%)]\tTotal Loss: 0.719985\n",
      "Reconstruction: 0.312883, Regularization: 0.407103\n",
      "2019-04-10 01:01:28,922 root         INFO     Train Epoch: 13 [6144/8000 (77%)]\tTotal Loss: 0.733047\n",
      "Reconstruction: 0.314724, Regularization: 0.418322\n",
      "2019-04-10 01:01:28,984 root         INFO     Train Epoch: 13 [6656/8000 (83%)]\tTotal Loss: 0.708386\n",
      "Reconstruction: 0.310024, Regularization: 0.398361\n",
      "2019-04-10 01:01:29,045 root         INFO     Train Epoch: 13 [7168/8000 (90%)]\tTotal Loss: 0.714265\n",
      "Reconstruction: 0.325706, Regularization: 0.388559\n",
      "2019-04-10 01:01:29,110 root         INFO     Train Epoch: 13 [7680/8000 (96%)]\tTotal Loss: 0.723016\n",
      "Reconstruction: 0.329076, Regularization: 0.393940\n",
      "2019-04-10 01:01:29,162 root         INFO     ====> Epoch: 13 Average loss: 0.7467\n",
      "2019-04-10 01:01:29,186 root         INFO     Train Epoch: 14 [0/8000 (0%)]\tTotal Loss: 0.690325\n",
      "Reconstruction: 0.307916, Regularization: 0.382408\n",
      "2019-04-10 01:01:29,250 root         INFO     Train Epoch: 14 [512/8000 (6%)]\tTotal Loss: 0.669669\n",
      "Reconstruction: 0.302144, Regularization: 0.367525\n",
      "2019-04-10 01:01:29,314 root         INFO     Train Epoch: 14 [1024/8000 (13%)]\tTotal Loss: 0.659161\n",
      "Reconstruction: 0.299027, Regularization: 0.360134\n",
      "2019-04-10 01:01:29,377 root         INFO     Train Epoch: 14 [1536/8000 (19%)]\tTotal Loss: 0.646772\n",
      "Reconstruction: 0.309194, Regularization: 0.337578\n",
      "2019-04-10 01:01:29,441 root         INFO     Train Epoch: 14 [2048/8000 (26%)]\tTotal Loss: 0.687428\n",
      "Reconstruction: 0.315805, Regularization: 0.371623\n",
      "2019-04-10 01:01:29,505 root         INFO     Train Epoch: 14 [2560/8000 (32%)]\tTotal Loss: 0.677699\n",
      "Reconstruction: 0.331456, Regularization: 0.346243\n",
      "2019-04-10 01:01:29,569 root         INFO     Train Epoch: 14 [3072/8000 (38%)]\tTotal Loss: 0.692221\n",
      "Reconstruction: 0.324820, Regularization: 0.367400\n",
      "2019-04-10 01:01:29,632 root         INFO     Train Epoch: 14 [3584/8000 (45%)]\tTotal Loss: 0.690241\n",
      "Reconstruction: 0.323864, Regularization: 0.366377\n",
      "2019-04-10 01:01:29,694 root         INFO     Train Epoch: 14 [4096/8000 (51%)]\tTotal Loss: 0.684658\n",
      "Reconstruction: 0.322217, Regularization: 0.362441\n",
      "2019-04-10 01:01:29,757 root         INFO     Train Epoch: 14 [4608/8000 (58%)]\tTotal Loss: 0.639784\n",
      "Reconstruction: 0.303461, Regularization: 0.336323\n",
      "2019-04-10 01:01:29,819 root         INFO     Train Epoch: 14 [5120/8000 (64%)]\tTotal Loss: 0.707595\n",
      "Reconstruction: 0.327057, Regularization: 0.380538\n",
      "2019-04-10 01:01:29,881 root         INFO     Train Epoch: 14 [5632/8000 (70%)]\tTotal Loss: 0.659049\n",
      "Reconstruction: 0.293571, Regularization: 0.365479\n",
      "2019-04-10 01:01:29,944 root         INFO     Train Epoch: 14 [6144/8000 (77%)]\tTotal Loss: 0.692757\n",
      "Reconstruction: 0.330038, Regularization: 0.362718\n",
      "2019-04-10 01:01:30,007 root         INFO     Train Epoch: 14 [6656/8000 (83%)]\tTotal Loss: 0.668424\n",
      "Reconstruction: 0.300936, Regularization: 0.367487\n",
      "2019-04-10 01:01:30,069 root         INFO     Train Epoch: 14 [7168/8000 (90%)]\tTotal Loss: 0.628401\n",
      "Reconstruction: 0.302614, Regularization: 0.325787\n",
      "2019-04-10 01:01:30,131 root         INFO     Train Epoch: 14 [7680/8000 (96%)]\tTotal Loss: 0.621474\n",
      "Reconstruction: 0.306688, Regularization: 0.314785\n",
      "2019-04-10 01:01:30,185 root         INFO     ====> Epoch: 14 Average loss: 0.6776\n",
      "2019-04-10 01:01:30,209 root         INFO     Train Epoch: 15 [0/8000 (0%)]\tTotal Loss: 0.693713\n",
      "Reconstruction: 0.325783, Regularization: 0.367930\n",
      "2019-04-10 01:01:30,273 root         INFO     Train Epoch: 15 [512/8000 (6%)]\tTotal Loss: 0.642200\n",
      "Reconstruction: 0.329704, Regularization: 0.312496\n",
      "2019-04-10 01:01:30,336 root         INFO     Train Epoch: 15 [1024/8000 (13%)]\tTotal Loss: 0.673455\n",
      "Reconstruction: 0.317585, Regularization: 0.355870\n",
      "2019-04-10 01:01:30,400 root         INFO     Train Epoch: 15 [1536/8000 (19%)]\tTotal Loss: 0.614568\n",
      "Reconstruction: 0.288842, Regularization: 0.325726\n",
      "2019-04-10 01:01:30,465 root         INFO     Train Epoch: 15 [2048/8000 (26%)]\tTotal Loss: 0.636739\n",
      "Reconstruction: 0.321038, Regularization: 0.315700\n",
      "2019-04-10 01:01:30,528 root         INFO     Train Epoch: 15 [2560/8000 (32%)]\tTotal Loss: 0.567084\n",
      "Reconstruction: 0.305739, Regularization: 0.261346\n",
      "2019-04-10 01:01:30,592 root         INFO     Train Epoch: 15 [3072/8000 (38%)]\tTotal Loss: 0.578489\n",
      "Reconstruction: 0.287320, Regularization: 0.291169\n",
      "2019-04-10 01:01:30,656 root         INFO     Train Epoch: 15 [3584/8000 (45%)]\tTotal Loss: 0.565775\n",
      "Reconstruction: 0.289002, Regularization: 0.276773\n",
      "2019-04-10 01:01:30,719 root         INFO     Train Epoch: 15 [4096/8000 (51%)]\tTotal Loss: 0.615201\n",
      "Reconstruction: 0.313257, Regularization: 0.301944\n",
      "2019-04-10 01:01:30,782 root         INFO     Train Epoch: 15 [4608/8000 (58%)]\tTotal Loss: 0.610595\n",
      "Reconstruction: 0.320228, Regularization: 0.290367\n",
      "2019-04-10 01:01:30,847 root         INFO     Train Epoch: 15 [5120/8000 (64%)]\tTotal Loss: 0.636445\n",
      "Reconstruction: 0.333807, Regularization: 0.302638\n",
      "2019-04-10 01:01:30,911 root         INFO     Train Epoch: 15 [5632/8000 (70%)]\tTotal Loss: 0.569710\n",
      "Reconstruction: 0.304269, Regularization: 0.265441\n",
      "2019-04-10 01:01:30,975 root         INFO     Train Epoch: 15 [6144/8000 (77%)]\tTotal Loss: 0.599280\n",
      "Reconstruction: 0.319818, Regularization: 0.279463\n",
      "2019-04-10 01:01:31,039 root         INFO     Train Epoch: 15 [6656/8000 (83%)]\tTotal Loss: 0.625932\n",
      "Reconstruction: 0.305143, Regularization: 0.320788\n",
      "2019-04-10 01:01:31,103 root         INFO     Train Epoch: 15 [7168/8000 (90%)]\tTotal Loss: 0.580597\n",
      "Reconstruction: 0.306815, Regularization: 0.273782\n",
      "2019-04-10 01:01:31,167 root         INFO     Train Epoch: 15 [7680/8000 (96%)]\tTotal Loss: 0.597014\n",
      "Reconstruction: 0.295999, Regularization: 0.301015\n",
      "2019-04-10 01:01:31,221 root         INFO     ====> Epoch: 15 Average loss: 0.6167\n",
      "2019-04-10 01:01:31,245 root         INFO     Train Epoch: 16 [0/8000 (0%)]\tTotal Loss: 0.587154\n",
      "Reconstruction: 0.303801, Regularization: 0.283353\n",
      "2019-04-10 01:01:31,308 root         INFO     Train Epoch: 16 [512/8000 (6%)]\tTotal Loss: 0.583435\n",
      "Reconstruction: 0.309557, Regularization: 0.273878\n",
      "2019-04-10 01:01:31,371 root         INFO     Train Epoch: 16 [1024/8000 (13%)]\tTotal Loss: 0.608624\n",
      "Reconstruction: 0.337843, Regularization: 0.270781\n",
      "2019-04-10 01:01:31,433 root         INFO     Train Epoch: 16 [1536/8000 (19%)]\tTotal Loss: 0.588761\n",
      "Reconstruction: 0.302482, Regularization: 0.286279\n",
      "2019-04-10 01:01:31,496 root         INFO     Train Epoch: 16 [2048/8000 (26%)]\tTotal Loss: 0.588637\n",
      "Reconstruction: 0.311875, Regularization: 0.276762\n",
      "2019-04-10 01:01:31,558 root         INFO     Train Epoch: 16 [2560/8000 (32%)]\tTotal Loss: 0.600152\n",
      "Reconstruction: 0.316231, Regularization: 0.283921\n",
      "2019-04-10 01:01:31,620 root         INFO     Train Epoch: 16 [3072/8000 (38%)]\tTotal Loss: 0.607760\n",
      "Reconstruction: 0.316701, Regularization: 0.291059\n",
      "2019-04-10 01:01:31,683 root         INFO     Train Epoch: 16 [3584/8000 (45%)]\tTotal Loss: 0.582758\n",
      "Reconstruction: 0.295700, Regularization: 0.287058\n",
      "2019-04-10 01:01:31,744 root         INFO     Train Epoch: 16 [4096/8000 (51%)]\tTotal Loss: 0.609291\n",
      "Reconstruction: 0.332149, Regularization: 0.277142\n",
      "2019-04-10 01:01:31,806 root         INFO     Train Epoch: 16 [4608/8000 (58%)]\tTotal Loss: 0.516646\n",
      "Reconstruction: 0.294636, Regularization: 0.222010\n",
      "2019-04-10 01:01:31,869 root         INFO     Train Epoch: 16 [5120/8000 (64%)]\tTotal Loss: 0.554759\n",
      "Reconstruction: 0.301500, Regularization: 0.253259\n",
      "2019-04-10 01:01:31,931 root         INFO     Train Epoch: 16 [5632/8000 (70%)]\tTotal Loss: 0.555393\n",
      "Reconstruction: 0.316765, Regularization: 0.238628\n",
      "2019-04-10 01:01:31,993 root         INFO     Train Epoch: 16 [6144/8000 (77%)]\tTotal Loss: 0.563208\n",
      "Reconstruction: 0.307012, Regularization: 0.256196\n",
      "2019-04-10 01:01:32,055 root         INFO     Train Epoch: 16 [6656/8000 (83%)]\tTotal Loss: 0.573109\n",
      "Reconstruction: 0.334074, Regularization: 0.239035\n",
      "2019-04-10 01:01:32,117 root         INFO     Train Epoch: 16 [7168/8000 (90%)]\tTotal Loss: 0.542575\n",
      "Reconstruction: 0.318426, Regularization: 0.224149\n",
      "2019-04-10 01:01:32,180 root         INFO     Train Epoch: 16 [7680/8000 (96%)]\tTotal Loss: 0.553036\n",
      "Reconstruction: 0.285517, Regularization: 0.267520\n",
      "2019-04-10 01:01:32,235 root         INFO     ====> Epoch: 16 Average loss: 0.5665\n",
      "2019-04-10 01:01:32,259 root         INFO     Train Epoch: 17 [0/8000 (0%)]\tTotal Loss: 0.522262\n",
      "Reconstruction: 0.289945, Regularization: 0.232317\n",
      "2019-04-10 01:01:32,323 root         INFO     Train Epoch: 17 [512/8000 (6%)]\tTotal Loss: 0.515174\n",
      "Reconstruction: 0.294651, Regularization: 0.220524\n",
      "2019-04-10 01:01:32,386 root         INFO     Train Epoch: 17 [1024/8000 (13%)]\tTotal Loss: 0.523396\n",
      "Reconstruction: 0.298892, Regularization: 0.224504\n",
      "2019-04-10 01:01:32,449 root         INFO     Train Epoch: 17 [1536/8000 (19%)]\tTotal Loss: 0.529022\n",
      "Reconstruction: 0.294564, Regularization: 0.234458\n",
      "2019-04-10 01:01:32,512 root         INFO     Train Epoch: 17 [2048/8000 (26%)]\tTotal Loss: 0.489679\n",
      "Reconstruction: 0.263743, Regularization: 0.225936\n",
      "2019-04-10 01:01:32,575 root         INFO     Train Epoch: 17 [2560/8000 (32%)]\tTotal Loss: 0.529035\n",
      "Reconstruction: 0.270463, Regularization: 0.258572\n",
      "2019-04-10 01:01:32,638 root         INFO     Train Epoch: 17 [3072/8000 (38%)]\tTotal Loss: 0.529302\n",
      "Reconstruction: 0.301435, Regularization: 0.227867\n",
      "2019-04-10 01:01:32,700 root         INFO     Train Epoch: 17 [3584/8000 (45%)]\tTotal Loss: 0.548150\n",
      "Reconstruction: 0.320013, Regularization: 0.228137\n",
      "2019-04-10 01:01:32,763 root         INFO     Train Epoch: 17 [4096/8000 (51%)]\tTotal Loss: 0.513510\n",
      "Reconstruction: 0.302178, Regularization: 0.211332\n",
      "2019-04-10 01:01:32,825 root         INFO     Train Epoch: 17 [4608/8000 (58%)]\tTotal Loss: 0.519948\n",
      "Reconstruction: 0.293029, Regularization: 0.226919\n",
      "2019-04-10 01:01:32,887 root         INFO     Train Epoch: 17 [5120/8000 (64%)]\tTotal Loss: 0.527106\n",
      "Reconstruction: 0.284474, Regularization: 0.242631\n",
      "2019-04-10 01:01:32,949 root         INFO     Train Epoch: 17 [5632/8000 (70%)]\tTotal Loss: 0.510082\n",
      "Reconstruction: 0.298076, Regularization: 0.212005\n",
      "2019-04-10 01:01:33,011 root         INFO     Train Epoch: 17 [6144/8000 (77%)]\tTotal Loss: 0.484995\n",
      "Reconstruction: 0.279780, Regularization: 0.205215\n",
      "2019-04-10 01:01:33,073 root         INFO     Train Epoch: 17 [6656/8000 (83%)]\tTotal Loss: 0.485613\n",
      "Reconstruction: 0.275009, Regularization: 0.210604\n",
      "2019-04-10 01:01:33,135 root         INFO     Train Epoch: 17 [7168/8000 (90%)]\tTotal Loss: 0.505472\n",
      "Reconstruction: 0.300168, Regularization: 0.205304\n",
      "2019-04-10 01:01:33,197 root         INFO     Train Epoch: 17 [7680/8000 (96%)]\tTotal Loss: 0.496544\n",
      "Reconstruction: 0.297264, Regularization: 0.199279\n",
      "2019-04-10 01:01:33,250 root         INFO     ====> Epoch: 17 Average loss: 0.5220\n",
      "2019-04-10 01:01:33,275 root         INFO     Train Epoch: 18 [0/8000 (0%)]\tTotal Loss: 0.494140\n",
      "Reconstruction: 0.293618, Regularization: 0.200522\n",
      "2019-04-10 01:01:33,338 root         INFO     Train Epoch: 18 [512/8000 (6%)]\tTotal Loss: 0.493674\n",
      "Reconstruction: 0.282045, Regularization: 0.211629\n",
      "2019-04-10 01:01:33,401 root         INFO     Train Epoch: 18 [1024/8000 (13%)]\tTotal Loss: 0.525905\n",
      "Reconstruction: 0.329618, Regularization: 0.196287\n",
      "2019-04-10 01:01:33,464 root         INFO     Train Epoch: 18 [1536/8000 (19%)]\tTotal Loss: 0.501097\n",
      "Reconstruction: 0.295428, Regularization: 0.205669\n",
      "2019-04-10 01:01:33,527 root         INFO     Train Epoch: 18 [2048/8000 (26%)]\tTotal Loss: 0.480932\n",
      "Reconstruction: 0.284466, Regularization: 0.196466\n",
      "2019-04-10 01:01:33,590 root         INFO     Train Epoch: 18 [2560/8000 (32%)]\tTotal Loss: 0.486963\n",
      "Reconstruction: 0.284468, Regularization: 0.202494\n",
      "2019-04-10 01:01:33,653 root         INFO     Train Epoch: 18 [3072/8000 (38%)]\tTotal Loss: 0.485638\n",
      "Reconstruction: 0.290137, Regularization: 0.195501\n",
      "2019-04-10 01:01:33,715 root         INFO     Train Epoch: 18 [3584/8000 (45%)]\tTotal Loss: 0.472692\n",
      "Reconstruction: 0.291386, Regularization: 0.181305\n",
      "2019-04-10 01:01:33,777 root         INFO     Train Epoch: 18 [4096/8000 (51%)]\tTotal Loss: 0.489846\n",
      "Reconstruction: 0.302246, Regularization: 0.187600\n",
      "2019-04-10 01:01:33,839 root         INFO     Train Epoch: 18 [4608/8000 (58%)]\tTotal Loss: 0.487990\n",
      "Reconstruction: 0.312214, Regularization: 0.175775\n",
      "2019-04-10 01:01:33,901 root         INFO     Train Epoch: 18 [5120/8000 (64%)]\tTotal Loss: 0.485102\n",
      "Reconstruction: 0.301510, Regularization: 0.183592\n",
      "2019-04-10 01:01:33,963 root         INFO     Train Epoch: 18 [5632/8000 (70%)]\tTotal Loss: 0.491839\n",
      "Reconstruction: 0.292485, Regularization: 0.199353\n",
      "2019-04-10 01:01:34,025 root         INFO     Train Epoch: 18 [6144/8000 (77%)]\tTotal Loss: 0.473141\n",
      "Reconstruction: 0.280696, Regularization: 0.192445\n",
      "2019-04-10 01:01:34,087 root         INFO     Train Epoch: 18 [6656/8000 (83%)]\tTotal Loss: 0.468744\n",
      "Reconstruction: 0.289777, Regularization: 0.178966\n",
      "2019-04-10 01:01:34,149 root         INFO     Train Epoch: 18 [7168/8000 (90%)]\tTotal Loss: 0.478368\n",
      "Reconstruction: 0.308381, Regularization: 0.169987\n",
      "2019-04-10 01:01:34,211 root         INFO     Train Epoch: 18 [7680/8000 (96%)]\tTotal Loss: 0.462800\n",
      "Reconstruction: 0.297467, Regularization: 0.165333\n",
      "2019-04-10 01:01:34,264 root         INFO     ====> Epoch: 18 Average loss: 0.4846\n",
      "2019-04-10 01:01:34,288 root         INFO     Train Epoch: 19 [0/8000 (0%)]\tTotal Loss: 0.468660\n",
      "Reconstruction: 0.301230, Regularization: 0.167430\n",
      "2019-04-10 01:01:34,352 root         INFO     Train Epoch: 19 [512/8000 (6%)]\tTotal Loss: 0.454908\n",
      "Reconstruction: 0.260487, Regularization: 0.194421\n",
      "2019-04-10 01:01:34,415 root         INFO     Train Epoch: 19 [1024/8000 (13%)]\tTotal Loss: 0.468884\n",
      "Reconstruction: 0.279361, Regularization: 0.189523\n",
      "2019-04-10 01:01:34,478 root         INFO     Train Epoch: 19 [1536/8000 (19%)]\tTotal Loss: 0.438928\n",
      "Reconstruction: 0.285109, Regularization: 0.153819\n",
      "2019-04-10 01:01:34,541 root         INFO     Train Epoch: 19 [2048/8000 (26%)]\tTotal Loss: 0.462491\n",
      "Reconstruction: 0.303093, Regularization: 0.159398\n",
      "2019-04-10 01:01:34,605 root         INFO     Train Epoch: 19 [2560/8000 (32%)]\tTotal Loss: 0.448891\n",
      "Reconstruction: 0.269273, Regularization: 0.179618\n",
      "2019-04-10 01:01:34,668 root         INFO     Train Epoch: 19 [3072/8000 (38%)]\tTotal Loss: 0.442092\n",
      "Reconstruction: 0.283084, Regularization: 0.159008\n",
      "2019-04-10 01:01:34,731 root         INFO     Train Epoch: 19 [3584/8000 (45%)]\tTotal Loss: 0.475105\n",
      "Reconstruction: 0.295820, Regularization: 0.179285\n",
      "2019-04-10 01:01:34,795 root         INFO     Train Epoch: 19 [4096/8000 (51%)]\tTotal Loss: 0.432127\n",
      "Reconstruction: 0.278943, Regularization: 0.153184\n",
      "2019-04-10 01:01:34,858 root         INFO     Train Epoch: 19 [4608/8000 (58%)]\tTotal Loss: 0.439260\n",
      "Reconstruction: 0.282938, Regularization: 0.156322\n",
      "2019-04-10 01:01:34,921 root         INFO     Train Epoch: 19 [5120/8000 (64%)]\tTotal Loss: 0.472469\n",
      "Reconstruction: 0.282143, Regularization: 0.190326\n",
      "2019-04-10 01:01:34,984 root         INFO     Train Epoch: 19 [5632/8000 (70%)]\tTotal Loss: 0.453805\n",
      "Reconstruction: 0.292069, Regularization: 0.161736\n",
      "2019-04-10 01:01:35,048 root         INFO     Train Epoch: 19 [6144/8000 (77%)]\tTotal Loss: 0.445642\n",
      "Reconstruction: 0.296012, Regularization: 0.149630\n",
      "2019-04-10 01:01:35,111 root         INFO     Train Epoch: 19 [6656/8000 (83%)]\tTotal Loss: 0.439079\n",
      "Reconstruction: 0.284470, Regularization: 0.154609\n",
      "2019-04-10 01:01:35,173 root         INFO     Train Epoch: 19 [7168/8000 (90%)]\tTotal Loss: 0.445098\n",
      "Reconstruction: 0.297915, Regularization: 0.147183\n",
      "2019-04-10 01:01:35,236 root         INFO     Train Epoch: 19 [7680/8000 (96%)]\tTotal Loss: 0.426863\n",
      "Reconstruction: 0.301258, Regularization: 0.125605\n",
      "2019-04-10 01:01:35,290 root         INFO     ====> Epoch: 19 Average loss: 0.4525\n",
      "2019-04-10 01:01:35,314 root         INFO     Train Epoch: 20 [0/8000 (0%)]\tTotal Loss: 0.447174\n",
      "Reconstruction: 0.284010, Regularization: 0.163164\n",
      "2019-04-10 01:01:35,377 root         INFO     Train Epoch: 20 [512/8000 (6%)]\tTotal Loss: 0.447679\n",
      "Reconstruction: 0.306282, Regularization: 0.141396\n",
      "2019-04-10 01:01:35,440 root         INFO     Train Epoch: 20 [1024/8000 (13%)]\tTotal Loss: 0.461206\n",
      "Reconstruction: 0.317880, Regularization: 0.143325\n",
      "2019-04-10 01:01:35,504 root         INFO     Train Epoch: 20 [1536/8000 (19%)]\tTotal Loss: 0.456031\n",
      "Reconstruction: 0.291700, Regularization: 0.164331\n",
      "2019-04-10 01:01:35,567 root         INFO     Train Epoch: 20 [2048/8000 (26%)]\tTotal Loss: 0.402170\n",
      "Reconstruction: 0.276818, Regularization: 0.125352\n",
      "2019-04-10 01:01:35,630 root         INFO     Train Epoch: 20 [2560/8000 (32%)]\tTotal Loss: 0.442239\n",
      "Reconstruction: 0.317269, Regularization: 0.124970\n",
      "2019-04-10 01:01:35,693 root         INFO     Train Epoch: 20 [3072/8000 (38%)]\tTotal Loss: 0.429103\n",
      "Reconstruction: 0.277484, Regularization: 0.151620\n",
      "2019-04-10 01:01:35,757 root         INFO     Train Epoch: 20 [3584/8000 (45%)]\tTotal Loss: 0.439030\n",
      "Reconstruction: 0.290385, Regularization: 0.148645\n",
      "2019-04-10 01:01:35,820 root         INFO     Train Epoch: 20 [4096/8000 (51%)]\tTotal Loss: 0.445782\n",
      "Reconstruction: 0.289105, Regularization: 0.156677\n",
      "2019-04-10 01:01:35,883 root         INFO     Train Epoch: 20 [4608/8000 (58%)]\tTotal Loss: 0.434036\n",
      "Reconstruction: 0.294596, Regularization: 0.139440\n",
      "2019-04-10 01:01:35,946 root         INFO     Train Epoch: 20 [5120/8000 (64%)]\tTotal Loss: 0.417046\n",
      "Reconstruction: 0.289060, Regularization: 0.127986\n",
      "2019-04-10 01:01:36,009 root         INFO     Train Epoch: 20 [5632/8000 (70%)]\tTotal Loss: 0.443902\n",
      "Reconstruction: 0.328312, Regularization: 0.115590\n",
      "2019-04-10 01:01:36,072 root         INFO     Train Epoch: 20 [6144/8000 (77%)]\tTotal Loss: 0.410145\n",
      "Reconstruction: 0.282239, Regularization: 0.127907\n",
      "2019-04-10 01:01:36,136 root         INFO     Train Epoch: 20 [6656/8000 (83%)]\tTotal Loss: 0.445093\n",
      "Reconstruction: 0.297876, Regularization: 0.147217\n",
      "2019-04-10 01:01:36,199 root         INFO     Train Epoch: 20 [7168/8000 (90%)]\tTotal Loss: 0.407380\n",
      "Reconstruction: 0.274094, Regularization: 0.133285\n",
      "2019-04-10 01:01:36,262 root         INFO     Train Epoch: 20 [7680/8000 (96%)]\tTotal Loss: 0.405838\n",
      "Reconstruction: 0.283719, Regularization: 0.122119\n",
      "2019-04-10 01:01:36,316 root         INFO     ====> Epoch: 20 Average loss: 0.4261\n",
      "2019-04-10 01:01:36,340 root         INFO     Train Epoch: 21 [0/8000 (0%)]\tTotal Loss: 0.426183\n",
      "Reconstruction: 0.282525, Regularization: 0.143658\n",
      "2019-04-10 01:01:36,404 root         INFO     Train Epoch: 21 [512/8000 (6%)]\tTotal Loss: 0.451528\n",
      "Reconstruction: 0.315925, Regularization: 0.135603\n",
      "2019-04-10 01:01:36,467 root         INFO     Train Epoch: 21 [1024/8000 (13%)]\tTotal Loss: 0.383411\n",
      "Reconstruction: 0.251725, Regularization: 0.131686\n",
      "2019-04-10 01:01:36,530 root         INFO     Train Epoch: 21 [1536/8000 (19%)]\tTotal Loss: 0.405010\n",
      "Reconstruction: 0.270780, Regularization: 0.134230\n",
      "2019-04-10 01:01:36,593 root         INFO     Train Epoch: 21 [2048/8000 (26%)]\tTotal Loss: 0.404658\n",
      "Reconstruction: 0.282926, Regularization: 0.121731\n",
      "2019-04-10 01:01:36,656 root         INFO     Train Epoch: 21 [2560/8000 (32%)]\tTotal Loss: 0.438183\n",
      "Reconstruction: 0.313832, Regularization: 0.124350\n",
      "2019-04-10 01:01:36,720 root         INFO     Train Epoch: 21 [3072/8000 (38%)]\tTotal Loss: 0.414327\n",
      "Reconstruction: 0.287648, Regularization: 0.126678\n",
      "2019-04-10 01:01:36,783 root         INFO     Train Epoch: 21 [3584/8000 (45%)]\tTotal Loss: 0.429760\n",
      "Reconstruction: 0.302236, Regularization: 0.127524\n",
      "2019-04-10 01:01:36,846 root         INFO     Train Epoch: 21 [4096/8000 (51%)]\tTotal Loss: 0.436837\n",
      "Reconstruction: 0.303094, Regularization: 0.133744\n",
      "2019-04-10 01:01:36,909 root         INFO     Train Epoch: 21 [4608/8000 (58%)]\tTotal Loss: 0.437823\n",
      "Reconstruction: 0.307003, Regularization: 0.130820\n",
      "2019-04-10 01:01:36,973 root         INFO     Train Epoch: 21 [5120/8000 (64%)]\tTotal Loss: 0.422832\n",
      "Reconstruction: 0.308754, Regularization: 0.114078\n",
      "2019-04-10 01:01:37,036 root         INFO     Train Epoch: 21 [5632/8000 (70%)]\tTotal Loss: 0.378501\n",
      "Reconstruction: 0.266202, Regularization: 0.112299\n",
      "2019-04-10 01:01:37,099 root         INFO     Train Epoch: 21 [6144/8000 (77%)]\tTotal Loss: 0.386225\n",
      "Reconstruction: 0.263975, Regularization: 0.122250\n",
      "2019-04-10 01:01:37,162 root         INFO     Train Epoch: 21 [6656/8000 (83%)]\tTotal Loss: 0.408136\n",
      "Reconstruction: 0.283608, Regularization: 0.124528\n",
      "2019-04-10 01:01:37,225 root         INFO     Train Epoch: 21 [7168/8000 (90%)]\tTotal Loss: 0.379375\n",
      "Reconstruction: 0.269955, Regularization: 0.109419\n",
      "2019-04-10 01:01:37,289 root         INFO     Train Epoch: 21 [7680/8000 (96%)]\tTotal Loss: 0.401935\n",
      "Reconstruction: 0.281381, Regularization: 0.120553\n",
      "2019-04-10 01:01:37,343 root         INFO     ====> Epoch: 21 Average loss: 0.4033\n",
      "2019-04-10 01:01:37,367 root         INFO     Train Epoch: 22 [0/8000 (0%)]\tTotal Loss: 0.378533\n",
      "Reconstruction: 0.265168, Regularization: 0.113365\n",
      "2019-04-10 01:01:37,429 root         INFO     Train Epoch: 22 [512/8000 (6%)]\tTotal Loss: 0.384555\n",
      "Reconstruction: 0.278462, Regularization: 0.106093\n",
      "2019-04-10 01:01:37,491 root         INFO     Train Epoch: 22 [1024/8000 (13%)]\tTotal Loss: 0.416013\n",
      "Reconstruction: 0.305605, Regularization: 0.110408\n",
      "2019-04-10 01:01:37,553 root         INFO     Train Epoch: 22 [1536/8000 (19%)]\tTotal Loss: 0.386673\n",
      "Reconstruction: 0.274040, Regularization: 0.112633\n",
      "2019-04-10 01:01:37,615 root         INFO     Train Epoch: 22 [2048/8000 (26%)]\tTotal Loss: 0.389108\n",
      "Reconstruction: 0.265882, Regularization: 0.123226\n",
      "2019-04-10 01:01:37,676 root         INFO     Train Epoch: 22 [2560/8000 (32%)]\tTotal Loss: 0.427645\n",
      "Reconstruction: 0.313746, Regularization: 0.113899\n",
      "2019-04-10 01:01:37,738 root         INFO     Train Epoch: 22 [3072/8000 (38%)]\tTotal Loss: 0.393202\n",
      "Reconstruction: 0.286981, Regularization: 0.106221\n",
      "2019-04-10 01:01:37,799 root         INFO     Train Epoch: 22 [3584/8000 (45%)]\tTotal Loss: 0.375765\n",
      "Reconstruction: 0.253599, Regularization: 0.122166\n",
      "2019-04-10 01:01:37,860 root         INFO     Train Epoch: 22 [4096/8000 (51%)]\tTotal Loss: 0.381704\n",
      "Reconstruction: 0.272051, Regularization: 0.109653\n",
      "2019-04-10 01:01:37,922 root         INFO     Train Epoch: 22 [4608/8000 (58%)]\tTotal Loss: 0.365908\n",
      "Reconstruction: 0.259301, Regularization: 0.106607\n",
      "2019-04-10 01:01:37,983 root         INFO     Train Epoch: 22 [5120/8000 (64%)]\tTotal Loss: 0.384869\n",
      "Reconstruction: 0.287511, Regularization: 0.097358\n",
      "2019-04-10 01:01:38,045 root         INFO     Train Epoch: 22 [5632/8000 (70%)]\tTotal Loss: 0.408453\n",
      "Reconstruction: 0.296686, Regularization: 0.111767\n",
      "2019-04-10 01:01:38,106 root         INFO     Train Epoch: 22 [6144/8000 (77%)]\tTotal Loss: 0.361718\n",
      "Reconstruction: 0.267659, Regularization: 0.094060\n",
      "2019-04-10 01:01:38,168 root         INFO     Train Epoch: 22 [6656/8000 (83%)]\tTotal Loss: 0.371947\n",
      "Reconstruction: 0.265503, Regularization: 0.106444\n",
      "2019-04-10 01:01:38,229 root         INFO     Train Epoch: 22 [7168/8000 (90%)]\tTotal Loss: 0.372236\n",
      "Reconstruction: 0.273923, Regularization: 0.098312\n",
      "2019-04-10 01:01:38,291 root         INFO     Train Epoch: 22 [7680/8000 (96%)]\tTotal Loss: 0.367553\n",
      "Reconstruction: 0.273111, Regularization: 0.094442\n",
      "2019-04-10 01:01:38,344 root         INFO     ====> Epoch: 22 Average loss: 0.3844\n",
      "2019-04-10 01:01:38,369 root         INFO     Train Epoch: 23 [0/8000 (0%)]\tTotal Loss: 0.383269\n",
      "Reconstruction: 0.272154, Regularization: 0.111115\n",
      "2019-04-10 01:01:38,433 root         INFO     Train Epoch: 23 [512/8000 (6%)]\tTotal Loss: 0.384294\n",
      "Reconstruction: 0.287472, Regularization: 0.096822\n",
      "2019-04-10 01:01:38,496 root         INFO     Train Epoch: 23 [1024/8000 (13%)]\tTotal Loss: 0.376871\n",
      "Reconstruction: 0.264874, Regularization: 0.111997\n",
      "2019-04-10 01:01:38,560 root         INFO     Train Epoch: 23 [1536/8000 (19%)]\tTotal Loss: 0.367216\n",
      "Reconstruction: 0.264547, Regularization: 0.102668\n",
      "2019-04-10 01:01:38,623 root         INFO     Train Epoch: 23 [2048/8000 (26%)]\tTotal Loss: 0.407684\n",
      "Reconstruction: 0.318026, Regularization: 0.089658\n",
      "2019-04-10 01:01:38,686 root         INFO     Train Epoch: 23 [2560/8000 (32%)]\tTotal Loss: 0.361666\n",
      "Reconstruction: 0.276167, Regularization: 0.085499\n",
      "2019-04-10 01:01:38,748 root         INFO     Train Epoch: 23 [3072/8000 (38%)]\tTotal Loss: 0.360469\n",
      "Reconstruction: 0.250365, Regularization: 0.110105\n",
      "2019-04-10 01:01:38,810 root         INFO     Train Epoch: 23 [3584/8000 (45%)]\tTotal Loss: 0.365658\n",
      "Reconstruction: 0.279008, Regularization: 0.086650\n",
      "2019-04-10 01:01:38,873 root         INFO     Train Epoch: 23 [4096/8000 (51%)]\tTotal Loss: 0.353526\n",
      "Reconstruction: 0.273778, Regularization: 0.079748\n",
      "2019-04-10 01:01:38,935 root         INFO     Train Epoch: 23 [4608/8000 (58%)]\tTotal Loss: 0.341614\n",
      "Reconstruction: 0.250342, Regularization: 0.091272\n",
      "2019-04-10 01:01:38,998 root         INFO     Train Epoch: 23 [5120/8000 (64%)]\tTotal Loss: 0.365923\n",
      "Reconstruction: 0.268539, Regularization: 0.097384\n",
      "2019-04-10 01:01:39,061 root         INFO     Train Epoch: 23 [5632/8000 (70%)]\tTotal Loss: 0.359091\n",
      "Reconstruction: 0.262773, Regularization: 0.096318\n",
      "2019-04-10 01:01:39,123 root         INFO     Train Epoch: 23 [6144/8000 (77%)]\tTotal Loss: 0.372690\n",
      "Reconstruction: 0.279889, Regularization: 0.092801\n",
      "2019-04-10 01:01:39,185 root         INFO     Train Epoch: 23 [6656/8000 (83%)]\tTotal Loss: 0.351428\n",
      "Reconstruction: 0.265793, Regularization: 0.085635\n",
      "2019-04-10 01:01:39,246 root         INFO     Train Epoch: 23 [7168/8000 (90%)]\tTotal Loss: 0.366561\n",
      "Reconstruction: 0.271264, Regularization: 0.095297\n",
      "2019-04-10 01:01:39,308 root         INFO     Train Epoch: 23 [7680/8000 (96%)]\tTotal Loss: 0.380414\n",
      "Reconstruction: 0.288448, Regularization: 0.091965\n",
      "2019-04-10 01:01:39,361 root         INFO     ====> Epoch: 23 Average loss: 0.3672\n",
      "2019-04-10 01:01:39,385 root         INFO     Train Epoch: 24 [0/8000 (0%)]\tTotal Loss: 0.349522\n",
      "Reconstruction: 0.264901, Regularization: 0.084621\n",
      "2019-04-10 01:01:39,448 root         INFO     Train Epoch: 24 [512/8000 (6%)]\tTotal Loss: 0.350476\n",
      "Reconstruction: 0.257048, Regularization: 0.093428\n",
      "2019-04-10 01:01:39,511 root         INFO     Train Epoch: 24 [1024/8000 (13%)]\tTotal Loss: 0.348029\n",
      "Reconstruction: 0.260269, Regularization: 0.087761\n",
      "2019-04-10 01:01:39,574 root         INFO     Train Epoch: 24 [1536/8000 (19%)]\tTotal Loss: 0.364587\n",
      "Reconstruction: 0.280961, Regularization: 0.083626\n",
      "2019-04-10 01:01:39,637 root         INFO     Train Epoch: 24 [2048/8000 (26%)]\tTotal Loss: 0.338561\n",
      "Reconstruction: 0.257917, Regularization: 0.080645\n",
      "2019-04-10 01:01:39,700 root         INFO     Train Epoch: 24 [2560/8000 (32%)]\tTotal Loss: 0.364748\n",
      "Reconstruction: 0.280710, Regularization: 0.084038\n",
      "2019-04-10 01:01:39,763 root         INFO     Train Epoch: 24 [3072/8000 (38%)]\tTotal Loss: 0.354566\n",
      "Reconstruction: 0.263015, Regularization: 0.091551\n",
      "2019-04-10 01:01:39,825 root         INFO     Train Epoch: 24 [3584/8000 (45%)]\tTotal Loss: 0.342571\n",
      "Reconstruction: 0.258737, Regularization: 0.083835\n",
      "2019-04-10 01:01:39,888 root         INFO     Train Epoch: 24 [4096/8000 (51%)]\tTotal Loss: 0.340058\n",
      "Reconstruction: 0.269893, Regularization: 0.070165\n",
      "2019-04-10 01:01:39,953 root         INFO     Train Epoch: 24 [4608/8000 (58%)]\tTotal Loss: 0.357706\n",
      "Reconstruction: 0.271310, Regularization: 0.086396\n",
      "2019-04-10 01:01:40,016 root         INFO     Train Epoch: 24 [5120/8000 (64%)]\tTotal Loss: 0.354789\n",
      "Reconstruction: 0.275886, Regularization: 0.078903\n",
      "2019-04-10 01:01:40,080 root         INFO     Train Epoch: 24 [5632/8000 (70%)]\tTotal Loss: 0.353659\n",
      "Reconstruction: 0.262193, Regularization: 0.091467\n",
      "2019-04-10 01:01:40,144 root         INFO     Train Epoch: 24 [6144/8000 (77%)]\tTotal Loss: 0.365572\n",
      "Reconstruction: 0.275621, Regularization: 0.089951\n",
      "2019-04-10 01:01:40,208 root         INFO     Train Epoch: 24 [6656/8000 (83%)]\tTotal Loss: 0.359018\n",
      "Reconstruction: 0.285430, Regularization: 0.073588\n",
      "2019-04-10 01:01:40,272 root         INFO     Train Epoch: 24 [7168/8000 (90%)]\tTotal Loss: 0.339069\n",
      "Reconstruction: 0.261973, Regularization: 0.077096\n",
      "2019-04-10 01:01:40,336 root         INFO     Train Epoch: 24 [7680/8000 (96%)]\tTotal Loss: 0.339154\n",
      "Reconstruction: 0.261044, Regularization: 0.078110\n",
      "2019-04-10 01:01:40,390 root         INFO     ====> Epoch: 24 Average loss: 0.3524\n",
      "2019-04-10 01:01:40,414 root         INFO     Train Epoch: 25 [0/8000 (0%)]\tTotal Loss: 0.362423\n",
      "Reconstruction: 0.272673, Regularization: 0.089750\n",
      "2019-04-10 01:01:40,478 root         INFO     Train Epoch: 25 [512/8000 (6%)]\tTotal Loss: 0.364681\n",
      "Reconstruction: 0.286401, Regularization: 0.078280\n",
      "2019-04-10 01:01:40,542 root         INFO     Train Epoch: 25 [1024/8000 (13%)]\tTotal Loss: 0.339468\n",
      "Reconstruction: 0.270340, Regularization: 0.069128\n",
      "2019-04-10 01:01:40,606 root         INFO     Train Epoch: 25 [1536/8000 (19%)]\tTotal Loss: 0.334963\n",
      "Reconstruction: 0.258583, Regularization: 0.076381\n",
      "2019-04-10 01:01:40,670 root         INFO     Train Epoch: 25 [2048/8000 (26%)]\tTotal Loss: 0.351284\n",
      "Reconstruction: 0.275567, Regularization: 0.075718\n",
      "2019-04-10 01:01:40,734 root         INFO     Train Epoch: 25 [2560/8000 (32%)]\tTotal Loss: 0.350350\n",
      "Reconstruction: 0.282804, Regularization: 0.067546\n",
      "2019-04-10 01:01:40,797 root         INFO     Train Epoch: 25 [3072/8000 (38%)]\tTotal Loss: 0.371085\n",
      "Reconstruction: 0.301220, Regularization: 0.069865\n",
      "2019-04-10 01:01:40,861 root         INFO     Train Epoch: 25 [3584/8000 (45%)]\tTotal Loss: 0.341859\n",
      "Reconstruction: 0.265000, Regularization: 0.076859\n",
      "2019-04-10 01:01:40,924 root         INFO     Train Epoch: 25 [4096/8000 (51%)]\tTotal Loss: 0.335687\n",
      "Reconstruction: 0.263695, Regularization: 0.071991\n",
      "2019-04-10 01:01:40,987 root         INFO     Train Epoch: 25 [4608/8000 (58%)]\tTotal Loss: 0.353099\n",
      "Reconstruction: 0.285102, Regularization: 0.067997\n",
      "2019-04-10 01:01:41,051 root         INFO     Train Epoch: 25 [5120/8000 (64%)]\tTotal Loss: 0.348880\n",
      "Reconstruction: 0.273791, Regularization: 0.075089\n",
      "2019-04-10 01:01:41,114 root         INFO     Train Epoch: 25 [5632/8000 (70%)]\tTotal Loss: 0.339552\n",
      "Reconstruction: 0.268900, Regularization: 0.070652\n",
      "2019-04-10 01:01:41,177 root         INFO     Train Epoch: 25 [6144/8000 (77%)]\tTotal Loss: 0.330280\n",
      "Reconstruction: 0.258656, Regularization: 0.071624\n",
      "2019-04-10 01:01:41,239 root         INFO     Train Epoch: 25 [6656/8000 (83%)]\tTotal Loss: 0.336987\n",
      "Reconstruction: 0.264819, Regularization: 0.072168\n",
      "2019-04-10 01:01:41,302 root         INFO     Train Epoch: 25 [7168/8000 (90%)]\tTotal Loss: 0.321907\n",
      "Reconstruction: 0.251256, Regularization: 0.070651\n",
      "2019-04-10 01:01:41,364 root         INFO     Train Epoch: 25 [7680/8000 (96%)]\tTotal Loss: 0.319769\n",
      "Reconstruction: 0.247881, Regularization: 0.071889\n",
      "2019-04-10 01:01:41,418 root         INFO     ====> Epoch: 25 Average loss: 0.3400\n",
      "2019-04-10 01:01:41,442 root         INFO     Train Epoch: 26 [0/8000 (0%)]\tTotal Loss: 0.329181\n",
      "Reconstruction: 0.259752, Regularization: 0.069429\n",
      "2019-04-10 01:01:41,505 root         INFO     Train Epoch: 26 [512/8000 (6%)]\tTotal Loss: 0.339074\n",
      "Reconstruction: 0.265452, Regularization: 0.073622\n",
      "2019-04-10 01:01:41,568 root         INFO     Train Epoch: 26 [1024/8000 (13%)]\tTotal Loss: 0.338006\n",
      "Reconstruction: 0.262895, Regularization: 0.075111\n",
      "2019-04-10 01:01:41,629 root         INFO     Train Epoch: 26 [1536/8000 (19%)]\tTotal Loss: 0.338833\n",
      "Reconstruction: 0.280938, Regularization: 0.057895\n",
      "2019-04-10 01:01:41,690 root         INFO     Train Epoch: 26 [2048/8000 (26%)]\tTotal Loss: 0.339896\n",
      "Reconstruction: 0.271879, Regularization: 0.068017\n",
      "2019-04-10 01:01:41,751 root         INFO     Train Epoch: 26 [2560/8000 (32%)]\tTotal Loss: 0.320275\n",
      "Reconstruction: 0.253662, Regularization: 0.066613\n",
      "2019-04-10 01:01:41,813 root         INFO     Train Epoch: 26 [3072/8000 (38%)]\tTotal Loss: 0.331047\n",
      "Reconstruction: 0.265145, Regularization: 0.065902\n",
      "2019-04-10 01:01:41,875 root         INFO     Train Epoch: 26 [3584/8000 (45%)]\tTotal Loss: 0.312071\n",
      "Reconstruction: 0.251672, Regularization: 0.060398\n",
      "2019-04-10 01:01:41,938 root         INFO     Train Epoch: 26 [4096/8000 (51%)]\tTotal Loss: 0.318539\n",
      "Reconstruction: 0.256186, Regularization: 0.062353\n",
      "2019-04-10 01:01:42,000 root         INFO     Train Epoch: 26 [4608/8000 (58%)]\tTotal Loss: 0.325810\n",
      "Reconstruction: 0.266585, Regularization: 0.059225\n",
      "2019-04-10 01:01:42,063 root         INFO     Train Epoch: 26 [5120/8000 (64%)]\tTotal Loss: 0.320724\n",
      "Reconstruction: 0.255028, Regularization: 0.065696\n",
      "2019-04-10 01:01:42,126 root         INFO     Train Epoch: 26 [5632/8000 (70%)]\tTotal Loss: 0.328843\n",
      "Reconstruction: 0.266066, Regularization: 0.062776\n",
      "2019-04-10 01:01:42,189 root         INFO     Train Epoch: 26 [6144/8000 (77%)]\tTotal Loss: 0.337799\n",
      "Reconstruction: 0.275140, Regularization: 0.062659\n",
      "2019-04-10 01:01:42,251 root         INFO     Train Epoch: 26 [6656/8000 (83%)]\tTotal Loss: 0.334435\n",
      "Reconstruction: 0.278524, Regularization: 0.055912\n",
      "2019-04-10 01:01:42,314 root         INFO     Train Epoch: 26 [7168/8000 (90%)]\tTotal Loss: 0.333735\n",
      "Reconstruction: 0.265304, Regularization: 0.068431\n",
      "2019-04-10 01:01:42,377 root         INFO     Train Epoch: 26 [7680/8000 (96%)]\tTotal Loss: 0.314461\n",
      "Reconstruction: 0.247704, Regularization: 0.066757\n",
      "2019-04-10 01:01:42,432 root         INFO     ====> Epoch: 26 Average loss: 0.3289\n",
      "2019-04-10 01:01:42,455 root         INFO     Train Epoch: 27 [0/8000 (0%)]\tTotal Loss: 0.327677\n",
      "Reconstruction: 0.268641, Regularization: 0.059036\n",
      "2019-04-10 01:01:42,519 root         INFO     Train Epoch: 27 [512/8000 (6%)]\tTotal Loss: 0.320490\n",
      "Reconstruction: 0.261906, Regularization: 0.058584\n",
      "2019-04-10 01:01:42,582 root         INFO     Train Epoch: 27 [1024/8000 (13%)]\tTotal Loss: 0.323241\n",
      "Reconstruction: 0.262126, Regularization: 0.061115\n",
      "2019-04-10 01:01:42,645 root         INFO     Train Epoch: 27 [1536/8000 (19%)]\tTotal Loss: 0.315691\n",
      "Reconstruction: 0.257178, Regularization: 0.058513\n",
      "2019-04-10 01:01:42,708 root         INFO     Train Epoch: 27 [2048/8000 (26%)]\tTotal Loss: 0.318949\n",
      "Reconstruction: 0.258294, Regularization: 0.060655\n",
      "2019-04-10 01:01:42,771 root         INFO     Train Epoch: 27 [2560/8000 (32%)]\tTotal Loss: 0.307532\n",
      "Reconstruction: 0.251265, Regularization: 0.056267\n",
      "2019-04-10 01:01:42,834 root         INFO     Train Epoch: 27 [3072/8000 (38%)]\tTotal Loss: 0.330075\n",
      "Reconstruction: 0.264695, Regularization: 0.065380\n",
      "2019-04-10 01:01:42,897 root         INFO     Train Epoch: 27 [3584/8000 (45%)]\tTotal Loss: 0.336645\n",
      "Reconstruction: 0.281316, Regularization: 0.055329\n",
      "2019-04-10 01:01:42,960 root         INFO     Train Epoch: 27 [4096/8000 (51%)]\tTotal Loss: 0.343316\n",
      "Reconstruction: 0.290505, Regularization: 0.052812\n",
      "2019-04-10 01:01:43,024 root         INFO     Train Epoch: 27 [4608/8000 (58%)]\tTotal Loss: 0.304666\n",
      "Reconstruction: 0.250747, Regularization: 0.053919\n",
      "2019-04-10 01:01:43,087 root         INFO     Train Epoch: 27 [5120/8000 (64%)]\tTotal Loss: 0.316608\n",
      "Reconstruction: 0.271431, Regularization: 0.045177\n",
      "2019-04-10 01:01:43,150 root         INFO     Train Epoch: 27 [5632/8000 (70%)]\tTotal Loss: 0.320483\n",
      "Reconstruction: 0.263576, Regularization: 0.056907\n",
      "2019-04-10 01:01:43,212 root         INFO     Train Epoch: 27 [6144/8000 (77%)]\tTotal Loss: 0.323176\n",
      "Reconstruction: 0.263577, Regularization: 0.059598\n",
      "2019-04-10 01:01:43,274 root         INFO     Train Epoch: 27 [6656/8000 (83%)]\tTotal Loss: 0.317511\n",
      "Reconstruction: 0.255587, Regularization: 0.061924\n",
      "2019-04-10 01:01:43,336 root         INFO     Train Epoch: 27 [7168/8000 (90%)]\tTotal Loss: 0.318561\n",
      "Reconstruction: 0.258260, Regularization: 0.060302\n",
      "2019-04-10 01:01:43,397 root         INFO     Train Epoch: 27 [7680/8000 (96%)]\tTotal Loss: 0.331354\n",
      "Reconstruction: 0.278357, Regularization: 0.052997\n",
      "2019-04-10 01:01:43,450 root         INFO     ====> Epoch: 27 Average loss: 0.3195\n",
      "2019-04-10 01:01:43,474 root         INFO     Train Epoch: 28 [0/8000 (0%)]\tTotal Loss: 0.301487\n",
      "Reconstruction: 0.252134, Regularization: 0.049353\n",
      "2019-04-10 01:01:43,538 root         INFO     Train Epoch: 28 [512/8000 (6%)]\tTotal Loss: 0.301927\n",
      "Reconstruction: 0.252686, Regularization: 0.049240\n",
      "2019-04-10 01:01:43,601 root         INFO     Train Epoch: 28 [1024/8000 (13%)]\tTotal Loss: 0.296091\n",
      "Reconstruction: 0.244346, Regularization: 0.051745\n",
      "2019-04-10 01:01:43,664 root         INFO     Train Epoch: 28 [1536/8000 (19%)]\tTotal Loss: 0.310200\n",
      "Reconstruction: 0.259185, Regularization: 0.051015\n",
      "2019-04-10 01:01:43,727 root         INFO     Train Epoch: 28 [2048/8000 (26%)]\tTotal Loss: 0.298138\n",
      "Reconstruction: 0.247375, Regularization: 0.050763\n",
      "2019-04-10 01:01:43,789 root         INFO     Train Epoch: 28 [2560/8000 (32%)]\tTotal Loss: 0.307150\n",
      "Reconstruction: 0.263705, Regularization: 0.043445\n",
      "2019-04-10 01:01:43,853 root         INFO     Train Epoch: 28 [3072/8000 (38%)]\tTotal Loss: 0.311655\n",
      "Reconstruction: 0.267565, Regularization: 0.044090\n",
      "2019-04-10 01:01:43,915 root         INFO     Train Epoch: 28 [3584/8000 (45%)]\tTotal Loss: 0.297251\n",
      "Reconstruction: 0.250893, Regularization: 0.046358\n",
      "2019-04-10 01:01:43,978 root         INFO     Train Epoch: 28 [4096/8000 (51%)]\tTotal Loss: 0.331146\n",
      "Reconstruction: 0.277898, Regularization: 0.053248\n",
      "2019-04-10 01:01:44,040 root         INFO     Train Epoch: 28 [4608/8000 (58%)]\tTotal Loss: 0.309454\n",
      "Reconstruction: 0.259111, Regularization: 0.050343\n",
      "2019-04-10 01:01:44,105 root         INFO     Train Epoch: 28 [5120/8000 (64%)]\tTotal Loss: 0.325261\n",
      "Reconstruction: 0.278131, Regularization: 0.047130\n",
      "2019-04-10 01:01:44,170 root         INFO     Train Epoch: 28 [5632/8000 (70%)]\tTotal Loss: 0.316934\n",
      "Reconstruction: 0.268030, Regularization: 0.048904\n",
      "2019-04-10 01:01:44,234 root         INFO     Train Epoch: 28 [6144/8000 (77%)]\tTotal Loss: 0.329153\n",
      "Reconstruction: 0.284643, Regularization: 0.044510\n",
      "2019-04-10 01:01:44,299 root         INFO     Train Epoch: 28 [6656/8000 (83%)]\tTotal Loss: 0.312197\n",
      "Reconstruction: 0.261945, Regularization: 0.050253\n",
      "2019-04-10 01:01:44,363 root         INFO     Train Epoch: 28 [7168/8000 (90%)]\tTotal Loss: 0.301205\n",
      "Reconstruction: 0.253165, Regularization: 0.048041\n",
      "2019-04-10 01:01:44,427 root         INFO     Train Epoch: 28 [7680/8000 (96%)]\tTotal Loss: 0.297278\n",
      "Reconstruction: 0.252257, Regularization: 0.045022\n",
      "2019-04-10 01:01:44,482 root         INFO     ====> Epoch: 28 Average loss: 0.3098\n",
      "2019-04-10 01:01:44,506 root         INFO     Train Epoch: 29 [0/8000 (0%)]\tTotal Loss: 0.305661\n",
      "Reconstruction: 0.260580, Regularization: 0.045082\n",
      "2019-04-10 01:01:44,567 root         INFO     Train Epoch: 29 [512/8000 (6%)]\tTotal Loss: 0.320858\n",
      "Reconstruction: 0.275519, Regularization: 0.045339\n",
      "2019-04-10 01:01:44,629 root         INFO     Train Epoch: 29 [1024/8000 (13%)]\tTotal Loss: 0.310593\n",
      "Reconstruction: 0.263996, Regularization: 0.046597\n",
      "2019-04-10 01:01:44,690 root         INFO     Train Epoch: 29 [1536/8000 (19%)]\tTotal Loss: 0.306390\n",
      "Reconstruction: 0.263679, Regularization: 0.042711\n",
      "2019-04-10 01:01:44,752 root         INFO     Train Epoch: 29 [2048/8000 (26%)]\tTotal Loss: 0.300899\n",
      "Reconstruction: 0.257846, Regularization: 0.043052\n",
      "2019-04-10 01:01:44,814 root         INFO     Train Epoch: 29 [2560/8000 (32%)]\tTotal Loss: 0.305329\n",
      "Reconstruction: 0.258126, Regularization: 0.047203\n",
      "2019-04-10 01:01:44,875 root         INFO     Train Epoch: 29 [3072/8000 (38%)]\tTotal Loss: 0.301786\n",
      "Reconstruction: 0.260750, Regularization: 0.041036\n",
      "2019-04-10 01:01:44,937 root         INFO     Train Epoch: 29 [3584/8000 (45%)]\tTotal Loss: 0.300217\n",
      "Reconstruction: 0.261663, Regularization: 0.038554\n",
      "2019-04-10 01:01:44,999 root         INFO     Train Epoch: 29 [4096/8000 (51%)]\tTotal Loss: 0.303270\n",
      "Reconstruction: 0.254366, Regularization: 0.048904\n",
      "2019-04-10 01:01:45,060 root         INFO     Train Epoch: 29 [4608/8000 (58%)]\tTotal Loss: 0.305639\n",
      "Reconstruction: 0.251698, Regularization: 0.053942\n",
      "2019-04-10 01:01:45,123 root         INFO     Train Epoch: 29 [5120/8000 (64%)]\tTotal Loss: 0.297975\n",
      "Reconstruction: 0.255089, Regularization: 0.042886\n",
      "2019-04-10 01:01:45,185 root         INFO     Train Epoch: 29 [5632/8000 (70%)]\tTotal Loss: 0.289382\n",
      "Reconstruction: 0.245201, Regularization: 0.044181\n",
      "2019-04-10 01:01:45,248 root         INFO     Train Epoch: 29 [6144/8000 (77%)]\tTotal Loss: 0.284777\n",
      "Reconstruction: 0.242105, Regularization: 0.042672\n",
      "2019-04-10 01:01:45,310 root         INFO     Train Epoch: 29 [6656/8000 (83%)]\tTotal Loss: 0.305632\n",
      "Reconstruction: 0.263487, Regularization: 0.042145\n",
      "2019-04-10 01:01:45,373 root         INFO     Train Epoch: 29 [7168/8000 (90%)]\tTotal Loss: 0.296280\n",
      "Reconstruction: 0.252698, Regularization: 0.043582\n",
      "2019-04-10 01:01:45,435 root         INFO     Train Epoch: 29 [7680/8000 (96%)]\tTotal Loss: 0.298536\n",
      "Reconstruction: 0.257867, Regularization: 0.040669\n",
      "2019-04-10 01:01:45,488 root         INFO     ====> Epoch: 29 Average loss: 0.3035\n",
      "2019-04-10 01:01:45,512 root         INFO     Train Epoch: 30 [0/8000 (0%)]\tTotal Loss: 0.306805\n",
      "Reconstruction: 0.261933, Regularization: 0.044872\n",
      "2019-04-10 01:01:45,575 root         INFO     Train Epoch: 30 [512/8000 (6%)]\tTotal Loss: 0.291609\n",
      "Reconstruction: 0.247039, Regularization: 0.044570\n",
      "2019-04-10 01:01:45,638 root         INFO     Train Epoch: 30 [1024/8000 (13%)]\tTotal Loss: 0.297330\n",
      "Reconstruction: 0.253535, Regularization: 0.043794\n",
      "2019-04-10 01:01:45,700 root         INFO     Train Epoch: 30 [1536/8000 (19%)]\tTotal Loss: 0.306112\n",
      "Reconstruction: 0.266974, Regularization: 0.039138\n",
      "2019-04-10 01:01:45,763 root         INFO     Train Epoch: 30 [2048/8000 (26%)]\tTotal Loss: 0.301290\n",
      "Reconstruction: 0.254765, Regularization: 0.046525\n",
      "2019-04-10 01:01:45,825 root         INFO     Train Epoch: 30 [2560/8000 (32%)]\tTotal Loss: 0.305068\n",
      "Reconstruction: 0.259291, Regularization: 0.045777\n",
      "2019-04-10 01:01:45,888 root         INFO     Train Epoch: 30 [3072/8000 (38%)]\tTotal Loss: 0.283929\n",
      "Reconstruction: 0.240564, Regularization: 0.043365\n",
      "2019-04-10 01:01:45,951 root         INFO     Train Epoch: 30 [3584/8000 (45%)]\tTotal Loss: 0.293952\n",
      "Reconstruction: 0.255597, Regularization: 0.038355\n",
      "2019-04-10 01:01:46,014 root         INFO     Train Epoch: 30 [4096/8000 (51%)]\tTotal Loss: 0.285836\n",
      "Reconstruction: 0.241968, Regularization: 0.043868\n",
      "2019-04-10 01:01:46,077 root         INFO     Train Epoch: 30 [4608/8000 (58%)]\tTotal Loss: 0.291069\n",
      "Reconstruction: 0.251603, Regularization: 0.039467\n",
      "2019-04-10 01:01:46,140 root         INFO     Train Epoch: 30 [5120/8000 (64%)]\tTotal Loss: 0.296693\n",
      "Reconstruction: 0.256742, Regularization: 0.039951\n",
      "2019-04-10 01:01:46,203 root         INFO     Train Epoch: 30 [5632/8000 (70%)]\tTotal Loss: 0.292653\n",
      "Reconstruction: 0.254431, Regularization: 0.038222\n",
      "2019-04-10 01:01:46,265 root         INFO     Train Epoch: 30 [6144/8000 (77%)]\tTotal Loss: 0.296176\n",
      "Reconstruction: 0.258793, Regularization: 0.037383\n",
      "2019-04-10 01:01:46,327 root         INFO     Train Epoch: 30 [6656/8000 (83%)]\tTotal Loss: 0.301981\n",
      "Reconstruction: 0.256680, Regularization: 0.045301\n",
      "2019-04-10 01:01:46,389 root         INFO     Train Epoch: 30 [7168/8000 (90%)]\tTotal Loss: 0.303932\n",
      "Reconstruction: 0.269185, Regularization: 0.034747\n",
      "2019-04-10 01:01:46,451 root         INFO     Train Epoch: 30 [7680/8000 (96%)]\tTotal Loss: 0.296192\n",
      "Reconstruction: 0.254350, Regularization: 0.041842\n",
      "2019-04-10 01:01:46,504 root         INFO     ====> Epoch: 30 Average loss: 0.2971\n",
      "2019-04-10 01:01:46,528 root         INFO     Train Epoch: 31 [0/8000 (0%)]\tTotal Loss: 0.287581\n",
      "Reconstruction: 0.256013, Regularization: 0.031568\n",
      "2019-04-10 01:01:46,592 root         INFO     Train Epoch: 31 [512/8000 (6%)]\tTotal Loss: 0.278502\n",
      "Reconstruction: 0.245338, Regularization: 0.033165\n",
      "2019-04-10 01:01:46,655 root         INFO     Train Epoch: 31 [1024/8000 (13%)]\tTotal Loss: 0.291381\n",
      "Reconstruction: 0.254876, Regularization: 0.036505\n",
      "2019-04-10 01:01:46,718 root         INFO     Train Epoch: 31 [1536/8000 (19%)]\tTotal Loss: 0.284452\n",
      "Reconstruction: 0.249550, Regularization: 0.034902\n",
      "2019-04-10 01:01:46,781 root         INFO     Train Epoch: 31 [2048/8000 (26%)]\tTotal Loss: 0.289093\n",
      "Reconstruction: 0.250113, Regularization: 0.038980\n",
      "2019-04-10 01:01:46,845 root         INFO     Train Epoch: 31 [2560/8000 (32%)]\tTotal Loss: 0.281423\n",
      "Reconstruction: 0.246142, Regularization: 0.035282\n",
      "2019-04-10 01:01:46,908 root         INFO     Train Epoch: 31 [3072/8000 (38%)]\tTotal Loss: 0.285098\n",
      "Reconstruction: 0.245809, Regularization: 0.039288\n",
      "2019-04-10 01:01:46,972 root         INFO     Train Epoch: 31 [3584/8000 (45%)]\tTotal Loss: 0.285563\n",
      "Reconstruction: 0.256334, Regularization: 0.029229\n",
      "2019-04-10 01:01:47,034 root         INFO     Train Epoch: 31 [4096/8000 (51%)]\tTotal Loss: 0.288585\n",
      "Reconstruction: 0.252365, Regularization: 0.036220\n",
      "2019-04-10 01:01:47,097 root         INFO     Train Epoch: 31 [4608/8000 (58%)]\tTotal Loss: 0.286104\n",
      "Reconstruction: 0.252222, Regularization: 0.033882\n",
      "2019-04-10 01:01:47,160 root         INFO     Train Epoch: 31 [5120/8000 (64%)]\tTotal Loss: 0.296261\n",
      "Reconstruction: 0.262792, Regularization: 0.033469\n",
      "2019-04-10 01:01:47,223 root         INFO     Train Epoch: 31 [5632/8000 (70%)]\tTotal Loss: 0.304206\n",
      "Reconstruction: 0.263789, Regularization: 0.040417\n",
      "2019-04-10 01:01:47,286 root         INFO     Train Epoch: 31 [6144/8000 (77%)]\tTotal Loss: 0.292192\n",
      "Reconstruction: 0.258365, Regularization: 0.033828\n",
      "2019-04-10 01:01:47,349 root         INFO     Train Epoch: 31 [6656/8000 (83%)]\tTotal Loss: 0.302483\n",
      "Reconstruction: 0.268862, Regularization: 0.033621\n",
      "2019-04-10 01:01:47,412 root         INFO     Train Epoch: 31 [7168/8000 (90%)]\tTotal Loss: 0.283476\n",
      "Reconstruction: 0.252682, Regularization: 0.030794\n",
      "2019-04-10 01:01:47,475 root         INFO     Train Epoch: 31 [7680/8000 (96%)]\tTotal Loss: 0.284986\n",
      "Reconstruction: 0.254883, Regularization: 0.030103\n",
      "2019-04-10 01:01:47,529 root         INFO     ====> Epoch: 31 Average loss: 0.2907\n",
      "2019-04-10 01:01:47,553 root         INFO     Train Epoch: 32 [0/8000 (0%)]\tTotal Loss: 0.266579\n",
      "Reconstruction: 0.236695, Regularization: 0.029884\n",
      "2019-04-10 01:01:47,617 root         INFO     Train Epoch: 32 [512/8000 (6%)]\tTotal Loss: 0.292350\n",
      "Reconstruction: 0.258108, Regularization: 0.034242\n",
      "2019-04-10 01:01:47,681 root         INFO     Train Epoch: 32 [1024/8000 (13%)]\tTotal Loss: 0.286928\n",
      "Reconstruction: 0.251245, Regularization: 0.035683\n",
      "2019-04-10 01:01:47,745 root         INFO     Train Epoch: 32 [1536/8000 (19%)]\tTotal Loss: 0.284245\n",
      "Reconstruction: 0.247104, Regularization: 0.037141\n",
      "2019-04-10 01:01:47,809 root         INFO     Train Epoch: 32 [2048/8000 (26%)]\tTotal Loss: 0.281492\n",
      "Reconstruction: 0.251120, Regularization: 0.030372\n",
      "2019-04-10 01:01:47,874 root         INFO     Train Epoch: 32 [2560/8000 (32%)]\tTotal Loss: 0.277745\n",
      "Reconstruction: 0.245980, Regularization: 0.031765\n",
      "2019-04-10 01:01:47,938 root         INFO     Train Epoch: 32 [3072/8000 (38%)]\tTotal Loss: 0.279782\n",
      "Reconstruction: 0.249221, Regularization: 0.030561\n",
      "2019-04-10 01:01:48,002 root         INFO     Train Epoch: 32 [3584/8000 (45%)]\tTotal Loss: 0.277346\n",
      "Reconstruction: 0.248504, Regularization: 0.028842\n",
      "2019-04-10 01:01:48,066 root         INFO     Train Epoch: 32 [4096/8000 (51%)]\tTotal Loss: 0.287337\n",
      "Reconstruction: 0.256734, Regularization: 0.030603\n",
      "2019-04-10 01:01:48,130 root         INFO     Train Epoch: 32 [4608/8000 (58%)]\tTotal Loss: 0.279494\n",
      "Reconstruction: 0.251160, Regularization: 0.028335\n",
      "2019-04-10 01:01:48,194 root         INFO     Train Epoch: 32 [5120/8000 (64%)]\tTotal Loss: 0.275257\n",
      "Reconstruction: 0.246199, Regularization: 0.029058\n",
      "2019-04-10 01:01:48,257 root         INFO     Train Epoch: 32 [5632/8000 (70%)]\tTotal Loss: 0.289963\n",
      "Reconstruction: 0.263301, Regularization: 0.026662\n",
      "2019-04-10 01:01:48,321 root         INFO     Train Epoch: 32 [6144/8000 (77%)]\tTotal Loss: 0.275334\n",
      "Reconstruction: 0.249342, Regularization: 0.025992\n",
      "2019-04-10 01:01:48,383 root         INFO     Train Epoch: 32 [6656/8000 (83%)]\tTotal Loss: 0.283515\n",
      "Reconstruction: 0.249903, Regularization: 0.033612\n",
      "2019-04-10 01:01:48,444 root         INFO     Train Epoch: 32 [7168/8000 (90%)]\tTotal Loss: 0.278800\n",
      "Reconstruction: 0.250954, Regularization: 0.027846\n",
      "2019-04-10 01:01:48,505 root         INFO     Train Epoch: 32 [7680/8000 (96%)]\tTotal Loss: 0.296337\n",
      "Reconstruction: 0.266293, Regularization: 0.030044\n",
      "2019-04-10 01:01:48,560 root         INFO     ====> Epoch: 32 Average loss: 0.2849\n",
      "2019-04-10 01:01:48,584 root         INFO     Train Epoch: 33 [0/8000 (0%)]\tTotal Loss: 0.284285\n",
      "Reconstruction: 0.256289, Regularization: 0.027996\n",
      "2019-04-10 01:01:48,648 root         INFO     Train Epoch: 33 [512/8000 (6%)]\tTotal Loss: 0.293407\n",
      "Reconstruction: 0.266278, Regularization: 0.027129\n",
      "2019-04-10 01:01:48,711 root         INFO     Train Epoch: 33 [1024/8000 (13%)]\tTotal Loss: 0.276274\n",
      "Reconstruction: 0.251725, Regularization: 0.024549\n",
      "2019-04-10 01:01:48,775 root         INFO     Train Epoch: 33 [1536/8000 (19%)]\tTotal Loss: 0.279991\n",
      "Reconstruction: 0.252120, Regularization: 0.027870\n",
      "2019-04-10 01:01:48,839 root         INFO     Train Epoch: 33 [2048/8000 (26%)]\tTotal Loss: 0.280830\n",
      "Reconstruction: 0.249299, Regularization: 0.031532\n",
      "2019-04-10 01:01:48,903 root         INFO     Train Epoch: 33 [2560/8000 (32%)]\tTotal Loss: 0.282414\n",
      "Reconstruction: 0.254647, Regularization: 0.027766\n",
      "2019-04-10 01:01:48,968 root         INFO     Train Epoch: 33 [3072/8000 (38%)]\tTotal Loss: 0.266068\n",
      "Reconstruction: 0.243266, Regularization: 0.022802\n",
      "2019-04-10 01:01:49,033 root         INFO     Train Epoch: 33 [3584/8000 (45%)]\tTotal Loss: 0.278576\n",
      "Reconstruction: 0.251385, Regularization: 0.027191\n",
      "2019-04-10 01:01:49,098 root         INFO     Train Epoch: 33 [4096/8000 (51%)]\tTotal Loss: 0.278781\n",
      "Reconstruction: 0.254775, Regularization: 0.024006\n",
      "2019-04-10 01:01:49,162 root         INFO     Train Epoch: 33 [4608/8000 (58%)]\tTotal Loss: 0.285338\n",
      "Reconstruction: 0.260451, Regularization: 0.024887\n",
      "2019-04-10 01:01:49,227 root         INFO     Train Epoch: 33 [5120/8000 (64%)]\tTotal Loss: 0.266721\n",
      "Reconstruction: 0.244498, Regularization: 0.022223\n",
      "2019-04-10 01:01:49,291 root         INFO     Train Epoch: 33 [5632/8000 (70%)]\tTotal Loss: 0.288558\n",
      "Reconstruction: 0.261788, Regularization: 0.026771\n",
      "2019-04-10 01:01:49,356 root         INFO     Train Epoch: 33 [6144/8000 (77%)]\tTotal Loss: 0.279738\n",
      "Reconstruction: 0.255775, Regularization: 0.023963\n",
      "2019-04-10 01:01:49,421 root         INFO     Train Epoch: 33 [6656/8000 (83%)]\tTotal Loss: 0.289422\n",
      "Reconstruction: 0.262356, Regularization: 0.027067\n",
      "2019-04-10 01:01:49,485 root         INFO     Train Epoch: 33 [7168/8000 (90%)]\tTotal Loss: 0.271427\n",
      "Reconstruction: 0.246308, Regularization: 0.025118\n",
      "2019-04-10 01:01:49,549 root         INFO     Train Epoch: 33 [7680/8000 (96%)]\tTotal Loss: 0.268308\n",
      "Reconstruction: 0.248254, Regularization: 0.020053\n",
      "2019-04-10 01:01:49,605 root         INFO     ====> Epoch: 33 Average loss: 0.2798\n",
      "2019-04-10 01:01:49,628 root         INFO     Train Epoch: 34 [0/8000 (0%)]\tTotal Loss: 0.276434\n",
      "Reconstruction: 0.251009, Regularization: 0.025424\n",
      "2019-04-10 01:01:49,692 root         INFO     Train Epoch: 34 [512/8000 (6%)]\tTotal Loss: 0.289074\n",
      "Reconstruction: 0.263521, Regularization: 0.025553\n",
      "2019-04-10 01:01:49,756 root         INFO     Train Epoch: 34 [1024/8000 (13%)]\tTotal Loss: 0.270299\n",
      "Reconstruction: 0.250285, Regularization: 0.020013\n",
      "2019-04-10 01:01:49,819 root         INFO     Train Epoch: 34 [1536/8000 (19%)]\tTotal Loss: 0.271561\n",
      "Reconstruction: 0.248780, Regularization: 0.022781\n",
      "2019-04-10 01:01:49,882 root         INFO     Train Epoch: 34 [2048/8000 (26%)]\tTotal Loss: 0.271473\n",
      "Reconstruction: 0.245136, Regularization: 0.026337\n",
      "2019-04-10 01:01:49,945 root         INFO     Train Epoch: 34 [2560/8000 (32%)]\tTotal Loss: 0.265586\n",
      "Reconstruction: 0.245789, Regularization: 0.019796\n",
      "2019-04-10 01:01:50,008 root         INFO     Train Epoch: 34 [3072/8000 (38%)]\tTotal Loss: 0.278894\n",
      "Reconstruction: 0.252797, Regularization: 0.026097\n",
      "2019-04-10 01:01:50,071 root         INFO     Train Epoch: 34 [3584/8000 (45%)]\tTotal Loss: 0.270795\n",
      "Reconstruction: 0.250185, Regularization: 0.020610\n",
      "2019-04-10 01:01:50,133 root         INFO     Train Epoch: 34 [4096/8000 (51%)]\tTotal Loss: 0.268778\n",
      "Reconstruction: 0.247357, Regularization: 0.021421\n",
      "2019-04-10 01:01:50,196 root         INFO     Train Epoch: 34 [4608/8000 (58%)]\tTotal Loss: 0.271909\n",
      "Reconstruction: 0.251290, Regularization: 0.020619\n",
      "2019-04-10 01:01:50,258 root         INFO     Train Epoch: 34 [5120/8000 (64%)]\tTotal Loss: 0.268736\n",
      "Reconstruction: 0.245547, Regularization: 0.023189\n",
      "2019-04-10 01:01:50,321 root         INFO     Train Epoch: 34 [5632/8000 (70%)]\tTotal Loss: 0.277827\n",
      "Reconstruction: 0.256115, Regularization: 0.021712\n",
      "2019-04-10 01:01:50,383 root         INFO     Train Epoch: 34 [6144/8000 (77%)]\tTotal Loss: 0.278557\n",
      "Reconstruction: 0.257501, Regularization: 0.021056\n",
      "2019-04-10 01:01:50,445 root         INFO     Train Epoch: 34 [6656/8000 (83%)]\tTotal Loss: 0.271688\n",
      "Reconstruction: 0.251243, Regularization: 0.020445\n",
      "2019-04-10 01:01:50,508 root         INFO     Train Epoch: 34 [7168/8000 (90%)]\tTotal Loss: 0.270023\n",
      "Reconstruction: 0.252338, Regularization: 0.017684\n",
      "2019-04-10 01:01:50,570 root         INFO     Train Epoch: 34 [7680/8000 (96%)]\tTotal Loss: 0.273769\n",
      "Reconstruction: 0.248971, Regularization: 0.024798\n",
      "2019-04-10 01:01:50,623 root         INFO     ====> Epoch: 34 Average loss: 0.2752\n",
      "2019-04-10 01:01:50,647 root         INFO     Train Epoch: 35 [0/8000 (0%)]\tTotal Loss: 0.269769\n",
      "Reconstruction: 0.242628, Regularization: 0.027141\n",
      "2019-04-10 01:01:50,709 root         INFO     Train Epoch: 35 [512/8000 (6%)]\tTotal Loss: 0.276078\n",
      "Reconstruction: 0.251176, Regularization: 0.024903\n",
      "2019-04-10 01:01:50,770 root         INFO     Train Epoch: 35 [1024/8000 (13%)]\tTotal Loss: 0.280650\n",
      "Reconstruction: 0.260153, Regularization: 0.020497\n",
      "2019-04-10 01:01:50,832 root         INFO     Train Epoch: 35 [1536/8000 (19%)]\tTotal Loss: 0.275051\n",
      "Reconstruction: 0.249613, Regularization: 0.025438\n",
      "2019-04-10 01:01:50,893 root         INFO     Train Epoch: 35 [2048/8000 (26%)]\tTotal Loss: 0.269315\n",
      "Reconstruction: 0.249968, Regularization: 0.019347\n",
      "2019-04-10 01:01:50,955 root         INFO     Train Epoch: 35 [2560/8000 (32%)]\tTotal Loss: 0.264745\n",
      "Reconstruction: 0.247196, Regularization: 0.017548\n",
      "2019-04-10 01:01:51,017 root         INFO     Train Epoch: 35 [3072/8000 (38%)]\tTotal Loss: 0.269590\n",
      "Reconstruction: 0.249240, Regularization: 0.020351\n",
      "2019-04-10 01:01:51,078 root         INFO     Train Epoch: 35 [3584/8000 (45%)]\tTotal Loss: 0.273931\n",
      "Reconstruction: 0.253291, Regularization: 0.020641\n",
      "2019-04-10 01:01:51,140 root         INFO     Train Epoch: 35 [4096/8000 (51%)]\tTotal Loss: 0.268691\n",
      "Reconstruction: 0.247418, Regularization: 0.021273\n",
      "2019-04-10 01:01:51,202 root         INFO     Train Epoch: 35 [4608/8000 (58%)]\tTotal Loss: 0.272800\n",
      "Reconstruction: 0.251411, Regularization: 0.021389\n",
      "2019-04-10 01:01:51,263 root         INFO     Train Epoch: 35 [5120/8000 (64%)]\tTotal Loss: 0.273007\n",
      "Reconstruction: 0.252715, Regularization: 0.020292\n",
      "2019-04-10 01:01:51,324 root         INFO     Train Epoch: 35 [5632/8000 (70%)]\tTotal Loss: 0.261441\n",
      "Reconstruction: 0.243743, Regularization: 0.017698\n",
      "2019-04-10 01:01:51,386 root         INFO     Train Epoch: 35 [6144/8000 (77%)]\tTotal Loss: 0.260889\n",
      "Reconstruction: 0.238936, Regularization: 0.021954\n",
      "2019-04-10 01:01:51,449 root         INFO     Train Epoch: 35 [6656/8000 (83%)]\tTotal Loss: 0.273278\n",
      "Reconstruction: 0.252631, Regularization: 0.020646\n",
      "2019-04-10 01:01:51,513 root         INFO     Train Epoch: 35 [7168/8000 (90%)]\tTotal Loss: 0.266999\n",
      "Reconstruction: 0.246787, Regularization: 0.020212\n",
      "2019-04-10 01:01:51,578 root         INFO     Train Epoch: 35 [7680/8000 (96%)]\tTotal Loss: 0.282428\n",
      "Reconstruction: 0.259840, Regularization: 0.022588\n",
      "2019-04-10 01:01:51,631 root         INFO     ====> Epoch: 35 Average loss: 0.2712\n",
      "2019-04-10 01:01:51,655 root         INFO     Train Epoch: 36 [0/8000 (0%)]\tTotal Loss: 0.281872\n",
      "Reconstruction: 0.259634, Regularization: 0.022238\n",
      "2019-04-10 01:01:51,718 root         INFO     Train Epoch: 36 [512/8000 (6%)]\tTotal Loss: 0.261717\n",
      "Reconstruction: 0.239956, Regularization: 0.021760\n",
      "2019-04-10 01:01:51,781 root         INFO     Train Epoch: 36 [1024/8000 (13%)]\tTotal Loss: 0.286613\n",
      "Reconstruction: 0.266477, Regularization: 0.020136\n",
      "2019-04-10 01:01:51,843 root         INFO     Train Epoch: 36 [1536/8000 (19%)]\tTotal Loss: 0.267439\n",
      "Reconstruction: 0.248418, Regularization: 0.019021\n",
      "2019-04-10 01:01:51,906 root         INFO     Train Epoch: 36 [2048/8000 (26%)]\tTotal Loss: 0.275316\n",
      "Reconstruction: 0.256534, Regularization: 0.018782\n",
      "2019-04-10 01:01:51,969 root         INFO     Train Epoch: 36 [2560/8000 (32%)]\tTotal Loss: 0.258124\n",
      "Reconstruction: 0.239378, Regularization: 0.018747\n",
      "2019-04-10 01:01:52,032 root         INFO     Train Epoch: 36 [3072/8000 (38%)]\tTotal Loss: 0.266716\n",
      "Reconstruction: 0.249733, Regularization: 0.016983\n",
      "2019-04-10 01:01:52,094 root         INFO     Train Epoch: 36 [3584/8000 (45%)]\tTotal Loss: 0.272148\n",
      "Reconstruction: 0.251651, Regularization: 0.020498\n",
      "2019-04-10 01:01:52,157 root         INFO     Train Epoch: 36 [4096/8000 (51%)]\tTotal Loss: 0.262007\n",
      "Reconstruction: 0.245078, Regularization: 0.016929\n",
      "2019-04-10 01:01:52,221 root         INFO     Train Epoch: 36 [4608/8000 (58%)]\tTotal Loss: 0.250698\n",
      "Reconstruction: 0.234942, Regularization: 0.015756\n",
      "2019-04-10 01:01:52,283 root         INFO     Train Epoch: 36 [5120/8000 (64%)]\tTotal Loss: 0.267691\n",
      "Reconstruction: 0.251033, Regularization: 0.016658\n",
      "2019-04-10 01:01:52,346 root         INFO     Train Epoch: 36 [5632/8000 (70%)]\tTotal Loss: 0.267102\n",
      "Reconstruction: 0.248137, Regularization: 0.018965\n",
      "2019-04-10 01:01:52,409 root         INFO     Train Epoch: 36 [6144/8000 (77%)]\tTotal Loss: 0.269510\n",
      "Reconstruction: 0.252184, Regularization: 0.017326\n",
      "2019-04-10 01:01:52,472 root         INFO     Train Epoch: 36 [6656/8000 (83%)]\tTotal Loss: 0.276654\n",
      "Reconstruction: 0.260946, Regularization: 0.015708\n",
      "2019-04-10 01:01:52,535 root         INFO     Train Epoch: 36 [7168/8000 (90%)]\tTotal Loss: 0.269926\n",
      "Reconstruction: 0.248431, Regularization: 0.021495\n",
      "2019-04-10 01:01:52,598 root         INFO     Train Epoch: 36 [7680/8000 (96%)]\tTotal Loss: 0.265395\n",
      "Reconstruction: 0.247325, Regularization: 0.018070\n",
      "2019-04-10 01:01:52,651 root         INFO     ====> Epoch: 36 Average loss: 0.2672\n",
      "2019-04-10 01:01:52,675 root         INFO     Train Epoch: 37 [0/8000 (0%)]\tTotal Loss: 0.266170\n",
      "Reconstruction: 0.248590, Regularization: 0.017580\n",
      "2019-04-10 01:01:52,738 root         INFO     Train Epoch: 37 [512/8000 (6%)]\tTotal Loss: 0.263224\n",
      "Reconstruction: 0.245681, Regularization: 0.017542\n",
      "2019-04-10 01:01:52,801 root         INFO     Train Epoch: 37 [1024/8000 (13%)]\tTotal Loss: 0.260181\n",
      "Reconstruction: 0.245764, Regularization: 0.014417\n",
      "2019-04-10 01:01:52,864 root         INFO     Train Epoch: 37 [1536/8000 (19%)]\tTotal Loss: 0.265516\n",
      "Reconstruction: 0.249077, Regularization: 0.016439\n",
      "2019-04-10 01:01:52,927 root         INFO     Train Epoch: 37 [2048/8000 (26%)]\tTotal Loss: 0.271900\n",
      "Reconstruction: 0.254696, Regularization: 0.017204\n",
      "2019-04-10 01:01:52,990 root         INFO     Train Epoch: 37 [2560/8000 (32%)]\tTotal Loss: 0.262282\n",
      "Reconstruction: 0.245732, Regularization: 0.016550\n",
      "2019-04-10 01:01:53,053 root         INFO     Train Epoch: 37 [3072/8000 (38%)]\tTotal Loss: 0.271633\n",
      "Reconstruction: 0.256263, Regularization: 0.015370\n",
      "2019-04-10 01:01:53,116 root         INFO     Train Epoch: 37 [3584/8000 (45%)]\tTotal Loss: 0.258508\n",
      "Reconstruction: 0.242961, Regularization: 0.015546\n",
      "2019-04-10 01:01:53,180 root         INFO     Train Epoch: 37 [4096/8000 (51%)]\tTotal Loss: 0.270700\n",
      "Reconstruction: 0.255768, Regularization: 0.014932\n",
      "2019-04-10 01:01:53,243 root         INFO     Train Epoch: 37 [4608/8000 (58%)]\tTotal Loss: 0.257869\n",
      "Reconstruction: 0.239733, Regularization: 0.018135\n",
      "2019-04-10 01:01:53,306 root         INFO     Train Epoch: 37 [5120/8000 (64%)]\tTotal Loss: 0.263460\n",
      "Reconstruction: 0.247328, Regularization: 0.016133\n",
      "2019-04-10 01:01:53,369 root         INFO     Train Epoch: 37 [5632/8000 (70%)]\tTotal Loss: 0.257241\n",
      "Reconstruction: 0.242676, Regularization: 0.014565\n",
      "2019-04-10 01:01:53,431 root         INFO     Train Epoch: 37 [6144/8000 (77%)]\tTotal Loss: 0.269197\n",
      "Reconstruction: 0.255105, Regularization: 0.014093\n",
      "2019-04-10 01:01:53,493 root         INFO     Train Epoch: 37 [6656/8000 (83%)]\tTotal Loss: 0.268889\n",
      "Reconstruction: 0.251879, Regularization: 0.017010\n",
      "2019-04-10 01:01:53,555 root         INFO     Train Epoch: 37 [7168/8000 (90%)]\tTotal Loss: 0.269961\n",
      "Reconstruction: 0.255135, Regularization: 0.014826\n",
      "2019-04-10 01:01:53,618 root         INFO     Train Epoch: 37 [7680/8000 (96%)]\tTotal Loss: 0.264713\n",
      "Reconstruction: 0.249544, Regularization: 0.015169\n",
      "2019-04-10 01:01:53,672 root         INFO     ====> Epoch: 37 Average loss: 0.2634\n",
      "2019-04-10 01:01:53,696 root         INFO     Train Epoch: 38 [0/8000 (0%)]\tTotal Loss: 0.259124\n",
      "Reconstruction: 0.244078, Regularization: 0.015046\n",
      "2019-04-10 01:01:53,759 root         INFO     Train Epoch: 38 [512/8000 (6%)]\tTotal Loss: 0.264850\n",
      "Reconstruction: 0.250500, Regularization: 0.014349\n",
      "2019-04-10 01:01:53,821 root         INFO     Train Epoch: 38 [1024/8000 (13%)]\tTotal Loss: 0.255630\n",
      "Reconstruction: 0.242000, Regularization: 0.013629\n",
      "2019-04-10 01:01:53,883 root         INFO     Train Epoch: 38 [1536/8000 (19%)]\tTotal Loss: 0.264839\n",
      "Reconstruction: 0.250121, Regularization: 0.014717\n",
      "2019-04-10 01:01:53,946 root         INFO     Train Epoch: 38 [2048/8000 (26%)]\tTotal Loss: 0.262686\n",
      "Reconstruction: 0.249204, Regularization: 0.013482\n",
      "2019-04-10 01:01:54,009 root         INFO     Train Epoch: 38 [2560/8000 (32%)]\tTotal Loss: 0.255965\n",
      "Reconstruction: 0.242610, Regularization: 0.013355\n",
      "2019-04-10 01:01:54,071 root         INFO     Train Epoch: 38 [3072/8000 (38%)]\tTotal Loss: 0.262183\n",
      "Reconstruction: 0.249699, Regularization: 0.012483\n",
      "2019-04-10 01:01:54,134 root         INFO     Train Epoch: 38 [3584/8000 (45%)]\tTotal Loss: 0.267860\n",
      "Reconstruction: 0.250770, Regularization: 0.017090\n",
      "2019-04-10 01:01:54,196 root         INFO     Train Epoch: 38 [4096/8000 (51%)]\tTotal Loss: 0.256172\n",
      "Reconstruction: 0.240835, Regularization: 0.015338\n",
      "2019-04-10 01:01:54,259 root         INFO     Train Epoch: 38 [4608/8000 (58%)]\tTotal Loss: 0.266094\n",
      "Reconstruction: 0.252655, Regularization: 0.013439\n",
      "2019-04-10 01:01:54,322 root         INFO     Train Epoch: 38 [5120/8000 (64%)]\tTotal Loss: 0.255313\n",
      "Reconstruction: 0.241104, Regularization: 0.014209\n",
      "2019-04-10 01:01:54,384 root         INFO     Train Epoch: 38 [5632/8000 (70%)]\tTotal Loss: 0.264576\n",
      "Reconstruction: 0.251314, Regularization: 0.013261\n",
      "2019-04-10 01:01:54,447 root         INFO     Train Epoch: 38 [6144/8000 (77%)]\tTotal Loss: 0.253799\n",
      "Reconstruction: 0.241681, Regularization: 0.012118\n",
      "2019-04-10 01:01:54,510 root         INFO     Train Epoch: 38 [6656/8000 (83%)]\tTotal Loss: 0.258790\n",
      "Reconstruction: 0.245094, Regularization: 0.013696\n",
      "2019-04-10 01:01:54,572 root         INFO     Train Epoch: 38 [7168/8000 (90%)]\tTotal Loss: 0.257864\n",
      "Reconstruction: 0.245716, Regularization: 0.012149\n",
      "2019-04-10 01:01:54,634 root         INFO     Train Epoch: 38 [7680/8000 (96%)]\tTotal Loss: 0.264625\n",
      "Reconstruction: 0.251390, Regularization: 0.013236\n",
      "2019-04-10 01:01:54,688 root         INFO     ====> Epoch: 38 Average loss: 0.2603\n",
      "2019-04-10 01:01:54,712 root         INFO     Train Epoch: 39 [0/8000 (0%)]\tTotal Loss: 0.260523\n",
      "Reconstruction: 0.247334, Regularization: 0.013189\n",
      "2019-04-10 01:01:54,775 root         INFO     Train Epoch: 39 [512/8000 (6%)]\tTotal Loss: 0.255236\n",
      "Reconstruction: 0.241537, Regularization: 0.013699\n",
      "2019-04-10 01:01:54,838 root         INFO     Train Epoch: 39 [1024/8000 (13%)]\tTotal Loss: 0.268528\n",
      "Reconstruction: 0.254165, Regularization: 0.014363\n",
      "2019-04-10 01:01:54,901 root         INFO     Train Epoch: 39 [1536/8000 (19%)]\tTotal Loss: 0.266237\n",
      "Reconstruction: 0.254316, Regularization: 0.011921\n",
      "2019-04-10 01:01:54,964 root         INFO     Train Epoch: 39 [2048/8000 (26%)]\tTotal Loss: 0.252779\n",
      "Reconstruction: 0.241922, Regularization: 0.010857\n",
      "2019-04-10 01:01:55,026 root         INFO     Train Epoch: 39 [2560/8000 (32%)]\tTotal Loss: 0.258798\n",
      "Reconstruction: 0.245565, Regularization: 0.013233\n",
      "2019-04-10 01:01:55,089 root         INFO     Train Epoch: 39 [3072/8000 (38%)]\tTotal Loss: 0.258131\n",
      "Reconstruction: 0.246401, Regularization: 0.011730\n",
      "2019-04-10 01:01:55,152 root         INFO     Train Epoch: 39 [3584/8000 (45%)]\tTotal Loss: 0.262903\n",
      "Reconstruction: 0.250953, Regularization: 0.011950\n",
      "2019-04-10 01:01:55,214 root         INFO     Train Epoch: 39 [4096/8000 (51%)]\tTotal Loss: 0.263305\n",
      "Reconstruction: 0.250676, Regularization: 0.012629\n",
      "2019-04-10 01:01:55,276 root         INFO     Train Epoch: 39 [4608/8000 (58%)]\tTotal Loss: 0.256144\n",
      "Reconstruction: 0.244738, Regularization: 0.011406\n",
      "2019-04-10 01:01:55,337 root         INFO     Train Epoch: 39 [5120/8000 (64%)]\tTotal Loss: 0.255170\n",
      "Reconstruction: 0.242792, Regularization: 0.012378\n",
      "2019-04-10 01:01:55,398 root         INFO     Train Epoch: 39 [5632/8000 (70%)]\tTotal Loss: 0.272045\n",
      "Reconstruction: 0.260114, Regularization: 0.011931\n",
      "2019-04-10 01:01:55,460 root         INFO     Train Epoch: 39 [6144/8000 (77%)]\tTotal Loss: 0.262695\n",
      "Reconstruction: 0.250637, Regularization: 0.012058\n",
      "2019-04-10 01:01:55,522 root         INFO     Train Epoch: 39 [6656/8000 (83%)]\tTotal Loss: 0.259913\n",
      "Reconstruction: 0.247869, Regularization: 0.012044\n",
      "2019-04-10 01:01:55,584 root         INFO     Train Epoch: 39 [7168/8000 (90%)]\tTotal Loss: 0.249808\n",
      "Reconstruction: 0.239312, Regularization: 0.010496\n",
      "2019-04-10 01:01:55,645 root         INFO     Train Epoch: 39 [7680/8000 (96%)]\tTotal Loss: 0.255199\n",
      "Reconstruction: 0.242608, Regularization: 0.012591\n",
      "2019-04-10 01:01:55,698 root         INFO     ====> Epoch: 39 Average loss: 0.2575\n",
      "2019-04-10 01:01:55,722 root         INFO     Train Epoch: 40 [0/8000 (0%)]\tTotal Loss: 0.255190\n",
      "Reconstruction: 0.244075, Regularization: 0.011115\n",
      "2019-04-10 01:01:55,785 root         INFO     Train Epoch: 40 [512/8000 (6%)]\tTotal Loss: 0.253966\n",
      "Reconstruction: 0.241326, Regularization: 0.012640\n",
      "2019-04-10 01:01:55,849 root         INFO     Train Epoch: 40 [1024/8000 (13%)]\tTotal Loss: 0.249510\n",
      "Reconstruction: 0.238206, Regularization: 0.011304\n",
      "2019-04-10 01:01:55,911 root         INFO     Train Epoch: 40 [1536/8000 (19%)]\tTotal Loss: 0.255623\n",
      "Reconstruction: 0.245494, Regularization: 0.010129\n",
      "2019-04-10 01:01:55,975 root         INFO     Train Epoch: 40 [2048/8000 (26%)]\tTotal Loss: 0.245352\n",
      "Reconstruction: 0.235444, Regularization: 0.009908\n",
      "2019-04-10 01:01:56,038 root         INFO     Train Epoch: 40 [2560/8000 (32%)]\tTotal Loss: 0.254676\n",
      "Reconstruction: 0.244015, Regularization: 0.010661\n",
      "2019-04-10 01:01:56,100 root         INFO     Train Epoch: 40 [3072/8000 (38%)]\tTotal Loss: 0.260132\n",
      "Reconstruction: 0.247872, Regularization: 0.012260\n",
      "2019-04-10 01:01:56,163 root         INFO     Train Epoch: 40 [3584/8000 (45%)]\tTotal Loss: 0.264862\n",
      "Reconstruction: 0.253518, Regularization: 0.011344\n",
      "2019-04-10 01:01:56,226 root         INFO     Train Epoch: 40 [4096/8000 (51%)]\tTotal Loss: 0.249826\n",
      "Reconstruction: 0.238650, Regularization: 0.011176\n",
      "2019-04-10 01:01:56,290 root         INFO     Train Epoch: 40 [4608/8000 (58%)]\tTotal Loss: 0.258050\n",
      "Reconstruction: 0.246782, Regularization: 0.011268\n",
      "2019-04-10 01:01:56,353 root         INFO     Train Epoch: 40 [5120/8000 (64%)]\tTotal Loss: 0.256560\n",
      "Reconstruction: 0.245841, Regularization: 0.010719\n",
      "2019-04-10 01:01:56,416 root         INFO     Train Epoch: 40 [5632/8000 (70%)]\tTotal Loss: 0.252732\n",
      "Reconstruction: 0.242314, Regularization: 0.010418\n",
      "2019-04-10 01:01:56,479 root         INFO     Train Epoch: 40 [6144/8000 (77%)]\tTotal Loss: 0.249722\n",
      "Reconstruction: 0.239148, Regularization: 0.010574\n",
      "2019-04-10 01:01:56,542 root         INFO     Train Epoch: 40 [6656/8000 (83%)]\tTotal Loss: 0.247986\n",
      "Reconstruction: 0.236958, Regularization: 0.011028\n",
      "2019-04-10 01:01:56,605 root         INFO     Train Epoch: 40 [7168/8000 (90%)]\tTotal Loss: 0.259353\n",
      "Reconstruction: 0.248871, Regularization: 0.010482\n",
      "2019-04-10 01:01:56,668 root         INFO     Train Epoch: 40 [7680/8000 (96%)]\tTotal Loss: 0.259222\n",
      "Reconstruction: 0.247829, Regularization: 0.011393\n",
      "2019-04-10 01:01:56,722 root         INFO     ====> Epoch: 40 Average loss: 0.2545\n",
      "2019-04-10 01:01:56,746 root         INFO     Train Epoch: 41 [0/8000 (0%)]\tTotal Loss: 0.256488\n",
      "Reconstruction: 0.247464, Regularization: 0.009024\n",
      "2019-04-10 01:01:56,808 root         INFO     Train Epoch: 41 [512/8000 (6%)]\tTotal Loss: 0.253895\n",
      "Reconstruction: 0.245017, Regularization: 0.008877\n",
      "2019-04-10 01:01:56,869 root         INFO     Train Epoch: 41 [1024/8000 (13%)]\tTotal Loss: 0.262355\n",
      "Reconstruction: 0.252930, Regularization: 0.009425\n",
      "2019-04-10 01:01:56,931 root         INFO     Train Epoch: 41 [1536/8000 (19%)]\tTotal Loss: 0.249435\n",
      "Reconstruction: 0.240696, Regularization: 0.008739\n",
      "2019-04-10 01:01:56,993 root         INFO     Train Epoch: 41 [2048/8000 (26%)]\tTotal Loss: 0.249666\n",
      "Reconstruction: 0.240441, Regularization: 0.009225\n",
      "2019-04-10 01:01:57,056 root         INFO     Train Epoch: 41 [2560/8000 (32%)]\tTotal Loss: 0.247734\n",
      "Reconstruction: 0.238755, Regularization: 0.008979\n",
      "2019-04-10 01:01:57,118 root         INFO     Train Epoch: 41 [3072/8000 (38%)]\tTotal Loss: 0.255015\n",
      "Reconstruction: 0.244653, Regularization: 0.010362\n",
      "2019-04-10 01:01:57,181 root         INFO     Train Epoch: 41 [3584/8000 (45%)]\tTotal Loss: 0.255180\n",
      "Reconstruction: 0.246096, Regularization: 0.009085\n",
      "2019-04-10 01:01:57,243 root         INFO     Train Epoch: 41 [4096/8000 (51%)]\tTotal Loss: 0.249844\n",
      "Reconstruction: 0.240972, Regularization: 0.008873\n",
      "2019-04-10 01:01:57,306 root         INFO     Train Epoch: 41 [4608/8000 (58%)]\tTotal Loss: 0.248082\n",
      "Reconstruction: 0.240803, Regularization: 0.007279\n",
      "2019-04-10 01:01:57,368 root         INFO     Train Epoch: 41 [5120/8000 (64%)]\tTotal Loss: 0.255439\n",
      "Reconstruction: 0.246668, Regularization: 0.008772\n",
      "2019-04-10 01:01:57,430 root         INFO     Train Epoch: 41 [5632/8000 (70%)]\tTotal Loss: 0.250731\n",
      "Reconstruction: 0.242276, Regularization: 0.008455\n",
      "2019-04-10 01:01:57,492 root         INFO     Train Epoch: 41 [6144/8000 (77%)]\tTotal Loss: 0.243497\n",
      "Reconstruction: 0.236100, Regularization: 0.007397\n",
      "2019-04-10 01:01:57,555 root         INFO     Train Epoch: 41 [6656/8000 (83%)]\tTotal Loss: 0.247911\n",
      "Reconstruction: 0.239974, Regularization: 0.007937\n",
      "2019-04-10 01:01:57,617 root         INFO     Train Epoch: 41 [7168/8000 (90%)]\tTotal Loss: 0.263153\n",
      "Reconstruction: 0.255086, Regularization: 0.008068\n",
      "2019-04-10 01:01:57,680 root         INFO     Train Epoch: 41 [7680/8000 (96%)]\tTotal Loss: 0.243949\n",
      "Reconstruction: 0.237049, Regularization: 0.006900\n",
      "2019-04-10 01:01:57,733 root         INFO     ====> Epoch: 41 Average loss: 0.2515\n",
      "2019-04-10 01:01:57,757 root         INFO     Train Epoch: 42 [0/8000 (0%)]\tTotal Loss: 0.255725\n",
      "Reconstruction: 0.247103, Regularization: 0.008622\n",
      "2019-04-10 01:01:57,821 root         INFO     Train Epoch: 42 [512/8000 (6%)]\tTotal Loss: 0.257551\n",
      "Reconstruction: 0.248411, Regularization: 0.009139\n",
      "2019-04-10 01:01:57,884 root         INFO     Train Epoch: 42 [1024/8000 (13%)]\tTotal Loss: 0.255035\n",
      "Reconstruction: 0.246805, Regularization: 0.008230\n",
      "2019-04-10 01:01:57,946 root         INFO     Train Epoch: 42 [1536/8000 (19%)]\tTotal Loss: 0.248955\n",
      "Reconstruction: 0.240268, Regularization: 0.008687\n",
      "2019-04-10 01:01:58,010 root         INFO     Train Epoch: 42 [2048/8000 (26%)]\tTotal Loss: 0.249822\n",
      "Reconstruction: 0.241429, Regularization: 0.008393\n",
      "2019-04-10 01:01:58,074 root         INFO     Train Epoch: 42 [2560/8000 (32%)]\tTotal Loss: 0.247010\n",
      "Reconstruction: 0.239718, Regularization: 0.007292\n",
      "2019-04-10 01:01:58,137 root         INFO     Train Epoch: 42 [3072/8000 (38%)]\tTotal Loss: 0.253261\n",
      "Reconstruction: 0.245030, Regularization: 0.008230\n",
      "2019-04-10 01:01:58,201 root         INFO     Train Epoch: 42 [3584/8000 (45%)]\tTotal Loss: 0.250950\n",
      "Reconstruction: 0.242923, Regularization: 0.008028\n",
      "2019-04-10 01:01:58,264 root         INFO     Train Epoch: 42 [4096/8000 (51%)]\tTotal Loss: 0.256346\n",
      "Reconstruction: 0.247886, Regularization: 0.008459\n",
      "2019-04-10 01:01:58,327 root         INFO     Train Epoch: 42 [4608/8000 (58%)]\tTotal Loss: 0.256450\n",
      "Reconstruction: 0.248207, Regularization: 0.008243\n",
      "2019-04-10 01:01:58,389 root         INFO     Train Epoch: 42 [5120/8000 (64%)]\tTotal Loss: 0.253219\n",
      "Reconstruction: 0.245561, Regularization: 0.007658\n",
      "2019-04-10 01:01:58,451 root         INFO     Train Epoch: 42 [5632/8000 (70%)]\tTotal Loss: 0.249157\n",
      "Reconstruction: 0.241802, Regularization: 0.007355\n",
      "2019-04-10 01:01:58,512 root         INFO     Train Epoch: 42 [6144/8000 (77%)]\tTotal Loss: 0.247304\n",
      "Reconstruction: 0.240846, Regularization: 0.006458\n",
      "2019-04-10 01:01:58,574 root         INFO     Train Epoch: 42 [6656/8000 (83%)]\tTotal Loss: 0.243527\n",
      "Reconstruction: 0.236060, Regularization: 0.007468\n",
      "2019-04-10 01:01:58,636 root         INFO     Train Epoch: 42 [7168/8000 (90%)]\tTotal Loss: 0.248360\n",
      "Reconstruction: 0.240911, Regularization: 0.007449\n",
      "2019-04-10 01:01:58,698 root         INFO     Train Epoch: 42 [7680/8000 (96%)]\tTotal Loss: 0.244442\n",
      "Reconstruction: 0.236453, Regularization: 0.007990\n",
      "2019-04-10 01:01:58,752 root         INFO     ====> Epoch: 42 Average loss: 0.2488\n",
      "2019-04-10 01:01:58,776 root         INFO     Train Epoch: 43 [0/8000 (0%)]\tTotal Loss: 0.244367\n",
      "Reconstruction: 0.236665, Regularization: 0.007702\n",
      "2019-04-10 01:01:58,840 root         INFO     Train Epoch: 43 [512/8000 (6%)]\tTotal Loss: 0.241970\n",
      "Reconstruction: 0.236218, Regularization: 0.005752\n",
      "2019-04-10 01:01:58,904 root         INFO     Train Epoch: 43 [1024/8000 (13%)]\tTotal Loss: 0.255301\n",
      "Reconstruction: 0.247989, Regularization: 0.007311\n",
      "2019-04-10 01:01:58,967 root         INFO     Train Epoch: 43 [1536/8000 (19%)]\tTotal Loss: 0.250560\n",
      "Reconstruction: 0.244213, Regularization: 0.006347\n",
      "2019-04-10 01:01:59,030 root         INFO     Train Epoch: 43 [2048/8000 (26%)]\tTotal Loss: 0.247893\n",
      "Reconstruction: 0.241032, Regularization: 0.006861\n",
      "2019-04-10 01:01:59,095 root         INFO     Train Epoch: 43 [2560/8000 (32%)]\tTotal Loss: 0.254892\n",
      "Reconstruction: 0.248439, Regularization: 0.006453\n",
      "2019-04-10 01:01:59,157 root         INFO     Train Epoch: 43 [3072/8000 (38%)]\tTotal Loss: 0.242193\n",
      "Reconstruction: 0.236889, Regularization: 0.005305\n",
      "2019-04-10 01:01:59,220 root         INFO     Train Epoch: 43 [3584/8000 (45%)]\tTotal Loss: 0.245155\n",
      "Reconstruction: 0.238799, Regularization: 0.006356\n",
      "2019-04-10 01:01:59,282 root         INFO     Train Epoch: 43 [4096/8000 (51%)]\tTotal Loss: 0.265107\n",
      "Reconstruction: 0.257795, Regularization: 0.007312\n",
      "2019-04-10 01:01:59,344 root         INFO     Train Epoch: 43 [4608/8000 (58%)]\tTotal Loss: 0.252712\n",
      "Reconstruction: 0.245087, Regularization: 0.007625\n",
      "2019-04-10 01:01:59,407 root         INFO     Train Epoch: 43 [5120/8000 (64%)]\tTotal Loss: 0.250275\n",
      "Reconstruction: 0.242852, Regularization: 0.007423\n",
      "2019-04-10 01:01:59,470 root         INFO     Train Epoch: 43 [5632/8000 (70%)]\tTotal Loss: 0.242902\n",
      "Reconstruction: 0.236374, Regularization: 0.006528\n",
      "2019-04-10 01:01:59,533 root         INFO     Train Epoch: 43 [6144/8000 (77%)]\tTotal Loss: 0.251526\n",
      "Reconstruction: 0.245472, Regularization: 0.006054\n",
      "2019-04-10 01:01:59,596 root         INFO     Train Epoch: 43 [6656/8000 (83%)]\tTotal Loss: 0.240391\n",
      "Reconstruction: 0.234698, Regularization: 0.005694\n",
      "2019-04-10 01:01:59,659 root         INFO     Train Epoch: 43 [7168/8000 (90%)]\tTotal Loss: 0.240462\n",
      "Reconstruction: 0.234185, Regularization: 0.006277\n",
      "2019-04-10 01:01:59,722 root         INFO     Train Epoch: 43 [7680/8000 (96%)]\tTotal Loss: 0.240075\n",
      "Reconstruction: 0.233638, Regularization: 0.006437\n",
      "2019-04-10 01:01:59,776 root         INFO     ====> Epoch: 43 Average loss: 0.2462\n",
      "2019-04-10 01:01:59,800 root         INFO     Train Epoch: 44 [0/8000 (0%)]\tTotal Loss: 0.233698\n",
      "Reconstruction: 0.228763, Regularization: 0.004935\n",
      "2019-04-10 01:01:59,863 root         INFO     Train Epoch: 44 [512/8000 (6%)]\tTotal Loss: 0.241620\n",
      "Reconstruction: 0.236103, Regularization: 0.005516\n",
      "2019-04-10 01:01:59,927 root         INFO     Train Epoch: 44 [1024/8000 (13%)]\tTotal Loss: 0.249113\n",
      "Reconstruction: 0.243133, Regularization: 0.005980\n",
      "2019-04-10 01:01:59,990 root         INFO     Train Epoch: 44 [1536/8000 (19%)]\tTotal Loss: 0.238875\n",
      "Reconstruction: 0.232867, Regularization: 0.006009\n",
      "2019-04-10 01:02:00,053 root         INFO     Train Epoch: 44 [2048/8000 (26%)]\tTotal Loss: 0.242241\n",
      "Reconstruction: 0.235983, Regularization: 0.006257\n",
      "2019-04-10 01:02:00,116 root         INFO     Train Epoch: 44 [2560/8000 (32%)]\tTotal Loss: 0.248785\n",
      "Reconstruction: 0.242227, Regularization: 0.006558\n",
      "2019-04-10 01:02:00,179 root         INFO     Train Epoch: 44 [3072/8000 (38%)]\tTotal Loss: 0.247895\n",
      "Reconstruction: 0.242151, Regularization: 0.005744\n",
      "2019-04-10 01:02:00,242 root         INFO     Train Epoch: 44 [3584/8000 (45%)]\tTotal Loss: 0.246302\n",
      "Reconstruction: 0.240528, Regularization: 0.005774\n",
      "2019-04-10 01:02:00,304 root         INFO     Train Epoch: 44 [4096/8000 (51%)]\tTotal Loss: 0.243787\n",
      "Reconstruction: 0.238303, Regularization: 0.005484\n",
      "2019-04-10 01:02:00,365 root         INFO     Train Epoch: 44 [4608/8000 (58%)]\tTotal Loss: 0.244223\n",
      "Reconstruction: 0.238816, Regularization: 0.005408\n",
      "2019-04-10 01:02:00,427 root         INFO     Train Epoch: 44 [5120/8000 (64%)]\tTotal Loss: 0.249136\n",
      "Reconstruction: 0.243195, Regularization: 0.005941\n",
      "2019-04-10 01:02:00,491 root         INFO     Train Epoch: 44 [5632/8000 (70%)]\tTotal Loss: 0.249059\n",
      "Reconstruction: 0.243268, Regularization: 0.005791\n",
      "2019-04-10 01:02:00,552 root         INFO     Train Epoch: 44 [6144/8000 (77%)]\tTotal Loss: 0.243646\n",
      "Reconstruction: 0.237724, Regularization: 0.005922\n",
      "2019-04-10 01:02:00,614 root         INFO     Train Epoch: 44 [6656/8000 (83%)]\tTotal Loss: 0.240220\n",
      "Reconstruction: 0.235733, Regularization: 0.004486\n",
      "2019-04-10 01:02:00,677 root         INFO     Train Epoch: 44 [7168/8000 (90%)]\tTotal Loss: 0.248164\n",
      "Reconstruction: 0.242483, Regularization: 0.005681\n",
      "2019-04-10 01:02:00,740 root         INFO     Train Epoch: 44 [7680/8000 (96%)]\tTotal Loss: 0.241389\n",
      "Reconstruction: 0.236741, Regularization: 0.004648\n",
      "2019-04-10 01:02:00,794 root         INFO     ====> Epoch: 44 Average loss: 0.2432\n",
      "2019-04-10 01:02:00,818 root         INFO     Train Epoch: 45 [0/8000 (0%)]\tTotal Loss: 0.251141\n",
      "Reconstruction: 0.245646, Regularization: 0.005494\n",
      "2019-04-10 01:02:00,881 root         INFO     Train Epoch: 45 [512/8000 (6%)]\tTotal Loss: 0.244734\n",
      "Reconstruction: 0.239515, Regularization: 0.005219\n",
      "2019-04-10 01:02:00,945 root         INFO     Train Epoch: 45 [1024/8000 (13%)]\tTotal Loss: 0.246245\n",
      "Reconstruction: 0.241066, Regularization: 0.005179\n",
      "2019-04-10 01:02:01,008 root         INFO     Train Epoch: 45 [1536/8000 (19%)]\tTotal Loss: 0.252612\n",
      "Reconstruction: 0.246854, Regularization: 0.005758\n",
      "2019-04-10 01:02:01,072 root         INFO     Train Epoch: 45 [2048/8000 (26%)]\tTotal Loss: 0.240680\n",
      "Reconstruction: 0.236102, Regularization: 0.004578\n",
      "2019-04-10 01:02:01,135 root         INFO     Train Epoch: 45 [2560/8000 (32%)]\tTotal Loss: 0.253000\n",
      "Reconstruction: 0.247246, Regularization: 0.005754\n",
      "2019-04-10 01:02:01,198 root         INFO     Train Epoch: 45 [3072/8000 (38%)]\tTotal Loss: 0.244877\n",
      "Reconstruction: 0.239995, Regularization: 0.004882\n",
      "2019-04-10 01:02:01,262 root         INFO     Train Epoch: 45 [3584/8000 (45%)]\tTotal Loss: 0.244735\n",
      "Reconstruction: 0.240002, Regularization: 0.004733\n",
      "2019-04-10 01:02:01,326 root         INFO     Train Epoch: 45 [4096/8000 (51%)]\tTotal Loss: 0.233266\n",
      "Reconstruction: 0.228908, Regularization: 0.004359\n",
      "2019-04-10 01:02:01,390 root         INFO     Train Epoch: 45 [4608/8000 (58%)]\tTotal Loss: 0.246454\n",
      "Reconstruction: 0.241139, Regularization: 0.005315\n",
      "2019-04-10 01:02:01,453 root         INFO     Train Epoch: 45 [5120/8000 (64%)]\tTotal Loss: 0.238073\n",
      "Reconstruction: 0.233919, Regularization: 0.004154\n",
      "2019-04-10 01:02:01,517 root         INFO     Train Epoch: 45 [5632/8000 (70%)]\tTotal Loss: 0.233324\n",
      "Reconstruction: 0.229719, Regularization: 0.003605\n",
      "2019-04-10 01:02:01,580 root         INFO     Train Epoch: 45 [6144/8000 (77%)]\tTotal Loss: 0.234226\n",
      "Reconstruction: 0.230201, Regularization: 0.004024\n",
      "2019-04-10 01:02:01,643 root         INFO     Train Epoch: 45 [6656/8000 (83%)]\tTotal Loss: 0.232822\n",
      "Reconstruction: 0.228859, Regularization: 0.003963\n",
      "2019-04-10 01:02:01,706 root         INFO     Train Epoch: 45 [7168/8000 (90%)]\tTotal Loss: 0.239992\n",
      "Reconstruction: 0.235808, Regularization: 0.004185\n",
      "2019-04-10 01:02:01,769 root         INFO     Train Epoch: 45 [7680/8000 (96%)]\tTotal Loss: 0.237402\n",
      "Reconstruction: 0.233384, Regularization: 0.004018\n",
      "2019-04-10 01:02:01,823 root         INFO     ====> Epoch: 45 Average loss: 0.2409\n",
      "2019-04-10 01:02:01,848 root         INFO     Train Epoch: 46 [0/8000 (0%)]\tTotal Loss: 0.239180\n",
      "Reconstruction: 0.235424, Regularization: 0.003755\n",
      "2019-04-10 01:02:01,912 root         INFO     Train Epoch: 46 [512/8000 (6%)]\tTotal Loss: 0.236454\n",
      "Reconstruction: 0.232962, Regularization: 0.003492\n",
      "2019-04-10 01:02:01,975 root         INFO     Train Epoch: 46 [1024/8000 (13%)]\tTotal Loss: 0.250512\n",
      "Reconstruction: 0.245904, Regularization: 0.004609\n",
      "2019-04-10 01:02:02,039 root         INFO     Train Epoch: 46 [1536/8000 (19%)]\tTotal Loss: 0.231164\n",
      "Reconstruction: 0.227569, Regularization: 0.003595\n",
      "2019-04-10 01:02:02,102 root         INFO     Train Epoch: 46 [2048/8000 (26%)]\tTotal Loss: 0.238044\n",
      "Reconstruction: 0.233602, Regularization: 0.004442\n",
      "2019-04-10 01:02:02,166 root         INFO     Train Epoch: 46 [2560/8000 (32%)]\tTotal Loss: 0.237679\n",
      "Reconstruction: 0.233671, Regularization: 0.004008\n",
      "2019-04-10 01:02:02,230 root         INFO     Train Epoch: 46 [3072/8000 (38%)]\tTotal Loss: 0.243089\n",
      "Reconstruction: 0.238570, Regularization: 0.004519\n",
      "2019-04-10 01:02:02,293 root         INFO     Train Epoch: 46 [3584/8000 (45%)]\tTotal Loss: 0.244415\n",
      "Reconstruction: 0.239582, Regularization: 0.004833\n",
      "2019-04-10 01:02:02,356 root         INFO     Train Epoch: 46 [4096/8000 (51%)]\tTotal Loss: 0.248121\n",
      "Reconstruction: 0.243484, Regularization: 0.004637\n",
      "2019-04-10 01:02:02,419 root         INFO     Train Epoch: 46 [4608/8000 (58%)]\tTotal Loss: 0.237704\n",
      "Reconstruction: 0.233869, Regularization: 0.003835\n",
      "2019-04-10 01:02:02,483 root         INFO     Train Epoch: 46 [5120/8000 (64%)]\tTotal Loss: 0.238896\n",
      "Reconstruction: 0.235021, Regularization: 0.003876\n",
      "2019-04-10 01:02:02,547 root         INFO     Train Epoch: 46 [5632/8000 (70%)]\tTotal Loss: 0.239798\n",
      "Reconstruction: 0.235922, Regularization: 0.003876\n",
      "2019-04-10 01:02:02,610 root         INFO     Train Epoch: 46 [6144/8000 (77%)]\tTotal Loss: 0.237841\n",
      "Reconstruction: 0.234288, Regularization: 0.003553\n",
      "2019-04-10 01:02:02,674 root         INFO     Train Epoch: 46 [6656/8000 (83%)]\tTotal Loss: 0.238833\n",
      "Reconstruction: 0.234621, Regularization: 0.004212\n",
      "2019-04-10 01:02:02,737 root         INFO     Train Epoch: 46 [7168/8000 (90%)]\tTotal Loss: 0.241573\n",
      "Reconstruction: 0.237669, Regularization: 0.003904\n",
      "2019-04-10 01:02:02,800 root         INFO     Train Epoch: 46 [7680/8000 (96%)]\tTotal Loss: 0.234916\n",
      "Reconstruction: 0.231433, Regularization: 0.003483\n",
      "2019-04-10 01:02:02,855 root         INFO     ====> Epoch: 46 Average loss: 0.2388\n",
      "2019-04-10 01:02:02,879 root         INFO     Train Epoch: 47 [0/8000 (0%)]\tTotal Loss: 0.236824\n",
      "Reconstruction: 0.232513, Regularization: 0.004311\n",
      "2019-04-10 01:02:02,942 root         INFO     Train Epoch: 47 [512/8000 (6%)]\tTotal Loss: 0.242658\n",
      "Reconstruction: 0.239094, Regularization: 0.003564\n",
      "2019-04-10 01:02:03,005 root         INFO     Train Epoch: 47 [1024/8000 (13%)]\tTotal Loss: 0.236225\n",
      "Reconstruction: 0.232515, Regularization: 0.003710\n",
      "2019-04-10 01:02:03,069 root         INFO     Train Epoch: 47 [1536/8000 (19%)]\tTotal Loss: 0.229869\n",
      "Reconstruction: 0.227083, Regularization: 0.002786\n",
      "2019-04-10 01:02:03,132 root         INFO     Train Epoch: 47 [2048/8000 (26%)]\tTotal Loss: 0.233726\n",
      "Reconstruction: 0.230332, Regularization: 0.003394\n",
      "2019-04-10 01:02:03,195 root         INFO     Train Epoch: 47 [2560/8000 (32%)]\tTotal Loss: 0.236610\n",
      "Reconstruction: 0.232933, Regularization: 0.003677\n",
      "2019-04-10 01:02:03,258 root         INFO     Train Epoch: 47 [3072/8000 (38%)]\tTotal Loss: 0.238837\n",
      "Reconstruction: 0.235263, Regularization: 0.003574\n",
      "2019-04-10 01:02:03,321 root         INFO     Train Epoch: 47 [3584/8000 (45%)]\tTotal Loss: 0.234106\n",
      "Reconstruction: 0.230995, Regularization: 0.003111\n",
      "2019-04-10 01:02:03,384 root         INFO     Train Epoch: 47 [4096/8000 (51%)]\tTotal Loss: 0.233044\n",
      "Reconstruction: 0.230261, Regularization: 0.002783\n",
      "2019-04-10 01:02:03,446 root         INFO     Train Epoch: 47 [4608/8000 (58%)]\tTotal Loss: 0.242700\n",
      "Reconstruction: 0.239212, Regularization: 0.003488\n",
      "2019-04-10 01:02:03,509 root         INFO     Train Epoch: 47 [5120/8000 (64%)]\tTotal Loss: 0.233803\n",
      "Reconstruction: 0.230849, Regularization: 0.002953\n",
      "2019-04-10 01:02:03,572 root         INFO     Train Epoch: 47 [5632/8000 (70%)]\tTotal Loss: 0.243297\n",
      "Reconstruction: 0.240093, Regularization: 0.003204\n",
      "2019-04-10 01:02:03,634 root         INFO     Train Epoch: 47 [6144/8000 (77%)]\tTotal Loss: 0.233283\n",
      "Reconstruction: 0.230231, Regularization: 0.003051\n",
      "2019-04-10 01:02:03,696 root         INFO     Train Epoch: 47 [6656/8000 (83%)]\tTotal Loss: 0.231729\n",
      "Reconstruction: 0.228708, Regularization: 0.003022\n",
      "2019-04-10 01:02:03,759 root         INFO     Train Epoch: 47 [7168/8000 (90%)]\tTotal Loss: 0.233222\n",
      "Reconstruction: 0.230204, Regularization: 0.003018\n",
      "2019-04-10 01:02:03,819 root         INFO     Train Epoch: 47 [7680/8000 (96%)]\tTotal Loss: 0.231181\n",
      "Reconstruction: 0.228470, Regularization: 0.002711\n",
      "2019-04-10 01:02:03,872 root         INFO     ====> Epoch: 47 Average loss: 0.2359\n",
      "2019-04-10 01:02:03,895 root         INFO     Train Epoch: 48 [0/8000 (0%)]\tTotal Loss: 0.238054\n",
      "Reconstruction: 0.235710, Regularization: 0.002343\n",
      "2019-04-10 01:02:03,957 root         INFO     Train Epoch: 48 [512/8000 (6%)]\tTotal Loss: 0.241053\n",
      "Reconstruction: 0.238295, Regularization: 0.002758\n",
      "2019-04-10 01:02:04,019 root         INFO     Train Epoch: 48 [1024/8000 (13%)]\tTotal Loss: 0.233769\n",
      "Reconstruction: 0.230882, Regularization: 0.002887\n",
      "2019-04-10 01:02:04,081 root         INFO     Train Epoch: 48 [1536/8000 (19%)]\tTotal Loss: 0.228433\n",
      "Reconstruction: 0.225687, Regularization: 0.002746\n",
      "2019-04-10 01:02:04,144 root         INFO     Train Epoch: 48 [2048/8000 (26%)]\tTotal Loss: 0.231921\n",
      "Reconstruction: 0.229144, Regularization: 0.002778\n",
      "2019-04-10 01:02:04,206 root         INFO     Train Epoch: 48 [2560/8000 (32%)]\tTotal Loss: 0.234450\n",
      "Reconstruction: 0.232090, Regularization: 0.002359\n",
      "2019-04-10 01:02:04,268 root         INFO     Train Epoch: 48 [3072/8000 (38%)]\tTotal Loss: 0.229726\n",
      "Reconstruction: 0.227397, Regularization: 0.002330\n",
      "2019-04-10 01:02:04,331 root         INFO     Train Epoch: 48 [3584/8000 (45%)]\tTotal Loss: 0.229598\n",
      "Reconstruction: 0.227167, Regularization: 0.002431\n",
      "2019-04-10 01:02:04,393 root         INFO     Train Epoch: 48 [4096/8000 (51%)]\tTotal Loss: 0.230113\n",
      "Reconstruction: 0.227865, Regularization: 0.002248\n",
      "2019-04-10 01:02:04,455 root         INFO     Train Epoch: 48 [4608/8000 (58%)]\tTotal Loss: 0.233300\n",
      "Reconstruction: 0.231169, Regularization: 0.002131\n",
      "2019-04-10 01:02:04,518 root         INFO     Train Epoch: 48 [5120/8000 (64%)]\tTotal Loss: 0.237208\n",
      "Reconstruction: 0.234285, Regularization: 0.002924\n",
      "2019-04-10 01:02:04,580 root         INFO     Train Epoch: 48 [5632/8000 (70%)]\tTotal Loss: 0.233455\n",
      "Reconstruction: 0.231296, Regularization: 0.002159\n",
      "2019-04-10 01:02:04,643 root         INFO     Train Epoch: 48 [6144/8000 (77%)]\tTotal Loss: 0.251628\n",
      "Reconstruction: 0.248556, Regularization: 0.003072\n",
      "2019-04-10 01:02:04,706 root         INFO     Train Epoch: 48 [6656/8000 (83%)]\tTotal Loss: 0.231230\n",
      "Reconstruction: 0.229068, Regularization: 0.002161\n",
      "2019-04-10 01:02:04,769 root         INFO     Train Epoch: 48 [7168/8000 (90%)]\tTotal Loss: 0.231602\n",
      "Reconstruction: 0.229336, Regularization: 0.002266\n",
      "2019-04-10 01:02:04,831 root         INFO     Train Epoch: 48 [7680/8000 (96%)]\tTotal Loss: 0.228330\n",
      "Reconstruction: 0.226239, Regularization: 0.002091\n",
      "2019-04-10 01:02:04,885 root         INFO     ====> Epoch: 48 Average loss: 0.2337\n",
      "2019-04-10 01:02:04,909 root         INFO     Train Epoch: 49 [0/8000 (0%)]\tTotal Loss: 0.228270\n",
      "Reconstruction: 0.225908, Regularization: 0.002362\n",
      "2019-04-10 01:02:04,973 root         INFO     Train Epoch: 49 [512/8000 (6%)]\tTotal Loss: 0.230940\n",
      "Reconstruction: 0.228856, Regularization: 0.002084\n",
      "2019-04-10 01:02:05,036 root         INFO     Train Epoch: 49 [1024/8000 (13%)]\tTotal Loss: 0.221634\n",
      "Reconstruction: 0.219359, Regularization: 0.002275\n",
      "2019-04-10 01:02:05,098 root         INFO     Train Epoch: 49 [1536/8000 (19%)]\tTotal Loss: 0.238614\n",
      "Reconstruction: 0.235894, Regularization: 0.002721\n",
      "2019-04-10 01:02:05,160 root         INFO     Train Epoch: 49 [2048/8000 (26%)]\tTotal Loss: 0.227733\n",
      "Reconstruction: 0.226108, Regularization: 0.001625\n",
      "2019-04-10 01:02:05,223 root         INFO     Train Epoch: 49 [2560/8000 (32%)]\tTotal Loss: 0.232567\n",
      "Reconstruction: 0.230534, Regularization: 0.002033\n",
      "2019-04-10 01:02:05,286 root         INFO     Train Epoch: 49 [3072/8000 (38%)]\tTotal Loss: 0.224947\n",
      "Reconstruction: 0.223019, Regularization: 0.001928\n",
      "2019-04-10 01:02:05,348 root         INFO     Train Epoch: 49 [3584/8000 (45%)]\tTotal Loss: 0.233994\n",
      "Reconstruction: 0.231550, Regularization: 0.002444\n",
      "2019-04-10 01:02:05,410 root         INFO     Train Epoch: 49 [4096/8000 (51%)]\tTotal Loss: 0.228335\n",
      "Reconstruction: 0.226451, Regularization: 0.001883\n",
      "2019-04-10 01:02:05,474 root         INFO     Train Epoch: 49 [4608/8000 (58%)]\tTotal Loss: 0.228076\n",
      "Reconstruction: 0.226034, Regularization: 0.002042\n",
      "2019-04-10 01:02:05,536 root         INFO     Train Epoch: 49 [5120/8000 (64%)]\tTotal Loss: 0.232050\n",
      "Reconstruction: 0.230280, Regularization: 0.001770\n",
      "2019-04-10 01:02:05,599 root         INFO     Train Epoch: 49 [5632/8000 (70%)]\tTotal Loss: 0.227261\n",
      "Reconstruction: 0.225534, Regularization: 0.001727\n",
      "2019-04-10 01:02:05,661 root         INFO     Train Epoch: 49 [6144/8000 (77%)]\tTotal Loss: 0.230615\n",
      "Reconstruction: 0.229064, Regularization: 0.001551\n",
      "2019-04-10 01:02:05,723 root         INFO     Train Epoch: 49 [6656/8000 (83%)]\tTotal Loss: 0.224387\n",
      "Reconstruction: 0.223113, Regularization: 0.001274\n",
      "2019-04-10 01:02:05,785 root         INFO     Train Epoch: 49 [7168/8000 (90%)]\tTotal Loss: 0.227675\n",
      "Reconstruction: 0.226047, Regularization: 0.001628\n",
      "2019-04-10 01:02:05,848 root         INFO     Train Epoch: 49 [7680/8000 (96%)]\tTotal Loss: 0.230587\n",
      "Reconstruction: 0.228752, Regularization: 0.001836\n",
      "2019-04-10 01:02:05,901 root         INFO     ====> Epoch: 49 Average loss: 0.2314\n",
      "2019-04-10 01:02:05,925 root         INFO     Train Epoch: 50 [0/8000 (0%)]\tTotal Loss: 0.227442\n",
      "Reconstruction: 0.225472, Regularization: 0.001970\n",
      "2019-04-10 01:02:05,989 root         INFO     Train Epoch: 50 [512/8000 (6%)]\tTotal Loss: 0.233031\n",
      "Reconstruction: 0.231239, Regularization: 0.001791\n",
      "2019-04-10 01:02:06,051 root         INFO     Train Epoch: 50 [1024/8000 (13%)]\tTotal Loss: 0.227964\n",
      "Reconstruction: 0.226207, Regularization: 0.001757\n",
      "2019-04-10 01:02:06,115 root         INFO     Train Epoch: 50 [1536/8000 (19%)]\tTotal Loss: 0.231692\n",
      "Reconstruction: 0.230026, Regularization: 0.001665\n",
      "2019-04-10 01:02:06,177 root         INFO     Train Epoch: 50 [2048/8000 (26%)]\tTotal Loss: 0.232760\n",
      "Reconstruction: 0.231201, Regularization: 0.001559\n",
      "2019-04-10 01:02:06,239 root         INFO     Train Epoch: 50 [2560/8000 (32%)]\tTotal Loss: 0.227578\n",
      "Reconstruction: 0.226060, Regularization: 0.001518\n",
      "2019-04-10 01:02:06,300 root         INFO     Train Epoch: 50 [3072/8000 (38%)]\tTotal Loss: 0.227769\n",
      "Reconstruction: 0.226267, Regularization: 0.001502\n",
      "2019-04-10 01:02:06,361 root         INFO     Train Epoch: 50 [3584/8000 (45%)]\tTotal Loss: 0.223627\n",
      "Reconstruction: 0.222178, Regularization: 0.001448\n",
      "2019-04-10 01:02:06,423 root         INFO     Train Epoch: 50 [4096/8000 (51%)]\tTotal Loss: 0.233839\n",
      "Reconstruction: 0.232464, Regularization: 0.001375\n",
      "2019-04-10 01:02:06,484 root         INFO     Train Epoch: 50 [4608/8000 (58%)]\tTotal Loss: 0.229700\n",
      "Reconstruction: 0.228052, Regularization: 0.001648\n",
      "2019-04-10 01:02:06,546 root         INFO     Train Epoch: 50 [5120/8000 (64%)]\tTotal Loss: 0.227945\n",
      "Reconstruction: 0.226328, Regularization: 0.001617\n",
      "2019-04-10 01:02:06,608 root         INFO     Train Epoch: 50 [5632/8000 (70%)]\tTotal Loss: 0.228728\n",
      "Reconstruction: 0.227088, Regularization: 0.001640\n",
      "2019-04-10 01:02:06,669 root         INFO     Train Epoch: 50 [6144/8000 (77%)]\tTotal Loss: 0.239318\n",
      "Reconstruction: 0.237619, Regularization: 0.001698\n",
      "2019-04-10 01:02:06,730 root         INFO     Train Epoch: 50 [6656/8000 (83%)]\tTotal Loss: 0.227753\n",
      "Reconstruction: 0.226164, Regularization: 0.001589\n",
      "2019-04-10 01:02:06,792 root         INFO     Train Epoch: 50 [7168/8000 (90%)]\tTotal Loss: 0.234567\n",
      "Reconstruction: 0.232959, Regularization: 0.001608\n",
      "2019-04-10 01:02:06,853 root         INFO     Train Epoch: 50 [7680/8000 (96%)]\tTotal Loss: 0.226963\n",
      "Reconstruction: 0.225592, Regularization: 0.001371\n",
      "2019-04-10 01:02:06,905 root         INFO     ====> Epoch: 50 Average loss: 0.2293\n",
      "2019-04-10 01:02:06,929 root         INFO     Train Epoch: 51 [0/8000 (0%)]\tTotal Loss: 0.226892\n",
      "Reconstruction: 0.225652, Regularization: 0.001240\n",
      "2019-04-10 01:02:06,992 root         INFO     Train Epoch: 51 [512/8000 (6%)]\tTotal Loss: 0.220669\n",
      "Reconstruction: 0.219641, Regularization: 0.001028\n",
      "2019-04-10 01:02:07,054 root         INFO     Train Epoch: 51 [1024/8000 (13%)]\tTotal Loss: 0.230581\n",
      "Reconstruction: 0.228924, Regularization: 0.001656\n",
      "2019-04-10 01:02:07,117 root         INFO     Train Epoch: 51 [1536/8000 (19%)]\tTotal Loss: 0.229381\n",
      "Reconstruction: 0.227839, Regularization: 0.001542\n",
      "2019-04-10 01:02:07,180 root         INFO     Train Epoch: 51 [2048/8000 (26%)]\tTotal Loss: 0.229707\n",
      "Reconstruction: 0.228506, Regularization: 0.001202\n",
      "2019-04-10 01:02:07,242 root         INFO     Train Epoch: 51 [2560/8000 (32%)]\tTotal Loss: 0.222334\n",
      "Reconstruction: 0.221316, Regularization: 0.001019\n",
      "2019-04-10 01:02:07,305 root         INFO     Train Epoch: 51 [3072/8000 (38%)]\tTotal Loss: 0.231977\n",
      "Reconstruction: 0.230408, Regularization: 0.001570\n",
      "2019-04-10 01:02:07,368 root         INFO     Train Epoch: 51 [3584/8000 (45%)]\tTotal Loss: 0.225058\n",
      "Reconstruction: 0.223640, Regularization: 0.001418\n",
      "2019-04-10 01:02:07,430 root         INFO     Train Epoch: 51 [4096/8000 (51%)]\tTotal Loss: 0.224355\n",
      "Reconstruction: 0.223323, Regularization: 0.001032\n",
      "2019-04-10 01:02:07,493 root         INFO     Train Epoch: 51 [4608/8000 (58%)]\tTotal Loss: 0.221928\n",
      "Reconstruction: 0.220690, Regularization: 0.001238\n",
      "2019-04-10 01:02:07,556 root         INFO     Train Epoch: 51 [5120/8000 (64%)]\tTotal Loss: 0.233214\n",
      "Reconstruction: 0.231622, Regularization: 0.001592\n",
      "2019-04-10 01:02:07,620 root         INFO     Train Epoch: 51 [5632/8000 (70%)]\tTotal Loss: 0.231163\n",
      "Reconstruction: 0.230159, Regularization: 0.001004\n",
      "2019-04-10 01:02:07,684 root         INFO     Train Epoch: 51 [6144/8000 (77%)]\tTotal Loss: 0.224180\n",
      "Reconstruction: 0.222997, Regularization: 0.001183\n",
      "2019-04-10 01:02:07,748 root         INFO     Train Epoch: 51 [6656/8000 (83%)]\tTotal Loss: 0.231770\n",
      "Reconstruction: 0.230683, Regularization: 0.001086\n",
      "2019-04-10 01:02:07,812 root         INFO     Train Epoch: 51 [7168/8000 (90%)]\tTotal Loss: 0.226325\n",
      "Reconstruction: 0.225206, Regularization: 0.001120\n",
      "2019-04-10 01:02:07,876 root         INFO     Train Epoch: 51 [7680/8000 (96%)]\tTotal Loss: 0.219959\n",
      "Reconstruction: 0.219104, Regularization: 0.000855\n",
      "2019-04-10 01:02:07,930 root         INFO     ====> Epoch: 51 Average loss: 0.2273\n",
      "2019-04-10 01:02:07,954 root         INFO     Train Epoch: 52 [0/8000 (0%)]\tTotal Loss: 0.221730\n",
      "Reconstruction: 0.220527, Regularization: 0.001203\n",
      "2019-04-10 01:02:08,019 root         INFO     Train Epoch: 52 [512/8000 (6%)]\tTotal Loss: 0.220573\n",
      "Reconstruction: 0.219372, Regularization: 0.001201\n",
      "2019-04-10 01:02:08,082 root         INFO     Train Epoch: 52 [1024/8000 (13%)]\tTotal Loss: 0.231944\n",
      "Reconstruction: 0.230821, Regularization: 0.001123\n",
      "2019-04-10 01:02:08,145 root         INFO     Train Epoch: 52 [1536/8000 (19%)]\tTotal Loss: 0.225398\n",
      "Reconstruction: 0.224005, Regularization: 0.001393\n",
      "2019-04-10 01:02:08,208 root         INFO     Train Epoch: 52 [2048/8000 (26%)]\tTotal Loss: 0.223354\n",
      "Reconstruction: 0.222508, Regularization: 0.000845\n",
      "2019-04-10 01:02:08,272 root         INFO     Train Epoch: 52 [2560/8000 (32%)]\tTotal Loss: 0.230270\n",
      "Reconstruction: 0.229192, Regularization: 0.001078\n",
      "2019-04-10 01:02:08,336 root         INFO     Train Epoch: 52 [3072/8000 (38%)]\tTotal Loss: 0.226825\n",
      "Reconstruction: 0.225992, Regularization: 0.000832\n",
      "2019-04-10 01:02:08,399 root         INFO     Train Epoch: 52 [3584/8000 (45%)]\tTotal Loss: 0.228932\n",
      "Reconstruction: 0.227850, Regularization: 0.001082\n",
      "2019-04-10 01:02:08,461 root         INFO     Train Epoch: 52 [4096/8000 (51%)]\tTotal Loss: 0.229310\n",
      "Reconstruction: 0.228324, Regularization: 0.000986\n",
      "2019-04-10 01:02:08,524 root         INFO     Train Epoch: 52 [4608/8000 (58%)]\tTotal Loss: 0.218736\n",
      "Reconstruction: 0.217754, Regularization: 0.000982\n",
      "2019-04-10 01:02:08,586 root         INFO     Train Epoch: 52 [5120/8000 (64%)]\tTotal Loss: 0.230488\n",
      "Reconstruction: 0.229133, Regularization: 0.001355\n",
      "2019-04-10 01:02:08,649 root         INFO     Train Epoch: 52 [5632/8000 (70%)]\tTotal Loss: 0.225749\n",
      "Reconstruction: 0.224657, Regularization: 0.001092\n",
      "2019-04-10 01:02:08,712 root         INFO     Train Epoch: 52 [6144/8000 (77%)]\tTotal Loss: 0.220883\n",
      "Reconstruction: 0.219693, Regularization: 0.001190\n",
      "2019-04-10 01:02:08,774 root         INFO     Train Epoch: 52 [6656/8000 (83%)]\tTotal Loss: 0.223527\n",
      "Reconstruction: 0.222553, Regularization: 0.000974\n",
      "2019-04-10 01:02:08,836 root         INFO     Train Epoch: 52 [7168/8000 (90%)]\tTotal Loss: 0.222509\n",
      "Reconstruction: 0.221708, Regularization: 0.000801\n",
      "2019-04-10 01:02:08,899 root         INFO     Train Epoch: 52 [7680/8000 (96%)]\tTotal Loss: 0.224171\n",
      "Reconstruction: 0.223168, Regularization: 0.001003\n",
      "2019-04-10 01:02:08,952 root         INFO     ====> Epoch: 52 Average loss: 0.2254\n",
      "2019-04-10 01:02:08,977 root         INFO     Train Epoch: 53 [0/8000 (0%)]\tTotal Loss: 0.227468\n",
      "Reconstruction: 0.226350, Regularization: 0.001118\n",
      "2019-04-10 01:02:09,041 root         INFO     Train Epoch: 53 [512/8000 (6%)]\tTotal Loss: 0.228224\n",
      "Reconstruction: 0.227347, Regularization: 0.000877\n",
      "2019-04-10 01:02:09,104 root         INFO     Train Epoch: 53 [1024/8000 (13%)]\tTotal Loss: 0.225148\n",
      "Reconstruction: 0.224461, Regularization: 0.000688\n",
      "2019-04-10 01:02:09,167 root         INFO     Train Epoch: 53 [1536/8000 (19%)]\tTotal Loss: 0.222490\n",
      "Reconstruction: 0.221377, Regularization: 0.001113\n",
      "2019-04-10 01:02:09,229 root         INFO     Train Epoch: 53 [2048/8000 (26%)]\tTotal Loss: 0.231207\n",
      "Reconstruction: 0.230495, Regularization: 0.000713\n",
      "2019-04-10 01:02:09,291 root         INFO     Train Epoch: 53 [2560/8000 (32%)]\tTotal Loss: 0.216469\n",
      "Reconstruction: 0.215503, Regularization: 0.000967\n",
      "2019-04-10 01:02:09,353 root         INFO     Train Epoch: 53 [3072/8000 (38%)]\tTotal Loss: 0.221155\n",
      "Reconstruction: 0.220551, Regularization: 0.000604\n",
      "2019-04-10 01:02:09,415 root         INFO     Train Epoch: 53 [3584/8000 (45%)]\tTotal Loss: 0.222659\n",
      "Reconstruction: 0.221549, Regularization: 0.001110\n",
      "2019-04-10 01:02:09,477 root         INFO     Train Epoch: 53 [4096/8000 (51%)]\tTotal Loss: 0.222733\n",
      "Reconstruction: 0.222050, Regularization: 0.000683\n",
      "2019-04-10 01:02:09,539 root         INFO     Train Epoch: 53 [4608/8000 (58%)]\tTotal Loss: 0.227575\n",
      "Reconstruction: 0.226786, Regularization: 0.000789\n",
      "2019-04-10 01:02:09,601 root         INFO     Train Epoch: 53 [5120/8000 (64%)]\tTotal Loss: 0.221137\n",
      "Reconstruction: 0.220283, Regularization: 0.000854\n",
      "2019-04-10 01:02:09,663 root         INFO     Train Epoch: 53 [5632/8000 (70%)]\tTotal Loss: 0.216508\n",
      "Reconstruction: 0.215865, Regularization: 0.000643\n",
      "2019-04-10 01:02:09,725 root         INFO     Train Epoch: 53 [6144/8000 (77%)]\tTotal Loss: 0.218249\n",
      "Reconstruction: 0.217558, Regularization: 0.000691\n",
      "2019-04-10 01:02:09,787 root         INFO     Train Epoch: 53 [6656/8000 (83%)]\tTotal Loss: 0.225390\n",
      "Reconstruction: 0.224555, Regularization: 0.000835\n",
      "2019-04-10 01:02:09,849 root         INFO     Train Epoch: 53 [7168/8000 (90%)]\tTotal Loss: 0.220221\n",
      "Reconstruction: 0.219367, Regularization: 0.000855\n",
      "2019-04-10 01:02:09,911 root         INFO     Train Epoch: 53 [7680/8000 (96%)]\tTotal Loss: 0.224668\n",
      "Reconstruction: 0.223810, Regularization: 0.000858\n",
      "2019-04-10 01:02:09,964 root         INFO     ====> Epoch: 53 Average loss: 0.2237\n",
      "2019-04-10 01:02:09,988 root         INFO     Train Epoch: 54 [0/8000 (0%)]\tTotal Loss: 0.220988\n",
      "Reconstruction: 0.220038, Regularization: 0.000950\n",
      "2019-04-10 01:02:10,052 root         INFO     Train Epoch: 54 [512/8000 (6%)]\tTotal Loss: 0.224918\n",
      "Reconstruction: 0.223856, Regularization: 0.001062\n",
      "2019-04-10 01:02:10,115 root         INFO     Train Epoch: 54 [1024/8000 (13%)]\tTotal Loss: 0.224491\n",
      "Reconstruction: 0.223648, Regularization: 0.000843\n",
      "2019-04-10 01:02:10,178 root         INFO     Train Epoch: 54 [1536/8000 (19%)]\tTotal Loss: 0.223177\n",
      "Reconstruction: 0.222569, Regularization: 0.000608\n",
      "2019-04-10 01:02:10,241 root         INFO     Train Epoch: 54 [2048/8000 (26%)]\tTotal Loss: 0.225555\n",
      "Reconstruction: 0.224453, Regularization: 0.001103\n",
      "2019-04-10 01:02:10,304 root         INFO     Train Epoch: 54 [2560/8000 (32%)]\tTotal Loss: 0.225176\n",
      "Reconstruction: 0.224511, Regularization: 0.000665\n",
      "2019-04-10 01:02:10,366 root         INFO     Train Epoch: 54 [3072/8000 (38%)]\tTotal Loss: 0.223712\n",
      "Reconstruction: 0.222718, Regularization: 0.000994\n",
      "2019-04-10 01:02:10,428 root         INFO     Train Epoch: 54 [3584/8000 (45%)]\tTotal Loss: 0.221855\n",
      "Reconstruction: 0.221304, Regularization: 0.000551\n",
      "2019-04-10 01:02:10,489 root         INFO     Train Epoch: 54 [4096/8000 (51%)]\tTotal Loss: 0.219314\n",
      "Reconstruction: 0.218580, Regularization: 0.000733\n",
      "2019-04-10 01:02:10,551 root         INFO     Train Epoch: 54 [4608/8000 (58%)]\tTotal Loss: 0.220758\n",
      "Reconstruction: 0.219822, Regularization: 0.000936\n",
      "2019-04-10 01:02:10,613 root         INFO     Train Epoch: 54 [5120/8000 (64%)]\tTotal Loss: 0.217077\n",
      "Reconstruction: 0.216671, Regularization: 0.000406\n",
      "2019-04-10 01:02:10,674 root         INFO     Train Epoch: 54 [5632/8000 (70%)]\tTotal Loss: 0.220455\n",
      "Reconstruction: 0.219972, Regularization: 0.000483\n",
      "2019-04-10 01:02:10,736 root         INFO     Train Epoch: 54 [6144/8000 (77%)]\tTotal Loss: 0.227085\n",
      "Reconstruction: 0.226465, Regularization: 0.000620\n",
      "2019-04-10 01:02:10,798 root         INFO     Train Epoch: 54 [6656/8000 (83%)]\tTotal Loss: 0.224308\n",
      "Reconstruction: 0.223398, Regularization: 0.000910\n",
      "2019-04-10 01:02:10,860 root         INFO     Train Epoch: 54 [7168/8000 (90%)]\tTotal Loss: 0.222586\n",
      "Reconstruction: 0.221925, Regularization: 0.000660\n",
      "2019-04-10 01:02:10,922 root         INFO     Train Epoch: 54 [7680/8000 (96%)]\tTotal Loss: 0.224074\n",
      "Reconstruction: 0.223601, Regularization: 0.000473\n",
      "2019-04-10 01:02:10,975 root         INFO     ====> Epoch: 54 Average loss: 0.2221\n",
      "2019-04-10 01:02:10,999 root         INFO     Train Epoch: 55 [0/8000 (0%)]\tTotal Loss: 0.220195\n",
      "Reconstruction: 0.219355, Regularization: 0.000840\n",
      "2019-04-10 01:02:11,063 root         INFO     Train Epoch: 55 [512/8000 (6%)]\tTotal Loss: 0.225987\n",
      "Reconstruction: 0.225279, Regularization: 0.000707\n",
      "2019-04-10 01:02:11,126 root         INFO     Train Epoch: 55 [1024/8000 (13%)]\tTotal Loss: 0.216690\n",
      "Reconstruction: 0.216059, Regularization: 0.000631\n",
      "2019-04-10 01:02:11,190 root         INFO     Train Epoch: 55 [1536/8000 (19%)]\tTotal Loss: 0.217851\n",
      "Reconstruction: 0.217194, Regularization: 0.000656\n",
      "2019-04-10 01:02:11,253 root         INFO     Train Epoch: 55 [2048/8000 (26%)]\tTotal Loss: 0.221629\n",
      "Reconstruction: 0.221060, Regularization: 0.000569\n",
      "2019-04-10 01:02:11,317 root         INFO     Train Epoch: 55 [2560/8000 (32%)]\tTotal Loss: 0.223953\n",
      "Reconstruction: 0.223293, Regularization: 0.000660\n",
      "2019-04-10 01:02:11,381 root         INFO     Train Epoch: 55 [3072/8000 (38%)]\tTotal Loss: 0.218233\n",
      "Reconstruction: 0.217515, Regularization: 0.000718\n",
      "2019-04-10 01:02:11,445 root         INFO     Train Epoch: 55 [3584/8000 (45%)]\tTotal Loss: 0.219111\n",
      "Reconstruction: 0.218466, Regularization: 0.000645\n",
      "2019-04-10 01:02:11,509 root         INFO     Train Epoch: 55 [4096/8000 (51%)]\tTotal Loss: 0.224211\n",
      "Reconstruction: 0.223630, Regularization: 0.000580\n",
      "2019-04-10 01:02:11,573 root         INFO     Train Epoch: 55 [4608/8000 (58%)]\tTotal Loss: 0.225356\n",
      "Reconstruction: 0.224683, Regularization: 0.000673\n",
      "2019-04-10 01:02:11,637 root         INFO     Train Epoch: 55 [5120/8000 (64%)]\tTotal Loss: 0.219181\n",
      "Reconstruction: 0.218737, Regularization: 0.000444\n",
      "2019-04-10 01:02:11,702 root         INFO     Train Epoch: 55 [5632/8000 (70%)]\tTotal Loss: 0.225413\n",
      "Reconstruction: 0.224735, Regularization: 0.000677\n",
      "2019-04-10 01:02:11,765 root         INFO     Train Epoch: 55 [6144/8000 (77%)]\tTotal Loss: 0.218784\n",
      "Reconstruction: 0.218341, Regularization: 0.000443\n",
      "2019-04-10 01:02:11,829 root         INFO     Train Epoch: 55 [6656/8000 (83%)]\tTotal Loss: 0.216162\n",
      "Reconstruction: 0.215649, Regularization: 0.000513\n",
      "2019-04-10 01:02:11,893 root         INFO     Train Epoch: 55 [7168/8000 (90%)]\tTotal Loss: 0.227506\n",
      "Reconstruction: 0.226720, Regularization: 0.000786\n",
      "2019-04-10 01:02:11,956 root         INFO     Train Epoch: 55 [7680/8000 (96%)]\tTotal Loss: 0.226073\n",
      "Reconstruction: 0.225317, Regularization: 0.000755\n",
      "2019-04-10 01:02:12,011 root         INFO     ====> Epoch: 55 Average loss: 0.2207\n",
      "2019-04-10 01:02:12,035 root         INFO     Train Epoch: 56 [0/8000 (0%)]\tTotal Loss: 0.219442\n",
      "Reconstruction: 0.218755, Regularization: 0.000687\n",
      "2019-04-10 01:02:12,099 root         INFO     Train Epoch: 56 [512/8000 (6%)]\tTotal Loss: 0.219503\n",
      "Reconstruction: 0.218761, Regularization: 0.000742\n",
      "2019-04-10 01:02:12,162 root         INFO     Train Epoch: 56 [1024/8000 (13%)]\tTotal Loss: 0.217766\n",
      "Reconstruction: 0.217066, Regularization: 0.000700\n",
      "2019-04-10 01:02:12,226 root         INFO     Train Epoch: 56 [1536/8000 (19%)]\tTotal Loss: 0.217969\n",
      "Reconstruction: 0.217477, Regularization: 0.000492\n",
      "2019-04-10 01:02:12,289 root         INFO     Train Epoch: 56 [2048/8000 (26%)]\tTotal Loss: 0.222196\n",
      "Reconstruction: 0.221672, Regularization: 0.000524\n",
      "2019-04-10 01:02:12,353 root         INFO     Train Epoch: 56 [2560/8000 (32%)]\tTotal Loss: 0.216181\n",
      "Reconstruction: 0.215659, Regularization: 0.000522\n",
      "2019-04-10 01:02:12,417 root         INFO     Train Epoch: 56 [3072/8000 (38%)]\tTotal Loss: 0.221114\n",
      "Reconstruction: 0.220494, Regularization: 0.000620\n",
      "2019-04-10 01:02:12,479 root         INFO     Train Epoch: 56 [3584/8000 (45%)]\tTotal Loss: 0.215921\n",
      "Reconstruction: 0.215382, Regularization: 0.000539\n",
      "2019-04-10 01:02:12,542 root         INFO     Train Epoch: 56 [4096/8000 (51%)]\tTotal Loss: 0.219594\n",
      "Reconstruction: 0.218929, Regularization: 0.000665\n",
      "2019-04-10 01:02:12,605 root         INFO     Train Epoch: 56 [4608/8000 (58%)]\tTotal Loss: 0.218364\n",
      "Reconstruction: 0.217871, Regularization: 0.000493\n",
      "2019-04-10 01:02:12,668 root         INFO     Train Epoch: 56 [5120/8000 (64%)]\tTotal Loss: 0.225994\n",
      "Reconstruction: 0.225303, Regularization: 0.000690\n",
      "2019-04-10 01:02:12,731 root         INFO     Train Epoch: 56 [5632/8000 (70%)]\tTotal Loss: 0.213754\n",
      "Reconstruction: 0.213156, Regularization: 0.000598\n",
      "2019-04-10 01:02:12,794 root         INFO     Train Epoch: 56 [6144/8000 (77%)]\tTotal Loss: 0.212996\n",
      "Reconstruction: 0.212484, Regularization: 0.000512\n",
      "2019-04-10 01:02:12,857 root         INFO     Train Epoch: 56 [6656/8000 (83%)]\tTotal Loss: 0.220257\n",
      "Reconstruction: 0.219688, Regularization: 0.000569\n",
      "2019-04-10 01:02:12,920 root         INFO     Train Epoch: 56 [7168/8000 (90%)]\tTotal Loss: 0.218062\n",
      "Reconstruction: 0.217557, Regularization: 0.000505\n",
      "2019-04-10 01:02:12,982 root         INFO     Train Epoch: 56 [7680/8000 (96%)]\tTotal Loss: 0.213644\n",
      "Reconstruction: 0.213167, Regularization: 0.000477\n",
      "2019-04-10 01:02:13,037 root         INFO     ====> Epoch: 56 Average loss: 0.2193\n",
      "2019-04-10 01:02:13,061 root         INFO     Train Epoch: 57 [0/8000 (0%)]\tTotal Loss: 0.219470\n",
      "Reconstruction: 0.218874, Regularization: 0.000596\n",
      "2019-04-10 01:02:13,125 root         INFO     Train Epoch: 57 [512/8000 (6%)]\tTotal Loss: 0.216137\n",
      "Reconstruction: 0.215543, Regularization: 0.000594\n",
      "2019-04-10 01:02:13,188 root         INFO     Train Epoch: 57 [1024/8000 (13%)]\tTotal Loss: 0.221107\n",
      "Reconstruction: 0.220378, Regularization: 0.000729\n",
      "2019-04-10 01:02:13,251 root         INFO     Train Epoch: 57 [1536/8000 (19%)]\tTotal Loss: 0.216571\n",
      "Reconstruction: 0.215819, Regularization: 0.000751\n",
      "2019-04-10 01:02:13,314 root         INFO     Train Epoch: 57 [2048/8000 (26%)]\tTotal Loss: 0.223492\n",
      "Reconstruction: 0.222999, Regularization: 0.000493\n",
      "2019-04-10 01:02:13,377 root         INFO     Train Epoch: 57 [2560/8000 (32%)]\tTotal Loss: 0.216211\n",
      "Reconstruction: 0.215772, Regularization: 0.000439\n",
      "2019-04-10 01:02:13,440 root         INFO     Train Epoch: 57 [3072/8000 (38%)]\tTotal Loss: 0.216535\n",
      "Reconstruction: 0.215866, Regularization: 0.000669\n",
      "2019-04-10 01:02:13,503 root         INFO     Train Epoch: 57 [3584/8000 (45%)]\tTotal Loss: 0.219824\n",
      "Reconstruction: 0.219180, Regularization: 0.000643\n",
      "2019-04-10 01:02:13,567 root         INFO     Train Epoch: 57 [4096/8000 (51%)]\tTotal Loss: 0.220006\n",
      "Reconstruction: 0.219392, Regularization: 0.000614\n",
      "2019-04-10 01:02:13,631 root         INFO     Train Epoch: 57 [4608/8000 (58%)]\tTotal Loss: 0.218521\n",
      "Reconstruction: 0.218050, Regularization: 0.000471\n",
      "2019-04-10 01:02:13,693 root         INFO     Train Epoch: 57 [5120/8000 (64%)]\tTotal Loss: 0.211444\n",
      "Reconstruction: 0.210898, Regularization: 0.000546\n",
      "2019-04-10 01:02:13,756 root         INFO     Train Epoch: 57 [5632/8000 (70%)]\tTotal Loss: 0.213279\n",
      "Reconstruction: 0.212765, Regularization: 0.000514\n",
      "2019-04-10 01:02:13,819 root         INFO     Train Epoch: 57 [6144/8000 (77%)]\tTotal Loss: 0.212373\n",
      "Reconstruction: 0.211683, Regularization: 0.000690\n",
      "2019-04-10 01:02:13,882 root         INFO     Train Epoch: 57 [6656/8000 (83%)]\tTotal Loss: 0.214993\n",
      "Reconstruction: 0.214442, Regularization: 0.000550\n",
      "2019-04-10 01:02:13,945 root         INFO     Train Epoch: 57 [7168/8000 (90%)]\tTotal Loss: 0.218689\n",
      "Reconstruction: 0.218318, Regularization: 0.000371\n",
      "2019-04-10 01:02:14,007 root         INFO     Train Epoch: 57 [7680/8000 (96%)]\tTotal Loss: 0.218989\n",
      "Reconstruction: 0.218252, Regularization: 0.000737\n",
      "2019-04-10 01:02:14,063 root         INFO     ====> Epoch: 57 Average loss: 0.2181\n",
      "2019-04-10 01:02:14,087 root         INFO     Train Epoch: 58 [0/8000 (0%)]\tTotal Loss: 0.217424\n",
      "Reconstruction: 0.216847, Regularization: 0.000576\n",
      "2019-04-10 01:02:14,151 root         INFO     Train Epoch: 58 [512/8000 (6%)]\tTotal Loss: 0.215764\n",
      "Reconstruction: 0.215280, Regularization: 0.000484\n",
      "2019-04-10 01:02:14,214 root         INFO     Train Epoch: 58 [1024/8000 (13%)]\tTotal Loss: 0.221073\n",
      "Reconstruction: 0.220444, Regularization: 0.000629\n",
      "2019-04-10 01:02:14,277 root         INFO     Train Epoch: 58 [1536/8000 (19%)]\tTotal Loss: 0.208546\n",
      "Reconstruction: 0.207989, Regularization: 0.000557\n",
      "2019-04-10 01:02:14,340 root         INFO     Train Epoch: 58 [2048/8000 (26%)]\tTotal Loss: 0.210741\n",
      "Reconstruction: 0.210172, Regularization: 0.000569\n",
      "2019-04-10 01:02:14,404 root         INFO     Train Epoch: 58 [2560/8000 (32%)]\tTotal Loss: 0.216195\n",
      "Reconstruction: 0.215787, Regularization: 0.000408\n",
      "2019-04-10 01:02:14,467 root         INFO     Train Epoch: 58 [3072/8000 (38%)]\tTotal Loss: 0.217249\n",
      "Reconstruction: 0.216705, Regularization: 0.000544\n",
      "2019-04-10 01:02:14,531 root         INFO     Train Epoch: 58 [3584/8000 (45%)]\tTotal Loss: 0.216629\n",
      "Reconstruction: 0.216060, Regularization: 0.000570\n",
      "2019-04-10 01:02:14,594 root         INFO     Train Epoch: 58 [4096/8000 (51%)]\tTotal Loss: 0.218005\n",
      "Reconstruction: 0.217289, Regularization: 0.000716\n",
      "2019-04-10 01:02:14,657 root         INFO     Train Epoch: 58 [4608/8000 (58%)]\tTotal Loss: 0.219827\n",
      "Reconstruction: 0.219201, Regularization: 0.000626\n",
      "2019-04-10 01:02:14,720 root         INFO     Train Epoch: 58 [5120/8000 (64%)]\tTotal Loss: 0.215987\n",
      "Reconstruction: 0.215599, Regularization: 0.000389\n",
      "2019-04-10 01:02:14,781 root         INFO     Train Epoch: 58 [5632/8000 (70%)]\tTotal Loss: 0.214537\n",
      "Reconstruction: 0.214068, Regularization: 0.000468\n",
      "2019-04-10 01:02:14,843 root         INFO     Train Epoch: 58 [6144/8000 (77%)]\tTotal Loss: 0.222399\n",
      "Reconstruction: 0.221841, Regularization: 0.000559\n",
      "2019-04-10 01:02:14,905 root         INFO     Train Epoch: 58 [6656/8000 (83%)]\tTotal Loss: 0.211410\n",
      "Reconstruction: 0.211150, Regularization: 0.000260\n",
      "2019-04-10 01:02:14,966 root         INFO     Train Epoch: 58 [7168/8000 (90%)]\tTotal Loss: 0.217979\n",
      "Reconstruction: 0.217498, Regularization: 0.000481\n",
      "2019-04-10 01:02:15,027 root         INFO     Train Epoch: 58 [7680/8000 (96%)]\tTotal Loss: 0.209679\n",
      "Reconstruction: 0.209407, Regularization: 0.000272\n",
      "2019-04-10 01:02:15,080 root         INFO     ====> Epoch: 58 Average loss: 0.2169\n",
      "2019-04-10 01:02:15,104 root         INFO     Train Epoch: 59 [0/8000 (0%)]\tTotal Loss: 0.218669\n",
      "Reconstruction: 0.218006, Regularization: 0.000663\n",
      "2019-04-10 01:02:15,168 root         INFO     Train Epoch: 59 [512/8000 (6%)]\tTotal Loss: 0.220617\n",
      "Reconstruction: 0.220057, Regularization: 0.000560\n",
      "2019-04-10 01:02:15,231 root         INFO     Train Epoch: 59 [1024/8000 (13%)]\tTotal Loss: 0.210129\n",
      "Reconstruction: 0.209816, Regularization: 0.000314\n",
      "2019-04-10 01:02:15,294 root         INFO     Train Epoch: 59 [1536/8000 (19%)]\tTotal Loss: 0.217216\n",
      "Reconstruction: 0.216740, Regularization: 0.000476\n",
      "2019-04-10 01:02:15,358 root         INFO     Train Epoch: 59 [2048/8000 (26%)]\tTotal Loss: 0.213737\n",
      "Reconstruction: 0.213115, Regularization: 0.000623\n",
      "2019-04-10 01:02:15,421 root         INFO     Train Epoch: 59 [2560/8000 (32%)]\tTotal Loss: 0.214725\n",
      "Reconstruction: 0.214310, Regularization: 0.000415\n",
      "2019-04-10 01:02:15,484 root         INFO     Train Epoch: 59 [3072/8000 (38%)]\tTotal Loss: 0.214624\n",
      "Reconstruction: 0.214185, Regularization: 0.000439\n",
      "2019-04-10 01:02:15,548 root         INFO     Train Epoch: 59 [3584/8000 (45%)]\tTotal Loss: 0.215477\n",
      "Reconstruction: 0.214908, Regularization: 0.000568\n",
      "2019-04-10 01:02:15,611 root         INFO     Train Epoch: 59 [4096/8000 (51%)]\tTotal Loss: 0.212265\n",
      "Reconstruction: 0.211833, Regularization: 0.000432\n",
      "2019-04-10 01:02:15,675 root         INFO     Train Epoch: 59 [4608/8000 (58%)]\tTotal Loss: 0.210843\n",
      "Reconstruction: 0.210301, Regularization: 0.000542\n",
      "2019-04-10 01:02:15,738 root         INFO     Train Epoch: 59 [5120/8000 (64%)]\tTotal Loss: 0.217098\n",
      "Reconstruction: 0.216738, Regularization: 0.000361\n",
      "2019-04-10 01:02:15,801 root         INFO     Train Epoch: 59 [5632/8000 (70%)]\tTotal Loss: 0.216398\n",
      "Reconstruction: 0.215978, Regularization: 0.000420\n",
      "2019-04-10 01:02:15,865 root         INFO     Train Epoch: 59 [6144/8000 (77%)]\tTotal Loss: 0.215265\n",
      "Reconstruction: 0.214878, Regularization: 0.000387\n",
      "2019-04-10 01:02:15,928 root         INFO     Train Epoch: 59 [6656/8000 (83%)]\tTotal Loss: 0.211715\n",
      "Reconstruction: 0.210862, Regularization: 0.000853\n",
      "2019-04-10 01:02:15,991 root         INFO     Train Epoch: 59 [7168/8000 (90%)]\tTotal Loss: 0.216260\n",
      "Reconstruction: 0.215887, Regularization: 0.000373\n",
      "2019-04-10 01:02:16,055 root         INFO     Train Epoch: 59 [7680/8000 (96%)]\tTotal Loss: 0.209742\n",
      "Reconstruction: 0.209099, Regularization: 0.000643\n",
      "2019-04-10 01:02:16,109 root         INFO     ====> Epoch: 59 Average loss: 0.2158\n",
      "2019-04-10 01:02:16,133 root         INFO     Train Epoch: 60 [0/8000 (0%)]\tTotal Loss: 0.223350\n",
      "Reconstruction: 0.222806, Regularization: 0.000544\n",
      "2019-04-10 01:02:16,197 root         INFO     Train Epoch: 60 [512/8000 (6%)]\tTotal Loss: 0.215704\n",
      "Reconstruction: 0.215269, Regularization: 0.000435\n",
      "2019-04-10 01:02:16,262 root         INFO     Train Epoch: 60 [1024/8000 (13%)]\tTotal Loss: 0.224595\n",
      "Reconstruction: 0.224232, Regularization: 0.000363\n",
      "2019-04-10 01:02:16,327 root         INFO     Train Epoch: 60 [1536/8000 (19%)]\tTotal Loss: 0.214169\n",
      "Reconstruction: 0.213707, Regularization: 0.000462\n",
      "2019-04-10 01:02:16,392 root         INFO     Train Epoch: 60 [2048/8000 (26%)]\tTotal Loss: 0.212637\n",
      "Reconstruction: 0.212230, Regularization: 0.000407\n",
      "2019-04-10 01:02:16,456 root         INFO     Train Epoch: 60 [2560/8000 (32%)]\tTotal Loss: 0.216007\n",
      "Reconstruction: 0.215462, Regularization: 0.000545\n",
      "2019-04-10 01:02:16,519 root         INFO     Train Epoch: 60 [3072/8000 (38%)]\tTotal Loss: 0.213529\n",
      "Reconstruction: 0.213163, Regularization: 0.000367\n",
      "2019-04-10 01:02:16,583 root         INFO     Train Epoch: 60 [3584/8000 (45%)]\tTotal Loss: 0.209741\n",
      "Reconstruction: 0.208872, Regularization: 0.000869\n",
      "2019-04-10 01:02:16,646 root         INFO     Train Epoch: 60 [4096/8000 (51%)]\tTotal Loss: 0.211366\n",
      "Reconstruction: 0.210920, Regularization: 0.000446\n",
      "2019-04-10 01:02:16,709 root         INFO     Train Epoch: 60 [4608/8000 (58%)]\tTotal Loss: 0.212900\n",
      "Reconstruction: 0.212534, Regularization: 0.000366\n",
      "2019-04-10 01:02:16,772 root         INFO     Train Epoch: 60 [5120/8000 (64%)]\tTotal Loss: 0.213479\n",
      "Reconstruction: 0.212952, Regularization: 0.000528\n",
      "2019-04-10 01:02:16,835 root         INFO     Train Epoch: 60 [5632/8000 (70%)]\tTotal Loss: 0.215908\n",
      "Reconstruction: 0.215334, Regularization: 0.000574\n",
      "2019-04-10 01:02:16,898 root         INFO     Train Epoch: 60 [6144/8000 (77%)]\tTotal Loss: 0.210236\n",
      "Reconstruction: 0.209765, Regularization: 0.000471\n",
      "2019-04-10 01:02:16,962 root         INFO     Train Epoch: 60 [6656/8000 (83%)]\tTotal Loss: 0.212502\n",
      "Reconstruction: 0.212111, Regularization: 0.000391\n",
      "2019-04-10 01:02:17,026 root         INFO     Train Epoch: 60 [7168/8000 (90%)]\tTotal Loss: 0.212399\n",
      "Reconstruction: 0.212068, Regularization: 0.000331\n",
      "2019-04-10 01:02:17,090 root         INFO     Train Epoch: 60 [7680/8000 (96%)]\tTotal Loss: 0.211972\n",
      "Reconstruction: 0.211446, Regularization: 0.000526\n",
      "2019-04-10 01:02:17,145 root         INFO     ====> Epoch: 60 Average loss: 0.2147\n",
      "2019-04-10 01:02:17,169 root         INFO     Train Epoch: 61 [0/8000 (0%)]\tTotal Loss: 0.221882\n",
      "Reconstruction: 0.221305, Regularization: 0.000577\n",
      "2019-04-10 01:02:17,233 root         INFO     Train Epoch: 61 [512/8000 (6%)]\tTotal Loss: 0.213404\n",
      "Reconstruction: 0.212994, Regularization: 0.000411\n",
      "2019-04-10 01:02:17,297 root         INFO     Train Epoch: 61 [1024/8000 (13%)]\tTotal Loss: 0.209963\n",
      "Reconstruction: 0.209589, Regularization: 0.000373\n",
      "2019-04-10 01:02:17,361 root         INFO     Train Epoch: 61 [1536/8000 (19%)]\tTotal Loss: 0.220844\n",
      "Reconstruction: 0.220217, Regularization: 0.000627\n",
      "2019-04-10 01:02:17,425 root         INFO     Train Epoch: 61 [2048/8000 (26%)]\tTotal Loss: 0.206903\n",
      "Reconstruction: 0.206410, Regularization: 0.000493\n",
      "2019-04-10 01:02:17,488 root         INFO     Train Epoch: 61 [2560/8000 (32%)]\tTotal Loss: 0.216769\n",
      "Reconstruction: 0.216388, Regularization: 0.000381\n",
      "2019-04-10 01:02:17,552 root         INFO     Train Epoch: 61 [3072/8000 (38%)]\tTotal Loss: 0.209973\n",
      "Reconstruction: 0.209491, Regularization: 0.000483\n",
      "2019-04-10 01:02:17,615 root         INFO     Train Epoch: 61 [3584/8000 (45%)]\tTotal Loss: 0.210185\n",
      "Reconstruction: 0.209694, Regularization: 0.000491\n",
      "2019-04-10 01:02:17,678 root         INFO     Train Epoch: 61 [4096/8000 (51%)]\tTotal Loss: 0.211642\n",
      "Reconstruction: 0.211122, Regularization: 0.000520\n",
      "2019-04-10 01:02:17,741 root         INFO     Train Epoch: 61 [4608/8000 (58%)]\tTotal Loss: 0.213494\n",
      "Reconstruction: 0.212733, Regularization: 0.000760\n",
      "2019-04-10 01:02:17,805 root         INFO     Train Epoch: 61 [5120/8000 (64%)]\tTotal Loss: 0.215214\n",
      "Reconstruction: 0.214906, Regularization: 0.000309\n",
      "2019-04-10 01:02:17,868 root         INFO     Train Epoch: 61 [5632/8000 (70%)]\tTotal Loss: 0.212332\n",
      "Reconstruction: 0.211826, Regularization: 0.000506\n",
      "2019-04-10 01:02:17,931 root         INFO     Train Epoch: 61 [6144/8000 (77%)]\tTotal Loss: 0.210368\n",
      "Reconstruction: 0.209847, Regularization: 0.000520\n",
      "2019-04-10 01:02:17,995 root         INFO     Train Epoch: 61 [6656/8000 (83%)]\tTotal Loss: 0.208589\n",
      "Reconstruction: 0.208156, Regularization: 0.000434\n",
      "2019-04-10 01:02:18,057 root         INFO     Train Epoch: 61 [7168/8000 (90%)]\tTotal Loss: 0.216275\n",
      "Reconstruction: 0.215972, Regularization: 0.000303\n",
      "2019-04-10 01:02:18,121 root         INFO     Train Epoch: 61 [7680/8000 (96%)]\tTotal Loss: 0.212755\n",
      "Reconstruction: 0.212396, Regularization: 0.000359\n",
      "2019-04-10 01:02:18,174 root         INFO     ====> Epoch: 61 Average loss: 0.2137\n",
      "2019-04-10 01:02:18,198 root         INFO     Train Epoch: 62 [0/8000 (0%)]\tTotal Loss: 0.208807\n",
      "Reconstruction: 0.208138, Regularization: 0.000669\n",
      "2019-04-10 01:02:18,263 root         INFO     Train Epoch: 62 [512/8000 (6%)]\tTotal Loss: 0.207246\n",
      "Reconstruction: 0.206658, Regularization: 0.000588\n",
      "2019-04-10 01:02:18,327 root         INFO     Train Epoch: 62 [1024/8000 (13%)]\tTotal Loss: 0.218294\n",
      "Reconstruction: 0.217894, Regularization: 0.000399\n",
      "2019-04-10 01:02:18,390 root         INFO     Train Epoch: 62 [1536/8000 (19%)]\tTotal Loss: 0.212638\n",
      "Reconstruction: 0.212064, Regularization: 0.000574\n",
      "2019-04-10 01:02:18,454 root         INFO     Train Epoch: 62 [2048/8000 (26%)]\tTotal Loss: 0.211302\n",
      "Reconstruction: 0.210797, Regularization: 0.000504\n",
      "2019-04-10 01:02:18,518 root         INFO     Train Epoch: 62 [2560/8000 (32%)]\tTotal Loss: 0.212764\n",
      "Reconstruction: 0.212390, Regularization: 0.000374\n",
      "2019-04-10 01:02:18,582 root         INFO     Train Epoch: 62 [3072/8000 (38%)]\tTotal Loss: 0.213049\n",
      "Reconstruction: 0.212712, Regularization: 0.000337\n",
      "2019-04-10 01:02:18,645 root         INFO     Train Epoch: 62 [3584/8000 (45%)]\tTotal Loss: 0.205968\n",
      "Reconstruction: 0.205491, Regularization: 0.000477\n",
      "2019-04-10 01:02:18,708 root         INFO     Train Epoch: 62 [4096/8000 (51%)]\tTotal Loss: 0.210914\n",
      "Reconstruction: 0.210478, Regularization: 0.000437\n",
      "2019-04-10 01:02:18,772 root         INFO     Train Epoch: 62 [4608/8000 (58%)]\tTotal Loss: 0.209987\n",
      "Reconstruction: 0.209455, Regularization: 0.000532\n",
      "2019-04-10 01:02:18,836 root         INFO     Train Epoch: 62 [5120/8000 (64%)]\tTotal Loss: 0.215037\n",
      "Reconstruction: 0.214668, Regularization: 0.000368\n",
      "2019-04-10 01:02:18,899 root         INFO     Train Epoch: 62 [5632/8000 (70%)]\tTotal Loss: 0.210972\n",
      "Reconstruction: 0.210646, Regularization: 0.000326\n",
      "2019-04-10 01:02:18,962 root         INFO     Train Epoch: 62 [6144/8000 (77%)]\tTotal Loss: 0.213351\n",
      "Reconstruction: 0.212771, Regularization: 0.000580\n",
      "2019-04-10 01:02:19,026 root         INFO     Train Epoch: 62 [6656/8000 (83%)]\tTotal Loss: 0.216865\n",
      "Reconstruction: 0.216285, Regularization: 0.000580\n",
      "2019-04-10 01:02:19,090 root         INFO     Train Epoch: 62 [7168/8000 (90%)]\tTotal Loss: 0.214584\n",
      "Reconstruction: 0.214161, Regularization: 0.000423\n",
      "2019-04-10 01:02:19,153 root         INFO     Train Epoch: 62 [7680/8000 (96%)]\tTotal Loss: 0.219956\n",
      "Reconstruction: 0.219394, Regularization: 0.000562\n",
      "2019-04-10 01:02:19,207 root         INFO     ====> Epoch: 62 Average loss: 0.2127\n",
      "2019-04-10 01:02:19,231 root         INFO     Train Epoch: 63 [0/8000 (0%)]\tTotal Loss: 0.212625\n",
      "Reconstruction: 0.211976, Regularization: 0.000649\n",
      "2019-04-10 01:02:19,295 root         INFO     Train Epoch: 63 [512/8000 (6%)]\tTotal Loss: 0.220205\n",
      "Reconstruction: 0.219485, Regularization: 0.000720\n",
      "2019-04-10 01:02:19,359 root         INFO     Train Epoch: 63 [1024/8000 (13%)]\tTotal Loss: 0.215156\n",
      "Reconstruction: 0.214827, Regularization: 0.000329\n",
      "2019-04-10 01:02:19,422 root         INFO     Train Epoch: 63 [1536/8000 (19%)]\tTotal Loss: 0.212736\n",
      "Reconstruction: 0.212007, Regularization: 0.000729\n",
      "2019-04-10 01:02:19,485 root         INFO     Train Epoch: 63 [2048/8000 (26%)]\tTotal Loss: 0.211414\n",
      "Reconstruction: 0.211032, Regularization: 0.000382\n",
      "2019-04-10 01:02:19,548 root         INFO     Train Epoch: 63 [2560/8000 (32%)]\tTotal Loss: 0.214610\n",
      "Reconstruction: 0.214258, Regularization: 0.000352\n",
      "2019-04-10 01:02:19,610 root         INFO     Train Epoch: 63 [3072/8000 (38%)]\tTotal Loss: 0.210798\n",
      "Reconstruction: 0.210290, Regularization: 0.000507\n",
      "2019-04-10 01:02:19,672 root         INFO     Train Epoch: 63 [3584/8000 (45%)]\tTotal Loss: 0.212497\n",
      "Reconstruction: 0.212062, Regularization: 0.000434\n",
      "2019-04-10 01:02:19,733 root         INFO     Train Epoch: 63 [4096/8000 (51%)]\tTotal Loss: 0.218500\n",
      "Reconstruction: 0.218022, Regularization: 0.000478\n",
      "2019-04-10 01:02:19,797 root         INFO     Train Epoch: 63 [4608/8000 (58%)]\tTotal Loss: 0.211698\n",
      "Reconstruction: 0.211350, Regularization: 0.000349\n",
      "2019-04-10 01:02:19,860 root         INFO     Train Epoch: 63 [5120/8000 (64%)]\tTotal Loss: 0.212726\n",
      "Reconstruction: 0.212321, Regularization: 0.000406\n",
      "2019-04-10 01:02:19,924 root         INFO     Train Epoch: 63 [5632/8000 (70%)]\tTotal Loss: 0.206487\n",
      "Reconstruction: 0.205997, Regularization: 0.000490\n",
      "2019-04-10 01:02:19,987 root         INFO     Train Epoch: 63 [6144/8000 (77%)]\tTotal Loss: 0.210200\n",
      "Reconstruction: 0.209752, Regularization: 0.000448\n",
      "2019-04-10 01:02:20,050 root         INFO     Train Epoch: 63 [6656/8000 (83%)]\tTotal Loss: 0.213582\n",
      "Reconstruction: 0.213013, Regularization: 0.000569\n",
      "2019-04-10 01:02:20,113 root         INFO     Train Epoch: 63 [7168/8000 (90%)]\tTotal Loss: 0.212397\n",
      "Reconstruction: 0.211957, Regularization: 0.000440\n",
      "2019-04-10 01:02:20,176 root         INFO     Train Epoch: 63 [7680/8000 (96%)]\tTotal Loss: 0.209845\n",
      "Reconstruction: 0.209525, Regularization: 0.000321\n",
      "2019-04-10 01:02:20,230 root         INFO     ====> Epoch: 63 Average loss: 0.2118\n",
      "2019-04-10 01:02:20,255 root         INFO     Train Epoch: 64 [0/8000 (0%)]\tTotal Loss: 0.219473\n",
      "Reconstruction: 0.219066, Regularization: 0.000407\n",
      "2019-04-10 01:02:20,319 root         INFO     Train Epoch: 64 [512/8000 (6%)]\tTotal Loss: 0.206762\n",
      "Reconstruction: 0.206388, Regularization: 0.000374\n",
      "2019-04-10 01:02:20,382 root         INFO     Train Epoch: 64 [1024/8000 (13%)]\tTotal Loss: 0.225354\n",
      "Reconstruction: 0.224887, Regularization: 0.000467\n",
      "2019-04-10 01:02:20,445 root         INFO     Train Epoch: 64 [1536/8000 (19%)]\tTotal Loss: 0.208936\n",
      "Reconstruction: 0.208459, Regularization: 0.000477\n",
      "2019-04-10 01:02:20,508 root         INFO     Train Epoch: 64 [2048/8000 (26%)]\tTotal Loss: 0.208041\n",
      "Reconstruction: 0.207550, Regularization: 0.000491\n",
      "2019-04-10 01:02:20,571 root         INFO     Train Epoch: 64 [2560/8000 (32%)]\tTotal Loss: 0.208088\n",
      "Reconstruction: 0.207373, Regularization: 0.000716\n",
      "2019-04-10 01:02:20,634 root         INFO     Train Epoch: 64 [3072/8000 (38%)]\tTotal Loss: 0.213416\n",
      "Reconstruction: 0.212878, Regularization: 0.000538\n",
      "2019-04-10 01:02:20,698 root         INFO     Train Epoch: 64 [3584/8000 (45%)]\tTotal Loss: 0.211207\n",
      "Reconstruction: 0.210772, Regularization: 0.000435\n",
      "2019-04-10 01:02:20,760 root         INFO     Train Epoch: 64 [4096/8000 (51%)]\tTotal Loss: 0.218635\n",
      "Reconstruction: 0.218227, Regularization: 0.000408\n",
      "2019-04-10 01:02:20,823 root         INFO     Train Epoch: 64 [4608/8000 (58%)]\tTotal Loss: 0.203818\n",
      "Reconstruction: 0.203262, Regularization: 0.000556\n",
      "2019-04-10 01:02:20,885 root         INFO     Train Epoch: 64 [5120/8000 (64%)]\tTotal Loss: 0.210833\n",
      "Reconstruction: 0.210349, Regularization: 0.000484\n",
      "2019-04-10 01:02:20,948 root         INFO     Train Epoch: 64 [5632/8000 (70%)]\tTotal Loss: 0.218221\n",
      "Reconstruction: 0.217749, Regularization: 0.000472\n",
      "2019-04-10 01:02:21,011 root         INFO     Train Epoch: 64 [6144/8000 (77%)]\tTotal Loss: 0.210118\n",
      "Reconstruction: 0.209770, Regularization: 0.000348\n",
      "2019-04-10 01:02:21,074 root         INFO     Train Epoch: 64 [6656/8000 (83%)]\tTotal Loss: 0.215117\n",
      "Reconstruction: 0.214570, Regularization: 0.000546\n",
      "2019-04-10 01:02:21,137 root         INFO     Train Epoch: 64 [7168/8000 (90%)]\tTotal Loss: 0.212971\n",
      "Reconstruction: 0.212510, Regularization: 0.000461\n",
      "2019-04-10 01:02:21,200 root         INFO     Train Epoch: 64 [7680/8000 (96%)]\tTotal Loss: 0.208550\n",
      "Reconstruction: 0.208126, Regularization: 0.000424\n",
      "2019-04-10 01:02:21,254 root         INFO     ====> Epoch: 64 Average loss: 0.2111\n",
      "2019-04-10 01:02:21,279 root         INFO     Train Epoch: 65 [0/8000 (0%)]\tTotal Loss: 0.216204\n",
      "Reconstruction: 0.215621, Regularization: 0.000583\n",
      "2019-04-10 01:02:21,343 root         INFO     Train Epoch: 65 [512/8000 (6%)]\tTotal Loss: 0.212854\n",
      "Reconstruction: 0.212458, Regularization: 0.000397\n",
      "2019-04-10 01:02:21,407 root         INFO     Train Epoch: 65 [1024/8000 (13%)]\tTotal Loss: 0.212463\n",
      "Reconstruction: 0.212104, Regularization: 0.000359\n",
      "2019-04-10 01:02:21,471 root         INFO     Train Epoch: 65 [1536/8000 (19%)]\tTotal Loss: 0.209596\n",
      "Reconstruction: 0.209103, Regularization: 0.000492\n",
      "2019-04-10 01:02:21,534 root         INFO     Train Epoch: 65 [2048/8000 (26%)]\tTotal Loss: 0.216978\n",
      "Reconstruction: 0.216650, Regularization: 0.000328\n",
      "2019-04-10 01:02:21,597 root         INFO     Train Epoch: 65 [2560/8000 (32%)]\tTotal Loss: 0.209202\n",
      "Reconstruction: 0.208715, Regularization: 0.000487\n",
      "2019-04-10 01:02:21,660 root         INFO     Train Epoch: 65 [3072/8000 (38%)]\tTotal Loss: 0.210079\n",
      "Reconstruction: 0.209549, Regularization: 0.000530\n",
      "2019-04-10 01:02:21,723 root         INFO     Train Epoch: 65 [3584/8000 (45%)]\tTotal Loss: 0.208506\n",
      "Reconstruction: 0.208039, Regularization: 0.000466\n",
      "2019-04-10 01:02:21,786 root         INFO     Train Epoch: 65 [4096/8000 (51%)]\tTotal Loss: 0.215353\n",
      "Reconstruction: 0.214857, Regularization: 0.000495\n",
      "2019-04-10 01:02:21,850 root         INFO     Train Epoch: 65 [4608/8000 (58%)]\tTotal Loss: 0.208998\n",
      "Reconstruction: 0.208460, Regularization: 0.000538\n",
      "2019-04-10 01:02:21,913 root         INFO     Train Epoch: 65 [5120/8000 (64%)]\tTotal Loss: 0.214027\n",
      "Reconstruction: 0.213609, Regularization: 0.000418\n",
      "2019-04-10 01:02:21,977 root         INFO     Train Epoch: 65 [5632/8000 (70%)]\tTotal Loss: 0.217772\n",
      "Reconstruction: 0.217484, Regularization: 0.000289\n",
      "2019-04-10 01:02:22,040 root         INFO     Train Epoch: 65 [6144/8000 (77%)]\tTotal Loss: 0.207707\n",
      "Reconstruction: 0.207284, Regularization: 0.000423\n",
      "2019-04-10 01:02:22,103 root         INFO     Train Epoch: 65 [6656/8000 (83%)]\tTotal Loss: 0.210189\n",
      "Reconstruction: 0.209569, Regularization: 0.000620\n",
      "2019-04-10 01:02:22,166 root         INFO     Train Epoch: 65 [7168/8000 (90%)]\tTotal Loss: 0.212311\n",
      "Reconstruction: 0.211875, Regularization: 0.000436\n",
      "2019-04-10 01:02:22,230 root         INFO     Train Epoch: 65 [7680/8000 (96%)]\tTotal Loss: 0.214852\n",
      "Reconstruction: 0.214485, Regularization: 0.000367\n",
      "2019-04-10 01:02:22,284 root         INFO     ====> Epoch: 65 Average loss: 0.2104\n",
      "2019-04-10 01:02:22,308 root         INFO     Train Epoch: 66 [0/8000 (0%)]\tTotal Loss: 0.211884\n",
      "Reconstruction: 0.211260, Regularization: 0.000624\n",
      "2019-04-10 01:02:22,372 root         INFO     Train Epoch: 66 [512/8000 (6%)]\tTotal Loss: 0.216293\n",
      "Reconstruction: 0.216065, Regularization: 0.000228\n",
      "2019-04-10 01:02:22,436 root         INFO     Train Epoch: 66 [1024/8000 (13%)]\tTotal Loss: 0.207702\n",
      "Reconstruction: 0.207041, Regularization: 0.000661\n",
      "2019-04-10 01:02:22,498 root         INFO     Train Epoch: 66 [1536/8000 (19%)]\tTotal Loss: 0.206311\n",
      "Reconstruction: 0.205978, Regularization: 0.000332\n",
      "2019-04-10 01:02:22,561 root         INFO     Train Epoch: 66 [2048/8000 (26%)]\tTotal Loss: 0.207836\n",
      "Reconstruction: 0.207382, Regularization: 0.000453\n",
      "2019-04-10 01:02:22,623 root         INFO     Train Epoch: 66 [2560/8000 (32%)]\tTotal Loss: 0.210233\n",
      "Reconstruction: 0.209952, Regularization: 0.000281\n",
      "2019-04-10 01:02:22,685 root         INFO     Train Epoch: 66 [3072/8000 (38%)]\tTotal Loss: 0.211677\n",
      "Reconstruction: 0.211209, Regularization: 0.000468\n",
      "2019-04-10 01:02:22,748 root         INFO     Train Epoch: 66 [3584/8000 (45%)]\tTotal Loss: 0.205823\n",
      "Reconstruction: 0.205336, Regularization: 0.000487\n",
      "2019-04-10 01:02:22,811 root         INFO     Train Epoch: 66 [4096/8000 (51%)]\tTotal Loss: 0.202277\n",
      "Reconstruction: 0.201862, Regularization: 0.000415\n",
      "2019-04-10 01:02:22,874 root         INFO     Train Epoch: 66 [4608/8000 (58%)]\tTotal Loss: 0.216283\n",
      "Reconstruction: 0.215910, Regularization: 0.000374\n",
      "2019-04-10 01:02:22,936 root         INFO     Train Epoch: 66 [5120/8000 (64%)]\tTotal Loss: 0.211435\n",
      "Reconstruction: 0.211078, Regularization: 0.000357\n",
      "2019-04-10 01:02:22,999 root         INFO     Train Epoch: 66 [5632/8000 (70%)]\tTotal Loss: 0.204458\n",
      "Reconstruction: 0.204055, Regularization: 0.000403\n",
      "2019-04-10 01:02:23,061 root         INFO     Train Epoch: 66 [6144/8000 (77%)]\tTotal Loss: 0.208477\n",
      "Reconstruction: 0.207923, Regularization: 0.000553\n",
      "2019-04-10 01:02:23,123 root         INFO     Train Epoch: 66 [6656/8000 (83%)]\tTotal Loss: 0.212814\n",
      "Reconstruction: 0.212486, Regularization: 0.000329\n",
      "2019-04-10 01:02:23,185 root         INFO     Train Epoch: 66 [7168/8000 (90%)]\tTotal Loss: 0.204297\n",
      "Reconstruction: 0.203809, Regularization: 0.000488\n",
      "2019-04-10 01:02:23,247 root         INFO     Train Epoch: 66 [7680/8000 (96%)]\tTotal Loss: 0.218511\n",
      "Reconstruction: 0.218143, Regularization: 0.000368\n",
      "2019-04-10 01:02:23,301 root         INFO     ====> Epoch: 66 Average loss: 0.2097\n",
      "2019-04-10 01:02:23,325 root         INFO     Train Epoch: 67 [0/8000 (0%)]\tTotal Loss: 0.206823\n",
      "Reconstruction: 0.206328, Regularization: 0.000495\n",
      "2019-04-10 01:02:23,388 root         INFO     Train Epoch: 67 [512/8000 (6%)]\tTotal Loss: 0.214218\n",
      "Reconstruction: 0.213853, Regularization: 0.000365\n",
      "2019-04-10 01:02:23,451 root         INFO     Train Epoch: 67 [1024/8000 (13%)]\tTotal Loss: 0.212011\n",
      "Reconstruction: 0.211613, Regularization: 0.000398\n",
      "2019-04-10 01:02:23,514 root         INFO     Train Epoch: 67 [1536/8000 (19%)]\tTotal Loss: 0.216706\n",
      "Reconstruction: 0.216271, Regularization: 0.000435\n",
      "2019-04-10 01:02:23,577 root         INFO     Train Epoch: 67 [2048/8000 (26%)]\tTotal Loss: 0.207853\n",
      "Reconstruction: 0.207471, Regularization: 0.000383\n",
      "2019-04-10 01:02:23,640 root         INFO     Train Epoch: 67 [2560/8000 (32%)]\tTotal Loss: 0.215041\n",
      "Reconstruction: 0.214635, Regularization: 0.000406\n",
      "2019-04-10 01:02:23,702 root         INFO     Train Epoch: 67 [3072/8000 (38%)]\tTotal Loss: 0.210320\n",
      "Reconstruction: 0.209996, Regularization: 0.000323\n",
      "2019-04-10 01:02:23,766 root         INFO     Train Epoch: 67 [3584/8000 (45%)]\tTotal Loss: 0.205335\n",
      "Reconstruction: 0.204940, Regularization: 0.000394\n",
      "2019-04-10 01:02:23,829 root         INFO     Train Epoch: 67 [4096/8000 (51%)]\tTotal Loss: 0.212243\n",
      "Reconstruction: 0.211666, Regularization: 0.000577\n",
      "2019-04-10 01:02:23,891 root         INFO     Train Epoch: 67 [4608/8000 (58%)]\tTotal Loss: 0.208951\n",
      "Reconstruction: 0.208596, Regularization: 0.000354\n",
      "2019-04-10 01:02:23,954 root         INFO     Train Epoch: 67 [5120/8000 (64%)]\tTotal Loss: 0.209884\n",
      "Reconstruction: 0.209363, Regularization: 0.000521\n",
      "2019-04-10 01:02:24,016 root         INFO     Train Epoch: 67 [5632/8000 (70%)]\tTotal Loss: 0.215289\n",
      "Reconstruction: 0.214479, Regularization: 0.000810\n",
      "2019-04-10 01:02:24,079 root         INFO     Train Epoch: 67 [6144/8000 (77%)]\tTotal Loss: 0.206183\n",
      "Reconstruction: 0.205882, Regularization: 0.000301\n",
      "2019-04-10 01:02:24,141 root         INFO     Train Epoch: 67 [6656/8000 (83%)]\tTotal Loss: 0.209763\n",
      "Reconstruction: 0.209555, Regularization: 0.000208\n",
      "2019-04-10 01:02:24,203 root         INFO     Train Epoch: 67 [7168/8000 (90%)]\tTotal Loss: 0.213303\n",
      "Reconstruction: 0.212872, Regularization: 0.000430\n",
      "2019-04-10 01:02:24,265 root         INFO     Train Epoch: 67 [7680/8000 (96%)]\tTotal Loss: 0.217672\n",
      "Reconstruction: 0.217253, Regularization: 0.000419\n",
      "2019-04-10 01:02:24,319 root         INFO     ====> Epoch: 67 Average loss: 0.2092\n",
      "2019-04-10 01:02:24,342 root         INFO     Train Epoch: 68 [0/8000 (0%)]\tTotal Loss: 0.206535\n",
      "Reconstruction: 0.206103, Regularization: 0.000433\n",
      "2019-04-10 01:02:24,406 root         INFO     Train Epoch: 68 [512/8000 (6%)]\tTotal Loss: 0.204276\n",
      "Reconstruction: 0.203780, Regularization: 0.000497\n",
      "2019-04-10 01:02:24,470 root         INFO     Train Epoch: 68 [1024/8000 (13%)]\tTotal Loss: 0.211978\n",
      "Reconstruction: 0.211533, Regularization: 0.000445\n",
      "2019-04-10 01:02:24,533 root         INFO     Train Epoch: 68 [1536/8000 (19%)]\tTotal Loss: 0.203824\n",
      "Reconstruction: 0.203451, Regularization: 0.000373\n",
      "2019-04-10 01:02:24,597 root         INFO     Train Epoch: 68 [2048/8000 (26%)]\tTotal Loss: 0.208991\n",
      "Reconstruction: 0.208482, Regularization: 0.000509\n",
      "2019-04-10 01:02:24,660 root         INFO     Train Epoch: 68 [2560/8000 (32%)]\tTotal Loss: 0.206401\n",
      "Reconstruction: 0.206014, Regularization: 0.000387\n",
      "2019-04-10 01:02:24,724 root         INFO     Train Epoch: 68 [3072/8000 (38%)]\tTotal Loss: 0.205125\n",
      "Reconstruction: 0.204763, Regularization: 0.000362\n",
      "2019-04-10 01:02:24,788 root         INFO     Train Epoch: 68 [3584/8000 (45%)]\tTotal Loss: 0.205116\n",
      "Reconstruction: 0.204768, Regularization: 0.000348\n",
      "2019-04-10 01:02:24,852 root         INFO     Train Epoch: 68 [4096/8000 (51%)]\tTotal Loss: 0.205423\n",
      "Reconstruction: 0.204990, Regularization: 0.000433\n",
      "2019-04-10 01:02:24,915 root         INFO     Train Epoch: 68 [4608/8000 (58%)]\tTotal Loss: 0.209762\n",
      "Reconstruction: 0.209386, Regularization: 0.000377\n",
      "2019-04-10 01:02:24,979 root         INFO     Train Epoch: 68 [5120/8000 (64%)]\tTotal Loss: 0.210071\n",
      "Reconstruction: 0.209840, Regularization: 0.000231\n",
      "2019-04-10 01:02:25,044 root         INFO     Train Epoch: 68 [5632/8000 (70%)]\tTotal Loss: 0.208574\n",
      "Reconstruction: 0.208170, Regularization: 0.000403\n",
      "2019-04-10 01:02:25,108 root         INFO     Train Epoch: 68 [6144/8000 (77%)]\tTotal Loss: 0.204216\n",
      "Reconstruction: 0.203595, Regularization: 0.000621\n",
      "2019-04-10 01:02:25,172 root         INFO     Train Epoch: 68 [6656/8000 (83%)]\tTotal Loss: 0.207044\n",
      "Reconstruction: 0.206646, Regularization: 0.000398\n",
      "2019-04-10 01:02:25,235 root         INFO     Train Epoch: 68 [7168/8000 (90%)]\tTotal Loss: 0.207666\n",
      "Reconstruction: 0.207195, Regularization: 0.000472\n",
      "2019-04-10 01:02:25,299 root         INFO     Train Epoch: 68 [7680/8000 (96%)]\tTotal Loss: 0.208482\n",
      "Reconstruction: 0.208078, Regularization: 0.000404\n",
      "2019-04-10 01:02:25,354 root         INFO     ====> Epoch: 68 Average loss: 0.2088\n",
      "2019-04-10 01:02:25,378 root         INFO     Train Epoch: 69 [0/8000 (0%)]\tTotal Loss: 0.203241\n",
      "Reconstruction: 0.202511, Regularization: 0.000730\n",
      "2019-04-10 01:02:25,440 root         INFO     Train Epoch: 69 [512/8000 (6%)]\tTotal Loss: 0.205015\n",
      "Reconstruction: 0.204638, Regularization: 0.000377\n",
      "2019-04-10 01:02:25,504 root         INFO     Train Epoch: 69 [1024/8000 (13%)]\tTotal Loss: 0.203789\n",
      "Reconstruction: 0.203409, Regularization: 0.000380\n",
      "2019-04-10 01:02:25,567 root         INFO     Train Epoch: 69 [1536/8000 (19%)]\tTotal Loss: 0.204809\n",
      "Reconstruction: 0.204366, Regularization: 0.000443\n",
      "2019-04-10 01:02:25,630 root         INFO     Train Epoch: 69 [2048/8000 (26%)]\tTotal Loss: 0.204558\n",
      "Reconstruction: 0.204209, Regularization: 0.000349\n",
      "2019-04-10 01:02:25,693 root         INFO     Train Epoch: 69 [2560/8000 (32%)]\tTotal Loss: 0.212464\n",
      "Reconstruction: 0.212159, Regularization: 0.000305\n",
      "2019-04-10 01:02:25,756 root         INFO     Train Epoch: 69 [3072/8000 (38%)]\tTotal Loss: 0.205984\n",
      "Reconstruction: 0.205474, Regularization: 0.000509\n",
      "2019-04-10 01:02:25,819 root         INFO     Train Epoch: 69 [3584/8000 (45%)]\tTotal Loss: 0.211205\n",
      "Reconstruction: 0.210968, Regularization: 0.000237\n",
      "2019-04-10 01:02:25,882 root         INFO     Train Epoch: 69 [4096/8000 (51%)]\tTotal Loss: 0.207107\n",
      "Reconstruction: 0.206652, Regularization: 0.000455\n",
      "2019-04-10 01:02:25,945 root         INFO     Train Epoch: 69 [4608/8000 (58%)]\tTotal Loss: 0.212175\n",
      "Reconstruction: 0.211742, Regularization: 0.000433\n",
      "2019-04-10 01:02:26,009 root         INFO     Train Epoch: 69 [5120/8000 (64%)]\tTotal Loss: 0.208687\n",
      "Reconstruction: 0.208396, Regularization: 0.000291\n",
      "2019-04-10 01:02:26,072 root         INFO     Train Epoch: 69 [5632/8000 (70%)]\tTotal Loss: 0.216977\n",
      "Reconstruction: 0.216251, Regularization: 0.000725\n",
      "2019-04-10 01:02:26,134 root         INFO     Train Epoch: 69 [6144/8000 (77%)]\tTotal Loss: 0.213686\n",
      "Reconstruction: 0.213172, Regularization: 0.000515\n",
      "2019-04-10 01:02:26,197 root         INFO     Train Epoch: 69 [6656/8000 (83%)]\tTotal Loss: 0.207122\n",
      "Reconstruction: 0.206771, Regularization: 0.000351\n",
      "2019-04-10 01:02:26,260 root         INFO     Train Epoch: 69 [7168/8000 (90%)]\tTotal Loss: 0.207251\n",
      "Reconstruction: 0.206821, Regularization: 0.000430\n",
      "2019-04-10 01:02:26,322 root         INFO     Train Epoch: 69 [7680/8000 (96%)]\tTotal Loss: 0.206116\n",
      "Reconstruction: 0.205604, Regularization: 0.000512\n",
      "2019-04-10 01:02:26,377 root         INFO     ====> Epoch: 69 Average loss: 0.2083\n",
      "2019-04-10 01:02:26,401 root         INFO     Train Epoch: 70 [0/8000 (0%)]\tTotal Loss: 0.213314\n",
      "Reconstruction: 0.212656, Regularization: 0.000658\n",
      "2019-04-10 01:02:26,465 root         INFO     Train Epoch: 70 [512/8000 (6%)]\tTotal Loss: 0.209076\n",
      "Reconstruction: 0.208677, Regularization: 0.000398\n",
      "2019-04-10 01:02:26,529 root         INFO     Train Epoch: 70 [1024/8000 (13%)]\tTotal Loss: 0.203763\n",
      "Reconstruction: 0.203193, Regularization: 0.000571\n",
      "2019-04-10 01:02:26,593 root         INFO     Train Epoch: 70 [1536/8000 (19%)]\tTotal Loss: 0.208203\n",
      "Reconstruction: 0.207900, Regularization: 0.000303\n",
      "2019-04-10 01:02:26,659 root         INFO     Train Epoch: 70 [2048/8000 (26%)]\tTotal Loss: 0.204443\n",
      "Reconstruction: 0.203724, Regularization: 0.000718\n",
      "2019-04-10 01:02:26,723 root         INFO     Train Epoch: 70 [2560/8000 (32%)]\tTotal Loss: 0.210547\n",
      "Reconstruction: 0.210191, Regularization: 0.000356\n",
      "2019-04-10 01:02:26,786 root         INFO     Train Epoch: 70 [3072/8000 (38%)]\tTotal Loss: 0.209417\n",
      "Reconstruction: 0.209143, Regularization: 0.000274\n",
      "2019-04-10 01:02:26,848 root         INFO     Train Epoch: 70 [3584/8000 (45%)]\tTotal Loss: 0.210230\n",
      "Reconstruction: 0.209750, Regularization: 0.000481\n",
      "2019-04-10 01:02:26,909 root         INFO     Train Epoch: 70 [4096/8000 (51%)]\tTotal Loss: 0.208009\n",
      "Reconstruction: 0.207394, Regularization: 0.000614\n",
      "2019-04-10 01:02:26,971 root         INFO     Train Epoch: 70 [4608/8000 (58%)]\tTotal Loss: 0.209370\n",
      "Reconstruction: 0.208747, Regularization: 0.000622\n",
      "2019-04-10 01:02:27,033 root         INFO     Train Epoch: 70 [5120/8000 (64%)]\tTotal Loss: 0.207184\n",
      "Reconstruction: 0.206791, Regularization: 0.000393\n",
      "2019-04-10 01:02:27,094 root         INFO     Train Epoch: 70 [5632/8000 (70%)]\tTotal Loss: 0.204084\n",
      "Reconstruction: 0.203663, Regularization: 0.000420\n",
      "2019-04-10 01:02:27,155 root         INFO     Train Epoch: 70 [6144/8000 (77%)]\tTotal Loss: 0.205162\n",
      "Reconstruction: 0.204743, Regularization: 0.000420\n",
      "2019-04-10 01:02:27,216 root         INFO     Train Epoch: 70 [6656/8000 (83%)]\tTotal Loss: 0.207112\n",
      "Reconstruction: 0.206653, Regularization: 0.000460\n",
      "2019-04-10 01:02:27,277 root         INFO     Train Epoch: 70 [7168/8000 (90%)]\tTotal Loss: 0.213687\n",
      "Reconstruction: 0.213146, Regularization: 0.000541\n",
      "2019-04-10 01:02:27,339 root         INFO     Train Epoch: 70 [7680/8000 (96%)]\tTotal Loss: 0.210257\n",
      "Reconstruction: 0.209880, Regularization: 0.000376\n",
      "2019-04-10 01:02:27,392 root         INFO     ====> Epoch: 70 Average loss: 0.2079\n",
      "2019-04-10 01:02:27,416 root         INFO     Train Epoch: 71 [0/8000 (0%)]\tTotal Loss: 0.204775\n",
      "Reconstruction: 0.204217, Regularization: 0.000558\n",
      "2019-04-10 01:02:27,481 root         INFO     Train Epoch: 71 [512/8000 (6%)]\tTotal Loss: 0.204872\n",
      "Reconstruction: 0.204424, Regularization: 0.000448\n",
      "2019-04-10 01:02:27,544 root         INFO     Train Epoch: 71 [1024/8000 (13%)]\tTotal Loss: 0.200865\n",
      "Reconstruction: 0.200339, Regularization: 0.000526\n",
      "2019-04-10 01:02:27,606 root         INFO     Train Epoch: 71 [1536/8000 (19%)]\tTotal Loss: 0.203163\n",
      "Reconstruction: 0.202810, Regularization: 0.000353\n",
      "2019-04-10 01:02:27,668 root         INFO     Train Epoch: 71 [2048/8000 (26%)]\tTotal Loss: 0.202993\n",
      "Reconstruction: 0.202577, Regularization: 0.000416\n",
      "2019-04-10 01:02:27,730 root         INFO     Train Epoch: 71 [2560/8000 (32%)]\tTotal Loss: 0.209779\n",
      "Reconstruction: 0.209358, Regularization: 0.000421\n",
      "2019-04-10 01:02:27,792 root         INFO     Train Epoch: 71 [3072/8000 (38%)]\tTotal Loss: 0.222432\n",
      "Reconstruction: 0.221955, Regularization: 0.000477\n",
      "2019-04-10 01:02:27,854 root         INFO     Train Epoch: 71 [3584/8000 (45%)]\tTotal Loss: 0.207937\n",
      "Reconstruction: 0.207692, Regularization: 0.000245\n",
      "2019-04-10 01:02:27,917 root         INFO     Train Epoch: 71 [4096/8000 (51%)]\tTotal Loss: 0.206980\n",
      "Reconstruction: 0.206530, Regularization: 0.000450\n",
      "2019-04-10 01:02:27,979 root         INFO     Train Epoch: 71 [4608/8000 (58%)]\tTotal Loss: 0.205189\n",
      "Reconstruction: 0.204821, Regularization: 0.000368\n",
      "2019-04-10 01:02:28,040 root         INFO     Train Epoch: 71 [5120/8000 (64%)]\tTotal Loss: 0.208962\n",
      "Reconstruction: 0.208494, Regularization: 0.000467\n",
      "2019-04-10 01:02:28,102 root         INFO     Train Epoch: 71 [5632/8000 (70%)]\tTotal Loss: 0.204578\n",
      "Reconstruction: 0.204167, Regularization: 0.000411\n",
      "2019-04-10 01:02:28,163 root         INFO     Train Epoch: 71 [6144/8000 (77%)]\tTotal Loss: 0.203720\n",
      "Reconstruction: 0.203247, Regularization: 0.000473\n",
      "2019-04-10 01:02:28,225 root         INFO     Train Epoch: 71 [6656/8000 (83%)]\tTotal Loss: 0.199353\n",
      "Reconstruction: 0.199037, Regularization: 0.000316\n",
      "2019-04-10 01:02:28,287 root         INFO     Train Epoch: 71 [7168/8000 (90%)]\tTotal Loss: 0.202434\n",
      "Reconstruction: 0.202102, Regularization: 0.000332\n",
      "2019-04-10 01:02:28,349 root         INFO     Train Epoch: 71 [7680/8000 (96%)]\tTotal Loss: 0.202217\n",
      "Reconstruction: 0.201767, Regularization: 0.000450\n",
      "2019-04-10 01:02:28,402 root         INFO     ====> Epoch: 71 Average loss: 0.2074\n",
      "2019-04-10 01:02:28,426 root         INFO     Train Epoch: 72 [0/8000 (0%)]\tTotal Loss: 0.200687\n",
      "Reconstruction: 0.200463, Regularization: 0.000224\n",
      "2019-04-10 01:02:28,491 root         INFO     Train Epoch: 72 [512/8000 (6%)]\tTotal Loss: 0.207195\n",
      "Reconstruction: 0.206838, Regularization: 0.000357\n",
      "2019-04-10 01:02:28,555 root         INFO     Train Epoch: 72 [1024/8000 (13%)]\tTotal Loss: 0.200835\n",
      "Reconstruction: 0.200482, Regularization: 0.000353\n",
      "2019-04-10 01:02:28,619 root         INFO     Train Epoch: 72 [1536/8000 (19%)]\tTotal Loss: 0.216045\n",
      "Reconstruction: 0.215552, Regularization: 0.000493\n",
      "2019-04-10 01:02:28,683 root         INFO     Train Epoch: 72 [2048/8000 (26%)]\tTotal Loss: 0.208504\n",
      "Reconstruction: 0.208167, Regularization: 0.000337\n",
      "2019-04-10 01:02:28,747 root         INFO     Train Epoch: 72 [2560/8000 (32%)]\tTotal Loss: 0.209380\n",
      "Reconstruction: 0.208797, Regularization: 0.000583\n",
      "2019-04-10 01:02:28,809 root         INFO     Train Epoch: 72 [3072/8000 (38%)]\tTotal Loss: 0.205656\n",
      "Reconstruction: 0.205191, Regularization: 0.000465\n",
      "2019-04-10 01:02:28,872 root         INFO     Train Epoch: 72 [3584/8000 (45%)]\tTotal Loss: 0.205708\n",
      "Reconstruction: 0.205409, Regularization: 0.000300\n",
      "2019-04-10 01:02:28,936 root         INFO     Train Epoch: 72 [4096/8000 (51%)]\tTotal Loss: 0.202059\n",
      "Reconstruction: 0.201589, Regularization: 0.000469\n",
      "2019-04-10 01:02:28,999 root         INFO     Train Epoch: 72 [4608/8000 (58%)]\tTotal Loss: 0.207942\n",
      "Reconstruction: 0.207632, Regularization: 0.000310\n",
      "2019-04-10 01:02:29,063 root         INFO     Train Epoch: 72 [5120/8000 (64%)]\tTotal Loss: 0.211517\n",
      "Reconstruction: 0.211194, Regularization: 0.000323\n",
      "2019-04-10 01:02:29,126 root         INFO     Train Epoch: 72 [5632/8000 (70%)]\tTotal Loss: 0.214135\n",
      "Reconstruction: 0.213736, Regularization: 0.000398\n",
      "2019-04-10 01:02:29,189 root         INFO     Train Epoch: 72 [6144/8000 (77%)]\tTotal Loss: 0.201288\n",
      "Reconstruction: 0.200895, Regularization: 0.000393\n",
      "2019-04-10 01:02:29,252 root         INFO     Train Epoch: 72 [6656/8000 (83%)]\tTotal Loss: 0.210219\n",
      "Reconstruction: 0.209818, Regularization: 0.000401\n",
      "2019-04-10 01:02:29,314 root         INFO     Train Epoch: 72 [7168/8000 (90%)]\tTotal Loss: 0.203659\n",
      "Reconstruction: 0.203350, Regularization: 0.000309\n",
      "2019-04-10 01:02:29,377 root         INFO     Train Epoch: 72 [7680/8000 (96%)]\tTotal Loss: 0.204738\n",
      "Reconstruction: 0.204348, Regularization: 0.000390\n",
      "2019-04-10 01:02:29,431 root         INFO     ====> Epoch: 72 Average loss: 0.2069\n",
      "2019-04-10 01:02:29,455 root         INFO     Train Epoch: 73 [0/8000 (0%)]\tTotal Loss: 0.196356\n",
      "Reconstruction: 0.195939, Regularization: 0.000417\n",
      "2019-04-10 01:02:29,520 root         INFO     Train Epoch: 73 [512/8000 (6%)]\tTotal Loss: 0.207562\n",
      "Reconstruction: 0.207235, Regularization: 0.000327\n",
      "2019-04-10 01:02:29,584 root         INFO     Train Epoch: 73 [1024/8000 (13%)]\tTotal Loss: 0.207903\n",
      "Reconstruction: 0.207590, Regularization: 0.000313\n",
      "2019-04-10 01:02:29,647 root         INFO     Train Epoch: 73 [1536/8000 (19%)]\tTotal Loss: 0.205839\n",
      "Reconstruction: 0.205344, Regularization: 0.000495\n",
      "2019-04-10 01:02:29,711 root         INFO     Train Epoch: 73 [2048/8000 (26%)]\tTotal Loss: 0.210203\n",
      "Reconstruction: 0.209912, Regularization: 0.000291\n",
      "2019-04-10 01:02:29,775 root         INFO     Train Epoch: 73 [2560/8000 (32%)]\tTotal Loss: 0.208663\n",
      "Reconstruction: 0.208262, Regularization: 0.000402\n",
      "2019-04-10 01:02:29,840 root         INFO     Train Epoch: 73 [3072/8000 (38%)]\tTotal Loss: 0.205229\n",
      "Reconstruction: 0.204648, Regularization: 0.000581\n",
      "2019-04-10 01:02:29,904 root         INFO     Train Epoch: 73 [3584/8000 (45%)]\tTotal Loss: 0.208094\n",
      "Reconstruction: 0.207852, Regularization: 0.000242\n",
      "2019-04-10 01:02:29,968 root         INFO     Train Epoch: 73 [4096/8000 (51%)]\tTotal Loss: 0.204782\n",
      "Reconstruction: 0.204229, Regularization: 0.000552\n",
      "2019-04-10 01:02:30,032 root         INFO     Train Epoch: 73 [4608/8000 (58%)]\tTotal Loss: 0.200954\n",
      "Reconstruction: 0.200609, Regularization: 0.000345\n",
      "2019-04-10 01:02:30,096 root         INFO     Train Epoch: 73 [5120/8000 (64%)]\tTotal Loss: 0.204824\n",
      "Reconstruction: 0.204452, Regularization: 0.000372\n",
      "2019-04-10 01:02:30,160 root         INFO     Train Epoch: 73 [5632/8000 (70%)]\tTotal Loss: 0.208556\n",
      "Reconstruction: 0.208227, Regularization: 0.000329\n",
      "2019-04-10 01:02:30,224 root         INFO     Train Epoch: 73 [6144/8000 (77%)]\tTotal Loss: 0.206834\n",
      "Reconstruction: 0.206501, Regularization: 0.000333\n",
      "2019-04-10 01:02:30,288 root         INFO     Train Epoch: 73 [6656/8000 (83%)]\tTotal Loss: 0.203295\n",
      "Reconstruction: 0.202965, Regularization: 0.000331\n",
      "2019-04-10 01:02:30,352 root         INFO     Train Epoch: 73 [7168/8000 (90%)]\tTotal Loss: 0.204370\n",
      "Reconstruction: 0.203894, Regularization: 0.000476\n",
      "2019-04-10 01:02:30,415 root         INFO     Train Epoch: 73 [7680/8000 (96%)]\tTotal Loss: 0.207859\n",
      "Reconstruction: 0.207537, Regularization: 0.000322\n",
      "2019-04-10 01:02:30,469 root         INFO     ====> Epoch: 73 Average loss: 0.2065\n",
      "2019-04-10 01:02:30,493 root         INFO     Train Epoch: 74 [0/8000 (0%)]\tTotal Loss: 0.211515\n",
      "Reconstruction: 0.211209, Regularization: 0.000306\n",
      "2019-04-10 01:02:30,558 root         INFO     Train Epoch: 74 [512/8000 (6%)]\tTotal Loss: 0.200426\n",
      "Reconstruction: 0.200134, Regularization: 0.000292\n",
      "2019-04-10 01:02:30,623 root         INFO     Train Epoch: 74 [1024/8000 (13%)]\tTotal Loss: 0.210631\n",
      "Reconstruction: 0.210213, Regularization: 0.000418\n",
      "2019-04-10 01:02:30,687 root         INFO     Train Epoch: 74 [1536/8000 (19%)]\tTotal Loss: 0.206034\n",
      "Reconstruction: 0.205569, Regularization: 0.000465\n",
      "2019-04-10 01:02:30,751 root         INFO     Train Epoch: 74 [2048/8000 (26%)]\tTotal Loss: 0.206179\n",
      "Reconstruction: 0.205902, Regularization: 0.000277\n",
      "2019-04-10 01:02:30,816 root         INFO     Train Epoch: 74 [2560/8000 (32%)]\tTotal Loss: 0.206317\n",
      "Reconstruction: 0.206022, Regularization: 0.000295\n",
      "2019-04-10 01:02:30,879 root         INFO     Train Epoch: 74 [3072/8000 (38%)]\tTotal Loss: 0.200809\n",
      "Reconstruction: 0.200202, Regularization: 0.000608\n",
      "2019-04-10 01:02:30,943 root         INFO     Train Epoch: 74 [3584/8000 (45%)]\tTotal Loss: 0.204431\n",
      "Reconstruction: 0.204060, Regularization: 0.000371\n",
      "2019-04-10 01:02:31,007 root         INFO     Train Epoch: 74 [4096/8000 (51%)]\tTotal Loss: 0.201952\n",
      "Reconstruction: 0.201696, Regularization: 0.000256\n",
      "2019-04-10 01:02:31,071 root         INFO     Train Epoch: 74 [4608/8000 (58%)]\tTotal Loss: 0.198049\n",
      "Reconstruction: 0.197761, Regularization: 0.000288\n",
      "2019-04-10 01:02:31,135 root         INFO     Train Epoch: 74 [5120/8000 (64%)]\tTotal Loss: 0.208842\n",
      "Reconstruction: 0.208367, Regularization: 0.000475\n",
      "2019-04-10 01:02:31,199 root         INFO     Train Epoch: 74 [5632/8000 (70%)]\tTotal Loss: 0.204863\n",
      "Reconstruction: 0.204436, Regularization: 0.000427\n",
      "2019-04-10 01:02:31,263 root         INFO     Train Epoch: 74 [6144/8000 (77%)]\tTotal Loss: 0.210947\n",
      "Reconstruction: 0.210641, Regularization: 0.000306\n",
      "2019-04-10 01:02:31,327 root         INFO     Train Epoch: 74 [6656/8000 (83%)]\tTotal Loss: 0.203739\n",
      "Reconstruction: 0.203317, Regularization: 0.000422\n",
      "2019-04-10 01:02:31,391 root         INFO     Train Epoch: 74 [7168/8000 (90%)]\tTotal Loss: 0.208811\n",
      "Reconstruction: 0.208500, Regularization: 0.000311\n",
      "2019-04-10 01:02:31,455 root         INFO     Train Epoch: 74 [7680/8000 (96%)]\tTotal Loss: 0.204079\n",
      "Reconstruction: 0.203829, Regularization: 0.000250\n",
      "2019-04-10 01:02:31,510 root         INFO     ====> Epoch: 74 Average loss: 0.2060\n",
      "2019-04-10 01:02:31,534 root         INFO     Train Epoch: 75 [0/8000 (0%)]\tTotal Loss: 0.202653\n",
      "Reconstruction: 0.202242, Regularization: 0.000412\n",
      "2019-04-10 01:02:31,597 root         INFO     Train Epoch: 75 [512/8000 (6%)]\tTotal Loss: 0.205690\n",
      "Reconstruction: 0.205397, Regularization: 0.000293\n",
      "2019-04-10 01:02:31,661 root         INFO     Train Epoch: 75 [1024/8000 (13%)]\tTotal Loss: 0.205740\n",
      "Reconstruction: 0.205317, Regularization: 0.000424\n",
      "2019-04-10 01:02:31,725 root         INFO     Train Epoch: 75 [1536/8000 (19%)]\tTotal Loss: 0.206196\n",
      "Reconstruction: 0.205844, Regularization: 0.000351\n",
      "2019-04-10 01:02:31,789 root         INFO     Train Epoch: 75 [2048/8000 (26%)]\tTotal Loss: 0.200016\n",
      "Reconstruction: 0.199594, Regularization: 0.000422\n",
      "2019-04-10 01:02:31,852 root         INFO     Train Epoch: 75 [2560/8000 (32%)]\tTotal Loss: 0.217387\n",
      "Reconstruction: 0.217083, Regularization: 0.000304\n",
      "2019-04-10 01:02:31,916 root         INFO     Train Epoch: 75 [3072/8000 (38%)]\tTotal Loss: 0.212032\n",
      "Reconstruction: 0.211618, Regularization: 0.000414\n",
      "2019-04-10 01:02:31,979 root         INFO     Train Epoch: 75 [3584/8000 (45%)]\tTotal Loss: 0.203977\n",
      "Reconstruction: 0.203575, Regularization: 0.000402\n",
      "2019-04-10 01:02:32,043 root         INFO     Train Epoch: 75 [4096/8000 (51%)]\tTotal Loss: 0.200514\n",
      "Reconstruction: 0.200233, Regularization: 0.000280\n",
      "2019-04-10 01:02:32,106 root         INFO     Train Epoch: 75 [4608/8000 (58%)]\tTotal Loss: 0.208396\n",
      "Reconstruction: 0.208107, Regularization: 0.000289\n",
      "2019-04-10 01:02:32,169 root         INFO     Train Epoch: 75 [5120/8000 (64%)]\tTotal Loss: 0.197717\n",
      "Reconstruction: 0.197354, Regularization: 0.000363\n",
      "2019-04-10 01:02:32,231 root         INFO     Train Epoch: 75 [5632/8000 (70%)]\tTotal Loss: 0.199071\n",
      "Reconstruction: 0.198695, Regularization: 0.000376\n",
      "2019-04-10 01:02:32,294 root         INFO     Train Epoch: 75 [6144/8000 (77%)]\tTotal Loss: 0.210287\n",
      "Reconstruction: 0.210005, Regularization: 0.000282\n",
      "2019-04-10 01:02:32,357 root         INFO     Train Epoch: 75 [6656/8000 (83%)]\tTotal Loss: 0.204712\n",
      "Reconstruction: 0.204132, Regularization: 0.000581\n",
      "2019-04-10 01:02:32,420 root         INFO     Train Epoch: 75 [7168/8000 (90%)]\tTotal Loss: 0.206223\n",
      "Reconstruction: 0.205851, Regularization: 0.000371\n",
      "2019-04-10 01:02:32,483 root         INFO     Train Epoch: 75 [7680/8000 (96%)]\tTotal Loss: 0.205517\n",
      "Reconstruction: 0.205211, Regularization: 0.000306\n",
      "2019-04-10 01:02:32,537 root         INFO     ====> Epoch: 75 Average loss: 0.2054\n",
      "2019-04-10 01:02:32,561 root         INFO     Train Epoch: 76 [0/8000 (0%)]\tTotal Loss: 0.207271\n",
      "Reconstruction: 0.206994, Regularization: 0.000276\n",
      "2019-04-10 01:02:32,625 root         INFO     Train Epoch: 76 [512/8000 (6%)]\tTotal Loss: 0.205840\n",
      "Reconstruction: 0.205581, Regularization: 0.000259\n",
      "2019-04-10 01:02:32,688 root         INFO     Train Epoch: 76 [1024/8000 (13%)]\tTotal Loss: 0.206596\n",
      "Reconstruction: 0.206053, Regularization: 0.000543\n",
      "2019-04-10 01:02:32,752 root         INFO     Train Epoch: 76 [1536/8000 (19%)]\tTotal Loss: 0.206050\n",
      "Reconstruction: 0.205734, Regularization: 0.000316\n",
      "2019-04-10 01:02:32,815 root         INFO     Train Epoch: 76 [2048/8000 (26%)]\tTotal Loss: 0.200457\n",
      "Reconstruction: 0.200231, Regularization: 0.000226\n",
      "2019-04-10 01:02:32,879 root         INFO     Train Epoch: 76 [2560/8000 (32%)]\tTotal Loss: 0.201267\n",
      "Reconstruction: 0.200931, Regularization: 0.000336\n",
      "2019-04-10 01:02:32,942 root         INFO     Train Epoch: 76 [3072/8000 (38%)]\tTotal Loss: 0.204867\n",
      "Reconstruction: 0.204564, Regularization: 0.000303\n",
      "2019-04-10 01:02:33,005 root         INFO     Train Epoch: 76 [3584/8000 (45%)]\tTotal Loss: 0.209233\n",
      "Reconstruction: 0.208886, Regularization: 0.000348\n",
      "2019-04-10 01:02:33,068 root         INFO     Train Epoch: 76 [4096/8000 (51%)]\tTotal Loss: 0.213943\n",
      "Reconstruction: 0.213624, Regularization: 0.000319\n",
      "2019-04-10 01:02:33,131 root         INFO     Train Epoch: 76 [4608/8000 (58%)]\tTotal Loss: 0.206814\n",
      "Reconstruction: 0.206442, Regularization: 0.000372\n",
      "2019-04-10 01:02:33,194 root         INFO     Train Epoch: 76 [5120/8000 (64%)]\tTotal Loss: 0.200304\n",
      "Reconstruction: 0.199887, Regularization: 0.000416\n",
      "2019-04-10 01:02:33,257 root         INFO     Train Epoch: 76 [5632/8000 (70%)]\tTotal Loss: 0.205396\n",
      "Reconstruction: 0.204942, Regularization: 0.000454\n",
      "2019-04-10 01:02:33,320 root         INFO     Train Epoch: 76 [6144/8000 (77%)]\tTotal Loss: 0.203045\n",
      "Reconstruction: 0.202711, Regularization: 0.000334\n",
      "2019-04-10 01:02:33,383 root         INFO     Train Epoch: 76 [6656/8000 (83%)]\tTotal Loss: 0.201349\n",
      "Reconstruction: 0.200986, Regularization: 0.000363\n",
      "2019-04-10 01:02:33,446 root         INFO     Train Epoch: 76 [7168/8000 (90%)]\tTotal Loss: 0.200760\n",
      "Reconstruction: 0.200284, Regularization: 0.000476\n",
      "2019-04-10 01:02:33,509 root         INFO     Train Epoch: 76 [7680/8000 (96%)]\tTotal Loss: 0.204608\n",
      "Reconstruction: 0.204315, Regularization: 0.000293\n",
      "2019-04-10 01:02:33,564 root         INFO     ====> Epoch: 76 Average loss: 0.2048\n",
      "2019-04-10 01:02:33,589 root         INFO     Train Epoch: 77 [0/8000 (0%)]\tTotal Loss: 0.206624\n",
      "Reconstruction: 0.206333, Regularization: 0.000291\n",
      "2019-04-10 01:02:33,653 root         INFO     Train Epoch: 77 [512/8000 (6%)]\tTotal Loss: 0.203661\n",
      "Reconstruction: 0.203364, Regularization: 0.000297\n",
      "2019-04-10 01:02:33,716 root         INFO     Train Epoch: 77 [1024/8000 (13%)]\tTotal Loss: 0.207075\n",
      "Reconstruction: 0.206616, Regularization: 0.000459\n",
      "2019-04-10 01:02:33,780 root         INFO     Train Epoch: 77 [1536/8000 (19%)]\tTotal Loss: 0.215408\n",
      "Reconstruction: 0.215056, Regularization: 0.000351\n",
      "2019-04-10 01:02:33,843 root         INFO     Train Epoch: 77 [2048/8000 (26%)]\tTotal Loss: 0.209005\n",
      "Reconstruction: 0.208704, Regularization: 0.000301\n",
      "2019-04-10 01:02:33,905 root         INFO     Train Epoch: 77 [2560/8000 (32%)]\tTotal Loss: 0.207092\n",
      "Reconstruction: 0.206795, Regularization: 0.000298\n",
      "2019-04-10 01:02:33,967 root         INFO     Train Epoch: 77 [3072/8000 (38%)]\tTotal Loss: 0.200915\n",
      "Reconstruction: 0.200638, Regularization: 0.000276\n",
      "2019-04-10 01:02:34,029 root         INFO     Train Epoch: 77 [3584/8000 (45%)]\tTotal Loss: 0.209439\n",
      "Reconstruction: 0.209157, Regularization: 0.000282\n",
      "2019-04-10 01:02:34,091 root         INFO     Train Epoch: 77 [4096/8000 (51%)]\tTotal Loss: 0.201265\n",
      "Reconstruction: 0.200964, Regularization: 0.000301\n",
      "2019-04-10 01:02:34,153 root         INFO     Train Epoch: 77 [4608/8000 (58%)]\tTotal Loss: 0.197817\n",
      "Reconstruction: 0.197543, Regularization: 0.000275\n",
      "2019-04-10 01:02:34,215 root         INFO     Train Epoch: 77 [5120/8000 (64%)]\tTotal Loss: 0.215501\n",
      "Reconstruction: 0.215152, Regularization: 0.000350\n",
      "2019-04-10 01:02:34,278 root         INFO     Train Epoch: 77 [5632/8000 (70%)]\tTotal Loss: 0.201980\n",
      "Reconstruction: 0.201489, Regularization: 0.000492\n",
      "2019-04-10 01:02:34,342 root         INFO     Train Epoch: 77 [6144/8000 (77%)]\tTotal Loss: 0.200333\n",
      "Reconstruction: 0.199941, Regularization: 0.000392\n",
      "2019-04-10 01:02:34,405 root         INFO     Train Epoch: 77 [6656/8000 (83%)]\tTotal Loss: 0.201906\n",
      "Reconstruction: 0.201476, Regularization: 0.000431\n",
      "2019-04-10 01:02:34,467 root         INFO     Train Epoch: 77 [7168/8000 (90%)]\tTotal Loss: 0.202030\n",
      "Reconstruction: 0.201672, Regularization: 0.000358\n",
      "2019-04-10 01:02:34,530 root         INFO     Train Epoch: 77 [7680/8000 (96%)]\tTotal Loss: 0.201145\n",
      "Reconstruction: 0.200822, Regularization: 0.000323\n",
      "2019-04-10 01:02:34,584 root         INFO     ====> Epoch: 77 Average loss: 0.2042\n",
      "2019-04-10 01:02:34,608 root         INFO     Train Epoch: 78 [0/8000 (0%)]\tTotal Loss: 0.195118\n",
      "Reconstruction: 0.194646, Regularization: 0.000472\n",
      "2019-04-10 01:02:34,671 root         INFO     Train Epoch: 78 [512/8000 (6%)]\tTotal Loss: 0.204146\n",
      "Reconstruction: 0.203833, Regularization: 0.000313\n",
      "2019-04-10 01:02:34,732 root         INFO     Train Epoch: 78 [1024/8000 (13%)]\tTotal Loss: 0.205071\n",
      "Reconstruction: 0.204819, Regularization: 0.000252\n",
      "2019-04-10 01:02:34,794 root         INFO     Train Epoch: 78 [1536/8000 (19%)]\tTotal Loss: 0.198763\n",
      "Reconstruction: 0.198506, Regularization: 0.000257\n",
      "2019-04-10 01:02:34,855 root         INFO     Train Epoch: 78 [2048/8000 (26%)]\tTotal Loss: 0.212069\n",
      "Reconstruction: 0.211782, Regularization: 0.000286\n",
      "2019-04-10 01:02:34,916 root         INFO     Train Epoch: 78 [2560/8000 (32%)]\tTotal Loss: 0.204450\n",
      "Reconstruction: 0.204216, Regularization: 0.000234\n",
      "2019-04-10 01:02:34,978 root         INFO     Train Epoch: 78 [3072/8000 (38%)]\tTotal Loss: 0.204980\n",
      "Reconstruction: 0.204530, Regularization: 0.000450\n",
      "2019-04-10 01:02:35,040 root         INFO     Train Epoch: 78 [3584/8000 (45%)]\tTotal Loss: 0.199635\n",
      "Reconstruction: 0.199263, Regularization: 0.000372\n",
      "2019-04-10 01:02:35,101 root         INFO     Train Epoch: 78 [4096/8000 (51%)]\tTotal Loss: 0.203586\n",
      "Reconstruction: 0.203167, Regularization: 0.000419\n",
      "2019-04-10 01:02:35,162 root         INFO     Train Epoch: 78 [4608/8000 (58%)]\tTotal Loss: 0.195148\n",
      "Reconstruction: 0.194919, Regularization: 0.000229\n",
      "2019-04-10 01:02:35,223 root         INFO     Train Epoch: 78 [5120/8000 (64%)]\tTotal Loss: 0.207083\n",
      "Reconstruction: 0.206857, Regularization: 0.000227\n",
      "2019-04-10 01:02:35,284 root         INFO     Train Epoch: 78 [5632/8000 (70%)]\tTotal Loss: 0.207108\n",
      "Reconstruction: 0.206691, Regularization: 0.000417\n",
      "2019-04-10 01:02:35,345 root         INFO     Train Epoch: 78 [6144/8000 (77%)]\tTotal Loss: 0.201403\n",
      "Reconstruction: 0.201101, Regularization: 0.000302\n",
      "2019-04-10 01:02:35,407 root         INFO     Train Epoch: 78 [6656/8000 (83%)]\tTotal Loss: 0.211553\n",
      "Reconstruction: 0.211223, Regularization: 0.000330\n",
      "2019-04-10 01:02:35,468 root         INFO     Train Epoch: 78 [7168/8000 (90%)]\tTotal Loss: 0.202699\n",
      "Reconstruction: 0.202376, Regularization: 0.000323\n",
      "2019-04-10 01:02:35,529 root         INFO     Train Epoch: 78 [7680/8000 (96%)]\tTotal Loss: 0.202339\n",
      "Reconstruction: 0.202059, Regularization: 0.000280\n",
      "2019-04-10 01:02:35,582 root         INFO     ====> Epoch: 78 Average loss: 0.2035\n",
      "2019-04-10 01:02:35,606 root         INFO     Train Epoch: 79 [0/8000 (0%)]\tTotal Loss: 0.203521\n",
      "Reconstruction: 0.203275, Regularization: 0.000246\n",
      "2019-04-10 01:02:35,668 root         INFO     Train Epoch: 79 [512/8000 (6%)]\tTotal Loss: 0.203764\n",
      "Reconstruction: 0.203558, Regularization: 0.000205\n",
      "2019-04-10 01:02:35,730 root         INFO     Train Epoch: 79 [1024/8000 (13%)]\tTotal Loss: 0.203884\n",
      "Reconstruction: 0.203671, Regularization: 0.000213\n",
      "2019-04-10 01:02:35,791 root         INFO     Train Epoch: 79 [1536/8000 (19%)]\tTotal Loss: 0.198631\n",
      "Reconstruction: 0.198235, Regularization: 0.000396\n",
      "2019-04-10 01:02:35,853 root         INFO     Train Epoch: 79 [2048/8000 (26%)]\tTotal Loss: 0.203483\n",
      "Reconstruction: 0.203219, Regularization: 0.000264\n",
      "2019-04-10 01:02:35,915 root         INFO     Train Epoch: 79 [2560/8000 (32%)]\tTotal Loss: 0.197725\n",
      "Reconstruction: 0.197418, Regularization: 0.000306\n",
      "2019-04-10 01:02:35,976 root         INFO     Train Epoch: 79 [3072/8000 (38%)]\tTotal Loss: 0.204761\n",
      "Reconstruction: 0.204469, Regularization: 0.000292\n",
      "2019-04-10 01:02:36,038 root         INFO     Train Epoch: 79 [3584/8000 (45%)]\tTotal Loss: 0.207221\n",
      "Reconstruction: 0.206901, Regularization: 0.000320\n",
      "2019-04-10 01:02:36,100 root         INFO     Train Epoch: 79 [4096/8000 (51%)]\tTotal Loss: 0.198647\n",
      "Reconstruction: 0.198107, Regularization: 0.000540\n",
      "2019-04-10 01:02:36,161 root         INFO     Train Epoch: 79 [4608/8000 (58%)]\tTotal Loss: 0.200493\n",
      "Reconstruction: 0.200245, Regularization: 0.000248\n",
      "2019-04-10 01:02:36,223 root         INFO     Train Epoch: 79 [5120/8000 (64%)]\tTotal Loss: 0.199384\n",
      "Reconstruction: 0.198998, Regularization: 0.000386\n",
      "2019-04-10 01:02:36,285 root         INFO     Train Epoch: 79 [5632/8000 (70%)]\tTotal Loss: 0.205414\n",
      "Reconstruction: 0.205121, Regularization: 0.000293\n",
      "2019-04-10 01:02:36,347 root         INFO     Train Epoch: 79 [6144/8000 (77%)]\tTotal Loss: 0.211844\n",
      "Reconstruction: 0.211545, Regularization: 0.000298\n",
      "2019-04-10 01:02:36,409 root         INFO     Train Epoch: 79 [6656/8000 (83%)]\tTotal Loss: 0.197779\n",
      "Reconstruction: 0.197365, Regularization: 0.000415\n",
      "2019-04-10 01:02:36,471 root         INFO     Train Epoch: 79 [7168/8000 (90%)]\tTotal Loss: 0.201598\n",
      "Reconstruction: 0.201335, Regularization: 0.000263\n",
      "2019-04-10 01:02:36,534 root         INFO     Train Epoch: 79 [7680/8000 (96%)]\tTotal Loss: 0.199948\n",
      "Reconstruction: 0.199487, Regularization: 0.000461\n",
      "2019-04-10 01:02:36,588 root         INFO     ====> Epoch: 79 Average loss: 0.2027\n",
      "2019-04-10 01:02:36,611 root         INFO     Train Epoch: 80 [0/8000 (0%)]\tTotal Loss: 0.201530\n",
      "Reconstruction: 0.201214, Regularization: 0.000316\n",
      "2019-04-10 01:02:36,674 root         INFO     Train Epoch: 80 [512/8000 (6%)]\tTotal Loss: 0.202970\n",
      "Reconstruction: 0.202786, Regularization: 0.000184\n",
      "2019-04-10 01:02:36,737 root         INFO     Train Epoch: 80 [1024/8000 (13%)]\tTotal Loss: 0.211559\n",
      "Reconstruction: 0.211158, Regularization: 0.000400\n",
      "2019-04-10 01:02:36,800 root         INFO     Train Epoch: 80 [1536/8000 (19%)]\tTotal Loss: 0.201576\n",
      "Reconstruction: 0.201186, Regularization: 0.000389\n",
      "2019-04-10 01:02:36,863 root         INFO     Train Epoch: 80 [2048/8000 (26%)]\tTotal Loss: 0.207426\n",
      "Reconstruction: 0.207072, Regularization: 0.000354\n",
      "2019-04-10 01:02:36,926 root         INFO     Train Epoch: 80 [2560/8000 (32%)]\tTotal Loss: 0.204444\n",
      "Reconstruction: 0.204167, Regularization: 0.000277\n",
      "2019-04-10 01:02:36,989 root         INFO     Train Epoch: 80 [3072/8000 (38%)]\tTotal Loss: 0.206774\n",
      "Reconstruction: 0.206461, Regularization: 0.000312\n",
      "2019-04-10 01:02:37,051 root         INFO     Train Epoch: 80 [3584/8000 (45%)]\tTotal Loss: 0.209119\n",
      "Reconstruction: 0.208780, Regularization: 0.000339\n",
      "2019-04-10 01:02:37,114 root         INFO     Train Epoch: 80 [4096/8000 (51%)]\tTotal Loss: 0.196622\n",
      "Reconstruction: 0.196269, Regularization: 0.000353\n",
      "2019-04-10 01:02:37,177 root         INFO     Train Epoch: 80 [4608/8000 (58%)]\tTotal Loss: 0.197192\n",
      "Reconstruction: 0.196687, Regularization: 0.000505\n",
      "2019-04-10 01:02:37,240 root         INFO     Train Epoch: 80 [5120/8000 (64%)]\tTotal Loss: 0.199828\n",
      "Reconstruction: 0.199512, Regularization: 0.000315\n",
      "2019-04-10 01:02:37,303 root         INFO     Train Epoch: 80 [5632/8000 (70%)]\tTotal Loss: 0.197416\n",
      "Reconstruction: 0.196976, Regularization: 0.000441\n",
      "2019-04-10 01:02:37,366 root         INFO     Train Epoch: 80 [6144/8000 (77%)]\tTotal Loss: 0.201117\n",
      "Reconstruction: 0.200910, Regularization: 0.000207\n",
      "2019-04-10 01:02:37,428 root         INFO     Train Epoch: 80 [6656/8000 (83%)]\tTotal Loss: 0.209250\n",
      "Reconstruction: 0.208896, Regularization: 0.000354\n",
      "2019-04-10 01:02:37,491 root         INFO     Train Epoch: 80 [7168/8000 (90%)]\tTotal Loss: 0.203845\n",
      "Reconstruction: 0.203251, Regularization: 0.000594\n",
      "2019-04-10 01:02:37,555 root         INFO     Train Epoch: 80 [7680/8000 (96%)]\tTotal Loss: 0.199037\n",
      "Reconstruction: 0.198750, Regularization: 0.000287\n",
      "2019-04-10 01:02:37,609 root         INFO     ====> Epoch: 80 Average loss: 0.2019\n",
      "2019-04-10 01:02:37,634 root         INFO     Train Epoch: 81 [0/8000 (0%)]\tTotal Loss: 0.198133\n",
      "Reconstruction: 0.197720, Regularization: 0.000413\n",
      "2019-04-10 01:02:37,698 root         INFO     Train Epoch: 81 [512/8000 (6%)]\tTotal Loss: 0.198635\n",
      "Reconstruction: 0.198348, Regularization: 0.000287\n",
      "2019-04-10 01:02:37,762 root         INFO     Train Epoch: 81 [1024/8000 (13%)]\tTotal Loss: 0.198051\n",
      "Reconstruction: 0.197855, Regularization: 0.000195\n",
      "2019-04-10 01:02:37,826 root         INFO     Train Epoch: 81 [1536/8000 (19%)]\tTotal Loss: 0.200728\n",
      "Reconstruction: 0.200499, Regularization: 0.000228\n",
      "2019-04-10 01:02:37,891 root         INFO     Train Epoch: 81 [2048/8000 (26%)]\tTotal Loss: 0.201661\n",
      "Reconstruction: 0.201387, Regularization: 0.000275\n",
      "2019-04-10 01:02:37,955 root         INFO     Train Epoch: 81 [2560/8000 (32%)]\tTotal Loss: 0.201198\n",
      "Reconstruction: 0.200728, Regularization: 0.000470\n",
      "2019-04-10 01:02:38,019 root         INFO     Train Epoch: 81 [3072/8000 (38%)]\tTotal Loss: 0.201415\n",
      "Reconstruction: 0.201049, Regularization: 0.000366\n",
      "2019-04-10 01:02:38,083 root         INFO     Train Epoch: 81 [3584/8000 (45%)]\tTotal Loss: 0.200265\n",
      "Reconstruction: 0.200049, Regularization: 0.000215\n",
      "2019-04-10 01:02:38,147 root         INFO     Train Epoch: 81 [4096/8000 (51%)]\tTotal Loss: 0.205168\n",
      "Reconstruction: 0.204807, Regularization: 0.000362\n",
      "2019-04-10 01:02:38,211 root         INFO     Train Epoch: 81 [4608/8000 (58%)]\tTotal Loss: 0.198758\n",
      "Reconstruction: 0.198374, Regularization: 0.000384\n",
      "2019-04-10 01:02:38,275 root         INFO     Train Epoch: 81 [5120/8000 (64%)]\tTotal Loss: 0.199917\n",
      "Reconstruction: 0.199435, Regularization: 0.000482\n",
      "2019-04-10 01:02:38,338 root         INFO     Train Epoch: 81 [5632/8000 (70%)]\tTotal Loss: 0.201487\n",
      "Reconstruction: 0.201229, Regularization: 0.000259\n",
      "2019-04-10 01:02:38,402 root         INFO     Train Epoch: 81 [6144/8000 (77%)]\tTotal Loss: 0.204694\n",
      "Reconstruction: 0.204382, Regularization: 0.000312\n",
      "2019-04-10 01:02:38,466 root         INFO     Train Epoch: 81 [6656/8000 (83%)]\tTotal Loss: 0.201343\n",
      "Reconstruction: 0.201154, Regularization: 0.000189\n",
      "2019-04-10 01:02:38,530 root         INFO     Train Epoch: 81 [7168/8000 (90%)]\tTotal Loss: 0.202137\n",
      "Reconstruction: 0.201964, Regularization: 0.000173\n",
      "2019-04-10 01:02:38,594 root         INFO     Train Epoch: 81 [7680/8000 (96%)]\tTotal Loss: 0.202519\n",
      "Reconstruction: 0.202197, Regularization: 0.000322\n",
      "2019-04-10 01:02:38,649 root         INFO     ====> Epoch: 81 Average loss: 0.2010\n",
      "2019-04-10 01:02:38,673 root         INFO     Train Epoch: 82 [0/8000 (0%)]\tTotal Loss: 0.196358\n",
      "Reconstruction: 0.196063, Regularization: 0.000296\n",
      "2019-04-10 01:02:38,737 root         INFO     Train Epoch: 82 [512/8000 (6%)]\tTotal Loss: 0.197540\n",
      "Reconstruction: 0.197038, Regularization: 0.000502\n",
      "2019-04-10 01:02:38,801 root         INFO     Train Epoch: 82 [1024/8000 (13%)]\tTotal Loss: 0.200767\n",
      "Reconstruction: 0.200442, Regularization: 0.000325\n",
      "2019-04-10 01:02:38,865 root         INFO     Train Epoch: 82 [1536/8000 (19%)]\tTotal Loss: 0.197182\n",
      "Reconstruction: 0.196952, Regularization: 0.000230\n",
      "2019-04-10 01:02:38,929 root         INFO     Train Epoch: 82 [2048/8000 (26%)]\tTotal Loss: 0.200230\n",
      "Reconstruction: 0.199947, Regularization: 0.000283\n",
      "2019-04-10 01:02:38,993 root         INFO     Train Epoch: 82 [2560/8000 (32%)]\tTotal Loss: 0.199946\n",
      "Reconstruction: 0.199633, Regularization: 0.000313\n",
      "2019-04-10 01:02:39,057 root         INFO     Train Epoch: 82 [3072/8000 (38%)]\tTotal Loss: 0.202269\n",
      "Reconstruction: 0.202065, Regularization: 0.000205\n",
      "2019-04-10 01:02:39,121 root         INFO     Train Epoch: 82 [3584/8000 (45%)]\tTotal Loss: 0.194451\n",
      "Reconstruction: 0.194101, Regularization: 0.000349\n",
      "2019-04-10 01:02:39,185 root         INFO     Train Epoch: 82 [4096/8000 (51%)]\tTotal Loss: 0.194079\n",
      "Reconstruction: 0.193660, Regularization: 0.000419\n",
      "2019-04-10 01:02:39,249 root         INFO     Train Epoch: 82 [4608/8000 (58%)]\tTotal Loss: 0.200570\n",
      "Reconstruction: 0.200148, Regularization: 0.000422\n",
      "2019-04-10 01:02:39,313 root         INFO     Train Epoch: 82 [5120/8000 (64%)]\tTotal Loss: 0.206099\n",
      "Reconstruction: 0.205942, Regularization: 0.000156\n",
      "2019-04-10 01:02:39,377 root         INFO     Train Epoch: 82 [5632/8000 (70%)]\tTotal Loss: 0.197148\n",
      "Reconstruction: 0.196792, Regularization: 0.000356\n",
      "2019-04-10 01:02:39,440 root         INFO     Train Epoch: 82 [6144/8000 (77%)]\tTotal Loss: 0.193955\n",
      "Reconstruction: 0.193742, Regularization: 0.000213\n",
      "2019-04-10 01:02:39,503 root         INFO     Train Epoch: 82 [6656/8000 (83%)]\tTotal Loss: 0.200757\n",
      "Reconstruction: 0.200395, Regularization: 0.000362\n",
      "2019-04-10 01:02:39,567 root         INFO     Train Epoch: 82 [7168/8000 (90%)]\tTotal Loss: 0.195560\n",
      "Reconstruction: 0.195185, Regularization: 0.000376\n",
      "2019-04-10 01:02:39,631 root         INFO     Train Epoch: 82 [7680/8000 (96%)]\tTotal Loss: 0.195680\n",
      "Reconstruction: 0.195392, Regularization: 0.000288\n",
      "2019-04-10 01:02:39,685 root         INFO     ====> Epoch: 82 Average loss: 0.2000\n",
      "2019-04-10 01:02:39,709 root         INFO     Train Epoch: 83 [0/8000 (0%)]\tTotal Loss: 0.201618\n",
      "Reconstruction: 0.201422, Regularization: 0.000196\n",
      "2019-04-10 01:02:39,772 root         INFO     Train Epoch: 83 [512/8000 (6%)]\tTotal Loss: 0.196023\n",
      "Reconstruction: 0.195821, Regularization: 0.000202\n",
      "2019-04-10 01:02:39,836 root         INFO     Train Epoch: 83 [1024/8000 (13%)]\tTotal Loss: 0.199399\n",
      "Reconstruction: 0.199080, Regularization: 0.000319\n",
      "2019-04-10 01:02:39,899 root         INFO     Train Epoch: 83 [1536/8000 (19%)]\tTotal Loss: 0.193918\n",
      "Reconstruction: 0.193632, Regularization: 0.000287\n",
      "2019-04-10 01:02:39,963 root         INFO     Train Epoch: 83 [2048/8000 (26%)]\tTotal Loss: 0.193257\n",
      "Reconstruction: 0.192970, Regularization: 0.000286\n",
      "2019-04-10 01:02:40,027 root         INFO     Train Epoch: 83 [2560/8000 (32%)]\tTotal Loss: 0.195873\n",
      "Reconstruction: 0.195564, Regularization: 0.000308\n",
      "2019-04-10 01:02:40,091 root         INFO     Train Epoch: 83 [3072/8000 (38%)]\tTotal Loss: 0.199254\n",
      "Reconstruction: 0.198929, Regularization: 0.000325\n",
      "2019-04-10 01:02:40,155 root         INFO     Train Epoch: 83 [3584/8000 (45%)]\tTotal Loss: 0.210218\n",
      "Reconstruction: 0.209878, Regularization: 0.000340\n",
      "2019-04-10 01:02:40,218 root         INFO     Train Epoch: 83 [4096/8000 (51%)]\tTotal Loss: 0.198495\n",
      "Reconstruction: 0.198142, Regularization: 0.000353\n",
      "2019-04-10 01:02:40,281 root         INFO     Train Epoch: 83 [4608/8000 (58%)]\tTotal Loss: 0.197118\n",
      "Reconstruction: 0.196819, Regularization: 0.000299\n",
      "2019-04-10 01:02:40,344 root         INFO     Train Epoch: 83 [5120/8000 (64%)]\tTotal Loss: 0.201197\n",
      "Reconstruction: 0.200975, Regularization: 0.000222\n",
      "2019-04-10 01:02:40,407 root         INFO     Train Epoch: 83 [5632/8000 (70%)]\tTotal Loss: 0.194219\n",
      "Reconstruction: 0.193906, Regularization: 0.000313\n",
      "2019-04-10 01:02:40,469 root         INFO     Train Epoch: 83 [6144/8000 (77%)]\tTotal Loss: 0.198319\n",
      "Reconstruction: 0.198169, Regularization: 0.000150\n",
      "2019-04-10 01:02:40,532 root         INFO     Train Epoch: 83 [6656/8000 (83%)]\tTotal Loss: 0.200034\n",
      "Reconstruction: 0.199728, Regularization: 0.000307\n",
      "2019-04-10 01:02:40,595 root         INFO     Train Epoch: 83 [7168/8000 (90%)]\tTotal Loss: 0.219785\n",
      "Reconstruction: 0.219465, Regularization: 0.000320\n",
      "2019-04-10 01:02:40,658 root         INFO     Train Epoch: 83 [7680/8000 (96%)]\tTotal Loss: 0.200378\n",
      "Reconstruction: 0.200176, Regularization: 0.000202\n",
      "2019-04-10 01:02:40,711 root         INFO     ====> Epoch: 83 Average loss: 0.1989\n",
      "2019-04-10 01:02:40,736 root         INFO     Train Epoch: 84 [0/8000 (0%)]\tTotal Loss: 0.202972\n",
      "Reconstruction: 0.202757, Regularization: 0.000214\n",
      "2019-04-10 01:02:40,800 root         INFO     Train Epoch: 84 [512/8000 (6%)]\tTotal Loss: 0.196571\n",
      "Reconstruction: 0.196264, Regularization: 0.000308\n",
      "2019-04-10 01:02:40,864 root         INFO     Train Epoch: 84 [1024/8000 (13%)]\tTotal Loss: 0.194094\n",
      "Reconstruction: 0.193813, Regularization: 0.000282\n",
      "2019-04-10 01:02:40,928 root         INFO     Train Epoch: 84 [1536/8000 (19%)]\tTotal Loss: 0.198797\n",
      "Reconstruction: 0.198345, Regularization: 0.000452\n",
      "2019-04-10 01:02:40,991 root         INFO     Train Epoch: 84 [2048/8000 (26%)]\tTotal Loss: 0.197791\n",
      "Reconstruction: 0.197474, Regularization: 0.000317\n",
      "2019-04-10 01:02:41,055 root         INFO     Train Epoch: 84 [2560/8000 (32%)]\tTotal Loss: 0.194686\n",
      "Reconstruction: 0.194347, Regularization: 0.000339\n",
      "2019-04-10 01:02:41,119 root         INFO     Train Epoch: 84 [3072/8000 (38%)]\tTotal Loss: 0.194317\n",
      "Reconstruction: 0.193948, Regularization: 0.000369\n",
      "2019-04-10 01:02:41,183 root         INFO     Train Epoch: 84 [3584/8000 (45%)]\tTotal Loss: 0.201969\n",
      "Reconstruction: 0.201567, Regularization: 0.000402\n",
      "2019-04-10 01:02:41,247 root         INFO     Train Epoch: 84 [4096/8000 (51%)]\tTotal Loss: 0.201778\n",
      "Reconstruction: 0.201519, Regularization: 0.000259\n",
      "2019-04-10 01:02:41,311 root         INFO     Train Epoch: 84 [4608/8000 (58%)]\tTotal Loss: 0.193665\n",
      "Reconstruction: 0.193456, Regularization: 0.000209\n",
      "2019-04-10 01:02:41,375 root         INFO     Train Epoch: 84 [5120/8000 (64%)]\tTotal Loss: 0.204198\n",
      "Reconstruction: 0.203715, Regularization: 0.000483\n",
      "2019-04-10 01:02:41,440 root         INFO     Train Epoch: 84 [5632/8000 (70%)]\tTotal Loss: 0.192239\n",
      "Reconstruction: 0.192003, Regularization: 0.000236\n",
      "2019-04-10 01:02:41,503 root         INFO     Train Epoch: 84 [6144/8000 (77%)]\tTotal Loss: 0.195013\n",
      "Reconstruction: 0.194713, Regularization: 0.000300\n",
      "2019-04-10 01:02:41,567 root         INFO     Train Epoch: 84 [6656/8000 (83%)]\tTotal Loss: 0.191442\n",
      "Reconstruction: 0.191064, Regularization: 0.000378\n",
      "2019-04-10 01:02:41,631 root         INFO     Train Epoch: 84 [7168/8000 (90%)]\tTotal Loss: 0.197087\n",
      "Reconstruction: 0.196797, Regularization: 0.000290\n",
      "2019-04-10 01:02:41,695 root         INFO     Train Epoch: 84 [7680/8000 (96%)]\tTotal Loss: 0.194811\n",
      "Reconstruction: 0.194637, Regularization: 0.000174\n",
      "2019-04-10 01:02:41,749 root         INFO     ====> Epoch: 84 Average loss: 0.1977\n",
      "2019-04-10 01:02:41,773 root         INFO     Train Epoch: 85 [0/8000 (0%)]\tTotal Loss: 0.193323\n",
      "Reconstruction: 0.193083, Regularization: 0.000240\n",
      "2019-04-10 01:02:41,837 root         INFO     Train Epoch: 85 [512/8000 (6%)]\tTotal Loss: 0.198617\n",
      "Reconstruction: 0.198252, Regularization: 0.000364\n",
      "2019-04-10 01:02:41,901 root         INFO     Train Epoch: 85 [1024/8000 (13%)]\tTotal Loss: 0.201597\n",
      "Reconstruction: 0.201053, Regularization: 0.000544\n",
      "2019-04-10 01:02:41,964 root         INFO     Train Epoch: 85 [1536/8000 (19%)]\tTotal Loss: 0.198444\n",
      "Reconstruction: 0.198005, Regularization: 0.000439\n",
      "2019-04-10 01:02:42,026 root         INFO     Train Epoch: 85 [2048/8000 (26%)]\tTotal Loss: 0.194167\n",
      "Reconstruction: 0.193888, Regularization: 0.000279\n",
      "2019-04-10 01:02:42,090 root         INFO     Train Epoch: 85 [2560/8000 (32%)]\tTotal Loss: 0.195816\n",
      "Reconstruction: 0.195539, Regularization: 0.000277\n",
      "2019-04-10 01:02:42,153 root         INFO     Train Epoch: 85 [3072/8000 (38%)]\tTotal Loss: 0.192525\n",
      "Reconstruction: 0.192295, Regularization: 0.000230\n",
      "2019-04-10 01:02:42,216 root         INFO     Train Epoch: 85 [3584/8000 (45%)]\tTotal Loss: 0.197863\n",
      "Reconstruction: 0.197635, Regularization: 0.000228\n",
      "2019-04-10 01:02:42,279 root         INFO     Train Epoch: 85 [4096/8000 (51%)]\tTotal Loss: 0.196047\n",
      "Reconstruction: 0.195747, Regularization: 0.000300\n",
      "2019-04-10 01:02:42,343 root         INFO     Train Epoch: 85 [4608/8000 (58%)]\tTotal Loss: 0.195861\n",
      "Reconstruction: 0.195637, Regularization: 0.000224\n",
      "2019-04-10 01:02:42,405 root         INFO     Train Epoch: 85 [5120/8000 (64%)]\tTotal Loss: 0.193133\n",
      "Reconstruction: 0.192758, Regularization: 0.000375\n",
      "2019-04-10 01:02:42,469 root         INFO     Train Epoch: 85 [5632/8000 (70%)]\tTotal Loss: 0.202139\n",
      "Reconstruction: 0.201719, Regularization: 0.000420\n",
      "2019-04-10 01:02:42,532 root         INFO     Train Epoch: 85 [6144/8000 (77%)]\tTotal Loss: 0.193676\n",
      "Reconstruction: 0.193446, Regularization: 0.000229\n",
      "2019-04-10 01:02:42,595 root         INFO     Train Epoch: 85 [6656/8000 (83%)]\tTotal Loss: 0.191938\n",
      "Reconstruction: 0.191543, Regularization: 0.000395\n",
      "2019-04-10 01:02:42,659 root         INFO     Train Epoch: 85 [7168/8000 (90%)]\tTotal Loss: 0.191429\n",
      "Reconstruction: 0.191073, Regularization: 0.000356\n",
      "2019-04-10 01:02:42,722 root         INFO     Train Epoch: 85 [7680/8000 (96%)]\tTotal Loss: 0.200737\n",
      "Reconstruction: 0.200305, Regularization: 0.000432\n",
      "2019-04-10 01:02:42,776 root         INFO     ====> Epoch: 85 Average loss: 0.1963\n",
      "2019-04-10 01:02:42,799 root         INFO     Train Epoch: 86 [0/8000 (0%)]\tTotal Loss: 0.196218\n",
      "Reconstruction: 0.195955, Regularization: 0.000263\n",
      "2019-04-10 01:02:42,864 root         INFO     Train Epoch: 86 [512/8000 (6%)]\tTotal Loss: 0.194683\n",
      "Reconstruction: 0.194384, Regularization: 0.000299\n",
      "2019-04-10 01:02:42,927 root         INFO     Train Epoch: 86 [1024/8000 (13%)]\tTotal Loss: 0.199677\n",
      "Reconstruction: 0.199197, Regularization: 0.000480\n",
      "2019-04-10 01:02:42,991 root         INFO     Train Epoch: 86 [1536/8000 (19%)]\tTotal Loss: 0.198688\n",
      "Reconstruction: 0.198426, Regularization: 0.000263\n",
      "2019-04-10 01:02:43,055 root         INFO     Train Epoch: 86 [2048/8000 (26%)]\tTotal Loss: 0.190601\n",
      "Reconstruction: 0.190380, Regularization: 0.000221\n",
      "2019-04-10 01:02:43,118 root         INFO     Train Epoch: 86 [2560/8000 (32%)]\tTotal Loss: 0.190066\n",
      "Reconstruction: 0.189774, Regularization: 0.000291\n",
      "2019-04-10 01:02:43,182 root         INFO     Train Epoch: 86 [3072/8000 (38%)]\tTotal Loss: 0.196344\n",
      "Reconstruction: 0.196120, Regularization: 0.000224\n",
      "2019-04-10 01:02:43,246 root         INFO     Train Epoch: 86 [3584/8000 (45%)]\tTotal Loss: 0.200458\n",
      "Reconstruction: 0.200087, Regularization: 0.000371\n",
      "2019-04-10 01:02:43,310 root         INFO     Train Epoch: 86 [4096/8000 (51%)]\tTotal Loss: 0.193911\n",
      "Reconstruction: 0.193657, Regularization: 0.000253\n",
      "2019-04-10 01:02:43,374 root         INFO     Train Epoch: 86 [4608/8000 (58%)]\tTotal Loss: 0.191953\n",
      "Reconstruction: 0.191772, Regularization: 0.000181\n",
      "2019-04-10 01:02:43,438 root         INFO     Train Epoch: 86 [5120/8000 (64%)]\tTotal Loss: 0.193872\n",
      "Reconstruction: 0.193572, Regularization: 0.000300\n",
      "2019-04-10 01:02:43,502 root         INFO     Train Epoch: 86 [5632/8000 (70%)]\tTotal Loss: 0.194198\n",
      "Reconstruction: 0.193776, Regularization: 0.000422\n",
      "2019-04-10 01:02:43,566 root         INFO     Train Epoch: 86 [6144/8000 (77%)]\tTotal Loss: 0.194602\n",
      "Reconstruction: 0.194333, Regularization: 0.000268\n",
      "2019-04-10 01:02:43,630 root         INFO     Train Epoch: 86 [6656/8000 (83%)]\tTotal Loss: 0.201498\n",
      "Reconstruction: 0.201179, Regularization: 0.000319\n",
      "2019-04-10 01:02:43,693 root         INFO     Train Epoch: 86 [7168/8000 (90%)]\tTotal Loss: 0.191770\n",
      "Reconstruction: 0.191412, Regularization: 0.000359\n",
      "2019-04-10 01:02:43,757 root         INFO     Train Epoch: 86 [7680/8000 (96%)]\tTotal Loss: 0.197587\n",
      "Reconstruction: 0.197278, Regularization: 0.000309\n",
      "2019-04-10 01:02:43,811 root         INFO     ====> Epoch: 86 Average loss: 0.1947\n",
      "2019-04-10 01:02:43,835 root         INFO     Train Epoch: 87 [0/8000 (0%)]\tTotal Loss: 0.189630\n",
      "Reconstruction: 0.189356, Regularization: 0.000274\n",
      "2019-04-10 01:02:43,899 root         INFO     Train Epoch: 87 [512/8000 (6%)]\tTotal Loss: 0.189336\n",
      "Reconstruction: 0.189092, Regularization: 0.000244\n",
      "2019-04-10 01:02:43,961 root         INFO     Train Epoch: 87 [1024/8000 (13%)]\tTotal Loss: 0.191056\n",
      "Reconstruction: 0.190777, Regularization: 0.000278\n",
      "2019-04-10 01:02:44,024 root         INFO     Train Epoch: 87 [1536/8000 (19%)]\tTotal Loss: 0.190836\n",
      "Reconstruction: 0.190514, Regularization: 0.000322\n",
      "2019-04-10 01:02:44,089 root         INFO     Train Epoch: 87 [2048/8000 (26%)]\tTotal Loss: 0.203017\n",
      "Reconstruction: 0.202733, Regularization: 0.000284\n",
      "2019-04-10 01:02:44,152 root         INFO     Train Epoch: 87 [2560/8000 (32%)]\tTotal Loss: 0.190301\n",
      "Reconstruction: 0.190073, Regularization: 0.000227\n",
      "2019-04-10 01:02:44,216 root         INFO     Train Epoch: 87 [3072/8000 (38%)]\tTotal Loss: 0.188560\n",
      "Reconstruction: 0.188258, Regularization: 0.000301\n",
      "2019-04-10 01:02:44,279 root         INFO     Train Epoch: 87 [3584/8000 (45%)]\tTotal Loss: 0.195476\n",
      "Reconstruction: 0.195207, Regularization: 0.000269\n",
      "2019-04-10 01:02:44,342 root         INFO     Train Epoch: 87 [4096/8000 (51%)]\tTotal Loss: 0.194198\n",
      "Reconstruction: 0.193936, Regularization: 0.000262\n",
      "2019-04-10 01:02:44,405 root         INFO     Train Epoch: 87 [4608/8000 (58%)]\tTotal Loss: 0.193462\n",
      "Reconstruction: 0.193158, Regularization: 0.000303\n",
      "2019-04-10 01:02:44,467 root         INFO     Train Epoch: 87 [5120/8000 (64%)]\tTotal Loss: 0.189002\n",
      "Reconstruction: 0.188781, Regularization: 0.000221\n",
      "2019-04-10 01:02:44,530 root         INFO     Train Epoch: 87 [5632/8000 (70%)]\tTotal Loss: 0.190186\n",
      "Reconstruction: 0.189897, Regularization: 0.000289\n",
      "2019-04-10 01:02:44,593 root         INFO     Train Epoch: 87 [6144/8000 (77%)]\tTotal Loss: 0.191673\n",
      "Reconstruction: 0.191296, Regularization: 0.000377\n",
      "2019-04-10 01:02:44,656 root         INFO     Train Epoch: 87 [6656/8000 (83%)]\tTotal Loss: 0.186802\n",
      "Reconstruction: 0.186589, Regularization: 0.000213\n",
      "2019-04-10 01:02:44,719 root         INFO     Train Epoch: 87 [7168/8000 (90%)]\tTotal Loss: 0.192040\n",
      "Reconstruction: 0.191735, Regularization: 0.000305\n",
      "2019-04-10 01:02:44,782 root         INFO     Train Epoch: 87 [7680/8000 (96%)]\tTotal Loss: 0.191628\n",
      "Reconstruction: 0.191425, Regularization: 0.000203\n",
      "2019-04-10 01:02:44,836 root         INFO     ====> Epoch: 87 Average loss: 0.1929\n",
      "2019-04-10 01:02:44,859 root         INFO     Train Epoch: 88 [0/8000 (0%)]\tTotal Loss: 0.193594\n",
      "Reconstruction: 0.193352, Regularization: 0.000241\n",
      "2019-04-10 01:02:44,924 root         INFO     Train Epoch: 88 [512/8000 (6%)]\tTotal Loss: 0.189647\n",
      "Reconstruction: 0.189270, Regularization: 0.000377\n",
      "2019-04-10 01:02:44,987 root         INFO     Train Epoch: 88 [1024/8000 (13%)]\tTotal Loss: 0.184571\n",
      "Reconstruction: 0.184300, Regularization: 0.000271\n",
      "2019-04-10 01:02:45,051 root         INFO     Train Epoch: 88 [1536/8000 (19%)]\tTotal Loss: 0.197885\n",
      "Reconstruction: 0.197589, Regularization: 0.000296\n",
      "2019-04-10 01:02:45,115 root         INFO     Train Epoch: 88 [2048/8000 (26%)]\tTotal Loss: 0.187926\n",
      "Reconstruction: 0.187762, Regularization: 0.000164\n",
      "2019-04-10 01:02:45,179 root         INFO     Train Epoch: 88 [2560/8000 (32%)]\tTotal Loss: 0.185160\n",
      "Reconstruction: 0.184817, Regularization: 0.000343\n",
      "2019-04-10 01:02:45,243 root         INFO     Train Epoch: 88 [3072/8000 (38%)]\tTotal Loss: 0.186863\n",
      "Reconstruction: 0.186659, Regularization: 0.000204\n",
      "2019-04-10 01:02:45,307 root         INFO     Train Epoch: 88 [3584/8000 (45%)]\tTotal Loss: 0.188358\n",
      "Reconstruction: 0.188189, Regularization: 0.000169\n",
      "2019-04-10 01:02:45,371 root         INFO     Train Epoch: 88 [4096/8000 (51%)]\tTotal Loss: 0.206218\n",
      "Reconstruction: 0.205844, Regularization: 0.000374\n",
      "2019-04-10 01:02:45,435 root         INFO     Train Epoch: 88 [4608/8000 (58%)]\tTotal Loss: 0.186161\n",
      "Reconstruction: 0.185974, Regularization: 0.000187\n",
      "2019-04-10 01:02:45,499 root         INFO     Train Epoch: 88 [5120/8000 (64%)]\tTotal Loss: 0.189091\n",
      "Reconstruction: 0.188848, Regularization: 0.000243\n",
      "2019-04-10 01:02:45,563 root         INFO     Train Epoch: 88 [5632/8000 (70%)]\tTotal Loss: 0.192056\n",
      "Reconstruction: 0.191679, Regularization: 0.000377\n",
      "2019-04-10 01:02:45,626 root         INFO     Train Epoch: 88 [6144/8000 (77%)]\tTotal Loss: 0.189445\n",
      "Reconstruction: 0.189249, Regularization: 0.000195\n",
      "2019-04-10 01:02:45,689 root         INFO     Train Epoch: 88 [6656/8000 (83%)]\tTotal Loss: 0.187058\n",
      "Reconstruction: 0.186856, Regularization: 0.000202\n",
      "2019-04-10 01:02:45,751 root         INFO     Train Epoch: 88 [7168/8000 (90%)]\tTotal Loss: 0.187409\n",
      "Reconstruction: 0.187120, Regularization: 0.000289\n",
      "2019-04-10 01:02:45,815 root         INFO     Train Epoch: 88 [7680/8000 (96%)]\tTotal Loss: 0.191292\n",
      "Reconstruction: 0.190968, Regularization: 0.000323\n",
      "2019-04-10 01:02:45,868 root         INFO     ====> Epoch: 88 Average loss: 0.1908\n",
      "2019-04-10 01:02:45,892 root         INFO     Train Epoch: 89 [0/8000 (0%)]\tTotal Loss: 0.183726\n",
      "Reconstruction: 0.183383, Regularization: 0.000343\n",
      "2019-04-10 01:02:45,955 root         INFO     Train Epoch: 89 [512/8000 (6%)]\tTotal Loss: 0.201801\n",
      "Reconstruction: 0.201356, Regularization: 0.000445\n",
      "2019-04-10 01:02:46,017 root         INFO     Train Epoch: 89 [1024/8000 (13%)]\tTotal Loss: 0.189718\n",
      "Reconstruction: 0.189475, Regularization: 0.000243\n",
      "2019-04-10 01:02:46,080 root         INFO     Train Epoch: 89 [1536/8000 (19%)]\tTotal Loss: 0.195939\n",
      "Reconstruction: 0.195656, Regularization: 0.000283\n",
      "2019-04-10 01:02:46,144 root         INFO     Train Epoch: 89 [2048/8000 (26%)]\tTotal Loss: 0.188067\n",
      "Reconstruction: 0.187687, Regularization: 0.000380\n",
      "2019-04-10 01:02:46,206 root         INFO     Train Epoch: 89 [2560/8000 (32%)]\tTotal Loss: 0.186867\n",
      "Reconstruction: 0.186536, Regularization: 0.000330\n",
      "2019-04-10 01:02:46,270 root         INFO     Train Epoch: 89 [3072/8000 (38%)]\tTotal Loss: 0.194614\n",
      "Reconstruction: 0.194283, Regularization: 0.000331\n",
      "2019-04-10 01:02:46,333 root         INFO     Train Epoch: 89 [3584/8000 (45%)]\tTotal Loss: 0.191084\n",
      "Reconstruction: 0.190795, Regularization: 0.000289\n",
      "2019-04-10 01:02:46,396 root         INFO     Train Epoch: 89 [4096/8000 (51%)]\tTotal Loss: 0.187922\n",
      "Reconstruction: 0.187643, Regularization: 0.000279\n",
      "2019-04-10 01:02:46,459 root         INFO     Train Epoch: 89 [4608/8000 (58%)]\tTotal Loss: 0.186194\n",
      "Reconstruction: 0.185889, Regularization: 0.000305\n",
      "2019-04-10 01:02:46,522 root         INFO     Train Epoch: 89 [5120/8000 (64%)]\tTotal Loss: 0.184587\n",
      "Reconstruction: 0.184280, Regularization: 0.000306\n",
      "2019-04-10 01:02:46,586 root         INFO     Train Epoch: 89 [5632/8000 (70%)]\tTotal Loss: 0.184310\n",
      "Reconstruction: 0.184049, Regularization: 0.000261\n",
      "2019-04-10 01:02:46,649 root         INFO     Train Epoch: 89 [6144/8000 (77%)]\tTotal Loss: 0.196010\n",
      "Reconstruction: 0.195731, Regularization: 0.000280\n",
      "2019-04-10 01:02:46,713 root         INFO     Train Epoch: 89 [6656/8000 (83%)]\tTotal Loss: 0.183605\n",
      "Reconstruction: 0.183425, Regularization: 0.000180\n",
      "2019-04-10 01:02:46,775 root         INFO     Train Epoch: 89 [7168/8000 (90%)]\tTotal Loss: 0.190443\n",
      "Reconstruction: 0.190222, Regularization: 0.000221\n",
      "2019-04-10 01:02:46,838 root         INFO     Train Epoch: 89 [7680/8000 (96%)]\tTotal Loss: 0.182111\n",
      "Reconstruction: 0.181787, Regularization: 0.000323\n",
      "2019-04-10 01:02:46,892 root         INFO     ====> Epoch: 89 Average loss: 0.1884\n",
      "2019-04-10 01:02:46,916 root         INFO     Train Epoch: 90 [0/8000 (0%)]\tTotal Loss: 0.185905\n",
      "Reconstruction: 0.185628, Regularization: 0.000277\n",
      "2019-04-10 01:02:46,979 root         INFO     Train Epoch: 90 [512/8000 (6%)]\tTotal Loss: 0.184226\n",
      "Reconstruction: 0.183834, Regularization: 0.000391\n",
      "2019-04-10 01:02:47,042 root         INFO     Train Epoch: 90 [1024/8000 (13%)]\tTotal Loss: 0.186843\n",
      "Reconstruction: 0.186584, Regularization: 0.000259\n",
      "2019-04-10 01:02:47,105 root         INFO     Train Epoch: 90 [1536/8000 (19%)]\tTotal Loss: 0.188393\n",
      "Reconstruction: 0.188114, Regularization: 0.000279\n",
      "2019-04-10 01:02:47,167 root         INFO     Train Epoch: 90 [2048/8000 (26%)]\tTotal Loss: 0.183622\n",
      "Reconstruction: 0.183317, Regularization: 0.000305\n",
      "2019-04-10 01:02:47,229 root         INFO     Train Epoch: 90 [2560/8000 (32%)]\tTotal Loss: 0.184437\n",
      "Reconstruction: 0.184216, Regularization: 0.000220\n",
      "2019-04-10 01:02:47,291 root         INFO     Train Epoch: 90 [3072/8000 (38%)]\tTotal Loss: 0.192269\n",
      "Reconstruction: 0.191749, Regularization: 0.000520\n",
      "2019-04-10 01:02:47,353 root         INFO     Train Epoch: 90 [3584/8000 (45%)]\tTotal Loss: 0.181997\n",
      "Reconstruction: 0.181679, Regularization: 0.000318\n",
      "2019-04-10 01:02:47,414 root         INFO     Train Epoch: 90 [4096/8000 (51%)]\tTotal Loss: 0.185504\n",
      "Reconstruction: 0.185056, Regularization: 0.000448\n",
      "2019-04-10 01:02:47,476 root         INFO     Train Epoch: 90 [4608/8000 (58%)]\tTotal Loss: 0.180128\n",
      "Reconstruction: 0.179918, Regularization: 0.000209\n",
      "2019-04-10 01:02:47,537 root         INFO     Train Epoch: 90 [5120/8000 (64%)]\tTotal Loss: 0.187871\n",
      "Reconstruction: 0.187629, Regularization: 0.000242\n",
      "2019-04-10 01:02:47,599 root         INFO     Train Epoch: 90 [5632/8000 (70%)]\tTotal Loss: 0.190242\n",
      "Reconstruction: 0.189812, Regularization: 0.000429\n",
      "2019-04-10 01:02:47,660 root         INFO     Train Epoch: 90 [6144/8000 (77%)]\tTotal Loss: 0.181702\n",
      "Reconstruction: 0.181386, Regularization: 0.000316\n",
      "2019-04-10 01:02:47,721 root         INFO     Train Epoch: 90 [6656/8000 (83%)]\tTotal Loss: 0.187765\n",
      "Reconstruction: 0.187536, Regularization: 0.000229\n",
      "2019-04-10 01:02:47,783 root         INFO     Train Epoch: 90 [7168/8000 (90%)]\tTotal Loss: 0.177590\n",
      "Reconstruction: 0.177355, Regularization: 0.000235\n",
      "2019-04-10 01:02:47,844 root         INFO     Train Epoch: 90 [7680/8000 (96%)]\tTotal Loss: 0.178892\n",
      "Reconstruction: 0.178629, Regularization: 0.000262\n",
      "2019-04-10 01:02:47,898 root         INFO     ====> Epoch: 90 Average loss: 0.1855\n",
      "2019-04-10 01:02:47,922 root         INFO     Train Epoch: 91 [0/8000 (0%)]\tTotal Loss: 0.182066\n",
      "Reconstruction: 0.181926, Regularization: 0.000141\n",
      "2019-04-10 01:02:47,985 root         INFO     Train Epoch: 91 [512/8000 (6%)]\tTotal Loss: 0.184897\n",
      "Reconstruction: 0.184628, Regularization: 0.000269\n",
      "2019-04-10 01:02:48,047 root         INFO     Train Epoch: 91 [1024/8000 (13%)]\tTotal Loss: 0.184103\n",
      "Reconstruction: 0.183834, Regularization: 0.000268\n",
      "2019-04-10 01:02:48,110 root         INFO     Train Epoch: 91 [1536/8000 (19%)]\tTotal Loss: 0.181509\n",
      "Reconstruction: 0.181175, Regularization: 0.000334\n",
      "2019-04-10 01:02:48,173 root         INFO     Train Epoch: 91 [2048/8000 (26%)]\tTotal Loss: 0.180746\n",
      "Reconstruction: 0.180488, Regularization: 0.000258\n",
      "2019-04-10 01:02:48,235 root         INFO     Train Epoch: 91 [2560/8000 (32%)]\tTotal Loss: 0.182127\n",
      "Reconstruction: 0.181829, Regularization: 0.000298\n",
      "2019-04-10 01:02:48,298 root         INFO     Train Epoch: 91 [3072/8000 (38%)]\tTotal Loss: 0.180164\n",
      "Reconstruction: 0.179859, Regularization: 0.000305\n",
      "2019-04-10 01:02:48,361 root         INFO     Train Epoch: 91 [3584/8000 (45%)]\tTotal Loss: 0.176677\n",
      "Reconstruction: 0.176477, Regularization: 0.000199\n",
      "2019-04-10 01:02:48,423 root         INFO     Train Epoch: 91 [4096/8000 (51%)]\tTotal Loss: 0.174394\n",
      "Reconstruction: 0.174177, Regularization: 0.000216\n",
      "2019-04-10 01:02:48,486 root         INFO     Train Epoch: 91 [4608/8000 (58%)]\tTotal Loss: 0.181177\n",
      "Reconstruction: 0.180834, Regularization: 0.000343\n",
      "2019-04-10 01:02:48,549 root         INFO     Train Epoch: 91 [5120/8000 (64%)]\tTotal Loss: 0.183703\n",
      "Reconstruction: 0.183315, Regularization: 0.000387\n",
      "2019-04-10 01:02:48,612 root         INFO     Train Epoch: 91 [5632/8000 (70%)]\tTotal Loss: 0.178511\n",
      "Reconstruction: 0.178279, Regularization: 0.000232\n",
      "2019-04-10 01:02:48,675 root         INFO     Train Epoch: 91 [6144/8000 (77%)]\tTotal Loss: 0.181041\n",
      "Reconstruction: 0.180697, Regularization: 0.000344\n",
      "2019-04-10 01:02:48,737 root         INFO     Train Epoch: 91 [6656/8000 (83%)]\tTotal Loss: 0.179103\n",
      "Reconstruction: 0.178754, Regularization: 0.000349\n",
      "2019-04-10 01:02:48,800 root         INFO     Train Epoch: 91 [7168/8000 (90%)]\tTotal Loss: 0.175393\n",
      "Reconstruction: 0.175233, Regularization: 0.000160\n",
      "2019-04-10 01:02:48,862 root         INFO     Train Epoch: 91 [7680/8000 (96%)]\tTotal Loss: 0.185090\n",
      "Reconstruction: 0.184874, Regularization: 0.000216\n",
      "2019-04-10 01:02:48,916 root         INFO     ====> Epoch: 91 Average loss: 0.1821\n",
      "2019-04-10 01:02:48,940 root         INFO     Train Epoch: 92 [0/8000 (0%)]\tTotal Loss: 0.179154\n",
      "Reconstruction: 0.178779, Regularization: 0.000375\n",
      "2019-04-10 01:02:49,003 root         INFO     Train Epoch: 92 [512/8000 (6%)]\tTotal Loss: 0.170994\n",
      "Reconstruction: 0.170660, Regularization: 0.000334\n",
      "2019-04-10 01:02:49,067 root         INFO     Train Epoch: 92 [1024/8000 (13%)]\tTotal Loss: 0.174009\n",
      "Reconstruction: 0.173686, Regularization: 0.000323\n",
      "2019-04-10 01:02:49,130 root         INFO     Train Epoch: 92 [1536/8000 (19%)]\tTotal Loss: 0.183785\n",
      "Reconstruction: 0.183421, Regularization: 0.000364\n",
      "2019-04-10 01:02:49,193 root         INFO     Train Epoch: 92 [2048/8000 (26%)]\tTotal Loss: 0.178085\n",
      "Reconstruction: 0.177732, Regularization: 0.000353\n",
      "2019-04-10 01:02:49,256 root         INFO     Train Epoch: 92 [2560/8000 (32%)]\tTotal Loss: 0.178674\n",
      "Reconstruction: 0.178421, Regularization: 0.000253\n",
      "2019-04-10 01:02:49,320 root         INFO     Train Epoch: 92 [3072/8000 (38%)]\tTotal Loss: 0.175923\n",
      "Reconstruction: 0.175718, Regularization: 0.000205\n",
      "2019-04-10 01:02:49,383 root         INFO     Train Epoch: 92 [3584/8000 (45%)]\tTotal Loss: 0.172950\n",
      "Reconstruction: 0.172688, Regularization: 0.000262\n",
      "2019-04-10 01:02:49,446 root         INFO     Train Epoch: 92 [4096/8000 (51%)]\tTotal Loss: 0.179665\n",
      "Reconstruction: 0.179338, Regularization: 0.000327\n",
      "2019-04-10 01:02:49,510 root         INFO     Train Epoch: 92 [4608/8000 (58%)]\tTotal Loss: 0.182245\n",
      "Reconstruction: 0.181988, Regularization: 0.000257\n",
      "2019-04-10 01:02:49,573 root         INFO     Train Epoch: 92 [5120/8000 (64%)]\tTotal Loss: 0.181032\n",
      "Reconstruction: 0.180659, Regularization: 0.000372\n",
      "2019-04-10 01:02:49,636 root         INFO     Train Epoch: 92 [5632/8000 (70%)]\tTotal Loss: 0.174527\n",
      "Reconstruction: 0.174293, Regularization: 0.000234\n",
      "2019-04-10 01:02:49,699 root         INFO     Train Epoch: 92 [6144/8000 (77%)]\tTotal Loss: 0.182356\n",
      "Reconstruction: 0.181907, Regularization: 0.000449\n",
      "2019-04-10 01:02:49,762 root         INFO     Train Epoch: 92 [6656/8000 (83%)]\tTotal Loss: 0.185451\n",
      "Reconstruction: 0.185093, Regularization: 0.000358\n",
      "2019-04-10 01:02:49,826 root         INFO     Train Epoch: 92 [7168/8000 (90%)]\tTotal Loss: 0.169329\n",
      "Reconstruction: 0.169135, Regularization: 0.000194\n",
      "2019-04-10 01:02:49,889 root         INFO     Train Epoch: 92 [7680/8000 (96%)]\tTotal Loss: 0.192107\n",
      "Reconstruction: 0.191708, Regularization: 0.000400\n",
      "2019-04-10 01:02:49,943 root         INFO     ====> Epoch: 92 Average loss: 0.1783\n",
      "2019-04-10 01:02:49,967 root         INFO     Train Epoch: 93 [0/8000 (0%)]\tTotal Loss: 0.178166\n",
      "Reconstruction: 0.177825, Regularization: 0.000342\n",
      "2019-04-10 01:02:50,031 root         INFO     Train Epoch: 93 [512/8000 (6%)]\tTotal Loss: 0.170501\n",
      "Reconstruction: 0.170159, Regularization: 0.000342\n",
      "2019-04-10 01:02:50,095 root         INFO     Train Epoch: 93 [1024/8000 (13%)]\tTotal Loss: 0.174025\n",
      "Reconstruction: 0.173733, Regularization: 0.000292\n",
      "2019-04-10 01:02:50,159 root         INFO     Train Epoch: 93 [1536/8000 (19%)]\tTotal Loss: 0.170203\n",
      "Reconstruction: 0.169886, Regularization: 0.000317\n",
      "2019-04-10 01:02:50,222 root         INFO     Train Epoch: 93 [2048/8000 (26%)]\tTotal Loss: 0.179451\n",
      "Reconstruction: 0.179085, Regularization: 0.000366\n",
      "2019-04-10 01:02:50,286 root         INFO     Train Epoch: 93 [2560/8000 (32%)]\tTotal Loss: 0.180834\n",
      "Reconstruction: 0.180547, Regularization: 0.000287\n",
      "2019-04-10 01:02:50,350 root         INFO     Train Epoch: 93 [3072/8000 (38%)]\tTotal Loss: 0.168139\n",
      "Reconstruction: 0.167880, Regularization: 0.000259\n",
      "2019-04-10 01:02:50,414 root         INFO     Train Epoch: 93 [3584/8000 (45%)]\tTotal Loss: 0.172997\n",
      "Reconstruction: 0.172664, Regularization: 0.000333\n",
      "2019-04-10 01:02:50,478 root         INFO     Train Epoch: 93 [4096/8000 (51%)]\tTotal Loss: 0.172035\n",
      "Reconstruction: 0.171689, Regularization: 0.000345\n",
      "2019-04-10 01:02:50,542 root         INFO     Train Epoch: 93 [4608/8000 (58%)]\tTotal Loss: 0.182384\n",
      "Reconstruction: 0.182066, Regularization: 0.000318\n",
      "2019-04-10 01:02:50,606 root         INFO     Train Epoch: 93 [5120/8000 (64%)]\tTotal Loss: 0.173383\n",
      "Reconstruction: 0.173111, Regularization: 0.000273\n",
      "2019-04-10 01:02:50,670 root         INFO     Train Epoch: 93 [5632/8000 (70%)]\tTotal Loss: 0.180760\n",
      "Reconstruction: 0.180350, Regularization: 0.000410\n",
      "2019-04-10 01:02:50,733 root         INFO     Train Epoch: 93 [6144/8000 (77%)]\tTotal Loss: 0.163541\n",
      "Reconstruction: 0.163191, Regularization: 0.000350\n",
      "2019-04-10 01:02:50,797 root         INFO     Train Epoch: 93 [6656/8000 (83%)]\tTotal Loss: 0.175864\n",
      "Reconstruction: 0.175445, Regularization: 0.000419\n",
      "2019-04-10 01:02:50,861 root         INFO     Train Epoch: 93 [7168/8000 (90%)]\tTotal Loss: 0.162951\n",
      "Reconstruction: 0.162727, Regularization: 0.000224\n",
      "2019-04-10 01:02:50,924 root         INFO     Train Epoch: 93 [7680/8000 (96%)]\tTotal Loss: 0.166102\n",
      "Reconstruction: 0.165808, Regularization: 0.000294\n",
      "2019-04-10 01:02:50,978 root         INFO     ====> Epoch: 93 Average loss: 0.1743\n",
      "2019-04-10 01:02:51,002 root         INFO     Train Epoch: 94 [0/8000 (0%)]\tTotal Loss: 0.170259\n",
      "Reconstruction: 0.170008, Regularization: 0.000251\n",
      "2019-04-10 01:02:51,065 root         INFO     Train Epoch: 94 [512/8000 (6%)]\tTotal Loss: 0.168029\n",
      "Reconstruction: 0.167679, Regularization: 0.000351\n",
      "2019-04-10 01:02:51,128 root         INFO     Train Epoch: 94 [1024/8000 (13%)]\tTotal Loss: 0.181436\n",
      "Reconstruction: 0.180840, Regularization: 0.000596\n",
      "2019-04-10 01:02:51,192 root         INFO     Train Epoch: 94 [1536/8000 (19%)]\tTotal Loss: 0.169029\n",
      "Reconstruction: 0.168696, Regularization: 0.000333\n",
      "2019-04-10 01:02:51,254 root         INFO     Train Epoch: 94 [2048/8000 (26%)]\tTotal Loss: 0.183309\n",
      "Reconstruction: 0.182978, Regularization: 0.000331\n",
      "2019-04-10 01:02:51,318 root         INFO     Train Epoch: 94 [2560/8000 (32%)]\tTotal Loss: 0.178228\n",
      "Reconstruction: 0.177768, Regularization: 0.000460\n",
      "2019-04-10 01:02:51,380 root         INFO     Train Epoch: 94 [3072/8000 (38%)]\tTotal Loss: 0.172008\n",
      "Reconstruction: 0.171652, Regularization: 0.000356\n",
      "2019-04-10 01:02:51,443 root         INFO     Train Epoch: 94 [3584/8000 (45%)]\tTotal Loss: 0.172834\n",
      "Reconstruction: 0.172458, Regularization: 0.000376\n",
      "2019-04-10 01:02:51,506 root         INFO     Train Epoch: 94 [4096/8000 (51%)]\tTotal Loss: 0.159547\n",
      "Reconstruction: 0.159295, Regularization: 0.000251\n",
      "2019-04-10 01:02:51,570 root         INFO     Train Epoch: 94 [4608/8000 (58%)]\tTotal Loss: 0.176954\n",
      "Reconstruction: 0.176512, Regularization: 0.000441\n",
      "2019-04-10 01:02:51,633 root         INFO     Train Epoch: 94 [5120/8000 (64%)]\tTotal Loss: 0.165769\n",
      "Reconstruction: 0.165385, Regularization: 0.000384\n",
      "2019-04-10 01:02:51,695 root         INFO     Train Epoch: 94 [5632/8000 (70%)]\tTotal Loss: 0.161802\n",
      "Reconstruction: 0.161444, Regularization: 0.000359\n",
      "2019-04-10 01:02:51,758 root         INFO     Train Epoch: 94 [6144/8000 (77%)]\tTotal Loss: 0.174507\n",
      "Reconstruction: 0.174116, Regularization: 0.000391\n",
      "2019-04-10 01:02:51,821 root         INFO     Train Epoch: 94 [6656/8000 (83%)]\tTotal Loss: 0.169669\n",
      "Reconstruction: 0.169309, Regularization: 0.000360\n",
      "2019-04-10 01:02:51,884 root         INFO     Train Epoch: 94 [7168/8000 (90%)]\tTotal Loss: 0.177440\n",
      "Reconstruction: 0.177088, Regularization: 0.000352\n",
      "2019-04-10 01:02:51,947 root         INFO     Train Epoch: 94 [7680/8000 (96%)]\tTotal Loss: 0.180378\n",
      "Reconstruction: 0.179818, Regularization: 0.000559\n",
      "2019-04-10 01:02:52,001 root         INFO     ====> Epoch: 94 Average loss: 0.1707\n",
      "2019-04-10 01:02:52,025 root         INFO     Train Epoch: 95 [0/8000 (0%)]\tTotal Loss: 0.168910\n",
      "Reconstruction: 0.168617, Regularization: 0.000293\n",
      "2019-04-10 01:02:52,088 root         INFO     Train Epoch: 95 [512/8000 (6%)]\tTotal Loss: 0.171733\n",
      "Reconstruction: 0.171393, Regularization: 0.000340\n",
      "2019-04-10 01:02:52,152 root         INFO     Train Epoch: 95 [1024/8000 (13%)]\tTotal Loss: 0.172661\n",
      "Reconstruction: 0.172104, Regularization: 0.000556\n",
      "2019-04-10 01:02:52,215 root         INFO     Train Epoch: 95 [1536/8000 (19%)]\tTotal Loss: 0.174690\n",
      "Reconstruction: 0.174326, Regularization: 0.000364\n",
      "2019-04-10 01:02:52,278 root         INFO     Train Epoch: 95 [2048/8000 (26%)]\tTotal Loss: 0.155242\n",
      "Reconstruction: 0.155025, Regularization: 0.000218\n",
      "2019-04-10 01:02:52,342 root         INFO     Train Epoch: 95 [2560/8000 (32%)]\tTotal Loss: 0.174956\n",
      "Reconstruction: 0.174585, Regularization: 0.000371\n",
      "2019-04-10 01:02:52,405 root         INFO     Train Epoch: 95 [3072/8000 (38%)]\tTotal Loss: 0.159322\n",
      "Reconstruction: 0.159067, Regularization: 0.000254\n",
      "2019-04-10 01:02:52,469 root         INFO     Train Epoch: 95 [3584/8000 (45%)]\tTotal Loss: 0.172349\n",
      "Reconstruction: 0.171955, Regularization: 0.000395\n",
      "2019-04-10 01:02:52,533 root         INFO     Train Epoch: 95 [4096/8000 (51%)]\tTotal Loss: 0.168479\n",
      "Reconstruction: 0.168052, Regularization: 0.000427\n",
      "2019-04-10 01:02:52,597 root         INFO     Train Epoch: 95 [4608/8000 (58%)]\tTotal Loss: 0.182011\n",
      "Reconstruction: 0.181569, Regularization: 0.000442\n",
      "2019-04-10 01:02:52,660 root         INFO     Train Epoch: 95 [5120/8000 (64%)]\tTotal Loss: 0.158721\n",
      "Reconstruction: 0.158440, Regularization: 0.000281\n",
      "2019-04-10 01:02:52,723 root         INFO     Train Epoch: 95 [5632/8000 (70%)]\tTotal Loss: 0.161296\n",
      "Reconstruction: 0.161026, Regularization: 0.000270\n",
      "2019-04-10 01:02:52,786 root         INFO     Train Epoch: 95 [6144/8000 (77%)]\tTotal Loss: 0.170610\n",
      "Reconstruction: 0.170212, Regularization: 0.000398\n",
      "2019-04-10 01:02:52,849 root         INFO     Train Epoch: 95 [6656/8000 (83%)]\tTotal Loss: 0.173195\n",
      "Reconstruction: 0.172709, Regularization: 0.000486\n",
      "2019-04-10 01:02:52,913 root         INFO     Train Epoch: 95 [7168/8000 (90%)]\tTotal Loss: 0.161405\n",
      "Reconstruction: 0.161094, Regularization: 0.000311\n",
      "2019-04-10 01:02:52,977 root         INFO     Train Epoch: 95 [7680/8000 (96%)]\tTotal Loss: 0.170163\n",
      "Reconstruction: 0.169677, Regularization: 0.000486\n",
      "2019-04-10 01:02:53,030 root         INFO     ====> Epoch: 95 Average loss: 0.1681\n",
      "2019-04-10 01:02:53,054 root         INFO     Train Epoch: 96 [0/8000 (0%)]\tTotal Loss: 0.160402\n",
      "Reconstruction: 0.160139, Regularization: 0.000263\n",
      "2019-04-10 01:02:53,119 root         INFO     Train Epoch: 96 [512/8000 (6%)]\tTotal Loss: 0.169825\n",
      "Reconstruction: 0.169413, Regularization: 0.000412\n",
      "2019-04-10 01:02:53,182 root         INFO     Train Epoch: 96 [1024/8000 (13%)]\tTotal Loss: 0.161632\n",
      "Reconstruction: 0.161392, Regularization: 0.000240\n",
      "2019-04-10 01:02:53,244 root         INFO     Train Epoch: 96 [1536/8000 (19%)]\tTotal Loss: 0.170123\n",
      "Reconstruction: 0.169815, Regularization: 0.000308\n",
      "2019-04-10 01:02:53,307 root         INFO     Train Epoch: 96 [2048/8000 (26%)]\tTotal Loss: 0.162451\n",
      "Reconstruction: 0.162191, Regularization: 0.000260\n",
      "2019-04-10 01:02:53,370 root         INFO     Train Epoch: 96 [2560/8000 (32%)]\tTotal Loss: 0.175805\n",
      "Reconstruction: 0.175399, Regularization: 0.000406\n",
      "2019-04-10 01:02:53,432 root         INFO     Train Epoch: 96 [3072/8000 (38%)]\tTotal Loss: 0.164823\n",
      "Reconstruction: 0.164474, Regularization: 0.000349\n",
      "2019-04-10 01:02:53,495 root         INFO     Train Epoch: 96 [3584/8000 (45%)]\tTotal Loss: 0.172586\n",
      "Reconstruction: 0.172224, Regularization: 0.000362\n",
      "2019-04-10 01:02:53,557 root         INFO     Train Epoch: 96 [4096/8000 (51%)]\tTotal Loss: 0.155268\n",
      "Reconstruction: 0.155062, Regularization: 0.000205\n",
      "2019-04-10 01:02:53,620 root         INFO     Train Epoch: 96 [4608/8000 (58%)]\tTotal Loss: 0.168509\n",
      "Reconstruction: 0.168192, Regularization: 0.000317\n",
      "2019-04-10 01:02:53,682 root         INFO     Train Epoch: 96 [5120/8000 (64%)]\tTotal Loss: 0.158101\n",
      "Reconstruction: 0.157750, Regularization: 0.000351\n",
      "2019-04-10 01:02:53,744 root         INFO     Train Epoch: 96 [5632/8000 (70%)]\tTotal Loss: 0.173043\n",
      "Reconstruction: 0.172480, Regularization: 0.000563\n",
      "2019-04-10 01:02:53,806 root         INFO     Train Epoch: 96 [6144/8000 (77%)]\tTotal Loss: 0.160644\n",
      "Reconstruction: 0.160297, Regularization: 0.000347\n",
      "2019-04-10 01:02:53,867 root         INFO     Train Epoch: 96 [6656/8000 (83%)]\tTotal Loss: 0.175240\n",
      "Reconstruction: 0.174773, Regularization: 0.000467\n",
      "2019-04-10 01:02:53,929 root         INFO     Train Epoch: 96 [7168/8000 (90%)]\tTotal Loss: 0.163122\n",
      "Reconstruction: 0.162785, Regularization: 0.000337\n",
      "2019-04-10 01:02:53,990 root         INFO     Train Epoch: 96 [7680/8000 (96%)]\tTotal Loss: 0.167731\n",
      "Reconstruction: 0.167434, Regularization: 0.000298\n",
      "2019-04-10 01:02:54,043 root         INFO     ====> Epoch: 96 Average loss: 0.1669\n",
      "2019-04-10 01:02:54,067 root         INFO     Train Epoch: 97 [0/8000 (0%)]\tTotal Loss: 0.163604\n",
      "Reconstruction: 0.163254, Regularization: 0.000350\n",
      "2019-04-10 01:02:54,130 root         INFO     Train Epoch: 97 [512/8000 (6%)]\tTotal Loss: 0.175447\n",
      "Reconstruction: 0.175042, Regularization: 0.000405\n",
      "2019-04-10 01:02:54,193 root         INFO     Train Epoch: 97 [1024/8000 (13%)]\tTotal Loss: 0.159013\n",
      "Reconstruction: 0.158747, Regularization: 0.000265\n",
      "2019-04-10 01:02:54,257 root         INFO     Train Epoch: 97 [1536/8000 (19%)]\tTotal Loss: 0.164686\n",
      "Reconstruction: 0.164321, Regularization: 0.000365\n",
      "2019-04-10 01:02:54,320 root         INFO     Train Epoch: 97 [2048/8000 (26%)]\tTotal Loss: 0.156234\n",
      "Reconstruction: 0.155986, Regularization: 0.000248\n",
      "2019-04-10 01:02:54,382 root         INFO     Train Epoch: 97 [2560/8000 (32%)]\tTotal Loss: 0.172931\n",
      "Reconstruction: 0.172575, Regularization: 0.000356\n",
      "2019-04-10 01:02:54,445 root         INFO     Train Epoch: 97 [3072/8000 (38%)]\tTotal Loss: 0.156894\n",
      "Reconstruction: 0.156584, Regularization: 0.000310\n",
      "2019-04-10 01:02:54,509 root         INFO     Train Epoch: 97 [3584/8000 (45%)]\tTotal Loss: 0.162325\n",
      "Reconstruction: 0.162072, Regularization: 0.000253\n",
      "2019-04-10 01:02:54,571 root         INFO     Train Epoch: 97 [4096/8000 (51%)]\tTotal Loss: 0.174083\n",
      "Reconstruction: 0.173760, Regularization: 0.000323\n",
      "2019-04-10 01:02:54,633 root         INFO     Train Epoch: 97 [4608/8000 (58%)]\tTotal Loss: 0.165198\n",
      "Reconstruction: 0.164748, Regularization: 0.000450\n",
      "2019-04-10 01:02:54,695 root         INFO     Train Epoch: 97 [5120/8000 (64%)]\tTotal Loss: 0.156206\n",
      "Reconstruction: 0.155947, Regularization: 0.000259\n",
      "2019-04-10 01:02:54,757 root         INFO     Train Epoch: 97 [5632/8000 (70%)]\tTotal Loss: 0.163719\n",
      "Reconstruction: 0.163415, Regularization: 0.000303\n",
      "2019-04-10 01:02:54,819 root         INFO     Train Epoch: 97 [6144/8000 (77%)]\tTotal Loss: 0.176972\n",
      "Reconstruction: 0.176581, Regularization: 0.000391\n",
      "2019-04-10 01:02:54,881 root         INFO     Train Epoch: 97 [6656/8000 (83%)]\tTotal Loss: 0.166089\n",
      "Reconstruction: 0.165709, Regularization: 0.000380\n",
      "2019-04-10 01:02:54,943 root         INFO     Train Epoch: 97 [7168/8000 (90%)]\tTotal Loss: 0.166850\n",
      "Reconstruction: 0.166505, Regularization: 0.000344\n",
      "2019-04-10 01:02:55,005 root         INFO     Train Epoch: 97 [7680/8000 (96%)]\tTotal Loss: 0.153875\n",
      "Reconstruction: 0.153591, Regularization: 0.000284\n",
      "2019-04-10 01:02:55,058 root         INFO     ====> Epoch: 97 Average loss: 0.1665\n",
      "2019-04-10 01:02:55,082 root         INFO     Train Epoch: 98 [0/8000 (0%)]\tTotal Loss: 0.161823\n",
      "Reconstruction: 0.161413, Regularization: 0.000410\n",
      "2019-04-10 01:02:55,146 root         INFO     Train Epoch: 98 [512/8000 (6%)]\tTotal Loss: 0.164129\n",
      "Reconstruction: 0.163671, Regularization: 0.000459\n",
      "2019-04-10 01:02:55,209 root         INFO     Train Epoch: 98 [1024/8000 (13%)]\tTotal Loss: 0.170398\n",
      "Reconstruction: 0.169895, Regularization: 0.000502\n",
      "2019-04-10 01:02:55,272 root         INFO     Train Epoch: 98 [1536/8000 (19%)]\tTotal Loss: 0.162672\n",
      "Reconstruction: 0.162270, Regularization: 0.000402\n",
      "2019-04-10 01:02:55,335 root         INFO     Train Epoch: 98 [2048/8000 (26%)]\tTotal Loss: 0.169773\n",
      "Reconstruction: 0.169345, Regularization: 0.000428\n",
      "2019-04-10 01:02:55,398 root         INFO     Train Epoch: 98 [2560/8000 (32%)]\tTotal Loss: 0.155486\n",
      "Reconstruction: 0.155233, Regularization: 0.000253\n",
      "2019-04-10 01:02:55,461 root         INFO     Train Epoch: 98 [3072/8000 (38%)]\tTotal Loss: 0.164045\n",
      "Reconstruction: 0.163709, Regularization: 0.000337\n",
      "2019-04-10 01:02:55,524 root         INFO     Train Epoch: 98 [3584/8000 (45%)]\tTotal Loss: 0.164688\n",
      "Reconstruction: 0.164299, Regularization: 0.000389\n",
      "2019-04-10 01:02:55,586 root         INFO     Train Epoch: 98 [4096/8000 (51%)]\tTotal Loss: 0.179911\n",
      "Reconstruction: 0.179490, Regularization: 0.000421\n",
      "2019-04-10 01:02:55,649 root         INFO     Train Epoch: 98 [4608/8000 (58%)]\tTotal Loss: 0.171204\n",
      "Reconstruction: 0.170710, Regularization: 0.000494\n",
      "2019-04-10 01:02:55,711 root         INFO     Train Epoch: 98 [5120/8000 (64%)]\tTotal Loss: 0.155962\n",
      "Reconstruction: 0.155669, Regularization: 0.000293\n",
      "2019-04-10 01:02:55,773 root         INFO     Train Epoch: 98 [5632/8000 (70%)]\tTotal Loss: 0.156098\n",
      "Reconstruction: 0.155787, Regularization: 0.000311\n",
      "2019-04-10 01:02:55,834 root         INFO     Train Epoch: 98 [6144/8000 (77%)]\tTotal Loss: 0.160597\n",
      "Reconstruction: 0.160298, Regularization: 0.000299\n",
      "2019-04-10 01:02:55,896 root         INFO     Train Epoch: 98 [6656/8000 (83%)]\tTotal Loss: 0.163097\n",
      "Reconstruction: 0.162713, Regularization: 0.000384\n",
      "2019-04-10 01:02:55,957 root         INFO     Train Epoch: 98 [7168/8000 (90%)]\tTotal Loss: 0.163077\n",
      "Reconstruction: 0.162633, Regularization: 0.000444\n",
      "2019-04-10 01:02:56,018 root         INFO     Train Epoch: 98 [7680/8000 (96%)]\tTotal Loss: 0.161608\n",
      "Reconstruction: 0.161261, Regularization: 0.000347\n",
      "2019-04-10 01:02:56,070 root         INFO     ====> Epoch: 98 Average loss: 0.1665\n",
      "2019-04-10 01:02:56,094 root         INFO     Train Epoch: 99 [0/8000 (0%)]\tTotal Loss: 0.157115\n",
      "Reconstruction: 0.156790, Regularization: 0.000325\n",
      "2019-04-10 01:02:56,159 root         INFO     Train Epoch: 99 [512/8000 (6%)]\tTotal Loss: 0.172789\n",
      "Reconstruction: 0.172268, Regularization: 0.000520\n",
      "2019-04-10 01:02:56,223 root         INFO     Train Epoch: 99 [1024/8000 (13%)]\tTotal Loss: 0.158476\n",
      "Reconstruction: 0.158149, Regularization: 0.000327\n",
      "2019-04-10 01:02:56,287 root         INFO     Train Epoch: 99 [1536/8000 (19%)]\tTotal Loss: 0.166852\n",
      "Reconstruction: 0.166433, Regularization: 0.000419\n",
      "2019-04-10 01:02:56,351 root         INFO     Train Epoch: 99 [2048/8000 (26%)]\tTotal Loss: 0.162260\n",
      "Reconstruction: 0.161950, Regularization: 0.000310\n",
      "2019-04-10 01:02:56,415 root         INFO     Train Epoch: 99 [2560/8000 (32%)]\tTotal Loss: 0.171101\n",
      "Reconstruction: 0.170708, Regularization: 0.000393\n",
      "2019-04-10 01:02:56,478 root         INFO     Train Epoch: 99 [3072/8000 (38%)]\tTotal Loss: 0.159687\n",
      "Reconstruction: 0.159341, Regularization: 0.000346\n",
      "2019-04-10 01:02:56,542 root         INFO     Train Epoch: 99 [3584/8000 (45%)]\tTotal Loss: 0.180526\n",
      "Reconstruction: 0.179999, Regularization: 0.000527\n",
      "2019-04-10 01:02:56,604 root         INFO     Train Epoch: 99 [4096/8000 (51%)]\tTotal Loss: 0.156691\n",
      "Reconstruction: 0.156426, Regularization: 0.000265\n",
      "2019-04-10 01:02:56,667 root         INFO     Train Epoch: 99 [4608/8000 (58%)]\tTotal Loss: 0.170223\n",
      "Reconstruction: 0.169738, Regularization: 0.000484\n",
      "2019-04-10 01:02:56,729 root         INFO     Train Epoch: 99 [5120/8000 (64%)]\tTotal Loss: 0.163795\n",
      "Reconstruction: 0.163441, Regularization: 0.000355\n",
      "2019-04-10 01:02:56,791 root         INFO     Train Epoch: 99 [5632/8000 (70%)]\tTotal Loss: 0.157808\n",
      "Reconstruction: 0.157502, Regularization: 0.000306\n",
      "2019-04-10 01:02:56,854 root         INFO     Train Epoch: 99 [6144/8000 (77%)]\tTotal Loss: 0.174395\n",
      "Reconstruction: 0.173807, Regularization: 0.000588\n",
      "2019-04-10 01:02:56,915 root         INFO     Train Epoch: 99 [6656/8000 (83%)]\tTotal Loss: 0.168405\n",
      "Reconstruction: 0.168059, Regularization: 0.000346\n",
      "2019-04-10 01:02:56,976 root         INFO     Train Epoch: 99 [7168/8000 (90%)]\tTotal Loss: 0.159714\n",
      "Reconstruction: 0.159404, Regularization: 0.000310\n",
      "2019-04-10 01:02:57,038 root         INFO     Train Epoch: 99 [7680/8000 (96%)]\tTotal Loss: 0.167814\n",
      "Reconstruction: 0.167401, Regularization: 0.000412\n",
      "2019-04-10 01:02:57,091 root         INFO     ====> Epoch: 99 Average loss: 0.1663\n",
      "2019-04-10 01:02:57,114 root         INFO     Train Epoch: 100 [0/8000 (0%)]\tTotal Loss: 0.164670\n",
      "Reconstruction: 0.164286, Regularization: 0.000384\n",
      "2019-04-10 01:02:57,178 root         INFO     Train Epoch: 100 [512/8000 (6%)]\tTotal Loss: 0.155844\n",
      "Reconstruction: 0.155610, Regularization: 0.000235\n",
      "2019-04-10 01:02:57,242 root         INFO     Train Epoch: 100 [1024/8000 (13%)]\tTotal Loss: 0.174114\n",
      "Reconstruction: 0.173649, Regularization: 0.000465\n",
      "2019-04-10 01:02:57,305 root         INFO     Train Epoch: 100 [1536/8000 (19%)]\tTotal Loss: 0.164238\n",
      "Reconstruction: 0.163897, Regularization: 0.000342\n",
      "2019-04-10 01:02:57,367 root         INFO     Train Epoch: 100 [2048/8000 (26%)]\tTotal Loss: 0.159120\n",
      "Reconstruction: 0.158818, Regularization: 0.000302\n",
      "2019-04-10 01:02:57,430 root         INFO     Train Epoch: 100 [2560/8000 (32%)]\tTotal Loss: 0.174008\n",
      "Reconstruction: 0.173457, Regularization: 0.000551\n",
      "2019-04-10 01:02:57,493 root         INFO     Train Epoch: 100 [3072/8000 (38%)]\tTotal Loss: 0.154304\n",
      "Reconstruction: 0.153948, Regularization: 0.000356\n",
      "2019-04-10 01:02:57,557 root         INFO     Train Epoch: 100 [3584/8000 (45%)]\tTotal Loss: 0.164833\n",
      "Reconstruction: 0.164425, Regularization: 0.000408\n",
      "2019-04-10 01:02:57,619 root         INFO     Train Epoch: 100 [4096/8000 (51%)]\tTotal Loss: 0.161032\n",
      "Reconstruction: 0.160659, Regularization: 0.000373\n",
      "2019-04-10 01:02:57,682 root         INFO     Train Epoch: 100 [4608/8000 (58%)]\tTotal Loss: 0.174309\n",
      "Reconstruction: 0.173884, Regularization: 0.000425\n",
      "2019-04-10 01:02:57,745 root         INFO     Train Epoch: 100 [5120/8000 (64%)]\tTotal Loss: 0.179784\n",
      "Reconstruction: 0.179237, Regularization: 0.000548\n",
      "2019-04-10 01:02:57,807 root         INFO     Train Epoch: 100 [5632/8000 (70%)]\tTotal Loss: 0.169185\n",
      "Reconstruction: 0.168757, Regularization: 0.000427\n",
      "2019-04-10 01:02:57,870 root         INFO     Train Epoch: 100 [6144/8000 (77%)]\tTotal Loss: 0.160209\n",
      "Reconstruction: 0.159854, Regularization: 0.000355\n",
      "2019-04-10 01:02:57,933 root         INFO     Train Epoch: 100 [6656/8000 (83%)]\tTotal Loss: 0.156835\n",
      "Reconstruction: 0.156548, Regularization: 0.000287\n",
      "2019-04-10 01:02:57,996 root         INFO     Train Epoch: 100 [7168/8000 (90%)]\tTotal Loss: 0.172523\n",
      "Reconstruction: 0.172053, Regularization: 0.000470\n",
      "2019-04-10 01:02:58,059 root         INFO     Train Epoch: 100 [7680/8000 (96%)]\tTotal Loss: 0.162880\n",
      "Reconstruction: 0.162562, Regularization: 0.000318\n",
      "2019-04-10 01:02:58,113 root         INFO     ====> Epoch: 100 Average loss: 0.1662\n",
      "2019-04-10 01:02:58,137 root         INFO     Train Epoch: 101 [0/8000 (0%)]\tTotal Loss: 0.164547\n",
      "Reconstruction: 0.164143, Regularization: 0.000404\n",
      "2019-04-10 01:02:58,200 root         INFO     Train Epoch: 101 [512/8000 (6%)]\tTotal Loss: 0.173026\n",
      "Reconstruction: 0.172669, Regularization: 0.000357\n",
      "2019-04-10 01:02:58,263 root         INFO     Train Epoch: 101 [1024/8000 (13%)]\tTotal Loss: 0.171220\n",
      "Reconstruction: 0.170734, Regularization: 0.000487\n",
      "2019-04-10 01:02:58,327 root         INFO     Train Epoch: 101 [1536/8000 (19%)]\tTotal Loss: 0.175698\n",
      "Reconstruction: 0.175309, Regularization: 0.000389\n",
      "2019-04-10 01:02:58,389 root         INFO     Train Epoch: 101 [2048/8000 (26%)]\tTotal Loss: 0.170936\n",
      "Reconstruction: 0.170538, Regularization: 0.000398\n",
      "2019-04-10 01:02:58,453 root         INFO     Train Epoch: 101 [2560/8000 (32%)]\tTotal Loss: 0.152098\n",
      "Reconstruction: 0.151866, Regularization: 0.000233\n",
      "2019-04-10 01:02:58,516 root         INFO     Train Epoch: 101 [3072/8000 (38%)]\tTotal Loss: 0.175608\n",
      "Reconstruction: 0.175096, Regularization: 0.000512\n",
      "2019-04-10 01:02:58,579 root         INFO     Train Epoch: 101 [3584/8000 (45%)]\tTotal Loss: 0.149306\n",
      "Reconstruction: 0.149086, Regularization: 0.000220\n",
      "2019-04-10 01:02:58,642 root         INFO     Train Epoch: 101 [4096/8000 (51%)]\tTotal Loss: 0.160010\n",
      "Reconstruction: 0.159705, Regularization: 0.000305\n",
      "2019-04-10 01:02:58,706 root         INFO     Train Epoch: 101 [4608/8000 (58%)]\tTotal Loss: 0.164138\n",
      "Reconstruction: 0.163750, Regularization: 0.000388\n",
      "2019-04-10 01:02:58,770 root         INFO     Train Epoch: 101 [5120/8000 (64%)]\tTotal Loss: 0.161985\n",
      "Reconstruction: 0.161673, Regularization: 0.000313\n",
      "2019-04-10 01:02:58,833 root         INFO     Train Epoch: 101 [5632/8000 (70%)]\tTotal Loss: 0.165986\n",
      "Reconstruction: 0.165587, Regularization: 0.000399\n",
      "2019-04-10 01:02:58,897 root         INFO     Train Epoch: 101 [6144/8000 (77%)]\tTotal Loss: 0.162278\n",
      "Reconstruction: 0.161984, Regularization: 0.000294\n",
      "2019-04-10 01:02:58,961 root         INFO     Train Epoch: 101 [6656/8000 (83%)]\tTotal Loss: 0.176461\n",
      "Reconstruction: 0.175965, Regularization: 0.000496\n",
      "2019-04-10 01:02:59,024 root         INFO     Train Epoch: 101 [7168/8000 (90%)]\tTotal Loss: 0.157234\n",
      "Reconstruction: 0.156889, Regularization: 0.000345\n",
      "2019-04-10 01:02:59,087 root         INFO     Train Epoch: 101 [7680/8000 (96%)]\tTotal Loss: 0.164931\n",
      "Reconstruction: 0.164543, Regularization: 0.000388\n",
      "2019-04-10 01:02:59,141 root         INFO     ====> Epoch: 101 Average loss: 0.1664\n",
      "2019-04-10 01:02:59,165 root         INFO     Train Epoch: 102 [0/8000 (0%)]\tTotal Loss: 0.170001\n",
      "Reconstruction: 0.169515, Regularization: 0.000486\n",
      "2019-04-10 01:02:59,229 root         INFO     Train Epoch: 102 [512/8000 (6%)]\tTotal Loss: 0.181070\n",
      "Reconstruction: 0.180542, Regularization: 0.000528\n",
      "2019-04-10 01:02:59,292 root         INFO     Train Epoch: 102 [1024/8000 (13%)]\tTotal Loss: 0.168826\n",
      "Reconstruction: 0.168394, Regularization: 0.000432\n",
      "2019-04-10 01:02:59,356 root         INFO     Train Epoch: 102 [1536/8000 (19%)]\tTotal Loss: 0.168315\n",
      "Reconstruction: 0.167862, Regularization: 0.000453\n",
      "2019-04-10 01:02:59,419 root         INFO     Train Epoch: 102 [2048/8000 (26%)]\tTotal Loss: 0.165210\n",
      "Reconstruction: 0.164889, Regularization: 0.000321\n",
      "2019-04-10 01:02:59,483 root         INFO     Train Epoch: 102 [2560/8000 (32%)]\tTotal Loss: 0.169417\n",
      "Reconstruction: 0.168984, Regularization: 0.000433\n",
      "2019-04-10 01:02:59,547 root         INFO     Train Epoch: 102 [3072/8000 (38%)]\tTotal Loss: 0.167438\n",
      "Reconstruction: 0.167118, Regularization: 0.000320\n",
      "2019-04-10 01:02:59,611 root         INFO     Train Epoch: 102 [3584/8000 (45%)]\tTotal Loss: 0.157640\n",
      "Reconstruction: 0.157275, Regularization: 0.000364\n",
      "2019-04-10 01:02:59,675 root         INFO     Train Epoch: 102 [4096/8000 (51%)]\tTotal Loss: 0.164690\n",
      "Reconstruction: 0.164337, Regularization: 0.000353\n",
      "2019-04-10 01:02:59,740 root         INFO     Train Epoch: 102 [4608/8000 (58%)]\tTotal Loss: 0.172378\n",
      "Reconstruction: 0.171878, Regularization: 0.000500\n",
      "2019-04-10 01:02:59,804 root         INFO     Train Epoch: 102 [5120/8000 (64%)]\tTotal Loss: 0.157669\n",
      "Reconstruction: 0.157411, Regularization: 0.000259\n",
      "2019-04-10 01:02:59,867 root         INFO     Train Epoch: 102 [5632/8000 (70%)]\tTotal Loss: 0.159745\n",
      "Reconstruction: 0.159485, Regularization: 0.000260\n",
      "2019-04-10 01:02:59,928 root         INFO     Train Epoch: 102 [6144/8000 (77%)]\tTotal Loss: 0.158785\n",
      "Reconstruction: 0.158397, Regularization: 0.000387\n",
      "2019-04-10 01:02:59,991 root         INFO     Train Epoch: 102 [6656/8000 (83%)]\tTotal Loss: 0.167633\n",
      "Reconstruction: 0.167211, Regularization: 0.000422\n",
      "2019-04-10 01:03:00,053 root         INFO     Train Epoch: 102 [7168/8000 (90%)]\tTotal Loss: 0.165936\n",
      "Reconstruction: 0.165507, Regularization: 0.000429\n",
      "2019-04-10 01:03:00,116 root         INFO     Train Epoch: 102 [7680/8000 (96%)]\tTotal Loss: 0.169039\n",
      "Reconstruction: 0.168546, Regularization: 0.000493\n",
      "2019-04-10 01:03:00,170 root         INFO     ====> Epoch: 102 Average loss: 0.1664\n",
      "2019-04-10 01:03:00,194 root         INFO     Train Epoch: 103 [0/8000 (0%)]\tTotal Loss: 0.158516\n",
      "Reconstruction: 0.158149, Regularization: 0.000367\n",
      "2019-04-10 01:03:00,258 root         INFO     Train Epoch: 103 [512/8000 (6%)]\tTotal Loss: 0.172043\n",
      "Reconstruction: 0.171616, Regularization: 0.000427\n",
      "2019-04-10 01:03:00,322 root         INFO     Train Epoch: 103 [1024/8000 (13%)]\tTotal Loss: 0.169831\n",
      "Reconstruction: 0.169360, Regularization: 0.000471\n",
      "2019-04-10 01:03:00,385 root         INFO     Train Epoch: 103 [1536/8000 (19%)]\tTotal Loss: 0.157635\n",
      "Reconstruction: 0.157345, Regularization: 0.000290\n",
      "2019-04-10 01:03:00,449 root         INFO     Train Epoch: 103 [2048/8000 (26%)]\tTotal Loss: 0.163536\n",
      "Reconstruction: 0.163159, Regularization: 0.000378\n",
      "2019-04-10 01:03:00,511 root         INFO     Train Epoch: 103 [2560/8000 (32%)]\tTotal Loss: 0.169585\n",
      "Reconstruction: 0.169073, Regularization: 0.000511\n",
      "2019-04-10 01:03:00,573 root         INFO     Train Epoch: 103 [3072/8000 (38%)]\tTotal Loss: 0.154700\n",
      "Reconstruction: 0.154439, Regularization: 0.000261\n",
      "2019-04-10 01:03:00,634 root         INFO     Train Epoch: 103 [3584/8000 (45%)]\tTotal Loss: 0.171041\n",
      "Reconstruction: 0.170498, Regularization: 0.000543\n",
      "2019-04-10 01:03:00,696 root         INFO     Train Epoch: 103 [4096/8000 (51%)]\tTotal Loss: 0.177718\n",
      "Reconstruction: 0.177188, Regularization: 0.000530\n",
      "2019-04-10 01:03:00,757 root         INFO     Train Epoch: 103 [4608/8000 (58%)]\tTotal Loss: 0.185130\n",
      "Reconstruction: 0.184434, Regularization: 0.000696\n",
      "2019-04-10 01:03:00,819 root         INFO     Train Epoch: 103 [5120/8000 (64%)]\tTotal Loss: 0.168754\n",
      "Reconstruction: 0.168184, Regularization: 0.000570\n",
      "2019-04-10 01:03:00,880 root         INFO     Train Epoch: 103 [5632/8000 (70%)]\tTotal Loss: 0.180022\n",
      "Reconstruction: 0.179564, Regularization: 0.000458\n",
      "2019-04-10 01:03:00,941 root         INFO     Train Epoch: 103 [6144/8000 (77%)]\tTotal Loss: 0.169439\n",
      "Reconstruction: 0.169051, Regularization: 0.000388\n",
      "2019-04-10 01:03:01,002 root         INFO     Train Epoch: 103 [6656/8000 (83%)]\tTotal Loss: 0.182997\n",
      "Reconstruction: 0.182385, Regularization: 0.000612\n",
      "2019-04-10 01:03:01,066 root         INFO     Train Epoch: 103 [7168/8000 (90%)]\tTotal Loss: 0.185528\n",
      "Reconstruction: 0.184978, Regularization: 0.000550\n",
      "2019-04-10 01:03:01,127 root         INFO     Train Epoch: 103 [7680/8000 (96%)]\tTotal Loss: 0.164343\n",
      "Reconstruction: 0.164016, Regularization: 0.000328\n",
      "2019-04-10 01:03:01,180 root         INFO     ====> Epoch: 103 Average loss: 0.1663\n",
      "2019-04-10 01:03:01,204 root         INFO     Train Epoch: 104 [0/8000 (0%)]\tTotal Loss: 0.163548\n",
      "Reconstruction: 0.163174, Regularization: 0.000374\n",
      "2019-04-10 01:03:01,268 root         INFO     Train Epoch: 104 [512/8000 (6%)]\tTotal Loss: 0.165959\n",
      "Reconstruction: 0.165527, Regularization: 0.000432\n",
      "2019-04-10 01:03:01,333 root         INFO     Train Epoch: 104 [1024/8000 (13%)]\tTotal Loss: 0.158683\n",
      "Reconstruction: 0.158348, Regularization: 0.000335\n",
      "2019-04-10 01:03:01,397 root         INFO     Train Epoch: 104 [1536/8000 (19%)]\tTotal Loss: 0.159970\n",
      "Reconstruction: 0.159702, Regularization: 0.000267\n",
      "2019-04-10 01:03:01,462 root         INFO     Train Epoch: 104 [2048/8000 (26%)]\tTotal Loss: 0.161467\n",
      "Reconstruction: 0.161081, Regularization: 0.000386\n",
      "2019-04-10 01:03:01,526 root         INFO     Train Epoch: 104 [2560/8000 (32%)]\tTotal Loss: 0.170664\n",
      "Reconstruction: 0.170086, Regularization: 0.000578\n",
      "2019-04-10 01:03:01,591 root         INFO     Train Epoch: 104 [3072/8000 (38%)]\tTotal Loss: 0.159255\n",
      "Reconstruction: 0.158933, Regularization: 0.000322\n",
      "2019-04-10 01:03:01,655 root         INFO     Train Epoch: 104 [3584/8000 (45%)]\tTotal Loss: 0.172952\n",
      "Reconstruction: 0.172491, Regularization: 0.000460\n",
      "2019-04-10 01:03:01,719 root         INFO     Train Epoch: 104 [4096/8000 (51%)]\tTotal Loss: 0.170065\n",
      "Reconstruction: 0.169458, Regularization: 0.000607\n",
      "2019-04-10 01:03:01,784 root         INFO     Train Epoch: 104 [4608/8000 (58%)]\tTotal Loss: 0.178793\n",
      "Reconstruction: 0.178284, Regularization: 0.000509\n",
      "2019-04-10 01:03:01,847 root         INFO     Train Epoch: 104 [5120/8000 (64%)]\tTotal Loss: 0.154112\n",
      "Reconstruction: 0.153777, Regularization: 0.000335\n",
      "2019-04-10 01:03:01,912 root         INFO     Train Epoch: 104 [5632/8000 (70%)]\tTotal Loss: 0.170277\n",
      "Reconstruction: 0.169694, Regularization: 0.000583\n",
      "2019-04-10 01:03:01,976 root         INFO     Train Epoch: 104 [6144/8000 (77%)]\tTotal Loss: 0.171787\n",
      "Reconstruction: 0.171359, Regularization: 0.000428\n",
      "2019-04-10 01:03:02,040 root         INFO     Train Epoch: 104 [6656/8000 (83%)]\tTotal Loss: 0.171979\n",
      "Reconstruction: 0.171527, Regularization: 0.000453\n",
      "2019-04-10 01:03:02,103 root         INFO     Train Epoch: 104 [7168/8000 (90%)]\tTotal Loss: 0.173200\n",
      "Reconstruction: 0.172741, Regularization: 0.000460\n",
      "2019-04-10 01:03:02,166 root         INFO     Train Epoch: 104 [7680/8000 (96%)]\tTotal Loss: 0.164888\n",
      "Reconstruction: 0.164413, Regularization: 0.000475\n",
      "2019-04-10 01:03:02,219 root         INFO     ====> Epoch: 104 Average loss: 0.1663\n",
      "2019-04-10 01:03:02,244 root         INFO     Train Epoch: 105 [0/8000 (0%)]\tTotal Loss: 0.161544\n",
      "Reconstruction: 0.161208, Regularization: 0.000336\n",
      "2019-04-10 01:03:02,308 root         INFO     Train Epoch: 105 [512/8000 (6%)]\tTotal Loss: 0.162419\n",
      "Reconstruction: 0.162085, Regularization: 0.000334\n",
      "2019-04-10 01:03:02,371 root         INFO     Train Epoch: 105 [1024/8000 (13%)]\tTotal Loss: 0.181222\n",
      "Reconstruction: 0.180596, Regularization: 0.000626\n",
      "2019-04-10 01:03:02,434 root         INFO     Train Epoch: 105 [1536/8000 (19%)]\tTotal Loss: 0.170668\n",
      "Reconstruction: 0.170237, Regularization: 0.000431\n",
      "2019-04-10 01:03:02,497 root         INFO     Train Epoch: 105 [2048/8000 (26%)]\tTotal Loss: 0.158467\n",
      "Reconstruction: 0.158126, Regularization: 0.000340\n",
      "2019-04-10 01:03:02,560 root         INFO     Train Epoch: 105 [2560/8000 (32%)]\tTotal Loss: 0.183779\n",
      "Reconstruction: 0.183156, Regularization: 0.000623\n",
      "2019-04-10 01:03:02,622 root         INFO     Train Epoch: 105 [3072/8000 (38%)]\tTotal Loss: 0.171231\n",
      "Reconstruction: 0.170743, Regularization: 0.000488\n",
      "2019-04-10 01:03:02,685 root         INFO     Train Epoch: 105 [3584/8000 (45%)]\tTotal Loss: 0.156896\n",
      "Reconstruction: 0.156594, Regularization: 0.000301\n",
      "2019-04-10 01:03:02,749 root         INFO     Train Epoch: 105 [4096/8000 (51%)]\tTotal Loss: 0.173572\n",
      "Reconstruction: 0.173072, Regularization: 0.000500\n",
      "2019-04-10 01:03:02,812 root         INFO     Train Epoch: 105 [4608/8000 (58%)]\tTotal Loss: 0.164354\n",
      "Reconstruction: 0.163952, Regularization: 0.000402\n",
      "2019-04-10 01:03:02,876 root         INFO     Train Epoch: 105 [5120/8000 (64%)]\tTotal Loss: 0.169776\n",
      "Reconstruction: 0.169297, Regularization: 0.000479\n",
      "2019-04-10 01:03:02,940 root         INFO     Train Epoch: 105 [5632/8000 (70%)]\tTotal Loss: 0.170844\n",
      "Reconstruction: 0.170349, Regularization: 0.000495\n",
      "2019-04-10 01:03:03,004 root         INFO     Train Epoch: 105 [6144/8000 (77%)]\tTotal Loss: 0.181636\n",
      "Reconstruction: 0.181144, Regularization: 0.000492\n",
      "2019-04-10 01:03:03,069 root         INFO     Train Epoch: 105 [6656/8000 (83%)]\tTotal Loss: 0.161931\n",
      "Reconstruction: 0.161509, Regularization: 0.000422\n",
      "2019-04-10 01:03:03,133 root         INFO     Train Epoch: 105 [7168/8000 (90%)]\tTotal Loss: 0.174969\n",
      "Reconstruction: 0.174444, Regularization: 0.000526\n",
      "2019-04-10 01:03:03,197 root         INFO     Train Epoch: 105 [7680/8000 (96%)]\tTotal Loss: 0.175829\n",
      "Reconstruction: 0.175216, Regularization: 0.000614\n",
      "2019-04-10 01:03:03,251 root         INFO     ====> Epoch: 105 Average loss: 0.1664\n",
      "2019-04-10 01:03:03,276 root         INFO     Train Epoch: 106 [0/8000 (0%)]\tTotal Loss: 0.161811\n",
      "Reconstruction: 0.161513, Regularization: 0.000298\n",
      "2019-04-10 01:03:03,340 root         INFO     Train Epoch: 106 [512/8000 (6%)]\tTotal Loss: 0.177994\n",
      "Reconstruction: 0.177466, Regularization: 0.000528\n",
      "2019-04-10 01:03:03,404 root         INFO     Train Epoch: 106 [1024/8000 (13%)]\tTotal Loss: 0.165034\n",
      "Reconstruction: 0.164620, Regularization: 0.000414\n",
      "2019-04-10 01:03:03,467 root         INFO     Train Epoch: 106 [1536/8000 (19%)]\tTotal Loss: 0.157679\n",
      "Reconstruction: 0.157321, Regularization: 0.000357\n",
      "2019-04-10 01:03:03,529 root         INFO     Train Epoch: 106 [2048/8000 (26%)]\tTotal Loss: 0.170695\n",
      "Reconstruction: 0.170270, Regularization: 0.000425\n",
      "2019-04-10 01:03:03,592 root         INFO     Train Epoch: 106 [2560/8000 (32%)]\tTotal Loss: 0.185545\n",
      "Reconstruction: 0.184903, Regularization: 0.000642\n",
      "2019-04-10 01:03:03,655 root         INFO     Train Epoch: 106 [3072/8000 (38%)]\tTotal Loss: 0.176980\n",
      "Reconstruction: 0.176391, Regularization: 0.000589\n",
      "2019-04-10 01:03:03,717 root         INFO     Train Epoch: 106 [3584/8000 (45%)]\tTotal Loss: 0.170147\n",
      "Reconstruction: 0.169696, Regularization: 0.000451\n",
      "2019-04-10 01:03:03,780 root         INFO     Train Epoch: 106 [4096/8000 (51%)]\tTotal Loss: 0.169051\n",
      "Reconstruction: 0.168564, Regularization: 0.000487\n",
      "2019-04-10 01:03:03,842 root         INFO     Train Epoch: 106 [4608/8000 (58%)]\tTotal Loss: 0.160894\n",
      "Reconstruction: 0.160539, Regularization: 0.000356\n",
      "2019-04-10 01:03:03,905 root         INFO     Train Epoch: 106 [5120/8000 (64%)]\tTotal Loss: 0.173513\n",
      "Reconstruction: 0.173063, Regularization: 0.000450\n",
      "2019-04-10 01:03:03,968 root         INFO     Train Epoch: 106 [5632/8000 (70%)]\tTotal Loss: 0.160545\n",
      "Reconstruction: 0.160288, Regularization: 0.000257\n",
      "2019-04-10 01:03:04,030 root         INFO     Train Epoch: 106 [6144/8000 (77%)]\tTotal Loss: 0.159813\n",
      "Reconstruction: 0.159501, Regularization: 0.000312\n",
      "2019-04-10 01:03:04,093 root         INFO     Train Epoch: 106 [6656/8000 (83%)]\tTotal Loss: 0.159926\n",
      "Reconstruction: 0.159596, Regularization: 0.000330\n",
      "2019-04-10 01:03:04,155 root         INFO     Train Epoch: 106 [7168/8000 (90%)]\tTotal Loss: 0.151777\n",
      "Reconstruction: 0.151540, Regularization: 0.000237\n",
      "2019-04-10 01:03:04,218 root         INFO     Train Epoch: 106 [7680/8000 (96%)]\tTotal Loss: 0.163850\n",
      "Reconstruction: 0.163491, Regularization: 0.000358\n",
      "2019-04-10 01:03:04,273 root         INFO     ====> Epoch: 106 Average loss: 0.1664\n",
      "2019-04-10 01:03:04,297 root         INFO     Train Epoch: 107 [0/8000 (0%)]\tTotal Loss: 0.166916\n",
      "Reconstruction: 0.166451, Regularization: 0.000465\n",
      "2019-04-10 01:03:04,361 root         INFO     Train Epoch: 107 [512/8000 (6%)]\tTotal Loss: 0.187682\n",
      "Reconstruction: 0.186946, Regularization: 0.000737\n",
      "2019-04-10 01:03:04,425 root         INFO     Train Epoch: 107 [1024/8000 (13%)]\tTotal Loss: 0.175776\n",
      "Reconstruction: 0.175234, Regularization: 0.000542\n",
      "2019-04-10 01:03:04,489 root         INFO     Train Epoch: 107 [1536/8000 (19%)]\tTotal Loss: 0.160666\n",
      "Reconstruction: 0.160290, Regularization: 0.000376\n",
      "2019-04-10 01:03:04,553 root         INFO     Train Epoch: 107 [2048/8000 (26%)]\tTotal Loss: 0.173815\n",
      "Reconstruction: 0.173275, Regularization: 0.000539\n",
      "2019-04-10 01:03:04,617 root         INFO     Train Epoch: 107 [2560/8000 (32%)]\tTotal Loss: 0.157561\n",
      "Reconstruction: 0.157247, Regularization: 0.000314\n",
      "2019-04-10 01:03:04,681 root         INFO     Train Epoch: 107 [3072/8000 (38%)]\tTotal Loss: 0.155111\n",
      "Reconstruction: 0.154808, Regularization: 0.000303\n",
      "2019-04-10 01:03:04,745 root         INFO     Train Epoch: 107 [3584/8000 (45%)]\tTotal Loss: 0.162887\n",
      "Reconstruction: 0.162534, Regularization: 0.000353\n",
      "2019-04-10 01:03:04,810 root         INFO     Train Epoch: 107 [4096/8000 (51%)]\tTotal Loss: 0.175302\n",
      "Reconstruction: 0.174684, Regularization: 0.000618\n",
      "2019-04-10 01:03:04,874 root         INFO     Train Epoch: 107 [4608/8000 (58%)]\tTotal Loss: 0.161231\n",
      "Reconstruction: 0.160775, Regularization: 0.000455\n",
      "2019-04-10 01:03:04,938 root         INFO     Train Epoch: 107 [5120/8000 (64%)]\tTotal Loss: 0.165526\n",
      "Reconstruction: 0.165135, Regularization: 0.000392\n",
      "2019-04-10 01:03:05,003 root         INFO     Train Epoch: 107 [5632/8000 (70%)]\tTotal Loss: 0.178119\n",
      "Reconstruction: 0.177695, Regularization: 0.000424\n",
      "2019-04-10 01:03:05,066 root         INFO     Train Epoch: 107 [6144/8000 (77%)]\tTotal Loss: 0.163612\n",
      "Reconstruction: 0.163272, Regularization: 0.000340\n",
      "2019-04-10 01:03:05,129 root         INFO     Train Epoch: 107 [6656/8000 (83%)]\tTotal Loss: 0.174697\n",
      "Reconstruction: 0.174112, Regularization: 0.000585\n",
      "2019-04-10 01:03:05,191 root         INFO     Train Epoch: 107 [7168/8000 (90%)]\tTotal Loss: 0.160345\n",
      "Reconstruction: 0.160008, Regularization: 0.000337\n",
      "2019-04-10 01:03:05,254 root         INFO     Train Epoch: 107 [7680/8000 (96%)]\tTotal Loss: 0.175533\n",
      "Reconstruction: 0.174965, Regularization: 0.000568\n",
      "2019-04-10 01:03:05,307 root         INFO     ====> Epoch: 107 Average loss: 0.1663\n",
      "2019-04-10 01:03:05,331 root         INFO     Train Epoch: 108 [0/8000 (0%)]\tTotal Loss: 0.160709\n",
      "Reconstruction: 0.160282, Regularization: 0.000426\n",
      "2019-04-10 01:03:05,394 root         INFO     Train Epoch: 108 [512/8000 (6%)]\tTotal Loss: 0.172926\n",
      "Reconstruction: 0.172359, Regularization: 0.000568\n",
      "2019-04-10 01:03:05,457 root         INFO     Train Epoch: 108 [1024/8000 (13%)]\tTotal Loss: 0.178734\n",
      "Reconstruction: 0.178129, Regularization: 0.000605\n",
      "2019-04-10 01:03:05,521 root         INFO     Train Epoch: 108 [1536/8000 (19%)]\tTotal Loss: 0.172935\n",
      "Reconstruction: 0.172344, Regularization: 0.000591\n",
      "2019-04-10 01:03:05,584 root         INFO     Train Epoch: 108 [2048/8000 (26%)]\tTotal Loss: 0.169632\n",
      "Reconstruction: 0.169208, Regularization: 0.000424\n",
      "2019-04-10 01:03:05,647 root         INFO     Train Epoch: 108 [2560/8000 (32%)]\tTotal Loss: 0.152765\n",
      "Reconstruction: 0.152522, Regularization: 0.000244\n",
      "2019-04-10 01:03:05,709 root         INFO     Train Epoch: 108 [3072/8000 (38%)]\tTotal Loss: 0.169798\n",
      "Reconstruction: 0.169371, Regularization: 0.000427\n",
      "2019-04-10 01:03:05,773 root         INFO     Train Epoch: 108 [3584/8000 (45%)]\tTotal Loss: 0.166241\n",
      "Reconstruction: 0.165809, Regularization: 0.000432\n",
      "2019-04-10 01:03:05,835 root         INFO     Train Epoch: 108 [4096/8000 (51%)]\tTotal Loss: 0.171933\n",
      "Reconstruction: 0.171451, Regularization: 0.000482\n",
      "2019-04-10 01:03:05,898 root         INFO     Train Epoch: 108 [4608/8000 (58%)]\tTotal Loss: 0.162348\n",
      "Reconstruction: 0.161973, Regularization: 0.000375\n",
      "2019-04-10 01:03:05,961 root         INFO     Train Epoch: 108 [5120/8000 (64%)]\tTotal Loss: 0.165441\n",
      "Reconstruction: 0.164983, Regularization: 0.000458\n",
      "2019-04-10 01:03:06,023 root         INFO     Train Epoch: 108 [5632/8000 (70%)]\tTotal Loss: 0.167713\n",
      "Reconstruction: 0.167204, Regularization: 0.000509\n",
      "2019-04-10 01:03:06,086 root         INFO     Train Epoch: 108 [6144/8000 (77%)]\tTotal Loss: 0.172168\n",
      "Reconstruction: 0.171698, Regularization: 0.000470\n",
      "2019-04-10 01:03:06,149 root         INFO     Train Epoch: 108 [6656/8000 (83%)]\tTotal Loss: 0.161562\n",
      "Reconstruction: 0.161180, Regularization: 0.000382\n",
      "2019-04-10 01:03:06,212 root         INFO     Train Epoch: 108 [7168/8000 (90%)]\tTotal Loss: 0.164057\n",
      "Reconstruction: 0.163698, Regularization: 0.000359\n",
      "2019-04-10 01:03:06,274 root         INFO     Train Epoch: 108 [7680/8000 (96%)]\tTotal Loss: 0.167640\n",
      "Reconstruction: 0.167179, Regularization: 0.000461\n",
      "2019-04-10 01:03:06,328 root         INFO     ====> Epoch: 108 Average loss: 0.1663\n",
      "2019-04-10 01:03:06,352 root         INFO     Train Epoch: 109 [0/8000 (0%)]\tTotal Loss: 0.174123\n",
      "Reconstruction: 0.173503, Regularization: 0.000621\n",
      "2019-04-10 01:03:06,415 root         INFO     Train Epoch: 109 [512/8000 (6%)]\tTotal Loss: 0.179080\n",
      "Reconstruction: 0.178471, Regularization: 0.000609\n",
      "2019-04-10 01:03:06,478 root         INFO     Train Epoch: 109 [1024/8000 (13%)]\tTotal Loss: 0.169089\n",
      "Reconstruction: 0.168653, Regularization: 0.000436\n",
      "2019-04-10 01:03:06,541 root         INFO     Train Epoch: 109 [1536/8000 (19%)]\tTotal Loss: 0.184875\n",
      "Reconstruction: 0.184166, Regularization: 0.000709\n",
      "2019-04-10 01:03:06,604 root         INFO     Train Epoch: 109 [2048/8000 (26%)]\tTotal Loss: 0.170621\n",
      "Reconstruction: 0.170169, Regularization: 0.000452\n",
      "2019-04-10 01:03:06,668 root         INFO     Train Epoch: 109 [2560/8000 (32%)]\tTotal Loss: 0.171019\n",
      "Reconstruction: 0.170483, Regularization: 0.000536\n",
      "2019-04-10 01:03:06,731 root         INFO     Train Epoch: 109 [3072/8000 (38%)]\tTotal Loss: 0.167987\n",
      "Reconstruction: 0.167565, Regularization: 0.000422\n",
      "2019-04-10 01:03:06,794 root         INFO     Train Epoch: 109 [3584/8000 (45%)]\tTotal Loss: 0.183102\n",
      "Reconstruction: 0.182368, Regularization: 0.000735\n",
      "2019-04-10 01:03:06,856 root         INFO     Train Epoch: 109 [4096/8000 (51%)]\tTotal Loss: 0.157338\n",
      "Reconstruction: 0.157000, Regularization: 0.000338\n",
      "2019-04-10 01:03:06,918 root         INFO     Train Epoch: 109 [4608/8000 (58%)]\tTotal Loss: 0.161645\n",
      "Reconstruction: 0.161267, Regularization: 0.000378\n",
      "2019-04-10 01:03:06,980 root         INFO     Train Epoch: 109 [5120/8000 (64%)]\tTotal Loss: 0.165765\n",
      "Reconstruction: 0.165378, Regularization: 0.000387\n",
      "2019-04-10 01:03:07,042 root         INFO     Train Epoch: 109 [5632/8000 (70%)]\tTotal Loss: 0.166902\n",
      "Reconstruction: 0.166459, Regularization: 0.000444\n",
      "2019-04-10 01:03:07,104 root         INFO     Train Epoch: 109 [6144/8000 (77%)]\tTotal Loss: 0.160426\n",
      "Reconstruction: 0.160026, Regularization: 0.000400\n",
      "2019-04-10 01:03:07,166 root         INFO     Train Epoch: 109 [6656/8000 (83%)]\tTotal Loss: 0.156011\n",
      "Reconstruction: 0.155748, Regularization: 0.000263\n",
      "2019-04-10 01:03:07,228 root         INFO     Train Epoch: 109 [7168/8000 (90%)]\tTotal Loss: 0.168467\n",
      "Reconstruction: 0.168014, Regularization: 0.000453\n",
      "2019-04-10 01:03:07,291 root         INFO     Train Epoch: 109 [7680/8000 (96%)]\tTotal Loss: 0.180364\n",
      "Reconstruction: 0.179834, Regularization: 0.000530\n",
      "2019-04-10 01:03:07,344 root         INFO     ====> Epoch: 109 Average loss: 0.1662\n",
      "2019-04-10 01:03:07,367 root         INFO     Train Epoch: 110 [0/8000 (0%)]\tTotal Loss: 0.158041\n",
      "Reconstruction: 0.157695, Regularization: 0.000346\n",
      "2019-04-10 01:03:07,431 root         INFO     Train Epoch: 110 [512/8000 (6%)]\tTotal Loss: 0.164473\n",
      "Reconstruction: 0.164081, Regularization: 0.000393\n",
      "2019-04-10 01:03:07,494 root         INFO     Train Epoch: 110 [1024/8000 (13%)]\tTotal Loss: 0.162838\n",
      "Reconstruction: 0.162392, Regularization: 0.000446\n",
      "2019-04-10 01:03:07,557 root         INFO     Train Epoch: 110 [1536/8000 (19%)]\tTotal Loss: 0.160040\n",
      "Reconstruction: 0.159658, Regularization: 0.000381\n",
      "2019-04-10 01:03:07,619 root         INFO     Train Epoch: 110 [2048/8000 (26%)]\tTotal Loss: 0.166375\n",
      "Reconstruction: 0.165859, Regularization: 0.000516\n",
      "2019-04-10 01:03:07,682 root         INFO     Train Epoch: 110 [2560/8000 (32%)]\tTotal Loss: 0.148042\n",
      "Reconstruction: 0.147815, Regularization: 0.000227\n",
      "2019-04-10 01:03:07,744 root         INFO     Train Epoch: 110 [3072/8000 (38%)]\tTotal Loss: 0.178356\n",
      "Reconstruction: 0.177830, Regularization: 0.000527\n",
      "2019-04-10 01:03:07,807 root         INFO     Train Epoch: 110 [3584/8000 (45%)]\tTotal Loss: 0.163233\n",
      "Reconstruction: 0.162752, Regularization: 0.000481\n",
      "2019-04-10 01:03:07,869 root         INFO     Train Epoch: 110 [4096/8000 (51%)]\tTotal Loss: 0.160753\n",
      "Reconstruction: 0.160444, Regularization: 0.000309\n",
      "2019-04-10 01:03:07,932 root         INFO     Train Epoch: 110 [4608/8000 (58%)]\tTotal Loss: 0.167251\n",
      "Reconstruction: 0.166808, Regularization: 0.000443\n",
      "2019-04-10 01:03:07,995 root         INFO     Train Epoch: 110 [5120/8000 (64%)]\tTotal Loss: 0.178533\n",
      "Reconstruction: 0.177967, Regularization: 0.000566\n",
      "2019-04-10 01:03:08,057 root         INFO     Train Epoch: 110 [5632/8000 (70%)]\tTotal Loss: 0.159235\n",
      "Reconstruction: 0.158836, Regularization: 0.000399\n",
      "2019-04-10 01:03:08,120 root         INFO     Train Epoch: 110 [6144/8000 (77%)]\tTotal Loss: 0.158791\n",
      "Reconstruction: 0.158452, Regularization: 0.000340\n",
      "2019-04-10 01:03:08,183 root         INFO     Train Epoch: 110 [6656/8000 (83%)]\tTotal Loss: 0.173399\n",
      "Reconstruction: 0.172808, Regularization: 0.000591\n",
      "2019-04-10 01:03:08,246 root         INFO     Train Epoch: 110 [7168/8000 (90%)]\tTotal Loss: 0.167651\n",
      "Reconstruction: 0.167110, Regularization: 0.000541\n",
      "2019-04-10 01:03:08,308 root         INFO     Train Epoch: 110 [7680/8000 (96%)]\tTotal Loss: 0.155633\n",
      "Reconstruction: 0.155335, Regularization: 0.000298\n",
      "2019-04-10 01:03:08,362 root         INFO     ====> Epoch: 110 Average loss: 0.1662\n",
      "2019-04-10 01:03:08,386 root         INFO     Train Epoch: 111 [0/8000 (0%)]\tTotal Loss: 0.167667\n",
      "Reconstruction: 0.167198, Regularization: 0.000469\n",
      "2019-04-10 01:03:08,449 root         INFO     Train Epoch: 111 [512/8000 (6%)]\tTotal Loss: 0.173397\n",
      "Reconstruction: 0.172855, Regularization: 0.000542\n",
      "2019-04-10 01:03:08,512 root         INFO     Train Epoch: 111 [1024/8000 (13%)]\tTotal Loss: 0.171603\n",
      "Reconstruction: 0.171033, Regularization: 0.000570\n",
      "2019-04-10 01:03:08,575 root         INFO     Train Epoch: 111 [1536/8000 (19%)]\tTotal Loss: 0.172947\n",
      "Reconstruction: 0.172303, Regularization: 0.000644\n",
      "2019-04-10 01:03:08,638 root         INFO     Train Epoch: 111 [2048/8000 (26%)]\tTotal Loss: 0.153747\n",
      "Reconstruction: 0.153505, Regularization: 0.000243\n",
      "2019-04-10 01:03:08,699 root         INFO     Train Epoch: 111 [2560/8000 (32%)]\tTotal Loss: 0.167517\n",
      "Reconstruction: 0.166916, Regularization: 0.000601\n",
      "2019-04-10 01:03:08,761 root         INFO     Train Epoch: 111 [3072/8000 (38%)]\tTotal Loss: 0.164208\n",
      "Reconstruction: 0.163747, Regularization: 0.000461\n",
      "2019-04-10 01:03:08,823 root         INFO     Train Epoch: 111 [3584/8000 (45%)]\tTotal Loss: 0.165933\n",
      "Reconstruction: 0.165453, Regularization: 0.000480\n",
      "2019-04-10 01:03:08,884 root         INFO     Train Epoch: 111 [4096/8000 (51%)]\tTotal Loss: 0.168747\n",
      "Reconstruction: 0.168230, Regularization: 0.000517\n",
      "2019-04-10 01:03:08,946 root         INFO     Train Epoch: 111 [4608/8000 (58%)]\tTotal Loss: 0.162623\n",
      "Reconstruction: 0.162266, Regularization: 0.000356\n",
      "2019-04-10 01:03:09,007 root         INFO     Train Epoch: 111 [5120/8000 (64%)]\tTotal Loss: 0.172623\n",
      "Reconstruction: 0.172081, Regularization: 0.000541\n",
      "2019-04-10 01:03:09,069 root         INFO     Train Epoch: 111 [5632/8000 (70%)]\tTotal Loss: 0.160541\n",
      "Reconstruction: 0.160202, Regularization: 0.000339\n",
      "2019-04-10 01:03:09,130 root         INFO     Train Epoch: 111 [6144/8000 (77%)]\tTotal Loss: 0.159649\n",
      "Reconstruction: 0.159308, Regularization: 0.000341\n",
      "2019-04-10 01:03:09,192 root         INFO     Train Epoch: 111 [6656/8000 (83%)]\tTotal Loss: 0.159785\n",
      "Reconstruction: 0.159441, Regularization: 0.000345\n",
      "2019-04-10 01:03:09,253 root         INFO     Train Epoch: 111 [7168/8000 (90%)]\tTotal Loss: 0.155148\n",
      "Reconstruction: 0.154813, Regularization: 0.000335\n",
      "2019-04-10 01:03:09,315 root         INFO     Train Epoch: 111 [7680/8000 (96%)]\tTotal Loss: 0.166747\n",
      "Reconstruction: 0.166276, Regularization: 0.000471\n",
      "2019-04-10 01:03:09,369 root         INFO     ====> Epoch: 111 Average loss: 0.1663\n",
      "2019-04-10 01:03:09,393 root         INFO     Train Epoch: 112 [0/8000 (0%)]\tTotal Loss: 0.163442\n",
      "Reconstruction: 0.163056, Regularization: 0.000387\n",
      "2019-04-10 01:03:09,455 root         INFO     Train Epoch: 112 [512/8000 (6%)]\tTotal Loss: 0.175664\n",
      "Reconstruction: 0.175022, Regularization: 0.000641\n",
      "2019-04-10 01:03:09,519 root         INFO     Train Epoch: 112 [1024/8000 (13%)]\tTotal Loss: 0.181056\n",
      "Reconstruction: 0.180229, Regularization: 0.000827\n",
      "2019-04-10 01:03:09,582 root         INFO     Train Epoch: 112 [1536/8000 (19%)]\tTotal Loss: 0.155269\n",
      "Reconstruction: 0.154923, Regularization: 0.000346\n",
      "2019-04-10 01:03:09,646 root         INFO     Train Epoch: 112 [2048/8000 (26%)]\tTotal Loss: 0.165976\n",
      "Reconstruction: 0.165474, Regularization: 0.000502\n",
      "2019-04-10 01:03:09,709 root         INFO     Train Epoch: 112 [2560/8000 (32%)]\tTotal Loss: 0.165574\n",
      "Reconstruction: 0.165072, Regularization: 0.000502\n",
      "2019-04-10 01:03:09,773 root         INFO     Train Epoch: 112 [3072/8000 (38%)]\tTotal Loss: 0.163004\n",
      "Reconstruction: 0.162547, Regularization: 0.000456\n",
      "2019-04-10 01:03:09,836 root         INFO     Train Epoch: 112 [3584/8000 (45%)]\tTotal Loss: 0.159708\n",
      "Reconstruction: 0.159293, Regularization: 0.000415\n",
      "2019-04-10 01:03:09,899 root         INFO     Train Epoch: 112 [4096/8000 (51%)]\tTotal Loss: 0.163372\n",
      "Reconstruction: 0.162883, Regularization: 0.000489\n",
      "2019-04-10 01:03:09,964 root         INFO     Train Epoch: 112 [4608/8000 (58%)]\tTotal Loss: 0.165184\n",
      "Reconstruction: 0.164718, Regularization: 0.000466\n",
      "2019-04-10 01:03:10,028 root         INFO     Train Epoch: 112 [5120/8000 (64%)]\tTotal Loss: 0.187387\n",
      "Reconstruction: 0.186549, Regularization: 0.000838\n",
      "2019-04-10 01:03:10,090 root         INFO     Train Epoch: 112 [5632/8000 (70%)]\tTotal Loss: 0.179232\n",
      "Reconstruction: 0.178539, Regularization: 0.000693\n",
      "2019-04-10 01:03:10,152 root         INFO     Train Epoch: 112 [6144/8000 (77%)]\tTotal Loss: 0.172676\n",
      "Reconstruction: 0.172027, Regularization: 0.000649\n",
      "2019-04-10 01:03:10,214 root         INFO     Train Epoch: 112 [6656/8000 (83%)]\tTotal Loss: 0.168319\n",
      "Reconstruction: 0.167826, Regularization: 0.000492\n",
      "2019-04-10 01:03:10,276 root         INFO     Train Epoch: 112 [7168/8000 (90%)]\tTotal Loss: 0.176004\n",
      "Reconstruction: 0.175282, Regularization: 0.000723\n",
      "2019-04-10 01:03:10,339 root         INFO     Train Epoch: 112 [7680/8000 (96%)]\tTotal Loss: 0.157767\n",
      "Reconstruction: 0.157351, Regularization: 0.000416\n",
      "2019-04-10 01:03:10,393 root         INFO     ====> Epoch: 112 Average loss: 0.1663\n",
      "2019-04-10 01:03:10,417 root         INFO     Train Epoch: 113 [0/8000 (0%)]\tTotal Loss: 0.157905\n",
      "Reconstruction: 0.157581, Regularization: 0.000324\n",
      "2019-04-10 01:03:10,480 root         INFO     Train Epoch: 113 [512/8000 (6%)]\tTotal Loss: 0.167154\n",
      "Reconstruction: 0.166645, Regularization: 0.000509\n",
      "2019-04-10 01:03:10,544 root         INFO     Train Epoch: 113 [1024/8000 (13%)]\tTotal Loss: 0.162389\n",
      "Reconstruction: 0.161988, Regularization: 0.000401\n",
      "2019-04-10 01:03:10,608 root         INFO     Train Epoch: 113 [1536/8000 (19%)]\tTotal Loss: 0.166842\n",
      "Reconstruction: 0.166311, Regularization: 0.000531\n",
      "2019-04-10 01:03:10,670 root         INFO     Train Epoch: 113 [2048/8000 (26%)]\tTotal Loss: 0.168770\n",
      "Reconstruction: 0.168172, Regularization: 0.000598\n",
      "2019-04-10 01:03:10,733 root         INFO     Train Epoch: 113 [2560/8000 (32%)]\tTotal Loss: 0.173741\n",
      "Reconstruction: 0.173128, Regularization: 0.000613\n",
      "2019-04-10 01:03:10,795 root         INFO     Train Epoch: 113 [3072/8000 (38%)]\tTotal Loss: 0.162028\n",
      "Reconstruction: 0.161635, Regularization: 0.000393\n",
      "2019-04-10 01:03:10,859 root         INFO     Train Epoch: 113 [3584/8000 (45%)]\tTotal Loss: 0.158353\n",
      "Reconstruction: 0.158030, Regularization: 0.000323\n",
      "2019-04-10 01:03:10,923 root         INFO     Train Epoch: 113 [4096/8000 (51%)]\tTotal Loss: 0.173013\n",
      "Reconstruction: 0.172368, Regularization: 0.000645\n",
      "2019-04-10 01:03:10,987 root         INFO     Train Epoch: 113 [4608/8000 (58%)]\tTotal Loss: 0.163273\n",
      "Reconstruction: 0.162768, Regularization: 0.000505\n",
      "2019-04-10 01:03:11,051 root         INFO     Train Epoch: 113 [5120/8000 (64%)]\tTotal Loss: 0.163627\n",
      "Reconstruction: 0.163223, Regularization: 0.000404\n",
      "2019-04-10 01:03:11,115 root         INFO     Train Epoch: 113 [5632/8000 (70%)]\tTotal Loss: 0.160010\n",
      "Reconstruction: 0.159671, Regularization: 0.000339\n",
      "2019-04-10 01:03:11,179 root         INFO     Train Epoch: 113 [6144/8000 (77%)]\tTotal Loss: 0.166328\n",
      "Reconstruction: 0.165843, Regularization: 0.000485\n",
      "2019-04-10 01:03:11,243 root         INFO     Train Epoch: 113 [6656/8000 (83%)]\tTotal Loss: 0.165552\n",
      "Reconstruction: 0.164979, Regularization: 0.000573\n",
      "2019-04-10 01:03:11,307 root         INFO     Train Epoch: 113 [7168/8000 (90%)]\tTotal Loss: 0.177135\n",
      "Reconstruction: 0.176425, Regularization: 0.000710\n",
      "2019-04-10 01:03:11,371 root         INFO     Train Epoch: 113 [7680/8000 (96%)]\tTotal Loss: 0.159671\n",
      "Reconstruction: 0.159250, Regularization: 0.000421\n",
      "2019-04-10 01:03:11,425 root         INFO     ====> Epoch: 113 Average loss: 0.1662\n",
      "2019-04-10 01:03:11,449 root         INFO     Train Epoch: 114 [0/8000 (0%)]\tTotal Loss: 0.165270\n",
      "Reconstruction: 0.164797, Regularization: 0.000473\n",
      "2019-04-10 01:03:11,515 root         INFO     Train Epoch: 114 [512/8000 (6%)]\tTotal Loss: 0.163766\n",
      "Reconstruction: 0.163257, Regularization: 0.000509\n",
      "2019-04-10 01:03:11,579 root         INFO     Train Epoch: 114 [1024/8000 (13%)]\tTotal Loss: 0.175112\n",
      "Reconstruction: 0.174460, Regularization: 0.000652\n",
      "2019-04-10 01:03:11,643 root         INFO     Train Epoch: 114 [1536/8000 (19%)]\tTotal Loss: 0.152625\n",
      "Reconstruction: 0.152346, Regularization: 0.000279\n",
      "2019-04-10 01:03:11,707 root         INFO     Train Epoch: 114 [2048/8000 (26%)]\tTotal Loss: 0.162811\n",
      "Reconstruction: 0.162378, Regularization: 0.000432\n",
      "2019-04-10 01:03:11,771 root         INFO     Train Epoch: 114 [2560/8000 (32%)]\tTotal Loss: 0.177167\n",
      "Reconstruction: 0.176561, Regularization: 0.000606\n",
      "2019-04-10 01:03:11,835 root         INFO     Train Epoch: 114 [3072/8000 (38%)]\tTotal Loss: 0.163201\n",
      "Reconstruction: 0.162763, Regularization: 0.000438\n",
      "2019-04-10 01:03:11,898 root         INFO     Train Epoch: 114 [3584/8000 (45%)]\tTotal Loss: 0.167643\n",
      "Reconstruction: 0.167079, Regularization: 0.000564\n",
      "2019-04-10 01:03:11,963 root         INFO     Train Epoch: 114 [4096/8000 (51%)]\tTotal Loss: 0.160627\n",
      "Reconstruction: 0.160218, Regularization: 0.000409\n",
      "2019-04-10 01:03:12,027 root         INFO     Train Epoch: 114 [4608/8000 (58%)]\tTotal Loss: 0.170886\n",
      "Reconstruction: 0.170266, Regularization: 0.000620\n",
      "2019-04-10 01:03:12,090 root         INFO     Train Epoch: 114 [5120/8000 (64%)]\tTotal Loss: 0.171025\n",
      "Reconstruction: 0.170430, Regularization: 0.000595\n",
      "2019-04-10 01:03:12,155 root         INFO     Train Epoch: 114 [5632/8000 (70%)]\tTotal Loss: 0.174037\n",
      "Reconstruction: 0.173341, Regularization: 0.000695\n",
      "2019-04-10 01:03:12,219 root         INFO     Train Epoch: 114 [6144/8000 (77%)]\tTotal Loss: 0.164231\n",
      "Reconstruction: 0.163646, Regularization: 0.000586\n",
      "2019-04-10 01:03:12,283 root         INFO     Train Epoch: 114 [6656/8000 (83%)]\tTotal Loss: 0.165377\n",
      "Reconstruction: 0.164967, Regularization: 0.000411\n",
      "2019-04-10 01:03:12,346 root         INFO     Train Epoch: 114 [7168/8000 (90%)]\tTotal Loss: 0.159946\n",
      "Reconstruction: 0.159529, Regularization: 0.000417\n",
      "2019-04-10 01:03:12,410 root         INFO     Train Epoch: 114 [7680/8000 (96%)]\tTotal Loss: 0.170273\n",
      "Reconstruction: 0.169774, Regularization: 0.000499\n",
      "2019-04-10 01:03:12,463 root         INFO     ====> Epoch: 114 Average loss: 0.1662\n",
      "2019-04-10 01:03:12,487 root         INFO     Train Epoch: 115 [0/8000 (0%)]\tTotal Loss: 0.155075\n",
      "Reconstruction: 0.154645, Regularization: 0.000431\n",
      "2019-04-10 01:03:12,551 root         INFO     Train Epoch: 115 [512/8000 (6%)]\tTotal Loss: 0.178446\n",
      "Reconstruction: 0.177727, Regularization: 0.000719\n",
      "2019-04-10 01:03:12,615 root         INFO     Train Epoch: 115 [1024/8000 (13%)]\tTotal Loss: 0.175749\n",
      "Reconstruction: 0.175097, Regularization: 0.000652\n",
      "2019-04-10 01:03:12,679 root         INFO     Train Epoch: 115 [1536/8000 (19%)]\tTotal Loss: 0.159776\n",
      "Reconstruction: 0.159354, Regularization: 0.000422\n",
      "2019-04-10 01:03:12,743 root         INFO     Train Epoch: 115 [2048/8000 (26%)]\tTotal Loss: 0.183660\n",
      "Reconstruction: 0.182928, Regularization: 0.000732\n",
      "2019-04-10 01:03:12,807 root         INFO     Train Epoch: 115 [2560/8000 (32%)]\tTotal Loss: 0.169510\n",
      "Reconstruction: 0.168916, Regularization: 0.000594\n",
      "2019-04-10 01:03:12,870 root         INFO     Train Epoch: 115 [3072/8000 (38%)]\tTotal Loss: 0.178823\n",
      "Reconstruction: 0.178154, Regularization: 0.000669\n",
      "2019-04-10 01:03:12,933 root         INFO     Train Epoch: 115 [3584/8000 (45%)]\tTotal Loss: 0.162366\n",
      "Reconstruction: 0.161908, Regularization: 0.000459\n",
      "2019-04-10 01:03:12,997 root         INFO     Train Epoch: 115 [4096/8000 (51%)]\tTotal Loss: 0.169526\n",
      "Reconstruction: 0.169062, Regularization: 0.000464\n",
      "2019-04-10 01:03:13,061 root         INFO     Train Epoch: 115 [4608/8000 (58%)]\tTotal Loss: 0.176594\n",
      "Reconstruction: 0.175944, Regularization: 0.000650\n",
      "2019-04-10 01:03:13,124 root         INFO     Train Epoch: 115 [5120/8000 (64%)]\tTotal Loss: 0.169786\n",
      "Reconstruction: 0.169176, Regularization: 0.000610\n",
      "2019-04-10 01:03:13,188 root         INFO     Train Epoch: 115 [5632/8000 (70%)]\tTotal Loss: 0.168943\n",
      "Reconstruction: 0.168383, Regularization: 0.000560\n",
      "2019-04-10 01:03:13,251 root         INFO     Train Epoch: 115 [6144/8000 (77%)]\tTotal Loss: 0.165706\n",
      "Reconstruction: 0.165182, Regularization: 0.000524\n",
      "2019-04-10 01:03:13,314 root         INFO     Train Epoch: 115 [6656/8000 (83%)]\tTotal Loss: 0.164598\n",
      "Reconstruction: 0.164168, Regularization: 0.000431\n",
      "2019-04-10 01:03:13,377 root         INFO     Train Epoch: 115 [7168/8000 (90%)]\tTotal Loss: 0.168152\n",
      "Reconstruction: 0.167494, Regularization: 0.000658\n",
      "2019-04-10 01:03:13,441 root         INFO     Train Epoch: 115 [7680/8000 (96%)]\tTotal Loss: 0.177339\n",
      "Reconstruction: 0.176683, Regularization: 0.000656\n",
      "2019-04-10 01:03:13,494 root         INFO     ====> Epoch: 115 Average loss: 0.1662\n",
      "2019-04-10 01:03:13,519 root         INFO     Train Epoch: 116 [0/8000 (0%)]\tTotal Loss: 0.177438\n",
      "Reconstruction: 0.176735, Regularization: 0.000703\n",
      "2019-04-10 01:03:13,582 root         INFO     Train Epoch: 116 [512/8000 (6%)]\tTotal Loss: 0.174344\n",
      "Reconstruction: 0.173758, Regularization: 0.000586\n",
      "2019-04-10 01:03:13,645 root         INFO     Train Epoch: 116 [1024/8000 (13%)]\tTotal Loss: 0.183299\n",
      "Reconstruction: 0.182378, Regularization: 0.000920\n",
      "2019-04-10 01:03:13,708 root         INFO     Train Epoch: 116 [1536/8000 (19%)]\tTotal Loss: 0.164865\n",
      "Reconstruction: 0.164270, Regularization: 0.000595\n",
      "2019-04-10 01:03:13,771 root         INFO     Train Epoch: 116 [2048/8000 (26%)]\tTotal Loss: 0.163335\n",
      "Reconstruction: 0.162837, Regularization: 0.000498\n",
      "2019-04-10 01:03:13,834 root         INFO     Train Epoch: 116 [2560/8000 (32%)]\tTotal Loss: 0.163108\n",
      "Reconstruction: 0.162672, Regularization: 0.000437\n",
      "2019-04-10 01:03:13,897 root         INFO     Train Epoch: 116 [3072/8000 (38%)]\tTotal Loss: 0.171412\n",
      "Reconstruction: 0.170731, Regularization: 0.000681\n",
      "2019-04-10 01:03:13,960 root         INFO     Train Epoch: 116 [3584/8000 (45%)]\tTotal Loss: 0.172743\n",
      "Reconstruction: 0.172143, Regularization: 0.000600\n",
      "2019-04-10 01:03:14,023 root         INFO     Train Epoch: 116 [4096/8000 (51%)]\tTotal Loss: 0.160473\n",
      "Reconstruction: 0.159975, Regularization: 0.000498\n",
      "2019-04-10 01:03:14,087 root         INFO     Train Epoch: 116 [4608/8000 (58%)]\tTotal Loss: 0.158365\n",
      "Reconstruction: 0.157984, Regularization: 0.000380\n",
      "2019-04-10 01:03:14,148 root         INFO     Train Epoch: 116 [5120/8000 (64%)]\tTotal Loss: 0.186550\n",
      "Reconstruction: 0.185637, Regularization: 0.000914\n",
      "2019-04-10 01:03:14,209 root         INFO     Train Epoch: 116 [5632/8000 (70%)]\tTotal Loss: 0.166361\n",
      "Reconstruction: 0.165800, Regularization: 0.000562\n",
      "2019-04-10 01:03:14,271 root         INFO     Train Epoch: 116 [6144/8000 (77%)]\tTotal Loss: 0.160705\n",
      "Reconstruction: 0.160237, Regularization: 0.000469\n",
      "2019-04-10 01:03:14,333 root         INFO     Train Epoch: 116 [6656/8000 (83%)]\tTotal Loss: 0.170408\n",
      "Reconstruction: 0.169802, Regularization: 0.000606\n",
      "2019-04-10 01:03:14,395 root         INFO     Train Epoch: 116 [7168/8000 (90%)]\tTotal Loss: 0.164815\n",
      "Reconstruction: 0.164339, Regularization: 0.000476\n",
      "2019-04-10 01:03:14,457 root         INFO     Train Epoch: 116 [7680/8000 (96%)]\tTotal Loss: 0.164762\n",
      "Reconstruction: 0.164202, Regularization: 0.000559\n",
      "2019-04-10 01:03:14,510 root         INFO     ====> Epoch: 116 Average loss: 0.1662\n",
      "2019-04-10 01:03:14,534 root         INFO     Train Epoch: 117 [0/8000 (0%)]\tTotal Loss: 0.171901\n",
      "Reconstruction: 0.171156, Regularization: 0.000746\n",
      "2019-04-10 01:03:14,597 root         INFO     Train Epoch: 117 [512/8000 (6%)]\tTotal Loss: 0.170962\n",
      "Reconstruction: 0.170243, Regularization: 0.000719\n",
      "2019-04-10 01:03:14,659 root         INFO     Train Epoch: 117 [1024/8000 (13%)]\tTotal Loss: 0.170335\n",
      "Reconstruction: 0.169697, Regularization: 0.000638\n",
      "2019-04-10 01:03:14,722 root         INFO     Train Epoch: 117 [1536/8000 (19%)]\tTotal Loss: 0.162311\n",
      "Reconstruction: 0.161828, Regularization: 0.000483\n",
      "2019-04-10 01:03:14,784 root         INFO     Train Epoch: 117 [2048/8000 (26%)]\tTotal Loss: 0.157088\n",
      "Reconstruction: 0.156662, Regularization: 0.000426\n",
      "2019-04-10 01:03:14,847 root         INFO     Train Epoch: 117 [2560/8000 (32%)]\tTotal Loss: 0.168399\n",
      "Reconstruction: 0.167766, Regularization: 0.000634\n",
      "2019-04-10 01:03:14,910 root         INFO     Train Epoch: 117 [3072/8000 (38%)]\tTotal Loss: 0.156641\n",
      "Reconstruction: 0.156262, Regularization: 0.000379\n",
      "2019-04-10 01:03:14,972 root         INFO     Train Epoch: 117 [3584/8000 (45%)]\tTotal Loss: 0.177405\n",
      "Reconstruction: 0.176711, Regularization: 0.000694\n",
      "2019-04-10 01:03:15,034 root         INFO     Train Epoch: 117 [4096/8000 (51%)]\tTotal Loss: 0.164373\n",
      "Reconstruction: 0.163800, Regularization: 0.000573\n",
      "2019-04-10 01:03:15,097 root         INFO     Train Epoch: 117 [4608/8000 (58%)]\tTotal Loss: 0.161592\n",
      "Reconstruction: 0.161065, Regularization: 0.000526\n",
      "2019-04-10 01:03:15,159 root         INFO     Train Epoch: 117 [5120/8000 (64%)]\tTotal Loss: 0.176265\n",
      "Reconstruction: 0.175527, Regularization: 0.000737\n",
      "2019-04-10 01:03:15,222 root         INFO     Train Epoch: 117 [5632/8000 (70%)]\tTotal Loss: 0.166515\n",
      "Reconstruction: 0.165861, Regularization: 0.000653\n",
      "2019-04-10 01:03:15,284 root         INFO     Train Epoch: 117 [6144/8000 (77%)]\tTotal Loss: 0.155948\n",
      "Reconstruction: 0.155508, Regularization: 0.000440\n",
      "2019-04-10 01:03:15,346 root         INFO     Train Epoch: 117 [6656/8000 (83%)]\tTotal Loss: 0.173862\n",
      "Reconstruction: 0.173085, Regularization: 0.000778\n",
      "2019-04-10 01:03:15,409 root         INFO     Train Epoch: 117 [7168/8000 (90%)]\tTotal Loss: 0.152899\n",
      "Reconstruction: 0.152556, Regularization: 0.000343\n",
      "2019-04-10 01:03:15,472 root         INFO     Train Epoch: 117 [7680/8000 (96%)]\tTotal Loss: 0.155382\n",
      "Reconstruction: 0.154962, Regularization: 0.000420\n",
      "2019-04-10 01:03:15,525 root         INFO     ====> Epoch: 117 Average loss: 0.1663\n",
      "2019-04-10 01:03:15,549 root         INFO     Train Epoch: 118 [0/8000 (0%)]\tTotal Loss: 0.168668\n",
      "Reconstruction: 0.168022, Regularization: 0.000646\n",
      "2019-04-10 01:03:15,613 root         INFO     Train Epoch: 118 [512/8000 (6%)]\tTotal Loss: 0.163078\n",
      "Reconstruction: 0.162535, Regularization: 0.000543\n",
      "2019-04-10 01:03:15,675 root         INFO     Train Epoch: 118 [1024/8000 (13%)]\tTotal Loss: 0.170415\n",
      "Reconstruction: 0.169780, Regularization: 0.000635\n",
      "2019-04-10 01:03:15,738 root         INFO     Train Epoch: 118 [1536/8000 (19%)]\tTotal Loss: 0.172545\n",
      "Reconstruction: 0.171762, Regularization: 0.000783\n",
      "2019-04-10 01:03:15,800 root         INFO     Train Epoch: 118 [2048/8000 (26%)]\tTotal Loss: 0.163792\n",
      "Reconstruction: 0.163243, Regularization: 0.000549\n",
      "2019-04-10 01:03:15,863 root         INFO     Train Epoch: 118 [2560/8000 (32%)]\tTotal Loss: 0.166920\n",
      "Reconstruction: 0.166332, Regularization: 0.000588\n",
      "2019-04-10 01:03:15,925 root         INFO     Train Epoch: 118 [3072/8000 (38%)]\tTotal Loss: 0.151636\n",
      "Reconstruction: 0.151341, Regularization: 0.000295\n",
      "2019-04-10 01:03:15,988 root         INFO     Train Epoch: 118 [3584/8000 (45%)]\tTotal Loss: 0.190971\n",
      "Reconstruction: 0.189899, Regularization: 0.001071\n",
      "2019-04-10 01:03:16,050 root         INFO     Train Epoch: 118 [4096/8000 (51%)]\tTotal Loss: 0.170440\n",
      "Reconstruction: 0.169793, Regularization: 0.000647\n",
      "2019-04-10 01:03:16,113 root         INFO     Train Epoch: 118 [4608/8000 (58%)]\tTotal Loss: 0.160421\n",
      "Reconstruction: 0.159975, Regularization: 0.000446\n",
      "2019-04-10 01:03:16,175 root         INFO     Train Epoch: 118 [5120/8000 (64%)]\tTotal Loss: 0.167093\n",
      "Reconstruction: 0.166446, Regularization: 0.000647\n",
      "2019-04-10 01:03:16,238 root         INFO     Train Epoch: 118 [5632/8000 (70%)]\tTotal Loss: 0.164862\n",
      "Reconstruction: 0.164249, Regularization: 0.000613\n",
      "2019-04-10 01:03:16,301 root         INFO     Train Epoch: 118 [6144/8000 (77%)]\tTotal Loss: 0.168633\n",
      "Reconstruction: 0.167982, Regularization: 0.000651\n",
      "2019-04-10 01:03:16,364 root         INFO     Train Epoch: 118 [6656/8000 (83%)]\tTotal Loss: 0.162245\n",
      "Reconstruction: 0.161739, Regularization: 0.000506\n",
      "2019-04-10 01:03:16,427 root         INFO     Train Epoch: 118 [7168/8000 (90%)]\tTotal Loss: 0.175533\n",
      "Reconstruction: 0.174775, Regularization: 0.000758\n",
      "2019-04-10 01:03:16,489 root         INFO     Train Epoch: 118 [7680/8000 (96%)]\tTotal Loss: 0.157248\n",
      "Reconstruction: 0.156801, Regularization: 0.000447\n",
      "2019-04-10 01:03:16,543 root         INFO     ====> Epoch: 118 Average loss: 0.1664\n",
      "2019-04-10 01:03:16,568 root         INFO     Train Epoch: 119 [0/8000 (0%)]\tTotal Loss: 0.166836\n",
      "Reconstruction: 0.166285, Regularization: 0.000552\n",
      "2019-04-10 01:03:16,631 root         INFO     Train Epoch: 119 [512/8000 (6%)]\tTotal Loss: 0.158924\n",
      "Reconstruction: 0.158443, Regularization: 0.000480\n",
      "2019-04-10 01:03:16,695 root         INFO     Train Epoch: 119 [1024/8000 (13%)]\tTotal Loss: 0.158187\n",
      "Reconstruction: 0.157716, Regularization: 0.000472\n",
      "2019-04-10 01:03:16,758 root         INFO     Train Epoch: 119 [1536/8000 (19%)]\tTotal Loss: 0.164139\n",
      "Reconstruction: 0.163565, Regularization: 0.000575\n",
      "2019-04-10 01:03:16,822 root         INFO     Train Epoch: 119 [2048/8000 (26%)]\tTotal Loss: 0.163573\n",
      "Reconstruction: 0.163068, Regularization: 0.000504\n",
      "2019-04-10 01:03:16,886 root         INFO     Train Epoch: 119 [2560/8000 (32%)]\tTotal Loss: 0.167511\n",
      "Reconstruction: 0.166927, Regularization: 0.000584\n",
      "2019-04-10 01:03:16,949 root         INFO     Train Epoch: 119 [3072/8000 (38%)]\tTotal Loss: 0.161481\n",
      "Reconstruction: 0.160976, Regularization: 0.000504\n",
      "2019-04-10 01:03:17,012 root         INFO     Train Epoch: 119 [3584/8000 (45%)]\tTotal Loss: 0.167942\n",
      "Reconstruction: 0.167401, Regularization: 0.000541\n",
      "2019-04-10 01:03:17,075 root         INFO     Train Epoch: 119 [4096/8000 (51%)]\tTotal Loss: 0.172135\n",
      "Reconstruction: 0.171431, Regularization: 0.000704\n",
      "2019-04-10 01:03:17,139 root         INFO     Train Epoch: 119 [4608/8000 (58%)]\tTotal Loss: 0.175520\n",
      "Reconstruction: 0.174647, Regularization: 0.000872\n",
      "2019-04-10 01:03:17,202 root         INFO     Train Epoch: 119 [5120/8000 (64%)]\tTotal Loss: 0.163961\n",
      "Reconstruction: 0.163427, Regularization: 0.000535\n",
      "2019-04-10 01:03:17,264 root         INFO     Train Epoch: 119 [5632/8000 (70%)]\tTotal Loss: 0.155876\n",
      "Reconstruction: 0.155462, Regularization: 0.000414\n",
      "2019-04-10 01:03:17,327 root         INFO     Train Epoch: 119 [6144/8000 (77%)]\tTotal Loss: 0.165942\n",
      "Reconstruction: 0.165316, Regularization: 0.000626\n",
      "2019-04-10 01:03:17,390 root         INFO     Train Epoch: 119 [6656/8000 (83%)]\tTotal Loss: 0.158323\n",
      "Reconstruction: 0.157849, Regularization: 0.000474\n",
      "2019-04-10 01:03:17,453 root         INFO     Train Epoch: 119 [7168/8000 (90%)]\tTotal Loss: 0.161764\n",
      "Reconstruction: 0.161284, Regularization: 0.000480\n",
      "2019-04-10 01:03:17,515 root         INFO     Train Epoch: 119 [7680/8000 (96%)]\tTotal Loss: 0.157743\n",
      "Reconstruction: 0.157293, Regularization: 0.000450\n",
      "2019-04-10 01:03:17,569 root         INFO     ====> Epoch: 119 Average loss: 0.1662\n",
      "2019-04-10 01:03:17,594 root         INFO     Train Epoch: 120 [0/8000 (0%)]\tTotal Loss: 0.167829\n",
      "Reconstruction: 0.167192, Regularization: 0.000637\n",
      "2019-04-10 01:03:17,656 root         INFO     Train Epoch: 120 [512/8000 (6%)]\tTotal Loss: 0.164022\n",
      "Reconstruction: 0.163518, Regularization: 0.000504\n",
      "2019-04-10 01:03:17,719 root         INFO     Train Epoch: 120 [1024/8000 (13%)]\tTotal Loss: 0.162189\n",
      "Reconstruction: 0.161715, Regularization: 0.000474\n",
      "2019-04-10 01:03:17,781 root         INFO     Train Epoch: 120 [1536/8000 (19%)]\tTotal Loss: 0.168191\n",
      "Reconstruction: 0.167538, Regularization: 0.000653\n",
      "2019-04-10 01:03:17,843 root         INFO     Train Epoch: 120 [2048/8000 (26%)]\tTotal Loss: 0.151547\n",
      "Reconstruction: 0.151207, Regularization: 0.000339\n",
      "2019-04-10 01:03:17,905 root         INFO     Train Epoch: 120 [2560/8000 (32%)]\tTotal Loss: 0.177945\n",
      "Reconstruction: 0.177225, Regularization: 0.000720\n",
      "2019-04-10 01:03:17,967 root         INFO     Train Epoch: 120 [3072/8000 (38%)]\tTotal Loss: 0.181647\n",
      "Reconstruction: 0.180758, Regularization: 0.000889\n",
      "2019-04-10 01:03:18,030 root         INFO     Train Epoch: 120 [3584/8000 (45%)]\tTotal Loss: 0.170195\n",
      "Reconstruction: 0.169531, Regularization: 0.000664\n",
      "2019-04-10 01:03:18,092 root         INFO     Train Epoch: 120 [4096/8000 (51%)]\tTotal Loss: 0.169601\n",
      "Reconstruction: 0.169004, Regularization: 0.000597\n",
      "2019-04-10 01:03:18,154 root         INFO     Train Epoch: 120 [4608/8000 (58%)]\tTotal Loss: 0.158436\n",
      "Reconstruction: 0.158011, Regularization: 0.000425\n",
      "2019-04-10 01:03:18,216 root         INFO     Train Epoch: 120 [5120/8000 (64%)]\tTotal Loss: 0.158450\n",
      "Reconstruction: 0.157991, Regularization: 0.000459\n",
      "2019-04-10 01:03:18,278 root         INFO     Train Epoch: 120 [5632/8000 (70%)]\tTotal Loss: 0.163493\n",
      "Reconstruction: 0.162933, Regularization: 0.000560\n",
      "2019-04-10 01:03:18,340 root         INFO     Train Epoch: 120 [6144/8000 (77%)]\tTotal Loss: 0.157097\n",
      "Reconstruction: 0.156648, Regularization: 0.000449\n",
      "2019-04-10 01:03:18,402 root         INFO     Train Epoch: 120 [6656/8000 (83%)]\tTotal Loss: 0.158960\n",
      "Reconstruction: 0.158506, Regularization: 0.000454\n",
      "2019-04-10 01:03:18,464 root         INFO     Train Epoch: 120 [7168/8000 (90%)]\tTotal Loss: 0.169286\n",
      "Reconstruction: 0.168607, Regularization: 0.000678\n",
      "2019-04-10 01:03:18,527 root         INFO     Train Epoch: 120 [7680/8000 (96%)]\tTotal Loss: 0.150397\n",
      "Reconstruction: 0.150138, Regularization: 0.000260\n",
      "2019-04-10 01:03:18,581 root         INFO     ====> Epoch: 120 Average loss: 0.1661\n",
      "2019-04-10 01:03:18,605 root         INFO     Train Epoch: 121 [0/8000 (0%)]\tTotal Loss: 0.159802\n",
      "Reconstruction: 0.159291, Regularization: 0.000512\n",
      "2019-04-10 01:03:18,668 root         INFO     Train Epoch: 121 [512/8000 (6%)]\tTotal Loss: 0.158695\n",
      "Reconstruction: 0.158195, Regularization: 0.000500\n",
      "2019-04-10 01:03:18,732 root         INFO     Train Epoch: 121 [1024/8000 (13%)]\tTotal Loss: 0.171277\n",
      "Reconstruction: 0.170571, Regularization: 0.000706\n",
      "2019-04-10 01:03:18,795 root         INFO     Train Epoch: 121 [1536/8000 (19%)]\tTotal Loss: 0.158531\n",
      "Reconstruction: 0.158114, Regularization: 0.000417\n",
      "2019-04-10 01:03:18,858 root         INFO     Train Epoch: 121 [2048/8000 (26%)]\tTotal Loss: 0.165791\n",
      "Reconstruction: 0.165237, Regularization: 0.000555\n",
      "2019-04-10 01:03:18,921 root         INFO     Train Epoch: 121 [2560/8000 (32%)]\tTotal Loss: 0.171470\n",
      "Reconstruction: 0.170696, Regularization: 0.000774\n",
      "2019-04-10 01:03:18,984 root         INFO     Train Epoch: 121 [3072/8000 (38%)]\tTotal Loss: 0.172421\n",
      "Reconstruction: 0.171815, Regularization: 0.000605\n",
      "2019-04-10 01:03:19,046 root         INFO     Train Epoch: 121 [3584/8000 (45%)]\tTotal Loss: 0.172710\n",
      "Reconstruction: 0.172005, Regularization: 0.000705\n",
      "2019-04-10 01:03:19,108 root         INFO     Train Epoch: 121 [4096/8000 (51%)]\tTotal Loss: 0.167723\n",
      "Reconstruction: 0.167078, Regularization: 0.000646\n",
      "2019-04-10 01:03:19,169 root         INFO     Train Epoch: 121 [4608/8000 (58%)]\tTotal Loss: 0.169034\n",
      "Reconstruction: 0.168271, Regularization: 0.000764\n",
      "2019-04-10 01:03:19,231 root         INFO     Train Epoch: 121 [5120/8000 (64%)]\tTotal Loss: 0.162710\n",
      "Reconstruction: 0.162174, Regularization: 0.000536\n",
      "2019-04-10 01:03:19,293 root         INFO     Train Epoch: 121 [5632/8000 (70%)]\tTotal Loss: 0.169910\n",
      "Reconstruction: 0.169133, Regularization: 0.000776\n",
      "2019-04-10 01:03:19,356 root         INFO     Train Epoch: 121 [6144/8000 (77%)]\tTotal Loss: 0.173506\n",
      "Reconstruction: 0.172616, Regularization: 0.000891\n",
      "2019-04-10 01:03:19,418 root         INFO     Train Epoch: 121 [6656/8000 (83%)]\tTotal Loss: 0.157689\n",
      "Reconstruction: 0.157290, Regularization: 0.000398\n",
      "2019-04-10 01:03:19,480 root         INFO     Train Epoch: 121 [7168/8000 (90%)]\tTotal Loss: 0.165685\n",
      "Reconstruction: 0.165059, Regularization: 0.000626\n",
      "2019-04-10 01:03:19,542 root         INFO     Train Epoch: 121 [7680/8000 (96%)]\tTotal Loss: 0.169444\n",
      "Reconstruction: 0.168815, Regularization: 0.000629\n",
      "2019-04-10 01:03:19,595 root         INFO     ====> Epoch: 121 Average loss: 0.1663\n",
      "2019-04-10 01:03:19,619 root         INFO     Train Epoch: 122 [0/8000 (0%)]\tTotal Loss: 0.159354\n",
      "Reconstruction: 0.158846, Regularization: 0.000508\n",
      "2019-04-10 01:03:19,681 root         INFO     Train Epoch: 122 [512/8000 (6%)]\tTotal Loss: 0.167065\n",
      "Reconstruction: 0.166492, Regularization: 0.000573\n",
      "2019-04-10 01:03:19,742 root         INFO     Train Epoch: 122 [1024/8000 (13%)]\tTotal Loss: 0.164423\n",
      "Reconstruction: 0.163826, Regularization: 0.000597\n",
      "2019-04-10 01:03:19,804 root         INFO     Train Epoch: 122 [1536/8000 (19%)]\tTotal Loss: 0.202439\n",
      "Reconstruction: 0.200987, Regularization: 0.001453\n",
      "2019-04-10 01:03:19,865 root         INFO     Train Epoch: 122 [2048/8000 (26%)]\tTotal Loss: 0.168650\n",
      "Reconstruction: 0.167977, Regularization: 0.000674\n",
      "2019-04-10 01:03:19,927 root         INFO     Train Epoch: 122 [2560/8000 (32%)]\tTotal Loss: 0.165490\n",
      "Reconstruction: 0.164915, Regularization: 0.000575\n",
      "2019-04-10 01:03:19,989 root         INFO     Train Epoch: 122 [3072/8000 (38%)]\tTotal Loss: 0.158328\n",
      "Reconstruction: 0.157854, Regularization: 0.000474\n",
      "2019-04-10 01:03:20,050 root         INFO     Train Epoch: 122 [3584/8000 (45%)]\tTotal Loss: 0.173936\n",
      "Reconstruction: 0.173165, Regularization: 0.000770\n",
      "2019-04-10 01:03:20,112 root         INFO     Train Epoch: 122 [4096/8000 (51%)]\tTotal Loss: 0.181842\n",
      "Reconstruction: 0.181020, Regularization: 0.000821\n",
      "2019-04-10 01:03:20,174 root         INFO     Train Epoch: 122 [4608/8000 (58%)]\tTotal Loss: 0.170695\n",
      "Reconstruction: 0.169981, Regularization: 0.000715\n",
      "2019-04-10 01:03:20,236 root         INFO     Train Epoch: 122 [5120/8000 (64%)]\tTotal Loss: 0.168817\n",
      "Reconstruction: 0.168224, Regularization: 0.000593\n",
      "2019-04-10 01:03:20,297 root         INFO     Train Epoch: 122 [5632/8000 (70%)]\tTotal Loss: 0.155797\n",
      "Reconstruction: 0.155399, Regularization: 0.000399\n",
      "2019-04-10 01:03:20,359 root         INFO     Train Epoch: 122 [6144/8000 (77%)]\tTotal Loss: 0.165903\n",
      "Reconstruction: 0.165238, Regularization: 0.000665\n",
      "2019-04-10 01:03:20,421 root         INFO     Train Epoch: 122 [6656/8000 (83%)]\tTotal Loss: 0.156543\n",
      "Reconstruction: 0.156085, Regularization: 0.000457\n",
      "2019-04-10 01:03:20,482 root         INFO     Train Epoch: 122 [7168/8000 (90%)]\tTotal Loss: 0.171426\n",
      "Reconstruction: 0.170630, Regularization: 0.000796\n",
      "2019-04-10 01:03:20,543 root         INFO     Train Epoch: 122 [7680/8000 (96%)]\tTotal Loss: 0.163177\n",
      "Reconstruction: 0.162636, Regularization: 0.000540\n",
      "2019-04-10 01:03:20,597 root         INFO     ====> Epoch: 122 Average loss: 0.1662\n",
      "2019-04-10 01:03:20,621 root         INFO     Train Epoch: 123 [0/8000 (0%)]\tTotal Loss: 0.175128\n",
      "Reconstruction: 0.174360, Regularization: 0.000768\n",
      "2019-04-10 01:03:20,685 root         INFO     Train Epoch: 123 [512/8000 (6%)]\tTotal Loss: 0.168264\n",
      "Reconstruction: 0.167552, Regularization: 0.000712\n",
      "2019-04-10 01:03:20,748 root         INFO     Train Epoch: 123 [1024/8000 (13%)]\tTotal Loss: 0.168962\n",
      "Reconstruction: 0.168207, Regularization: 0.000756\n",
      "2019-04-10 01:03:20,811 root         INFO     Train Epoch: 123 [1536/8000 (19%)]\tTotal Loss: 0.158286\n",
      "Reconstruction: 0.157797, Regularization: 0.000489\n",
      "2019-04-10 01:03:20,874 root         INFO     Train Epoch: 123 [2048/8000 (26%)]\tTotal Loss: 0.175117\n",
      "Reconstruction: 0.174424, Regularization: 0.000693\n",
      "2019-04-10 01:03:20,937 root         INFO     Train Epoch: 123 [2560/8000 (32%)]\tTotal Loss: 0.154308\n",
      "Reconstruction: 0.153945, Regularization: 0.000364\n",
      "2019-04-10 01:03:21,000 root         INFO     Train Epoch: 123 [3072/8000 (38%)]\tTotal Loss: 0.166351\n",
      "Reconstruction: 0.165689, Regularization: 0.000663\n",
      "2019-04-10 01:03:21,063 root         INFO     Train Epoch: 123 [3584/8000 (45%)]\tTotal Loss: 0.165547\n",
      "Reconstruction: 0.164996, Regularization: 0.000551\n",
      "2019-04-10 01:03:21,126 root         INFO     Train Epoch: 123 [4096/8000 (51%)]\tTotal Loss: 0.156230\n",
      "Reconstruction: 0.155772, Regularization: 0.000458\n",
      "2019-04-10 01:03:21,189 root         INFO     Train Epoch: 123 [4608/8000 (58%)]\tTotal Loss: 0.175133\n",
      "Reconstruction: 0.174326, Regularization: 0.000807\n",
      "2019-04-10 01:03:21,252 root         INFO     Train Epoch: 123 [5120/8000 (64%)]\tTotal Loss: 0.172573\n",
      "Reconstruction: 0.171864, Regularization: 0.000709\n",
      "2019-04-10 01:03:21,314 root         INFO     Train Epoch: 123 [5632/8000 (70%)]\tTotal Loss: 0.173424\n",
      "Reconstruction: 0.172635, Regularization: 0.000789\n",
      "2019-04-10 01:03:21,377 root         INFO     Train Epoch: 123 [6144/8000 (77%)]\tTotal Loss: 0.172057\n",
      "Reconstruction: 0.171393, Regularization: 0.000664\n",
      "2019-04-10 01:03:21,440 root         INFO     Train Epoch: 123 [6656/8000 (83%)]\tTotal Loss: 0.155828\n",
      "Reconstruction: 0.155424, Regularization: 0.000404\n",
      "2019-04-10 01:03:21,502 root         INFO     Train Epoch: 123 [7168/8000 (90%)]\tTotal Loss: 0.168009\n",
      "Reconstruction: 0.167339, Regularization: 0.000670\n",
      "2019-04-10 01:03:21,566 root         INFO     Train Epoch: 123 [7680/8000 (96%)]\tTotal Loss: 0.158571\n",
      "Reconstruction: 0.158085, Regularization: 0.000486\n",
      "2019-04-10 01:03:21,619 root         INFO     ====> Epoch: 123 Average loss: 0.1661\n",
      "2019-04-10 01:03:21,643 root         INFO     Train Epoch: 124 [0/8000 (0%)]\tTotal Loss: 0.170218\n",
      "Reconstruction: 0.169532, Regularization: 0.000686\n",
      "2019-04-10 01:03:21,706 root         INFO     Train Epoch: 124 [512/8000 (6%)]\tTotal Loss: 0.171684\n",
      "Reconstruction: 0.170995, Regularization: 0.000689\n",
      "2019-04-10 01:03:21,769 root         INFO     Train Epoch: 124 [1024/8000 (13%)]\tTotal Loss: 0.155008\n",
      "Reconstruction: 0.154560, Regularization: 0.000448\n",
      "2019-04-10 01:03:21,832 root         INFO     Train Epoch: 124 [1536/8000 (19%)]\tTotal Loss: 0.169156\n",
      "Reconstruction: 0.168454, Regularization: 0.000702\n",
      "2019-04-10 01:03:21,895 root         INFO     Train Epoch: 124 [2048/8000 (26%)]\tTotal Loss: 0.162713\n",
      "Reconstruction: 0.162175, Regularization: 0.000539\n",
      "2019-04-10 01:03:21,958 root         INFO     Train Epoch: 124 [2560/8000 (32%)]\tTotal Loss: 0.162675\n",
      "Reconstruction: 0.162121, Regularization: 0.000554\n",
      "2019-04-10 01:03:22,021 root         INFO     Train Epoch: 124 [3072/8000 (38%)]\tTotal Loss: 0.173406\n",
      "Reconstruction: 0.172611, Regularization: 0.000794\n",
      "2019-04-10 01:03:22,083 root         INFO     Train Epoch: 124 [3584/8000 (45%)]\tTotal Loss: 0.158840\n",
      "Reconstruction: 0.158373, Regularization: 0.000468\n",
      "2019-04-10 01:03:22,146 root         INFO     Train Epoch: 124 [4096/8000 (51%)]\tTotal Loss: 0.158877\n",
      "Reconstruction: 0.158364, Regularization: 0.000513\n",
      "2019-04-10 01:03:22,208 root         INFO     Train Epoch: 124 [4608/8000 (58%)]\tTotal Loss: 0.166217\n",
      "Reconstruction: 0.165611, Regularization: 0.000606\n",
      "2019-04-10 01:03:22,271 root         INFO     Train Epoch: 124 [5120/8000 (64%)]\tTotal Loss: 0.163893\n",
      "Reconstruction: 0.163224, Regularization: 0.000669\n",
      "2019-04-10 01:03:22,334 root         INFO     Train Epoch: 124 [5632/8000 (70%)]\tTotal Loss: 0.147686\n",
      "Reconstruction: 0.147423, Regularization: 0.000263\n",
      "2019-04-10 01:03:22,396 root         INFO     Train Epoch: 124 [6144/8000 (77%)]\tTotal Loss: 0.165809\n",
      "Reconstruction: 0.165180, Regularization: 0.000629\n",
      "2019-04-10 01:03:22,459 root         INFO     Train Epoch: 124 [6656/8000 (83%)]\tTotal Loss: 0.171499\n",
      "Reconstruction: 0.170631, Regularization: 0.000869\n",
      "2019-04-10 01:03:22,522 root         INFO     Train Epoch: 124 [7168/8000 (90%)]\tTotal Loss: 0.161816\n",
      "Reconstruction: 0.161244, Regularization: 0.000572\n",
      "2019-04-10 01:03:22,585 root         INFO     Train Epoch: 124 [7680/8000 (96%)]\tTotal Loss: 0.156246\n",
      "Reconstruction: 0.155753, Regularization: 0.000493\n",
      "2019-04-10 01:03:22,638 root         INFO     ====> Epoch: 124 Average loss: 0.1661\n",
      "2019-04-10 01:03:22,663 root         INFO     Train Epoch: 125 [0/8000 (0%)]\tTotal Loss: 0.176425\n",
      "Reconstruction: 0.175548, Regularization: 0.000878\n",
      "2019-04-10 01:03:22,727 root         INFO     Train Epoch: 125 [512/8000 (6%)]\tTotal Loss: 0.157798\n",
      "Reconstruction: 0.157297, Regularization: 0.000502\n",
      "2019-04-10 01:03:22,790 root         INFO     Train Epoch: 125 [1024/8000 (13%)]\tTotal Loss: 0.168090\n",
      "Reconstruction: 0.167308, Regularization: 0.000782\n",
      "2019-04-10 01:03:22,853 root         INFO     Train Epoch: 125 [1536/8000 (19%)]\tTotal Loss: 0.182971\n",
      "Reconstruction: 0.181921, Regularization: 0.001050\n",
      "2019-04-10 01:03:22,917 root         INFO     Train Epoch: 125 [2048/8000 (26%)]\tTotal Loss: 0.165777\n",
      "Reconstruction: 0.165091, Regularization: 0.000686\n",
      "2019-04-10 01:03:22,980 root         INFO     Train Epoch: 125 [2560/8000 (32%)]\tTotal Loss: 0.170009\n",
      "Reconstruction: 0.169230, Regularization: 0.000779\n",
      "2019-04-10 01:03:23,042 root         INFO     Train Epoch: 125 [3072/8000 (38%)]\tTotal Loss: 0.175378\n",
      "Reconstruction: 0.174430, Regularization: 0.000947\n",
      "2019-04-10 01:03:23,105 root         INFO     Train Epoch: 125 [3584/8000 (45%)]\tTotal Loss: 0.163074\n",
      "Reconstruction: 0.162444, Regularization: 0.000630\n",
      "2019-04-10 01:03:23,167 root         INFO     Train Epoch: 125 [4096/8000 (51%)]\tTotal Loss: 0.169254\n",
      "Reconstruction: 0.168521, Regularization: 0.000733\n",
      "2019-04-10 01:03:23,230 root         INFO     Train Epoch: 125 [4608/8000 (58%)]\tTotal Loss: 0.158318\n",
      "Reconstruction: 0.157875, Regularization: 0.000443\n",
      "2019-04-10 01:03:23,294 root         INFO     Train Epoch: 125 [5120/8000 (64%)]\tTotal Loss: 0.170205\n",
      "Reconstruction: 0.169476, Regularization: 0.000730\n",
      "2019-04-10 01:03:23,357 root         INFO     Train Epoch: 125 [5632/8000 (70%)]\tTotal Loss: 0.162296\n",
      "Reconstruction: 0.161721, Regularization: 0.000575\n",
      "2019-04-10 01:03:23,421 root         INFO     Train Epoch: 125 [6144/8000 (77%)]\tTotal Loss: 0.151613\n",
      "Reconstruction: 0.151283, Regularization: 0.000330\n",
      "2019-04-10 01:03:23,484 root         INFO     Train Epoch: 125 [6656/8000 (83%)]\tTotal Loss: 0.171257\n",
      "Reconstruction: 0.170508, Regularization: 0.000749\n",
      "2019-04-10 01:03:23,546 root         INFO     Train Epoch: 125 [7168/8000 (90%)]\tTotal Loss: 0.167881\n",
      "Reconstruction: 0.167041, Regularization: 0.000840\n",
      "2019-04-10 01:03:23,610 root         INFO     Train Epoch: 125 [7680/8000 (96%)]\tTotal Loss: 0.170938\n",
      "Reconstruction: 0.170031, Regularization: 0.000906\n",
      "2019-04-10 01:03:23,664 root         INFO     ====> Epoch: 125 Average loss: 0.1661\n",
      "2019-04-10 01:03:23,688 root         INFO     Train Epoch: 126 [0/8000 (0%)]\tTotal Loss: 0.172834\n",
      "Reconstruction: 0.171918, Regularization: 0.000915\n",
      "2019-04-10 01:03:23,751 root         INFO     Train Epoch: 126 [512/8000 (6%)]\tTotal Loss: 0.155799\n",
      "Reconstruction: 0.155339, Regularization: 0.000460\n",
      "2019-04-10 01:03:23,814 root         INFO     Train Epoch: 126 [1024/8000 (13%)]\tTotal Loss: 0.161343\n",
      "Reconstruction: 0.160706, Regularization: 0.000637\n",
      "2019-04-10 01:03:23,878 root         INFO     Train Epoch: 126 [1536/8000 (19%)]\tTotal Loss: 0.169115\n",
      "Reconstruction: 0.168296, Regularization: 0.000819\n",
      "2019-04-10 01:03:23,941 root         INFO     Train Epoch: 126 [2048/8000 (26%)]\tTotal Loss: 0.161146\n",
      "Reconstruction: 0.160481, Regularization: 0.000665\n",
      "2019-04-10 01:03:24,005 root         INFO     Train Epoch: 126 [2560/8000 (32%)]\tTotal Loss: 0.168095\n",
      "Reconstruction: 0.167385, Regularization: 0.000710\n",
      "2019-04-10 01:03:24,068 root         INFO     Train Epoch: 126 [3072/8000 (38%)]\tTotal Loss: 0.156476\n",
      "Reconstruction: 0.155925, Regularization: 0.000551\n",
      "2019-04-10 01:03:24,132 root         INFO     Train Epoch: 126 [3584/8000 (45%)]\tTotal Loss: 0.163272\n",
      "Reconstruction: 0.162614, Regularization: 0.000657\n",
      "2019-04-10 01:03:24,196 root         INFO     Train Epoch: 126 [4096/8000 (51%)]\tTotal Loss: 0.176974\n",
      "Reconstruction: 0.175996, Regularization: 0.000979\n",
      "2019-04-10 01:03:24,260 root         INFO     Train Epoch: 126 [4608/8000 (58%)]\tTotal Loss: 0.166271\n",
      "Reconstruction: 0.165612, Regularization: 0.000659\n",
      "2019-04-10 01:03:24,324 root         INFO     Train Epoch: 126 [5120/8000 (64%)]\tTotal Loss: 0.170027\n",
      "Reconstruction: 0.169267, Regularization: 0.000760\n",
      "2019-04-10 01:03:24,389 root         INFO     Train Epoch: 126 [5632/8000 (70%)]\tTotal Loss: 0.157819\n",
      "Reconstruction: 0.157291, Regularization: 0.000528\n",
      "2019-04-10 01:03:24,453 root         INFO     Train Epoch: 126 [6144/8000 (77%)]\tTotal Loss: 0.166839\n",
      "Reconstruction: 0.166040, Regularization: 0.000800\n",
      "2019-04-10 01:03:24,517 root         INFO     Train Epoch: 126 [6656/8000 (83%)]\tTotal Loss: 0.167740\n",
      "Reconstruction: 0.166993, Regularization: 0.000747\n",
      "2019-04-10 01:03:24,582 root         INFO     Train Epoch: 126 [7168/8000 (90%)]\tTotal Loss: 0.186047\n",
      "Reconstruction: 0.184810, Regularization: 0.001237\n",
      "2019-04-10 01:03:24,646 root         INFO     Train Epoch: 126 [7680/8000 (96%)]\tTotal Loss: 0.172261\n",
      "Reconstruction: 0.171366, Regularization: 0.000895\n",
      "2019-04-10 01:03:24,701 root         INFO     ====> Epoch: 126 Average loss: 0.1663\n",
      "2019-04-10 01:03:24,725 root         INFO     Train Epoch: 127 [0/8000 (0%)]\tTotal Loss: 0.157189\n",
      "Reconstruction: 0.156677, Regularization: 0.000512\n",
      "2019-04-10 01:03:24,790 root         INFO     Train Epoch: 127 [512/8000 (6%)]\tTotal Loss: 0.178327\n",
      "Reconstruction: 0.177434, Regularization: 0.000893\n",
      "2019-04-10 01:03:24,854 root         INFO     Train Epoch: 127 [1024/8000 (13%)]\tTotal Loss: 0.178636\n",
      "Reconstruction: 0.177584, Regularization: 0.001052\n",
      "2019-04-10 01:03:24,919 root         INFO     Train Epoch: 127 [1536/8000 (19%)]\tTotal Loss: 0.168161\n",
      "Reconstruction: 0.167480, Regularization: 0.000681\n",
      "2019-04-10 01:03:24,983 root         INFO     Train Epoch: 127 [2048/8000 (26%)]\tTotal Loss: 0.169348\n",
      "Reconstruction: 0.168488, Regularization: 0.000860\n",
      "2019-04-10 01:03:25,048 root         INFO     Train Epoch: 127 [2560/8000 (32%)]\tTotal Loss: 0.166600\n",
      "Reconstruction: 0.165858, Regularization: 0.000742\n",
      "2019-04-10 01:03:25,112 root         INFO     Train Epoch: 127 [3072/8000 (38%)]\tTotal Loss: 0.172807\n",
      "Reconstruction: 0.171967, Regularization: 0.000840\n",
      "2019-04-10 01:03:25,177 root         INFO     Train Epoch: 127 [3584/8000 (45%)]\tTotal Loss: 0.160072\n",
      "Reconstruction: 0.159535, Regularization: 0.000537\n",
      "2019-04-10 01:03:25,241 root         INFO     Train Epoch: 127 [4096/8000 (51%)]\tTotal Loss: 0.172493\n",
      "Reconstruction: 0.171610, Regularization: 0.000883\n",
      "2019-04-10 01:03:25,305 root         INFO     Train Epoch: 127 [4608/8000 (58%)]\tTotal Loss: 0.168718\n",
      "Reconstruction: 0.167904, Regularization: 0.000815\n",
      "2019-04-10 01:03:25,370 root         INFO     Train Epoch: 127 [5120/8000 (64%)]\tTotal Loss: 0.162995\n",
      "Reconstruction: 0.162357, Regularization: 0.000638\n",
      "2019-04-10 01:03:25,434 root         INFO     Train Epoch: 127 [5632/8000 (70%)]\tTotal Loss: 0.175845\n",
      "Reconstruction: 0.174844, Regularization: 0.001001\n",
      "2019-04-10 01:03:25,497 root         INFO     Train Epoch: 127 [6144/8000 (77%)]\tTotal Loss: 0.164035\n",
      "Reconstruction: 0.163372, Regularization: 0.000663\n",
      "2019-04-10 01:03:25,561 root         INFO     Train Epoch: 127 [6656/8000 (83%)]\tTotal Loss: 0.164566\n",
      "Reconstruction: 0.163891, Regularization: 0.000676\n",
      "2019-04-10 01:03:25,625 root         INFO     Train Epoch: 127 [7168/8000 (90%)]\tTotal Loss: 0.159561\n",
      "Reconstruction: 0.158939, Regularization: 0.000622\n",
      "2019-04-10 01:03:25,689 root         INFO     Train Epoch: 127 [7680/8000 (96%)]\tTotal Loss: 0.167854\n",
      "Reconstruction: 0.167102, Regularization: 0.000752\n",
      "2019-04-10 01:03:25,743 root         INFO     ====> Epoch: 127 Average loss: 0.1661\n",
      "2019-04-10 01:03:25,767 root         INFO     Train Epoch: 128 [0/8000 (0%)]\tTotal Loss: 0.170261\n",
      "Reconstruction: 0.169469, Regularization: 0.000792\n",
      "2019-04-10 01:03:25,832 root         INFO     Train Epoch: 128 [512/8000 (6%)]\tTotal Loss: 0.165127\n",
      "Reconstruction: 0.164381, Regularization: 0.000746\n",
      "2019-04-10 01:03:25,896 root         INFO     Train Epoch: 128 [1024/8000 (13%)]\tTotal Loss: 0.166206\n",
      "Reconstruction: 0.165448, Regularization: 0.000758\n",
      "2019-04-10 01:03:25,959 root         INFO     Train Epoch: 128 [1536/8000 (19%)]\tTotal Loss: 0.165312\n",
      "Reconstruction: 0.164618, Regularization: 0.000695\n",
      "2019-04-10 01:03:26,023 root         INFO     Train Epoch: 128 [2048/8000 (26%)]\tTotal Loss: 0.178366\n",
      "Reconstruction: 0.177416, Regularization: 0.000950\n",
      "2019-04-10 01:03:26,087 root         INFO     Train Epoch: 128 [2560/8000 (32%)]\tTotal Loss: 0.162078\n",
      "Reconstruction: 0.161485, Regularization: 0.000593\n",
      "2019-04-10 01:03:26,151 root         INFO     Train Epoch: 128 [3072/8000 (38%)]\tTotal Loss: 0.159270\n",
      "Reconstruction: 0.158564, Regularization: 0.000706\n",
      "2019-04-10 01:03:26,216 root         INFO     Train Epoch: 128 [3584/8000 (45%)]\tTotal Loss: 0.172902\n",
      "Reconstruction: 0.171911, Regularization: 0.000991\n",
      "2019-04-10 01:03:26,279 root         INFO     Train Epoch: 128 [4096/8000 (51%)]\tTotal Loss: 0.164579\n",
      "Reconstruction: 0.163761, Regularization: 0.000818\n",
      "2019-04-10 01:03:26,343 root         INFO     Train Epoch: 128 [4608/8000 (58%)]\tTotal Loss: 0.161126\n",
      "Reconstruction: 0.160447, Regularization: 0.000679\n",
      "2019-04-10 01:03:26,407 root         INFO     Train Epoch: 128 [5120/8000 (64%)]\tTotal Loss: 0.149955\n",
      "Reconstruction: 0.149553, Regularization: 0.000403\n",
      "2019-04-10 01:03:26,471 root         INFO     Train Epoch: 128 [5632/8000 (70%)]\tTotal Loss: 0.157605\n",
      "Reconstruction: 0.157085, Regularization: 0.000520\n",
      "2019-04-10 01:03:26,536 root         INFO     Train Epoch: 128 [6144/8000 (77%)]\tTotal Loss: 0.161554\n",
      "Reconstruction: 0.160917, Regularization: 0.000637\n",
      "2019-04-10 01:03:26,600 root         INFO     Train Epoch: 128 [6656/8000 (83%)]\tTotal Loss: 0.154195\n",
      "Reconstruction: 0.153683, Regularization: 0.000511\n",
      "2019-04-10 01:03:26,663 root         INFO     Train Epoch: 128 [7168/8000 (90%)]\tTotal Loss: 0.161454\n",
      "Reconstruction: 0.160869, Regularization: 0.000584\n",
      "2019-04-10 01:03:26,727 root         INFO     Train Epoch: 128 [7680/8000 (96%)]\tTotal Loss: 0.168095\n",
      "Reconstruction: 0.167300, Regularization: 0.000795\n",
      "2019-04-10 01:03:26,782 root         INFO     ====> Epoch: 128 Average loss: 0.1663\n",
      "2019-04-10 01:03:26,806 root         INFO     Train Epoch: 129 [0/8000 (0%)]\tTotal Loss: 0.178314\n",
      "Reconstruction: 0.177360, Regularization: 0.000954\n",
      "2019-04-10 01:03:26,869 root         INFO     Train Epoch: 129 [512/8000 (6%)]\tTotal Loss: 0.165428\n",
      "Reconstruction: 0.164669, Regularization: 0.000759\n",
      "2019-04-10 01:03:26,933 root         INFO     Train Epoch: 129 [1024/8000 (13%)]\tTotal Loss: 0.165433\n",
      "Reconstruction: 0.164643, Regularization: 0.000790\n",
      "2019-04-10 01:03:26,997 root         INFO     Train Epoch: 129 [1536/8000 (19%)]\tTotal Loss: 0.168817\n",
      "Reconstruction: 0.167916, Regularization: 0.000901\n",
      "2019-04-10 01:03:27,060 root         INFO     Train Epoch: 129 [2048/8000 (26%)]\tTotal Loss: 0.158942\n",
      "Reconstruction: 0.158359, Regularization: 0.000582\n",
      "2019-04-10 01:03:27,124 root         INFO     Train Epoch: 129 [2560/8000 (32%)]\tTotal Loss: 0.164846\n",
      "Reconstruction: 0.164072, Regularization: 0.000773\n",
      "2019-04-10 01:03:27,187 root         INFO     Train Epoch: 129 [3072/8000 (38%)]\tTotal Loss: 0.167619\n",
      "Reconstruction: 0.166789, Regularization: 0.000830\n",
      "2019-04-10 01:03:27,251 root         INFO     Train Epoch: 129 [3584/8000 (45%)]\tTotal Loss: 0.162692\n",
      "Reconstruction: 0.162028, Regularization: 0.000664\n",
      "2019-04-10 01:03:27,314 root         INFO     Train Epoch: 129 [4096/8000 (51%)]\tTotal Loss: 0.158516\n",
      "Reconstruction: 0.157822, Regularization: 0.000694\n",
      "2019-04-10 01:03:27,378 root         INFO     Train Epoch: 129 [4608/8000 (58%)]\tTotal Loss: 0.163943\n",
      "Reconstruction: 0.163184, Regularization: 0.000760\n",
      "2019-04-10 01:03:27,442 root         INFO     Train Epoch: 129 [5120/8000 (64%)]\tTotal Loss: 0.166419\n",
      "Reconstruction: 0.165690, Regularization: 0.000729\n",
      "2019-04-10 01:03:27,506 root         INFO     Train Epoch: 129 [5632/8000 (70%)]\tTotal Loss: 0.163834\n",
      "Reconstruction: 0.163094, Regularization: 0.000740\n",
      "2019-04-10 01:03:27,569 root         INFO     Train Epoch: 129 [6144/8000 (77%)]\tTotal Loss: 0.161863\n",
      "Reconstruction: 0.161208, Regularization: 0.000655\n",
      "2019-04-10 01:03:27,633 root         INFO     Train Epoch: 129 [6656/8000 (83%)]\tTotal Loss: 0.163245\n",
      "Reconstruction: 0.162537, Regularization: 0.000708\n",
      "2019-04-10 01:03:27,698 root         INFO     Train Epoch: 129 [7168/8000 (90%)]\tTotal Loss: 0.160030\n",
      "Reconstruction: 0.159320, Regularization: 0.000711\n",
      "2019-04-10 01:03:27,762 root         INFO     Train Epoch: 129 [7680/8000 (96%)]\tTotal Loss: 0.167139\n",
      "Reconstruction: 0.166252, Regularization: 0.000887\n",
      "2019-04-10 01:03:27,816 root         INFO     ====> Epoch: 129 Average loss: 0.1661\n",
      "2019-04-10 01:03:27,840 root         INFO     Train Epoch: 130 [0/8000 (0%)]\tTotal Loss: 0.164914\n",
      "Reconstruction: 0.164128, Regularization: 0.000786\n",
      "2019-04-10 01:03:27,905 root         INFO     Train Epoch: 130 [512/8000 (6%)]\tTotal Loss: 0.154134\n",
      "Reconstruction: 0.153681, Regularization: 0.000453\n",
      "2019-04-10 01:03:27,968 root         INFO     Train Epoch: 130 [1024/8000 (13%)]\tTotal Loss: 0.158151\n",
      "Reconstruction: 0.157558, Regularization: 0.000593\n",
      "2019-04-10 01:03:28,031 root         INFO     Train Epoch: 130 [1536/8000 (19%)]\tTotal Loss: 0.165886\n",
      "Reconstruction: 0.165160, Regularization: 0.000726\n",
      "2019-04-10 01:03:28,093 root         INFO     Train Epoch: 130 [2048/8000 (26%)]\tTotal Loss: 0.178566\n",
      "Reconstruction: 0.177468, Regularization: 0.001097\n",
      "2019-04-10 01:03:28,157 root         INFO     Train Epoch: 130 [2560/8000 (32%)]\tTotal Loss: 0.171114\n",
      "Reconstruction: 0.170085, Regularization: 0.001029\n",
      "2019-04-10 01:03:28,220 root         INFO     Train Epoch: 130 [3072/8000 (38%)]\tTotal Loss: 0.156205\n",
      "Reconstruction: 0.155623, Regularization: 0.000581\n",
      "2019-04-10 01:03:28,282 root         INFO     Train Epoch: 130 [3584/8000 (45%)]\tTotal Loss: 0.165163\n",
      "Reconstruction: 0.164451, Regularization: 0.000713\n",
      "2019-04-10 01:03:28,346 root         INFO     Train Epoch: 130 [4096/8000 (51%)]\tTotal Loss: 0.169159\n",
      "Reconstruction: 0.168204, Regularization: 0.000955\n",
      "2019-04-10 01:03:28,409 root         INFO     Train Epoch: 130 [4608/8000 (58%)]\tTotal Loss: 0.160703\n",
      "Reconstruction: 0.160038, Regularization: 0.000665\n",
      "2019-04-10 01:03:28,473 root         INFO     Train Epoch: 130 [5120/8000 (64%)]\tTotal Loss: 0.162556\n",
      "Reconstruction: 0.161793, Regularization: 0.000763\n",
      "2019-04-10 01:03:28,536 root         INFO     Train Epoch: 130 [5632/8000 (70%)]\tTotal Loss: 0.169691\n",
      "Reconstruction: 0.168687, Regularization: 0.001004\n",
      "2019-04-10 01:03:28,598 root         INFO     Train Epoch: 130 [6144/8000 (77%)]\tTotal Loss: 0.171131\n",
      "Reconstruction: 0.170294, Regularization: 0.000837\n",
      "2019-04-10 01:03:28,661 root         INFO     Train Epoch: 130 [6656/8000 (83%)]\tTotal Loss: 0.158187\n",
      "Reconstruction: 0.157628, Regularization: 0.000559\n",
      "2019-04-10 01:03:28,724 root         INFO     Train Epoch: 130 [7168/8000 (90%)]\tTotal Loss: 0.168491\n",
      "Reconstruction: 0.167709, Regularization: 0.000782\n",
      "2019-04-10 01:03:28,787 root         INFO     Train Epoch: 130 [7680/8000 (96%)]\tTotal Loss: 0.164874\n",
      "Reconstruction: 0.164135, Regularization: 0.000739\n",
      "2019-04-10 01:03:28,841 root         INFO     ====> Epoch: 130 Average loss: 0.1661\n",
      "2019-04-10 01:03:28,865 root         INFO     Train Epoch: 131 [0/8000 (0%)]\tTotal Loss: 0.174238\n",
      "Reconstruction: 0.173166, Regularization: 0.001072\n",
      "2019-04-10 01:03:28,930 root         INFO     Train Epoch: 131 [512/8000 (6%)]\tTotal Loss: 0.160835\n",
      "Reconstruction: 0.160126, Regularization: 0.000708\n",
      "2019-04-10 01:03:28,994 root         INFO     Train Epoch: 131 [1024/8000 (13%)]\tTotal Loss: 0.155136\n",
      "Reconstruction: 0.154566, Regularization: 0.000569\n",
      "2019-04-10 01:03:29,059 root         INFO     Train Epoch: 131 [1536/8000 (19%)]\tTotal Loss: 0.168814\n",
      "Reconstruction: 0.168037, Regularization: 0.000777\n",
      "2019-04-10 01:03:29,122 root         INFO     Train Epoch: 131 [2048/8000 (26%)]\tTotal Loss: 0.153136\n",
      "Reconstruction: 0.152625, Regularization: 0.000511\n",
      "2019-04-10 01:03:29,185 root         INFO     Train Epoch: 131 [2560/8000 (32%)]\tTotal Loss: 0.169956\n",
      "Reconstruction: 0.169085, Regularization: 0.000871\n",
      "2019-04-10 01:03:29,247 root         INFO     Train Epoch: 131 [3072/8000 (38%)]\tTotal Loss: 0.161625\n",
      "Reconstruction: 0.160968, Regularization: 0.000658\n",
      "2019-04-10 01:03:29,311 root         INFO     Train Epoch: 131 [3584/8000 (45%)]\tTotal Loss: 0.158747\n",
      "Reconstruction: 0.158130, Regularization: 0.000617\n",
      "2019-04-10 01:03:29,374 root         INFO     Train Epoch: 131 [4096/8000 (51%)]\tTotal Loss: 0.155133\n",
      "Reconstruction: 0.154561, Regularization: 0.000572\n",
      "2019-04-10 01:03:29,437 root         INFO     Train Epoch: 131 [4608/8000 (58%)]\tTotal Loss: 0.152765\n",
      "Reconstruction: 0.152325, Regularization: 0.000441\n",
      "2019-04-10 01:03:29,501 root         INFO     Train Epoch: 131 [5120/8000 (64%)]\tTotal Loss: 0.159710\n",
      "Reconstruction: 0.159001, Regularization: 0.000709\n",
      "2019-04-10 01:03:29,565 root         INFO     Train Epoch: 131 [5632/8000 (70%)]\tTotal Loss: 0.171029\n",
      "Reconstruction: 0.170001, Regularization: 0.001027\n",
      "2019-04-10 01:03:29,628 root         INFO     Train Epoch: 131 [6144/8000 (77%)]\tTotal Loss: 0.167976\n",
      "Reconstruction: 0.167131, Regularization: 0.000845\n",
      "2019-04-10 01:03:29,691 root         INFO     Train Epoch: 131 [6656/8000 (83%)]\tTotal Loss: 0.154124\n",
      "Reconstruction: 0.153566, Regularization: 0.000558\n",
      "2019-04-10 01:03:29,754 root         INFO     Train Epoch: 131 [7168/8000 (90%)]\tTotal Loss: 0.162732\n",
      "Reconstruction: 0.162084, Regularization: 0.000648\n",
      "2019-04-10 01:03:29,816 root         INFO     Train Epoch: 131 [7680/8000 (96%)]\tTotal Loss: 0.164128\n",
      "Reconstruction: 0.163322, Regularization: 0.000806\n",
      "2019-04-10 01:03:29,869 root         INFO     ====> Epoch: 131 Average loss: 0.1661\n",
      "2019-04-10 01:03:29,894 root         INFO     Train Epoch: 132 [0/8000 (0%)]\tTotal Loss: 0.157175\n",
      "Reconstruction: 0.156622, Regularization: 0.000552\n",
      "2019-04-10 01:03:29,958 root         INFO     Train Epoch: 132 [512/8000 (6%)]\tTotal Loss: 0.163948\n",
      "Reconstruction: 0.163257, Regularization: 0.000691\n",
      "2019-04-10 01:03:30,023 root         INFO     Train Epoch: 132 [1024/8000 (13%)]\tTotal Loss: 0.155384\n",
      "Reconstruction: 0.154831, Regularization: 0.000553\n",
      "2019-04-10 01:03:30,087 root         INFO     Train Epoch: 132 [1536/8000 (19%)]\tTotal Loss: 0.164796\n",
      "Reconstruction: 0.163984, Regularization: 0.000812\n",
      "2019-04-10 01:03:30,150 root         INFO     Train Epoch: 132 [2048/8000 (26%)]\tTotal Loss: 0.156582\n",
      "Reconstruction: 0.155933, Regularization: 0.000649\n",
      "2019-04-10 01:03:30,214 root         INFO     Train Epoch: 132 [2560/8000 (32%)]\tTotal Loss: 0.162320\n",
      "Reconstruction: 0.161555, Regularization: 0.000765\n",
      "2019-04-10 01:03:30,278 root         INFO     Train Epoch: 132 [3072/8000 (38%)]\tTotal Loss: 0.161212\n",
      "Reconstruction: 0.160420, Regularization: 0.000791\n",
      "2019-04-10 01:03:30,341 root         INFO     Train Epoch: 132 [3584/8000 (45%)]\tTotal Loss: 0.157594\n",
      "Reconstruction: 0.156977, Regularization: 0.000618\n",
      "2019-04-10 01:03:30,405 root         INFO     Train Epoch: 132 [4096/8000 (51%)]\tTotal Loss: 0.160370\n",
      "Reconstruction: 0.159630, Regularization: 0.000740\n",
      "2019-04-10 01:03:30,468 root         INFO     Train Epoch: 132 [4608/8000 (58%)]\tTotal Loss: 0.156342\n",
      "Reconstruction: 0.155754, Regularization: 0.000588\n",
      "2019-04-10 01:03:30,531 root         INFO     Train Epoch: 132 [5120/8000 (64%)]\tTotal Loss: 0.155450\n",
      "Reconstruction: 0.154894, Regularization: 0.000556\n",
      "2019-04-10 01:03:30,595 root         INFO     Train Epoch: 132 [5632/8000 (70%)]\tTotal Loss: 0.168429\n",
      "Reconstruction: 0.167531, Regularization: 0.000898\n",
      "2019-04-10 01:03:30,659 root         INFO     Train Epoch: 132 [6144/8000 (77%)]\tTotal Loss: 0.170690\n",
      "Reconstruction: 0.169711, Regularization: 0.000979\n",
      "2019-04-10 01:03:30,722 root         INFO     Train Epoch: 132 [6656/8000 (83%)]\tTotal Loss: 0.165798\n",
      "Reconstruction: 0.164898, Regularization: 0.000900\n",
      "2019-04-10 01:03:30,786 root         INFO     Train Epoch: 132 [7168/8000 (90%)]\tTotal Loss: 0.160618\n",
      "Reconstruction: 0.159863, Regularization: 0.000756\n",
      "2019-04-10 01:03:30,849 root         INFO     Train Epoch: 132 [7680/8000 (96%)]\tTotal Loss: 0.163533\n",
      "Reconstruction: 0.162771, Regularization: 0.000762\n",
      "2019-04-10 01:03:30,903 root         INFO     ====> Epoch: 132 Average loss: 0.1663\n",
      "2019-04-10 01:03:30,926 root         INFO     Train Epoch: 133 [0/8000 (0%)]\tTotal Loss: 0.165035\n",
      "Reconstruction: 0.164123, Regularization: 0.000912\n",
      "2019-04-10 01:03:30,990 root         INFO     Train Epoch: 133 [512/8000 (6%)]\tTotal Loss: 0.173946\n",
      "Reconstruction: 0.172929, Regularization: 0.001017\n",
      "2019-04-10 01:03:31,053 root         INFO     Train Epoch: 133 [1024/8000 (13%)]\tTotal Loss: 0.169088\n",
      "Reconstruction: 0.168115, Regularization: 0.000973\n",
      "2019-04-10 01:03:31,117 root         INFO     Train Epoch: 133 [1536/8000 (19%)]\tTotal Loss: 0.168539\n",
      "Reconstruction: 0.167640, Regularization: 0.000899\n",
      "2019-04-10 01:03:31,180 root         INFO     Train Epoch: 133 [2048/8000 (26%)]\tTotal Loss: 0.167304\n",
      "Reconstruction: 0.166384, Regularization: 0.000920\n",
      "2019-04-10 01:03:31,244 root         INFO     Train Epoch: 133 [2560/8000 (32%)]\tTotal Loss: 0.180228\n",
      "Reconstruction: 0.178954, Regularization: 0.001274\n",
      "2019-04-10 01:03:31,307 root         INFO     Train Epoch: 133 [3072/8000 (38%)]\tTotal Loss: 0.158078\n",
      "Reconstruction: 0.157444, Regularization: 0.000634\n",
      "2019-04-10 01:03:31,370 root         INFO     Train Epoch: 133 [3584/8000 (45%)]\tTotal Loss: 0.164905\n",
      "Reconstruction: 0.164032, Regularization: 0.000873\n",
      "2019-04-10 01:03:31,434 root         INFO     Train Epoch: 133 [4096/8000 (51%)]\tTotal Loss: 0.160279\n",
      "Reconstruction: 0.159495, Regularization: 0.000784\n",
      "2019-04-10 01:03:31,497 root         INFO     Train Epoch: 133 [4608/8000 (58%)]\tTotal Loss: 0.164904\n",
      "Reconstruction: 0.164038, Regularization: 0.000866\n",
      "2019-04-10 01:03:31,561 root         INFO     Train Epoch: 133 [5120/8000 (64%)]\tTotal Loss: 0.170324\n",
      "Reconstruction: 0.169307, Regularization: 0.001017\n",
      "2019-04-10 01:03:31,624 root         INFO     Train Epoch: 133 [5632/8000 (70%)]\tTotal Loss: 0.181481\n",
      "Reconstruction: 0.180315, Regularization: 0.001166\n",
      "2019-04-10 01:03:31,687 root         INFO     Train Epoch: 133 [6144/8000 (77%)]\tTotal Loss: 0.157044\n",
      "Reconstruction: 0.156449, Regularization: 0.000595\n",
      "2019-04-10 01:03:31,750 root         INFO     Train Epoch: 133 [6656/8000 (83%)]\tTotal Loss: 0.168229\n",
      "Reconstruction: 0.167356, Regularization: 0.000872\n",
      "2019-04-10 01:03:31,814 root         INFO     Train Epoch: 133 [7168/8000 (90%)]\tTotal Loss: 0.159202\n",
      "Reconstruction: 0.158649, Regularization: 0.000552\n",
      "2019-04-10 01:03:31,877 root         INFO     Train Epoch: 133 [7680/8000 (96%)]\tTotal Loss: 0.175014\n",
      "Reconstruction: 0.173944, Regularization: 0.001070\n",
      "2019-04-10 01:03:31,930 root         INFO     ====> Epoch: 133 Average loss: 0.1661\n",
      "2019-04-10 01:03:31,955 root         INFO     Train Epoch: 134 [0/8000 (0%)]\tTotal Loss: 0.159651\n",
      "Reconstruction: 0.158958, Regularization: 0.000693\n",
      "2019-04-10 01:03:32,019 root         INFO     Train Epoch: 134 [512/8000 (6%)]\tTotal Loss: 0.166523\n",
      "Reconstruction: 0.165636, Regularization: 0.000887\n",
      "2019-04-10 01:03:32,082 root         INFO     Train Epoch: 134 [1024/8000 (13%)]\tTotal Loss: 0.185328\n",
      "Reconstruction: 0.184074, Regularization: 0.001254\n",
      "2019-04-10 01:03:32,145 root         INFO     Train Epoch: 134 [1536/8000 (19%)]\tTotal Loss: 0.160367\n",
      "Reconstruction: 0.159617, Regularization: 0.000750\n",
      "2019-04-10 01:03:32,208 root         INFO     Train Epoch: 134 [2048/8000 (26%)]\tTotal Loss: 0.171442\n",
      "Reconstruction: 0.170432, Regularization: 0.001010\n",
      "2019-04-10 01:03:32,272 root         INFO     Train Epoch: 134 [2560/8000 (32%)]\tTotal Loss: 0.168937\n",
      "Reconstruction: 0.168112, Regularization: 0.000824\n",
      "2019-04-10 01:03:32,335 root         INFO     Train Epoch: 134 [3072/8000 (38%)]\tTotal Loss: 0.167436\n",
      "Reconstruction: 0.166522, Regularization: 0.000914\n",
      "2019-04-10 01:03:32,399 root         INFO     Train Epoch: 134 [3584/8000 (45%)]\tTotal Loss: 0.158954\n",
      "Reconstruction: 0.158215, Regularization: 0.000738\n",
      "2019-04-10 01:03:32,462 root         INFO     Train Epoch: 134 [4096/8000 (51%)]\tTotal Loss: 0.160248\n",
      "Reconstruction: 0.159554, Regularization: 0.000694\n",
      "2019-04-10 01:03:32,526 root         INFO     Train Epoch: 134 [4608/8000 (58%)]\tTotal Loss: 0.180140\n",
      "Reconstruction: 0.178852, Regularization: 0.001288\n",
      "2019-04-10 01:03:32,590 root         INFO     Train Epoch: 134 [5120/8000 (64%)]\tTotal Loss: 0.160963\n",
      "Reconstruction: 0.160269, Regularization: 0.000694\n",
      "2019-04-10 01:03:32,653 root         INFO     Train Epoch: 134 [5632/8000 (70%)]\tTotal Loss: 0.166988\n",
      "Reconstruction: 0.166126, Regularization: 0.000862\n",
      "2019-04-10 01:03:32,717 root         INFO     Train Epoch: 134 [6144/8000 (77%)]\tTotal Loss: 0.160569\n",
      "Reconstruction: 0.159820, Regularization: 0.000749\n",
      "2019-04-10 01:03:32,779 root         INFO     Train Epoch: 134 [6656/8000 (83%)]\tTotal Loss: 0.156003\n",
      "Reconstruction: 0.155344, Regularization: 0.000659\n",
      "2019-04-10 01:03:32,842 root         INFO     Train Epoch: 134 [7168/8000 (90%)]\tTotal Loss: 0.162576\n",
      "Reconstruction: 0.161835, Regularization: 0.000741\n",
      "2019-04-10 01:03:32,905 root         INFO     Train Epoch: 134 [7680/8000 (96%)]\tTotal Loss: 0.164180\n",
      "Reconstruction: 0.163255, Regularization: 0.000925\n",
      "2019-04-10 01:03:32,959 root         INFO     ====> Epoch: 134 Average loss: 0.1661\n",
      "2019-04-10 01:03:32,983 root         INFO     Train Epoch: 135 [0/8000 (0%)]\tTotal Loss: 0.166373\n",
      "Reconstruction: 0.165500, Regularization: 0.000874\n",
      "2019-04-10 01:03:33,047 root         INFO     Train Epoch: 135 [512/8000 (6%)]\tTotal Loss: 0.171747\n",
      "Reconstruction: 0.170712, Regularization: 0.001035\n",
      "2019-04-10 01:03:33,112 root         INFO     Train Epoch: 135 [1024/8000 (13%)]\tTotal Loss: 0.171575\n",
      "Reconstruction: 0.170526, Regularization: 0.001048\n",
      "2019-04-10 01:03:33,176 root         INFO     Train Epoch: 135 [1536/8000 (19%)]\tTotal Loss: 0.165898\n",
      "Reconstruction: 0.164835, Regularization: 0.001063\n",
      "2019-04-10 01:03:33,239 root         INFO     Train Epoch: 135 [2048/8000 (26%)]\tTotal Loss: 0.171078\n",
      "Reconstruction: 0.169906, Regularization: 0.001172\n",
      "2019-04-10 01:03:33,303 root         INFO     Train Epoch: 135 [2560/8000 (32%)]\tTotal Loss: 0.167814\n",
      "Reconstruction: 0.166859, Regularization: 0.000954\n",
      "2019-04-10 01:03:33,367 root         INFO     Train Epoch: 135 [3072/8000 (38%)]\tTotal Loss: 0.159141\n",
      "Reconstruction: 0.158456, Regularization: 0.000685\n",
      "2019-04-10 01:03:33,431 root         INFO     Train Epoch: 135 [3584/8000 (45%)]\tTotal Loss: 0.167988\n",
      "Reconstruction: 0.166956, Regularization: 0.001032\n",
      "2019-04-10 01:03:33,494 root         INFO     Train Epoch: 135 [4096/8000 (51%)]\tTotal Loss: 0.163554\n",
      "Reconstruction: 0.162647, Regularization: 0.000907\n",
      "2019-04-10 01:03:33,557 root         INFO     Train Epoch: 135 [4608/8000 (58%)]\tTotal Loss: 0.173928\n",
      "Reconstruction: 0.172836, Regularization: 0.001092\n",
      "2019-04-10 01:03:33,621 root         INFO     Train Epoch: 135 [5120/8000 (64%)]\tTotal Loss: 0.166853\n",
      "Reconstruction: 0.165921, Regularization: 0.000933\n",
      "2019-04-10 01:03:33,684 root         INFO     Train Epoch: 135 [5632/8000 (70%)]\tTotal Loss: 0.165251\n",
      "Reconstruction: 0.164354, Regularization: 0.000896\n",
      "2019-04-10 01:03:33,748 root         INFO     Train Epoch: 135 [6144/8000 (77%)]\tTotal Loss: 0.162458\n",
      "Reconstruction: 0.161609, Regularization: 0.000849\n",
      "2019-04-10 01:03:33,812 root         INFO     Train Epoch: 135 [6656/8000 (83%)]\tTotal Loss: 0.149889\n",
      "Reconstruction: 0.149424, Regularization: 0.000466\n",
      "2019-04-10 01:03:33,876 root         INFO     Train Epoch: 135 [7168/8000 (90%)]\tTotal Loss: 0.166755\n",
      "Reconstruction: 0.165925, Regularization: 0.000830\n",
      "2019-04-10 01:03:33,939 root         INFO     Train Epoch: 135 [7680/8000 (96%)]\tTotal Loss: 0.162873\n",
      "Reconstruction: 0.162101, Regularization: 0.000772\n",
      "2019-04-10 01:03:33,993 root         INFO     ====> Epoch: 135 Average loss: 0.1661\n",
      "2019-04-10 01:03:34,017 root         INFO     Train Epoch: 136 [0/8000 (0%)]\tTotal Loss: 0.169341\n",
      "Reconstruction: 0.168314, Regularization: 0.001028\n",
      "2019-04-10 01:03:34,081 root         INFO     Train Epoch: 136 [512/8000 (6%)]\tTotal Loss: 0.166025\n",
      "Reconstruction: 0.165008, Regularization: 0.001017\n",
      "2019-04-10 01:03:34,145 root         INFO     Train Epoch: 136 [1024/8000 (13%)]\tTotal Loss: 0.160237\n",
      "Reconstruction: 0.159432, Regularization: 0.000805\n",
      "2019-04-10 01:03:34,208 root         INFO     Train Epoch: 136 [1536/8000 (19%)]\tTotal Loss: 0.156176\n",
      "Reconstruction: 0.155587, Regularization: 0.000589\n",
      "2019-04-10 01:03:34,272 root         INFO     Train Epoch: 136 [2048/8000 (26%)]\tTotal Loss: 0.162812\n",
      "Reconstruction: 0.161986, Regularization: 0.000826\n",
      "2019-04-10 01:03:34,335 root         INFO     Train Epoch: 136 [2560/8000 (32%)]\tTotal Loss: 0.187366\n",
      "Reconstruction: 0.185766, Regularization: 0.001600\n",
      "2019-04-10 01:03:34,398 root         INFO     Train Epoch: 136 [3072/8000 (38%)]\tTotal Loss: 0.149288\n",
      "Reconstruction: 0.148747, Regularization: 0.000541\n",
      "2019-04-10 01:03:34,462 root         INFO     Train Epoch: 136 [3584/8000 (45%)]\tTotal Loss: 0.167606\n",
      "Reconstruction: 0.166654, Regularization: 0.000952\n",
      "2019-04-10 01:03:34,525 root         INFO     Train Epoch: 136 [4096/8000 (51%)]\tTotal Loss: 0.168476\n",
      "Reconstruction: 0.167579, Regularization: 0.000896\n",
      "2019-04-10 01:03:34,589 root         INFO     Train Epoch: 136 [4608/8000 (58%)]\tTotal Loss: 0.169454\n",
      "Reconstruction: 0.168442, Regularization: 0.001012\n",
      "2019-04-10 01:03:34,653 root         INFO     Train Epoch: 136 [5120/8000 (64%)]\tTotal Loss: 0.173213\n",
      "Reconstruction: 0.171876, Regularization: 0.001338\n",
      "2019-04-10 01:03:34,717 root         INFO     Train Epoch: 136 [5632/8000 (70%)]\tTotal Loss: 0.169596\n",
      "Reconstruction: 0.168616, Regularization: 0.000980\n",
      "2019-04-10 01:03:34,780 root         INFO     Train Epoch: 136 [6144/8000 (77%)]\tTotal Loss: 0.170795\n",
      "Reconstruction: 0.169772, Regularization: 0.001022\n",
      "2019-04-10 01:03:34,844 root         INFO     Train Epoch: 136 [6656/8000 (83%)]\tTotal Loss: 0.165753\n",
      "Reconstruction: 0.164786, Regularization: 0.000967\n",
      "2019-04-10 01:03:34,907 root         INFO     Train Epoch: 136 [7168/8000 (90%)]\tTotal Loss: 0.181370\n",
      "Reconstruction: 0.179972, Regularization: 0.001398\n",
      "2019-04-10 01:03:34,971 root         INFO     Train Epoch: 136 [7680/8000 (96%)]\tTotal Loss: 0.157704\n",
      "Reconstruction: 0.156962, Regularization: 0.000742\n",
      "2019-04-10 01:03:35,025 root         INFO     ====> Epoch: 136 Average loss: 0.1659\n",
      "2019-04-10 01:03:35,049 root         INFO     Train Epoch: 137 [0/8000 (0%)]\tTotal Loss: 0.153803\n",
      "Reconstruction: 0.153144, Regularization: 0.000659\n",
      "2019-04-10 01:03:35,113 root         INFO     Train Epoch: 137 [512/8000 (6%)]\tTotal Loss: 0.163668\n",
      "Reconstruction: 0.162797, Regularization: 0.000871\n",
      "2019-04-10 01:03:35,176 root         INFO     Train Epoch: 137 [1024/8000 (13%)]\tTotal Loss: 0.170105\n",
      "Reconstruction: 0.169126, Regularization: 0.000979\n",
      "2019-04-10 01:03:35,240 root         INFO     Train Epoch: 137 [1536/8000 (19%)]\tTotal Loss: 0.165549\n",
      "Reconstruction: 0.164579, Regularization: 0.000970\n",
      "2019-04-10 01:03:35,304 root         INFO     Train Epoch: 137 [2048/8000 (26%)]\tTotal Loss: 0.163808\n",
      "Reconstruction: 0.162962, Regularization: 0.000846\n",
      "2019-04-10 01:03:35,368 root         INFO     Train Epoch: 137 [2560/8000 (32%)]\tTotal Loss: 0.176798\n",
      "Reconstruction: 0.175535, Regularization: 0.001263\n",
      "2019-04-10 01:03:35,432 root         INFO     Train Epoch: 137 [3072/8000 (38%)]\tTotal Loss: 0.173785\n",
      "Reconstruction: 0.172599, Regularization: 0.001186\n",
      "2019-04-10 01:03:35,495 root         INFO     Train Epoch: 137 [3584/8000 (45%)]\tTotal Loss: 0.170464\n",
      "Reconstruction: 0.169370, Regularization: 0.001094\n",
      "2019-04-10 01:03:35,558 root         INFO     Train Epoch: 137 [4096/8000 (51%)]\tTotal Loss: 0.152503\n",
      "Reconstruction: 0.151977, Regularization: 0.000526\n",
      "2019-04-10 01:03:35,620 root         INFO     Train Epoch: 137 [4608/8000 (58%)]\tTotal Loss: 0.171309\n",
      "Reconstruction: 0.170249, Regularization: 0.001060\n",
      "2019-04-10 01:03:35,683 root         INFO     Train Epoch: 137 [5120/8000 (64%)]\tTotal Loss: 0.172246\n",
      "Reconstruction: 0.171073, Regularization: 0.001173\n",
      "2019-04-10 01:03:35,745 root         INFO     Train Epoch: 137 [5632/8000 (70%)]\tTotal Loss: 0.160636\n",
      "Reconstruction: 0.159781, Regularization: 0.000855\n",
      "2019-04-10 01:03:35,807 root         INFO     Train Epoch: 137 [6144/8000 (77%)]\tTotal Loss: 0.171351\n",
      "Reconstruction: 0.170217, Regularization: 0.001134\n",
      "2019-04-10 01:03:35,870 root         INFO     Train Epoch: 137 [6656/8000 (83%)]\tTotal Loss: 0.161445\n",
      "Reconstruction: 0.160566, Regularization: 0.000880\n",
      "2019-04-10 01:03:35,933 root         INFO     Train Epoch: 137 [7168/8000 (90%)]\tTotal Loss: 0.172729\n",
      "Reconstruction: 0.171530, Regularization: 0.001199\n",
      "2019-04-10 01:03:35,995 root         INFO     Train Epoch: 137 [7680/8000 (96%)]\tTotal Loss: 0.159005\n",
      "Reconstruction: 0.158037, Regularization: 0.000968\n",
      "2019-04-10 01:03:36,048 root         INFO     ====> Epoch: 137 Average loss: 0.1658\n",
      "2019-04-10 01:03:36,072 root         INFO     Train Epoch: 138 [0/8000 (0%)]\tTotal Loss: 0.156060\n",
      "Reconstruction: 0.155383, Regularization: 0.000678\n",
      "2019-04-10 01:03:36,136 root         INFO     Train Epoch: 138 [512/8000 (6%)]\tTotal Loss: 0.185531\n",
      "Reconstruction: 0.183877, Regularization: 0.001654\n",
      "2019-04-10 01:03:36,199 root         INFO     Train Epoch: 138 [1024/8000 (13%)]\tTotal Loss: 0.162763\n",
      "Reconstruction: 0.161920, Regularization: 0.000843\n",
      "2019-04-10 01:03:36,262 root         INFO     Train Epoch: 138 [1536/8000 (19%)]\tTotal Loss: 0.170282\n",
      "Reconstruction: 0.169185, Regularization: 0.001096\n",
      "2019-04-10 01:03:36,326 root         INFO     Train Epoch: 138 [2048/8000 (26%)]\tTotal Loss: 0.180701\n",
      "Reconstruction: 0.179013, Regularization: 0.001687\n",
      "2019-04-10 01:03:36,389 root         INFO     Train Epoch: 138 [2560/8000 (32%)]\tTotal Loss: 0.166246\n",
      "Reconstruction: 0.165223, Regularization: 0.001024\n",
      "2019-04-10 01:03:36,453 root         INFO     Train Epoch: 138 [3072/8000 (38%)]\tTotal Loss: 0.174898\n",
      "Reconstruction: 0.173681, Regularization: 0.001217\n",
      "2019-04-10 01:03:36,517 root         INFO     Train Epoch: 138 [3584/8000 (45%)]\tTotal Loss: 0.173396\n",
      "Reconstruction: 0.172110, Regularization: 0.001286\n",
      "2019-04-10 01:03:36,580 root         INFO     Train Epoch: 138 [4096/8000 (51%)]\tTotal Loss: 0.161745\n",
      "Reconstruction: 0.160778, Regularization: 0.000967\n",
      "2019-04-10 01:03:36,644 root         INFO     Train Epoch: 138 [4608/8000 (58%)]\tTotal Loss: 0.165814\n",
      "Reconstruction: 0.164707, Regularization: 0.001107\n",
      "2019-04-10 01:03:36,707 root         INFO     Train Epoch: 138 [5120/8000 (64%)]\tTotal Loss: 0.153967\n",
      "Reconstruction: 0.153405, Regularization: 0.000562\n",
      "2019-04-10 01:03:36,771 root         INFO     Train Epoch: 138 [5632/8000 (70%)]\tTotal Loss: 0.170419\n",
      "Reconstruction: 0.169243, Regularization: 0.001177\n",
      "2019-04-10 01:03:36,834 root         INFO     Train Epoch: 138 [6144/8000 (77%)]\tTotal Loss: 0.170990\n",
      "Reconstruction: 0.169912, Regularization: 0.001078\n",
      "2019-04-10 01:03:36,898 root         INFO     Train Epoch: 138 [6656/8000 (83%)]\tTotal Loss: 0.165163\n",
      "Reconstruction: 0.164208, Regularization: 0.000954\n",
      "2019-04-10 01:03:36,961 root         INFO     Train Epoch: 138 [7168/8000 (90%)]\tTotal Loss: 0.187990\n",
      "Reconstruction: 0.186069, Regularization: 0.001922\n",
      "2019-04-10 01:03:37,025 root         INFO     Train Epoch: 138 [7680/8000 (96%)]\tTotal Loss: 0.158260\n",
      "Reconstruction: 0.157362, Regularization: 0.000898\n",
      "2019-04-10 01:03:37,079 root         INFO     ====> Epoch: 138 Average loss: 0.1660\n",
      "2019-04-10 01:03:37,103 root         INFO     Train Epoch: 139 [0/8000 (0%)]\tTotal Loss: 0.172007\n",
      "Reconstruction: 0.170717, Regularization: 0.001290\n",
      "2019-04-10 01:03:37,167 root         INFO     Train Epoch: 139 [512/8000 (6%)]\tTotal Loss: 0.163855\n",
      "Reconstruction: 0.162861, Regularization: 0.000994\n",
      "2019-04-10 01:03:37,230 root         INFO     Train Epoch: 139 [1024/8000 (13%)]\tTotal Loss: 0.162074\n",
      "Reconstruction: 0.161237, Regularization: 0.000837\n",
      "2019-04-10 01:03:37,294 root         INFO     Train Epoch: 139 [1536/8000 (19%)]\tTotal Loss: 0.160317\n",
      "Reconstruction: 0.159412, Regularization: 0.000905\n",
      "2019-04-10 01:03:37,359 root         INFO     Train Epoch: 139 [2048/8000 (26%)]\tTotal Loss: 0.165809\n",
      "Reconstruction: 0.164819, Regularization: 0.000991\n",
      "2019-04-10 01:03:37,423 root         INFO     Train Epoch: 139 [2560/8000 (32%)]\tTotal Loss: 0.164227\n",
      "Reconstruction: 0.163162, Regularization: 0.001064\n",
      "2019-04-10 01:03:37,487 root         INFO     Train Epoch: 139 [3072/8000 (38%)]\tTotal Loss: 0.170120\n",
      "Reconstruction: 0.168995, Regularization: 0.001124\n",
      "2019-04-10 01:03:37,550 root         INFO     Train Epoch: 139 [3584/8000 (45%)]\tTotal Loss: 0.160067\n",
      "Reconstruction: 0.159191, Regularization: 0.000876\n",
      "2019-04-10 01:03:37,613 root         INFO     Train Epoch: 139 [4096/8000 (51%)]\tTotal Loss: 0.164335\n",
      "Reconstruction: 0.163290, Regularization: 0.001045\n",
      "2019-04-10 01:03:37,676 root         INFO     Train Epoch: 139 [4608/8000 (58%)]\tTotal Loss: 0.166728\n",
      "Reconstruction: 0.165562, Regularization: 0.001166\n",
      "2019-04-10 01:03:37,739 root         INFO     Train Epoch: 139 [5120/8000 (64%)]\tTotal Loss: 0.178586\n",
      "Reconstruction: 0.177062, Regularization: 0.001524\n",
      "2019-04-10 01:03:37,803 root         INFO     Train Epoch: 139 [5632/8000 (70%)]\tTotal Loss: 0.167467\n",
      "Reconstruction: 0.166159, Regularization: 0.001308\n",
      "2019-04-10 01:03:37,866 root         INFO     Train Epoch: 139 [6144/8000 (77%)]\tTotal Loss: 0.162427\n",
      "Reconstruction: 0.161346, Regularization: 0.001081\n",
      "2019-04-10 01:03:37,930 root         INFO     Train Epoch: 139 [6656/8000 (83%)]\tTotal Loss: 0.154979\n",
      "Reconstruction: 0.154236, Regularization: 0.000743\n",
      "2019-04-10 01:03:37,994 root         INFO     Train Epoch: 139 [7168/8000 (90%)]\tTotal Loss: 0.166805\n",
      "Reconstruction: 0.165654, Regularization: 0.001151\n",
      "2019-04-10 01:03:38,057 root         INFO     Train Epoch: 139 [7680/8000 (96%)]\tTotal Loss: 0.171611\n",
      "Reconstruction: 0.170330, Regularization: 0.001280\n",
      "2019-04-10 01:03:38,111 root         INFO     ====> Epoch: 139 Average loss: 0.1662\n",
      "2019-04-10 01:03:38,135 root         INFO     Train Epoch: 140 [0/8000 (0%)]\tTotal Loss: 0.184229\n",
      "Reconstruction: 0.182554, Regularization: 0.001674\n",
      "2019-04-10 01:03:38,199 root         INFO     Train Epoch: 140 [512/8000 (6%)]\tTotal Loss: 0.170527\n",
      "Reconstruction: 0.169347, Regularization: 0.001180\n",
      "2019-04-10 01:03:38,263 root         INFO     Train Epoch: 140 [1024/8000 (13%)]\tTotal Loss: 0.169417\n",
      "Reconstruction: 0.168193, Regularization: 0.001224\n",
      "2019-04-10 01:03:38,326 root         INFO     Train Epoch: 140 [1536/8000 (19%)]\tTotal Loss: 0.161411\n",
      "Reconstruction: 0.160551, Regularization: 0.000860\n",
      "2019-04-10 01:03:38,389 root         INFO     Train Epoch: 140 [2048/8000 (26%)]\tTotal Loss: 0.196436\n",
      "Reconstruction: 0.194021, Regularization: 0.002415\n",
      "2019-04-10 01:03:38,452 root         INFO     Train Epoch: 140 [2560/8000 (32%)]\tTotal Loss: 0.157284\n",
      "Reconstruction: 0.156584, Regularization: 0.000700\n",
      "2019-04-10 01:03:38,515 root         INFO     Train Epoch: 140 [3072/8000 (38%)]\tTotal Loss: 0.175191\n",
      "Reconstruction: 0.173803, Regularization: 0.001388\n",
      "2019-04-10 01:03:38,579 root         INFO     Train Epoch: 140 [3584/8000 (45%)]\tTotal Loss: 0.167774\n",
      "Reconstruction: 0.166585, Regularization: 0.001189\n",
      "2019-04-10 01:03:38,642 root         INFO     Train Epoch: 140 [4096/8000 (51%)]\tTotal Loss: 0.160645\n",
      "Reconstruction: 0.159765, Regularization: 0.000881\n",
      "2019-04-10 01:03:38,705 root         INFO     Train Epoch: 140 [4608/8000 (58%)]\tTotal Loss: 0.169944\n",
      "Reconstruction: 0.168784, Regularization: 0.001161\n",
      "2019-04-10 01:03:38,768 root         INFO     Train Epoch: 140 [5120/8000 (64%)]\tTotal Loss: 0.159991\n",
      "Reconstruction: 0.159132, Regularization: 0.000859\n",
      "2019-04-10 01:03:38,831 root         INFO     Train Epoch: 140 [5632/8000 (70%)]\tTotal Loss: 0.161506\n",
      "Reconstruction: 0.160419, Regularization: 0.001087\n",
      "2019-04-10 01:03:38,893 root         INFO     Train Epoch: 140 [6144/8000 (77%)]\tTotal Loss: 0.155636\n",
      "Reconstruction: 0.154948, Regularization: 0.000689\n",
      "2019-04-10 01:03:38,956 root         INFO     Train Epoch: 140 [6656/8000 (83%)]\tTotal Loss: 0.170979\n",
      "Reconstruction: 0.169539, Regularization: 0.001440\n",
      "2019-04-10 01:03:39,019 root         INFO     Train Epoch: 140 [7168/8000 (90%)]\tTotal Loss: 0.163219\n",
      "Reconstruction: 0.162163, Regularization: 0.001056\n",
      "2019-04-10 01:03:39,081 root         INFO     Train Epoch: 140 [7680/8000 (96%)]\tTotal Loss: 0.153961\n",
      "Reconstruction: 0.153282, Regularization: 0.000678\n",
      "2019-04-10 01:03:39,135 root         INFO     ====> Epoch: 140 Average loss: 0.1660\n",
      "2019-04-10 01:03:39,159 root         INFO     Train Epoch: 141 [0/8000 (0%)]\tTotal Loss: 0.175482\n",
      "Reconstruction: 0.174186, Regularization: 0.001296\n",
      "2019-04-10 01:03:39,223 root         INFO     Train Epoch: 141 [512/8000 (6%)]\tTotal Loss: 0.162933\n",
      "Reconstruction: 0.161960, Regularization: 0.000973\n",
      "2019-04-10 01:03:39,286 root         INFO     Train Epoch: 141 [1024/8000 (13%)]\tTotal Loss: 0.175619\n",
      "Reconstruction: 0.174108, Regularization: 0.001512\n",
      "2019-04-10 01:03:39,350 root         INFO     Train Epoch: 141 [1536/8000 (19%)]\tTotal Loss: 0.177104\n",
      "Reconstruction: 0.175543, Regularization: 0.001561\n",
      "2019-04-10 01:03:39,413 root         INFO     Train Epoch: 141 [2048/8000 (26%)]\tTotal Loss: 0.161581\n",
      "Reconstruction: 0.160629, Regularization: 0.000951\n",
      "2019-04-10 01:03:39,477 root         INFO     Train Epoch: 141 [2560/8000 (32%)]\tTotal Loss: 0.162906\n",
      "Reconstruction: 0.161860, Regularization: 0.001046\n",
      "2019-04-10 01:03:39,540 root         INFO     Train Epoch: 141 [3072/8000 (38%)]\tTotal Loss: 0.186125\n",
      "Reconstruction: 0.184184, Regularization: 0.001941\n",
      "2019-04-10 01:03:39,602 root         INFO     Train Epoch: 141 [3584/8000 (45%)]\tTotal Loss: 0.175761\n",
      "Reconstruction: 0.174464, Regularization: 0.001298\n",
      "2019-04-10 01:03:39,666 root         INFO     Train Epoch: 141 [4096/8000 (51%)]\tTotal Loss: 0.178701\n",
      "Reconstruction: 0.177077, Regularization: 0.001623\n",
      "2019-04-10 01:03:39,729 root         INFO     Train Epoch: 141 [4608/8000 (58%)]\tTotal Loss: 0.175497\n",
      "Reconstruction: 0.173974, Regularization: 0.001523\n",
      "2019-04-10 01:03:39,791 root         INFO     Train Epoch: 141 [5120/8000 (64%)]\tTotal Loss: 0.163453\n",
      "Reconstruction: 0.162448, Regularization: 0.001005\n",
      "2019-04-10 01:03:39,854 root         INFO     Train Epoch: 141 [5632/8000 (70%)]\tTotal Loss: 0.173871\n",
      "Reconstruction: 0.172514, Regularization: 0.001356\n",
      "2019-04-10 01:03:39,916 root         INFO     Train Epoch: 141 [6144/8000 (77%)]\tTotal Loss: 0.169268\n",
      "Reconstruction: 0.168017, Regularization: 0.001251\n",
      "2019-04-10 01:03:39,979 root         INFO     Train Epoch: 141 [6656/8000 (83%)]\tTotal Loss: 0.163858\n",
      "Reconstruction: 0.162851, Regularization: 0.001006\n",
      "2019-04-10 01:03:40,042 root         INFO     Train Epoch: 141 [7168/8000 (90%)]\tTotal Loss: 0.169655\n",
      "Reconstruction: 0.168292, Regularization: 0.001363\n",
      "2019-04-10 01:03:40,105 root         INFO     Train Epoch: 141 [7680/8000 (96%)]\tTotal Loss: 0.167633\n",
      "Reconstruction: 0.166399, Regularization: 0.001234\n",
      "2019-04-10 01:03:40,160 root         INFO     ====> Epoch: 141 Average loss: 0.1662\n",
      "2019-04-10 01:03:40,184 root         INFO     Train Epoch: 142 [0/8000 (0%)]\tTotal Loss: 0.169117\n",
      "Reconstruction: 0.167940, Regularization: 0.001177\n",
      "2019-04-10 01:03:40,249 root         INFO     Train Epoch: 142 [512/8000 (6%)]\tTotal Loss: 0.174599\n",
      "Reconstruction: 0.173101, Regularization: 0.001499\n",
      "2019-04-10 01:03:40,313 root         INFO     Train Epoch: 142 [1024/8000 (13%)]\tTotal Loss: 0.167346\n",
      "Reconstruction: 0.166051, Regularization: 0.001295\n",
      "2019-04-10 01:03:40,377 root         INFO     Train Epoch: 142 [1536/8000 (19%)]\tTotal Loss: 0.171282\n",
      "Reconstruction: 0.169977, Regularization: 0.001306\n",
      "2019-04-10 01:03:40,441 root         INFO     Train Epoch: 142 [2048/8000 (26%)]\tTotal Loss: 0.172146\n",
      "Reconstruction: 0.170762, Regularization: 0.001384\n",
      "2019-04-10 01:03:40,504 root         INFO     Train Epoch: 142 [2560/8000 (32%)]\tTotal Loss: 0.168349\n",
      "Reconstruction: 0.167052, Regularization: 0.001297\n",
      "2019-04-10 01:03:40,568 root         INFO     Train Epoch: 142 [3072/8000 (38%)]\tTotal Loss: 0.163420\n",
      "Reconstruction: 0.162171, Regularization: 0.001249\n",
      "2019-04-10 01:03:40,633 root         INFO     Train Epoch: 142 [3584/8000 (45%)]\tTotal Loss: 0.158074\n",
      "Reconstruction: 0.157271, Regularization: 0.000803\n",
      "2019-04-10 01:03:40,696 root         INFO     Train Epoch: 142 [4096/8000 (51%)]\tTotal Loss: 0.168677\n",
      "Reconstruction: 0.167351, Regularization: 0.001326\n",
      "2019-04-10 01:03:40,760 root         INFO     Train Epoch: 142 [4608/8000 (58%)]\tTotal Loss: 0.166177\n",
      "Reconstruction: 0.165099, Regularization: 0.001078\n",
      "2019-04-10 01:03:40,825 root         INFO     Train Epoch: 142 [5120/8000 (64%)]\tTotal Loss: 0.165008\n",
      "Reconstruction: 0.163672, Regularization: 0.001337\n",
      "2019-04-10 01:03:40,889 root         INFO     Train Epoch: 142 [5632/8000 (70%)]\tTotal Loss: 0.167468\n",
      "Reconstruction: 0.166156, Regularization: 0.001312\n",
      "2019-04-10 01:03:40,952 root         INFO     Train Epoch: 142 [6144/8000 (77%)]\tTotal Loss: 0.161383\n",
      "Reconstruction: 0.160323, Regularization: 0.001060\n",
      "2019-04-10 01:03:41,016 root         INFO     Train Epoch: 142 [6656/8000 (83%)]\tTotal Loss: 0.156836\n",
      "Reconstruction: 0.155838, Regularization: 0.000999\n",
      "2019-04-10 01:03:41,080 root         INFO     Train Epoch: 142 [7168/8000 (90%)]\tTotal Loss: 0.156469\n",
      "Reconstruction: 0.155622, Regularization: 0.000847\n",
      "2019-04-10 01:03:41,144 root         INFO     Train Epoch: 142 [7680/8000 (96%)]\tTotal Loss: 0.166747\n",
      "Reconstruction: 0.165440, Regularization: 0.001307\n",
      "2019-04-10 01:03:41,198 root         INFO     ====> Epoch: 142 Average loss: 0.1660\n",
      "2019-04-10 01:03:41,222 root         INFO     Train Epoch: 143 [0/8000 (0%)]\tTotal Loss: 0.169385\n",
      "Reconstruction: 0.167895, Regularization: 0.001490\n",
      "2019-04-10 01:03:41,287 root         INFO     Train Epoch: 143 [512/8000 (6%)]\tTotal Loss: 0.165677\n",
      "Reconstruction: 0.164491, Regularization: 0.001186\n",
      "2019-04-10 01:03:41,351 root         INFO     Train Epoch: 143 [1024/8000 (13%)]\tTotal Loss: 0.162195\n",
      "Reconstruction: 0.160946, Regularization: 0.001250\n",
      "2019-04-10 01:03:41,415 root         INFO     Train Epoch: 143 [1536/8000 (19%)]\tTotal Loss: 0.166542\n",
      "Reconstruction: 0.165385, Regularization: 0.001157\n",
      "2019-04-10 01:03:41,479 root         INFO     Train Epoch: 143 [2048/8000 (26%)]\tTotal Loss: 0.171840\n",
      "Reconstruction: 0.170578, Regularization: 0.001261\n",
      "2019-04-10 01:03:41,544 root         INFO     Train Epoch: 143 [2560/8000 (32%)]\tTotal Loss: 0.174071\n",
      "Reconstruction: 0.172583, Regularization: 0.001487\n",
      "2019-04-10 01:03:41,608 root         INFO     Train Epoch: 143 [3072/8000 (38%)]\tTotal Loss: 0.161906\n",
      "Reconstruction: 0.160854, Regularization: 0.001052\n",
      "2019-04-10 01:03:41,671 root         INFO     Train Epoch: 143 [3584/8000 (45%)]\tTotal Loss: 0.166537\n",
      "Reconstruction: 0.165329, Regularization: 0.001208\n",
      "2019-04-10 01:03:41,735 root         INFO     Train Epoch: 143 [4096/8000 (51%)]\tTotal Loss: 0.163615\n",
      "Reconstruction: 0.162441, Regularization: 0.001174\n",
      "2019-04-10 01:03:41,799 root         INFO     Train Epoch: 143 [4608/8000 (58%)]\tTotal Loss: 0.155598\n",
      "Reconstruction: 0.154726, Regularization: 0.000873\n",
      "2019-04-10 01:03:41,863 root         INFO     Train Epoch: 143 [5120/8000 (64%)]\tTotal Loss: 0.177390\n",
      "Reconstruction: 0.175601, Regularization: 0.001789\n",
      "2019-04-10 01:03:41,927 root         INFO     Train Epoch: 143 [5632/8000 (70%)]\tTotal Loss: 0.156577\n",
      "Reconstruction: 0.155697, Regularization: 0.000880\n",
      "2019-04-10 01:03:41,992 root         INFO     Train Epoch: 143 [6144/8000 (77%)]\tTotal Loss: 0.165967\n",
      "Reconstruction: 0.164706, Regularization: 0.001261\n",
      "2019-04-10 01:03:42,056 root         INFO     Train Epoch: 143 [6656/8000 (83%)]\tTotal Loss: 0.164725\n",
      "Reconstruction: 0.163588, Regularization: 0.001137\n",
      "2019-04-10 01:03:42,120 root         INFO     Train Epoch: 143 [7168/8000 (90%)]\tTotal Loss: 0.165650\n",
      "Reconstruction: 0.164409, Regularization: 0.001240\n",
      "2019-04-10 01:03:42,184 root         INFO     Train Epoch: 143 [7680/8000 (96%)]\tTotal Loss: 0.156865\n",
      "Reconstruction: 0.155888, Regularization: 0.000977\n",
      "2019-04-10 01:03:42,238 root         INFO     ====> Epoch: 143 Average loss: 0.1662\n",
      "2019-04-10 01:03:42,262 root         INFO     Train Epoch: 144 [0/8000 (0%)]\tTotal Loss: 0.160890\n",
      "Reconstruction: 0.159730, Regularization: 0.001161\n",
      "2019-04-10 01:03:42,324 root         INFO     Train Epoch: 144 [512/8000 (6%)]\tTotal Loss: 0.168915\n",
      "Reconstruction: 0.167494, Regularization: 0.001422\n",
      "2019-04-10 01:03:42,387 root         INFO     Train Epoch: 144 [1024/8000 (13%)]\tTotal Loss: 0.156148\n",
      "Reconstruction: 0.155277, Regularization: 0.000871\n",
      "2019-04-10 01:03:42,450 root         INFO     Train Epoch: 144 [1536/8000 (19%)]\tTotal Loss: 0.175117\n",
      "Reconstruction: 0.173466, Regularization: 0.001651\n",
      "2019-04-10 01:03:42,513 root         INFO     Train Epoch: 144 [2048/8000 (26%)]\tTotal Loss: 0.165927\n",
      "Reconstruction: 0.164811, Regularization: 0.001116\n",
      "2019-04-10 01:03:42,577 root         INFO     Train Epoch: 144 [2560/8000 (32%)]\tTotal Loss: 0.171447\n",
      "Reconstruction: 0.170135, Regularization: 0.001312\n",
      "2019-04-10 01:03:42,642 root         INFO     Train Epoch: 144 [3072/8000 (38%)]\tTotal Loss: 0.167810\n",
      "Reconstruction: 0.166466, Regularization: 0.001343\n",
      "2019-04-10 01:03:42,705 root         INFO     Train Epoch: 144 [3584/8000 (45%)]\tTotal Loss: 0.155379\n",
      "Reconstruction: 0.154571, Regularization: 0.000808\n",
      "2019-04-10 01:03:42,769 root         INFO     Train Epoch: 144 [4096/8000 (51%)]\tTotal Loss: 0.157981\n",
      "Reconstruction: 0.157084, Regularization: 0.000896\n",
      "2019-04-10 01:03:42,833 root         INFO     Train Epoch: 144 [4608/8000 (58%)]\tTotal Loss: 0.153438\n",
      "Reconstruction: 0.152660, Regularization: 0.000777\n",
      "2019-04-10 01:03:42,897 root         INFO     Train Epoch: 144 [5120/8000 (64%)]\tTotal Loss: 0.176380\n",
      "Reconstruction: 0.174813, Regularization: 0.001567\n",
      "2019-04-10 01:03:42,961 root         INFO     Train Epoch: 144 [5632/8000 (70%)]\tTotal Loss: 0.150540\n",
      "Reconstruction: 0.149784, Regularization: 0.000756\n",
      "2019-04-10 01:03:43,025 root         INFO     Train Epoch: 144 [6144/8000 (77%)]\tTotal Loss: 0.173729\n",
      "Reconstruction: 0.172187, Regularization: 0.001542\n",
      "2019-04-10 01:03:43,089 root         INFO     Train Epoch: 144 [6656/8000 (83%)]\tTotal Loss: 0.174924\n",
      "Reconstruction: 0.173071, Regularization: 0.001853\n",
      "2019-04-10 01:03:43,152 root         INFO     Train Epoch: 144 [7168/8000 (90%)]\tTotal Loss: 0.166211\n",
      "Reconstruction: 0.164797, Regularization: 0.001415\n",
      "2019-04-10 01:03:43,216 root         INFO     Train Epoch: 144 [7680/8000 (96%)]\tTotal Loss: 0.169963\n",
      "Reconstruction: 0.168558, Regularization: 0.001405\n",
      "2019-04-10 01:03:43,270 root         INFO     ====> Epoch: 144 Average loss: 0.1660\n",
      "2019-04-10 01:03:43,294 root         INFO     Train Epoch: 145 [0/8000 (0%)]\tTotal Loss: 0.169883\n",
      "Reconstruction: 0.168407, Regularization: 0.001476\n",
      "2019-04-10 01:03:43,358 root         INFO     Train Epoch: 145 [512/8000 (6%)]\tTotal Loss: 0.170878\n",
      "Reconstruction: 0.169399, Regularization: 0.001479\n",
      "2019-04-10 01:03:43,420 root         INFO     Train Epoch: 145 [1024/8000 (13%)]\tTotal Loss: 0.163649\n",
      "Reconstruction: 0.162372, Regularization: 0.001276\n",
      "2019-04-10 01:03:43,482 root         INFO     Train Epoch: 145 [1536/8000 (19%)]\tTotal Loss: 0.182467\n",
      "Reconstruction: 0.180681, Regularization: 0.001785\n",
      "2019-04-10 01:03:43,544 root         INFO     Train Epoch: 145 [2048/8000 (26%)]\tTotal Loss: 0.164917\n",
      "Reconstruction: 0.163609, Regularization: 0.001308\n",
      "2019-04-10 01:03:43,607 root         INFO     Train Epoch: 145 [2560/8000 (32%)]\tTotal Loss: 0.152586\n",
      "Reconstruction: 0.151777, Regularization: 0.000809\n",
      "2019-04-10 01:03:43,669 root         INFO     Train Epoch: 145 [3072/8000 (38%)]\tTotal Loss: 0.164858\n",
      "Reconstruction: 0.163582, Regularization: 0.001276\n",
      "2019-04-10 01:03:43,731 root         INFO     Train Epoch: 145 [3584/8000 (45%)]\tTotal Loss: 0.158615\n",
      "Reconstruction: 0.157578, Regularization: 0.001037\n",
      "2019-04-10 01:03:43,793 root         INFO     Train Epoch: 145 [4096/8000 (51%)]\tTotal Loss: 0.168081\n",
      "Reconstruction: 0.166667, Regularization: 0.001414\n",
      "2019-04-10 01:03:43,855 root         INFO     Train Epoch: 145 [4608/8000 (58%)]\tTotal Loss: 0.175347\n",
      "Reconstruction: 0.173729, Regularization: 0.001617\n",
      "2019-04-10 01:03:43,916 root         INFO     Train Epoch: 145 [5120/8000 (64%)]\tTotal Loss: 0.174798\n",
      "Reconstruction: 0.173300, Regularization: 0.001498\n",
      "2019-04-10 01:03:43,978 root         INFO     Train Epoch: 145 [5632/8000 (70%)]\tTotal Loss: 0.177645\n",
      "Reconstruction: 0.175930, Regularization: 0.001714\n",
      "2019-04-10 01:03:44,040 root         INFO     Train Epoch: 145 [6144/8000 (77%)]\tTotal Loss: 0.156331\n",
      "Reconstruction: 0.155439, Regularization: 0.000892\n",
      "2019-04-10 01:03:44,103 root         INFO     Train Epoch: 145 [6656/8000 (83%)]\tTotal Loss: 0.168289\n",
      "Reconstruction: 0.166992, Regularization: 0.001297\n",
      "2019-04-10 01:03:44,165 root         INFO     Train Epoch: 145 [7168/8000 (90%)]\tTotal Loss: 0.169222\n",
      "Reconstruction: 0.167973, Regularization: 0.001249\n",
      "2019-04-10 01:03:44,226 root         INFO     Train Epoch: 145 [7680/8000 (96%)]\tTotal Loss: 0.161526\n",
      "Reconstruction: 0.160473, Regularization: 0.001053\n",
      "2019-04-10 01:03:44,279 root         INFO     ====> Epoch: 145 Average loss: 0.1661\n",
      "2019-04-10 01:03:44,303 root         INFO     Train Epoch: 146 [0/8000 (0%)]\tTotal Loss: 0.166299\n",
      "Reconstruction: 0.164814, Regularization: 0.001485\n",
      "2019-04-10 01:03:44,366 root         INFO     Train Epoch: 146 [512/8000 (6%)]\tTotal Loss: 0.163397\n",
      "Reconstruction: 0.162109, Regularization: 0.001288\n",
      "2019-04-10 01:03:44,429 root         INFO     Train Epoch: 146 [1024/8000 (13%)]\tTotal Loss: 0.164793\n",
      "Reconstruction: 0.163370, Regularization: 0.001424\n",
      "2019-04-10 01:03:44,492 root         INFO     Train Epoch: 146 [1536/8000 (19%)]\tTotal Loss: 0.152127\n",
      "Reconstruction: 0.151319, Regularization: 0.000807\n",
      "2019-04-10 01:03:44,554 root         INFO     Train Epoch: 146 [2048/8000 (26%)]\tTotal Loss: 0.162760\n",
      "Reconstruction: 0.161568, Regularization: 0.001192\n",
      "2019-04-10 01:03:44,615 root         INFO     Train Epoch: 146 [2560/8000 (32%)]\tTotal Loss: 0.167366\n",
      "Reconstruction: 0.165945, Regularization: 0.001421\n",
      "2019-04-10 01:03:44,678 root         INFO     Train Epoch: 146 [3072/8000 (38%)]\tTotal Loss: 0.174631\n",
      "Reconstruction: 0.172670, Regularization: 0.001960\n",
      "2019-04-10 01:03:44,739 root         INFO     Train Epoch: 146 [3584/8000 (45%)]\tTotal Loss: 0.170626\n",
      "Reconstruction: 0.169005, Regularization: 0.001621\n",
      "2019-04-10 01:03:44,801 root         INFO     Train Epoch: 146 [4096/8000 (51%)]\tTotal Loss: 0.172934\n",
      "Reconstruction: 0.171468, Regularization: 0.001467\n",
      "2019-04-10 01:03:44,863 root         INFO     Train Epoch: 146 [4608/8000 (58%)]\tTotal Loss: 0.167754\n",
      "Reconstruction: 0.166391, Regularization: 0.001363\n",
      "2019-04-10 01:03:44,925 root         INFO     Train Epoch: 146 [5120/8000 (64%)]\tTotal Loss: 0.165648\n",
      "Reconstruction: 0.164363, Regularization: 0.001285\n",
      "2019-04-10 01:03:44,987 root         INFO     Train Epoch: 146 [5632/8000 (70%)]\tTotal Loss: 0.169414\n",
      "Reconstruction: 0.167849, Regularization: 0.001565\n",
      "2019-04-10 01:03:45,049 root         INFO     Train Epoch: 146 [6144/8000 (77%)]\tTotal Loss: 0.165689\n",
      "Reconstruction: 0.164401, Regularization: 0.001288\n",
      "2019-04-10 01:03:45,111 root         INFO     Train Epoch: 146 [6656/8000 (83%)]\tTotal Loss: 0.156806\n",
      "Reconstruction: 0.155739, Regularization: 0.001068\n",
      "2019-04-10 01:03:45,174 root         INFO     Train Epoch: 146 [7168/8000 (90%)]\tTotal Loss: 0.155670\n",
      "Reconstruction: 0.154650, Regularization: 0.001020\n",
      "2019-04-10 01:03:45,237 root         INFO     Train Epoch: 146 [7680/8000 (96%)]\tTotal Loss: 0.163498\n",
      "Reconstruction: 0.162196, Regularization: 0.001303\n",
      "2019-04-10 01:03:45,291 root         INFO     ====> Epoch: 146 Average loss: 0.1661\n",
      "2019-04-10 01:03:45,315 root         INFO     Train Epoch: 147 [0/8000 (0%)]\tTotal Loss: 0.167917\n",
      "Reconstruction: 0.166469, Regularization: 0.001447\n",
      "2019-04-10 01:03:45,379 root         INFO     Train Epoch: 147 [512/8000 (6%)]\tTotal Loss: 0.165615\n",
      "Reconstruction: 0.164127, Regularization: 0.001488\n",
      "2019-04-10 01:03:45,442 root         INFO     Train Epoch: 147 [1024/8000 (13%)]\tTotal Loss: 0.152577\n",
      "Reconstruction: 0.151804, Regularization: 0.000773\n",
      "2019-04-10 01:03:45,504 root         INFO     Train Epoch: 147 [1536/8000 (19%)]\tTotal Loss: 0.171579\n",
      "Reconstruction: 0.169751, Regularization: 0.001828\n",
      "2019-04-10 01:03:45,567 root         INFO     Train Epoch: 147 [2048/8000 (26%)]\tTotal Loss: 0.166983\n",
      "Reconstruction: 0.165645, Regularization: 0.001338\n",
      "2019-04-10 01:03:45,630 root         INFO     Train Epoch: 147 [2560/8000 (32%)]\tTotal Loss: 0.160417\n",
      "Reconstruction: 0.159319, Regularization: 0.001098\n",
      "2019-04-10 01:03:45,693 root         INFO     Train Epoch: 147 [3072/8000 (38%)]\tTotal Loss: 0.152067\n",
      "Reconstruction: 0.151139, Regularization: 0.000928\n",
      "2019-04-10 01:03:45,755 root         INFO     Train Epoch: 147 [3584/8000 (45%)]\tTotal Loss: 0.169706\n",
      "Reconstruction: 0.168253, Regularization: 0.001453\n",
      "2019-04-10 01:03:45,819 root         INFO     Train Epoch: 147 [4096/8000 (51%)]\tTotal Loss: 0.164038\n",
      "Reconstruction: 0.162710, Regularization: 0.001328\n",
      "2019-04-10 01:03:45,882 root         INFO     Train Epoch: 147 [4608/8000 (58%)]\tTotal Loss: 0.161171\n",
      "Reconstruction: 0.160004, Regularization: 0.001167\n",
      "2019-04-10 01:03:45,945 root         INFO     Train Epoch: 147 [5120/8000 (64%)]\tTotal Loss: 0.166087\n",
      "Reconstruction: 0.164675, Regularization: 0.001412\n",
      "2019-04-10 01:03:46,008 root         INFO     Train Epoch: 147 [5632/8000 (70%)]\tTotal Loss: 0.152951\n",
      "Reconstruction: 0.152064, Regularization: 0.000887\n",
      "2019-04-10 01:03:46,071 root         INFO     Train Epoch: 147 [6144/8000 (77%)]\tTotal Loss: 0.164168\n",
      "Reconstruction: 0.162839, Regularization: 0.001328\n",
      "2019-04-10 01:03:46,134 root         INFO     Train Epoch: 147 [6656/8000 (83%)]\tTotal Loss: 0.174206\n",
      "Reconstruction: 0.172520, Regularization: 0.001685\n",
      "2019-04-10 01:03:46,197 root         INFO     Train Epoch: 147 [7168/8000 (90%)]\tTotal Loss: 0.163074\n",
      "Reconstruction: 0.161850, Regularization: 0.001224\n",
      "2019-04-10 01:03:46,260 root         INFO     Train Epoch: 147 [7680/8000 (96%)]\tTotal Loss: 0.159413\n",
      "Reconstruction: 0.158273, Regularization: 0.001140\n",
      "2019-04-10 01:03:46,314 root         INFO     ====> Epoch: 147 Average loss: 0.1662\n",
      "2019-04-10 01:03:46,338 root         INFO     Train Epoch: 148 [0/8000 (0%)]\tTotal Loss: 0.163427\n",
      "Reconstruction: 0.162080, Regularization: 0.001347\n",
      "2019-04-10 01:03:46,401 root         INFO     Train Epoch: 148 [512/8000 (6%)]\tTotal Loss: 0.171089\n",
      "Reconstruction: 0.169572, Regularization: 0.001517\n",
      "2019-04-10 01:03:46,463 root         INFO     Train Epoch: 148 [1024/8000 (13%)]\tTotal Loss: 0.170620\n",
      "Reconstruction: 0.169089, Regularization: 0.001531\n",
      "2019-04-10 01:03:46,526 root         INFO     Train Epoch: 148 [1536/8000 (19%)]\tTotal Loss: 0.165612\n",
      "Reconstruction: 0.164237, Regularization: 0.001375\n",
      "2019-04-10 01:03:46,589 root         INFO     Train Epoch: 148 [2048/8000 (26%)]\tTotal Loss: 0.159289\n",
      "Reconstruction: 0.158296, Regularization: 0.000992\n",
      "2019-04-10 01:03:46,653 root         INFO     Train Epoch: 148 [2560/8000 (32%)]\tTotal Loss: 0.153653\n",
      "Reconstruction: 0.152673, Regularization: 0.000980\n",
      "2019-04-10 01:03:46,717 root         INFO     Train Epoch: 148 [3072/8000 (38%)]\tTotal Loss: 0.169905\n",
      "Reconstruction: 0.168226, Regularization: 0.001679\n",
      "2019-04-10 01:03:46,778 root         INFO     Train Epoch: 148 [3584/8000 (45%)]\tTotal Loss: 0.159010\n",
      "Reconstruction: 0.157917, Regularization: 0.001093\n",
      "2019-04-10 01:03:46,839 root         INFO     Train Epoch: 148 [4096/8000 (51%)]\tTotal Loss: 0.173267\n",
      "Reconstruction: 0.171528, Regularization: 0.001739\n",
      "2019-04-10 01:03:46,900 root         INFO     Train Epoch: 148 [4608/8000 (58%)]\tTotal Loss: 0.160895\n",
      "Reconstruction: 0.159748, Regularization: 0.001146\n",
      "2019-04-10 01:03:46,962 root         INFO     Train Epoch: 148 [5120/8000 (64%)]\tTotal Loss: 0.163123\n",
      "Reconstruction: 0.161764, Regularization: 0.001360\n",
      "2019-04-10 01:03:47,023 root         INFO     Train Epoch: 148 [5632/8000 (70%)]\tTotal Loss: 0.170431\n",
      "Reconstruction: 0.168763, Regularization: 0.001668\n",
      "2019-04-10 01:03:47,084 root         INFO     Train Epoch: 148 [6144/8000 (77%)]\tTotal Loss: 0.175579\n",
      "Reconstruction: 0.173877, Regularization: 0.001702\n",
      "2019-04-10 01:03:47,145 root         INFO     Train Epoch: 148 [6656/8000 (83%)]\tTotal Loss: 0.176715\n",
      "Reconstruction: 0.174788, Regularization: 0.001928\n",
      "2019-04-10 01:03:47,206 root         INFO     Train Epoch: 148 [7168/8000 (90%)]\tTotal Loss: 0.166595\n",
      "Reconstruction: 0.165119, Regularization: 0.001477\n",
      "2019-04-10 01:03:47,268 root         INFO     Train Epoch: 148 [7680/8000 (96%)]\tTotal Loss: 0.170678\n",
      "Reconstruction: 0.168808, Regularization: 0.001870\n",
      "2019-04-10 01:03:47,321 root         INFO     ====> Epoch: 148 Average loss: 0.1658\n",
      "2019-04-10 01:03:47,345 root         INFO     Train Epoch: 149 [0/8000 (0%)]\tTotal Loss: 0.167926\n",
      "Reconstruction: 0.166520, Regularization: 0.001406\n",
      "2019-04-10 01:03:47,409 root         INFO     Train Epoch: 149 [512/8000 (6%)]\tTotal Loss: 0.155826\n",
      "Reconstruction: 0.154785, Regularization: 0.001040\n",
      "2019-04-10 01:03:47,471 root         INFO     Train Epoch: 149 [1024/8000 (13%)]\tTotal Loss: 0.161195\n",
      "Reconstruction: 0.160042, Regularization: 0.001154\n",
      "2019-04-10 01:03:47,535 root         INFO     Train Epoch: 149 [1536/8000 (19%)]\tTotal Loss: 0.157582\n",
      "Reconstruction: 0.156778, Regularization: 0.000804\n",
      "2019-04-10 01:03:47,597 root         INFO     Train Epoch: 149 [2048/8000 (26%)]\tTotal Loss: 0.155981\n",
      "Reconstruction: 0.155029, Regularization: 0.000952\n",
      "2019-04-10 01:03:47,658 root         INFO     Train Epoch: 149 [2560/8000 (32%)]\tTotal Loss: 0.175494\n",
      "Reconstruction: 0.173728, Regularization: 0.001767\n",
      "2019-04-10 01:03:47,721 root         INFO     Train Epoch: 149 [3072/8000 (38%)]\tTotal Loss: 0.167877\n",
      "Reconstruction: 0.166157, Regularization: 0.001720\n",
      "2019-04-10 01:03:47,784 root         INFO     Train Epoch: 149 [3584/8000 (45%)]\tTotal Loss: 0.158702\n",
      "Reconstruction: 0.157528, Regularization: 0.001174\n",
      "2019-04-10 01:03:47,847 root         INFO     Train Epoch: 149 [4096/8000 (51%)]\tTotal Loss: 0.157018\n",
      "Reconstruction: 0.155899, Regularization: 0.001119\n",
      "2019-04-10 01:03:47,910 root         INFO     Train Epoch: 149 [4608/8000 (58%)]\tTotal Loss: 0.164261\n",
      "Reconstruction: 0.162633, Regularization: 0.001627\n",
      "2019-04-10 01:03:47,974 root         INFO     Train Epoch: 149 [5120/8000 (64%)]\tTotal Loss: 0.173171\n",
      "Reconstruction: 0.171354, Regularization: 0.001817\n",
      "2019-04-10 01:03:48,037 root         INFO     Train Epoch: 149 [5632/8000 (70%)]\tTotal Loss: 0.174288\n",
      "Reconstruction: 0.172463, Regularization: 0.001826\n",
      "2019-04-10 01:03:48,100 root         INFO     Train Epoch: 149 [6144/8000 (77%)]\tTotal Loss: 0.163688\n",
      "Reconstruction: 0.162285, Regularization: 0.001404\n",
      "2019-04-10 01:03:48,162 root         INFO     Train Epoch: 149 [6656/8000 (83%)]\tTotal Loss: 0.174409\n",
      "Reconstruction: 0.172507, Regularization: 0.001902\n",
      "2019-04-10 01:03:48,223 root         INFO     Train Epoch: 149 [7168/8000 (90%)]\tTotal Loss: 0.166740\n",
      "Reconstruction: 0.165367, Regularization: 0.001374\n",
      "2019-04-10 01:03:48,284 root         INFO     Train Epoch: 149 [7680/8000 (96%)]\tTotal Loss: 0.168229\n",
      "Reconstruction: 0.166648, Regularization: 0.001581\n",
      "2019-04-10 01:03:48,337 root         INFO     ====> Epoch: 149 Average loss: 0.1659\n",
      "2019-04-10 01:03:48,361 root         INFO     Train Epoch: 150 [0/8000 (0%)]\tTotal Loss: 0.165154\n",
      "Reconstruction: 0.163634, Regularization: 0.001520\n",
      "2019-04-10 01:03:48,425 root         INFO     Train Epoch: 150 [512/8000 (6%)]\tTotal Loss: 0.160563\n",
      "Reconstruction: 0.159124, Regularization: 0.001439\n",
      "2019-04-10 01:03:48,488 root         INFO     Train Epoch: 150 [1024/8000 (13%)]\tTotal Loss: 0.165887\n",
      "Reconstruction: 0.164297, Regularization: 0.001590\n",
      "2019-04-10 01:03:48,551 root         INFO     Train Epoch: 150 [1536/8000 (19%)]\tTotal Loss: 0.169015\n",
      "Reconstruction: 0.167292, Regularization: 0.001723\n",
      "2019-04-10 01:03:48,614 root         INFO     Train Epoch: 150 [2048/8000 (26%)]\tTotal Loss: 0.172986\n",
      "Reconstruction: 0.171169, Regularization: 0.001817\n",
      "2019-04-10 01:03:48,677 root         INFO     Train Epoch: 150 [2560/8000 (32%)]\tTotal Loss: 0.174479\n",
      "Reconstruction: 0.172740, Regularization: 0.001739\n",
      "2019-04-10 01:03:48,740 root         INFO     Train Epoch: 150 [3072/8000 (38%)]\tTotal Loss: 0.157258\n",
      "Reconstruction: 0.156178, Regularization: 0.001080\n",
      "2019-04-10 01:03:48,803 root         INFO     Train Epoch: 150 [3584/8000 (45%)]\tTotal Loss: 0.158485\n",
      "Reconstruction: 0.157345, Regularization: 0.001140\n",
      "2019-04-10 01:03:48,866 root         INFO     Train Epoch: 150 [4096/8000 (51%)]\tTotal Loss: 0.180329\n",
      "Reconstruction: 0.178187, Regularization: 0.002142\n",
      "2019-04-10 01:03:48,929 root         INFO     Train Epoch: 150 [4608/8000 (58%)]\tTotal Loss: 0.171292\n",
      "Reconstruction: 0.169500, Regularization: 0.001792\n",
      "2019-04-10 01:03:48,992 root         INFO     Train Epoch: 150 [5120/8000 (64%)]\tTotal Loss: 0.150889\n",
      "Reconstruction: 0.149914, Regularization: 0.000975\n",
      "2019-04-10 01:03:49,055 root         INFO     Train Epoch: 150 [5632/8000 (70%)]\tTotal Loss: 0.165318\n",
      "Reconstruction: 0.163782, Regularization: 0.001536\n",
      "2019-04-10 01:03:49,117 root         INFO     Train Epoch: 150 [6144/8000 (77%)]\tTotal Loss: 0.165763\n",
      "Reconstruction: 0.164199, Regularization: 0.001564\n",
      "2019-04-10 01:03:49,180 root         INFO     Train Epoch: 150 [6656/8000 (83%)]\tTotal Loss: 0.167324\n",
      "Reconstruction: 0.165622, Regularization: 0.001702\n",
      "2019-04-10 01:03:49,243 root         INFO     Train Epoch: 150 [7168/8000 (90%)]\tTotal Loss: 0.165338\n",
      "Reconstruction: 0.163773, Regularization: 0.001565\n",
      "2019-04-10 01:03:49,306 root         INFO     Train Epoch: 150 [7680/8000 (96%)]\tTotal Loss: 0.157658\n",
      "Reconstruction: 0.156518, Regularization: 0.001140\n",
      "2019-04-10 01:03:49,360 root         INFO     ====> Epoch: 150 Average loss: 0.1661\n",
      "2019-04-10 01:03:49,384 root         INFO     Train Epoch: 151 [0/8000 (0%)]\tTotal Loss: 0.156643\n",
      "Reconstruction: 0.155524, Regularization: 0.001119\n",
      "2019-04-10 01:03:49,447 root         INFO     Train Epoch: 151 [512/8000 (6%)]\tTotal Loss: 0.160972\n",
      "Reconstruction: 0.159578, Regularization: 0.001393\n",
      "2019-04-10 01:03:49,510 root         INFO     Train Epoch: 151 [1024/8000 (13%)]\tTotal Loss: 0.165288\n",
      "Reconstruction: 0.163558, Regularization: 0.001730\n",
      "2019-04-10 01:03:49,574 root         INFO     Train Epoch: 151 [1536/8000 (19%)]\tTotal Loss: 0.174244\n",
      "Reconstruction: 0.172332, Regularization: 0.001912\n",
      "2019-04-10 01:03:49,637 root         INFO     Train Epoch: 151 [2048/8000 (26%)]\tTotal Loss: 0.160088\n",
      "Reconstruction: 0.158912, Regularization: 0.001176\n",
      "2019-04-10 01:03:49,700 root         INFO     Train Epoch: 151 [2560/8000 (32%)]\tTotal Loss: 0.158759\n",
      "Reconstruction: 0.157537, Regularization: 0.001222\n",
      "2019-04-10 01:03:49,762 root         INFO     Train Epoch: 151 [3072/8000 (38%)]\tTotal Loss: 0.174446\n",
      "Reconstruction: 0.172752, Regularization: 0.001694\n",
      "2019-04-10 01:03:49,824 root         INFO     Train Epoch: 151 [3584/8000 (45%)]\tTotal Loss: 0.166461\n",
      "Reconstruction: 0.164981, Regularization: 0.001480\n",
      "2019-04-10 01:03:49,886 root         INFO     Train Epoch: 151 [4096/8000 (51%)]\tTotal Loss: 0.167288\n",
      "Reconstruction: 0.165757, Regularization: 0.001531\n",
      "2019-04-10 01:03:49,949 root         INFO     Train Epoch: 151 [4608/8000 (58%)]\tTotal Loss: 0.160139\n",
      "Reconstruction: 0.158942, Regularization: 0.001197\n",
      "2019-04-10 01:03:50,011 root         INFO     Train Epoch: 151 [5120/8000 (64%)]\tTotal Loss: 0.167811\n",
      "Reconstruction: 0.166093, Regularization: 0.001717\n",
      "2019-04-10 01:03:50,073 root         INFO     Train Epoch: 151 [5632/8000 (70%)]\tTotal Loss: 0.164606\n",
      "Reconstruction: 0.163274, Regularization: 0.001332\n",
      "2019-04-10 01:03:50,134 root         INFO     Train Epoch: 151 [6144/8000 (77%)]\tTotal Loss: 0.170517\n",
      "Reconstruction: 0.168593, Regularization: 0.001923\n",
      "2019-04-10 01:03:50,195 root         INFO     Train Epoch: 151 [6656/8000 (83%)]\tTotal Loss: 0.160503\n",
      "Reconstruction: 0.159213, Regularization: 0.001290\n",
      "2019-04-10 01:03:50,257 root         INFO     Train Epoch: 151 [7168/8000 (90%)]\tTotal Loss: 0.163417\n",
      "Reconstruction: 0.162159, Regularization: 0.001259\n",
      "2019-04-10 01:03:50,319 root         INFO     Train Epoch: 151 [7680/8000 (96%)]\tTotal Loss: 0.161341\n",
      "Reconstruction: 0.159921, Regularization: 0.001420\n",
      "2019-04-10 01:03:50,373 root         INFO     ====> Epoch: 151 Average loss: 0.1660\n",
      "2019-04-10 01:03:50,398 root         INFO     Train Epoch: 152 [0/8000 (0%)]\tTotal Loss: 0.173090\n",
      "Reconstruction: 0.171278, Regularization: 0.001812\n",
      "2019-04-10 01:03:50,460 root         INFO     Train Epoch: 152 [512/8000 (6%)]\tTotal Loss: 0.172809\n",
      "Reconstruction: 0.170765, Regularization: 0.002043\n",
      "2019-04-10 01:03:50,522 root         INFO     Train Epoch: 152 [1024/8000 (13%)]\tTotal Loss: 0.164840\n",
      "Reconstruction: 0.163055, Regularization: 0.001785\n",
      "2019-04-10 01:03:50,584 root         INFO     Train Epoch: 152 [1536/8000 (19%)]\tTotal Loss: 0.165614\n",
      "Reconstruction: 0.164221, Regularization: 0.001393\n",
      "2019-04-10 01:03:50,645 root         INFO     Train Epoch: 152 [2048/8000 (26%)]\tTotal Loss: 0.153691\n",
      "Reconstruction: 0.152786, Regularization: 0.000905\n",
      "2019-04-10 01:03:50,706 root         INFO     Train Epoch: 152 [2560/8000 (32%)]\tTotal Loss: 0.178935\n",
      "Reconstruction: 0.176926, Regularization: 0.002010\n",
      "2019-04-10 01:03:50,768 root         INFO     Train Epoch: 152 [3072/8000 (38%)]\tTotal Loss: 0.157156\n",
      "Reconstruction: 0.155933, Regularization: 0.001223\n",
      "2019-04-10 01:03:50,829 root         INFO     Train Epoch: 152 [3584/8000 (45%)]\tTotal Loss: 0.164774\n",
      "Reconstruction: 0.163343, Regularization: 0.001431\n",
      "2019-04-10 01:03:50,890 root         INFO     Train Epoch: 152 [4096/8000 (51%)]\tTotal Loss: 0.172788\n",
      "Reconstruction: 0.171039, Regularization: 0.001750\n",
      "2019-04-10 01:03:50,951 root         INFO     Train Epoch: 152 [4608/8000 (58%)]\tTotal Loss: 0.166216\n",
      "Reconstruction: 0.164594, Regularization: 0.001622\n",
      "2019-04-10 01:03:51,012 root         INFO     Train Epoch: 152 [5120/8000 (64%)]\tTotal Loss: 0.167233\n",
      "Reconstruction: 0.165343, Regularization: 0.001891\n",
      "2019-04-10 01:03:51,074 root         INFO     Train Epoch: 152 [5632/8000 (70%)]\tTotal Loss: 0.164281\n",
      "Reconstruction: 0.162889, Regularization: 0.001392\n",
      "2019-04-10 01:03:51,135 root         INFO     Train Epoch: 152 [6144/8000 (77%)]\tTotal Loss: 0.171581\n",
      "Reconstruction: 0.169789, Regularization: 0.001792\n",
      "2019-04-10 01:03:51,196 root         INFO     Train Epoch: 152 [6656/8000 (83%)]\tTotal Loss: 0.162776\n",
      "Reconstruction: 0.161207, Regularization: 0.001569\n",
      "2019-04-10 01:03:51,258 root         INFO     Train Epoch: 152 [7168/8000 (90%)]\tTotal Loss: 0.163525\n",
      "Reconstruction: 0.162095, Regularization: 0.001430\n",
      "2019-04-10 01:03:51,319 root         INFO     Train Epoch: 152 [7680/8000 (96%)]\tTotal Loss: 0.163618\n",
      "Reconstruction: 0.162180, Regularization: 0.001438\n",
      "2019-04-10 01:03:51,372 root         INFO     ====> Epoch: 152 Average loss: 0.1660\n",
      "2019-04-10 01:03:51,396 root         INFO     Train Epoch: 153 [0/8000 (0%)]\tTotal Loss: 0.174211\n",
      "Reconstruction: 0.172179, Regularization: 0.002032\n",
      "2019-04-10 01:03:51,460 root         INFO     Train Epoch: 153 [512/8000 (6%)]\tTotal Loss: 0.168606\n",
      "Reconstruction: 0.166605, Regularization: 0.002002\n",
      "2019-04-10 01:03:51,523 root         INFO     Train Epoch: 153 [1024/8000 (13%)]\tTotal Loss: 0.162852\n",
      "Reconstruction: 0.161433, Regularization: 0.001419\n",
      "2019-04-10 01:03:51,586 root         INFO     Train Epoch: 153 [1536/8000 (19%)]\tTotal Loss: 0.155128\n",
      "Reconstruction: 0.153876, Regularization: 0.001252\n",
      "2019-04-10 01:03:51,649 root         INFO     Train Epoch: 153 [2048/8000 (26%)]\tTotal Loss: 0.168369\n",
      "Reconstruction: 0.166695, Regularization: 0.001674\n",
      "2019-04-10 01:03:51,713 root         INFO     Train Epoch: 153 [2560/8000 (32%)]\tTotal Loss: 0.169365\n",
      "Reconstruction: 0.167639, Regularization: 0.001725\n",
      "2019-04-10 01:03:51,776 root         INFO     Train Epoch: 153 [3072/8000 (38%)]\tTotal Loss: 0.190033\n",
      "Reconstruction: 0.187482, Regularization: 0.002552\n",
      "2019-04-10 01:03:51,839 root         INFO     Train Epoch: 153 [3584/8000 (45%)]\tTotal Loss: 0.161096\n",
      "Reconstruction: 0.159935, Regularization: 0.001160\n",
      "2019-04-10 01:03:51,902 root         INFO     Train Epoch: 153 [4096/8000 (51%)]\tTotal Loss: 0.168924\n",
      "Reconstruction: 0.166721, Regularization: 0.002203\n",
      "2019-04-10 01:03:51,965 root         INFO     Train Epoch: 153 [4608/8000 (58%)]\tTotal Loss: 0.161002\n",
      "Reconstruction: 0.159664, Regularization: 0.001338\n",
      "2019-04-10 01:03:52,028 root         INFO     Train Epoch: 153 [5120/8000 (64%)]\tTotal Loss: 0.161533\n",
      "Reconstruction: 0.160024, Regularization: 0.001509\n",
      "2019-04-10 01:03:52,091 root         INFO     Train Epoch: 153 [5632/8000 (70%)]\tTotal Loss: 0.170818\n",
      "Reconstruction: 0.168941, Regularization: 0.001878\n",
      "2019-04-10 01:03:52,154 root         INFO     Train Epoch: 153 [6144/8000 (77%)]\tTotal Loss: 0.164880\n",
      "Reconstruction: 0.163442, Regularization: 0.001438\n",
      "2019-04-10 01:03:52,217 root         INFO     Train Epoch: 153 [6656/8000 (83%)]\tTotal Loss: 0.168834\n",
      "Reconstruction: 0.167081, Regularization: 0.001753\n",
      "2019-04-10 01:03:52,280 root         INFO     Train Epoch: 153 [7168/8000 (90%)]\tTotal Loss: 0.164601\n",
      "Reconstruction: 0.163049, Regularization: 0.001551\n",
      "2019-04-10 01:03:52,343 root         INFO     Train Epoch: 153 [7680/8000 (96%)]\tTotal Loss: 0.173358\n",
      "Reconstruction: 0.171255, Regularization: 0.002103\n",
      "2019-04-10 01:03:52,396 root         INFO     ====> Epoch: 153 Average loss: 0.1660\n",
      "2019-04-10 01:03:52,421 root         INFO     Train Epoch: 154 [0/8000 (0%)]\tTotal Loss: 0.170576\n",
      "Reconstruction: 0.168928, Regularization: 0.001647\n",
      "2019-04-10 01:03:52,485 root         INFO     Train Epoch: 154 [512/8000 (6%)]\tTotal Loss: 0.178304\n",
      "Reconstruction: 0.176104, Regularization: 0.002200\n",
      "2019-04-10 01:03:52,549 root         INFO     Train Epoch: 154 [1024/8000 (13%)]\tTotal Loss: 0.177471\n",
      "Reconstruction: 0.175307, Regularization: 0.002165\n",
      "2019-04-10 01:03:52,613 root         INFO     Train Epoch: 154 [1536/8000 (19%)]\tTotal Loss: 0.161376\n",
      "Reconstruction: 0.160074, Regularization: 0.001302\n",
      "2019-04-10 01:03:52,677 root         INFO     Train Epoch: 154 [2048/8000 (26%)]\tTotal Loss: 0.166542\n",
      "Reconstruction: 0.164840, Regularization: 0.001702\n",
      "2019-04-10 01:03:52,741 root         INFO     Train Epoch: 154 [2560/8000 (32%)]\tTotal Loss: 0.170264\n",
      "Reconstruction: 0.168224, Regularization: 0.002040\n",
      "2019-04-10 01:03:52,805 root         INFO     Train Epoch: 154 [3072/8000 (38%)]\tTotal Loss: 0.172913\n",
      "Reconstruction: 0.170888, Regularization: 0.002025\n",
      "2019-04-10 01:03:52,869 root         INFO     Train Epoch: 154 [3584/8000 (45%)]\tTotal Loss: 0.159233\n",
      "Reconstruction: 0.157827, Regularization: 0.001406\n",
      "2019-04-10 01:03:52,933 root         INFO     Train Epoch: 154 [4096/8000 (51%)]\tTotal Loss: 0.169684\n",
      "Reconstruction: 0.167963, Regularization: 0.001720\n",
      "2019-04-10 01:03:52,996 root         INFO     Train Epoch: 154 [4608/8000 (58%)]\tTotal Loss: 0.163766\n",
      "Reconstruction: 0.162231, Regularization: 0.001535\n",
      "2019-04-10 01:03:53,059 root         INFO     Train Epoch: 154 [5120/8000 (64%)]\tTotal Loss: 0.156392\n",
      "Reconstruction: 0.155260, Regularization: 0.001132\n",
      "2019-04-10 01:03:53,122 root         INFO     Train Epoch: 154 [5632/8000 (70%)]\tTotal Loss: 0.164364\n",
      "Reconstruction: 0.162683, Regularization: 0.001681\n",
      "2019-04-10 01:03:53,185 root         INFO     Train Epoch: 154 [6144/8000 (77%)]\tTotal Loss: 0.167556\n",
      "Reconstruction: 0.165804, Regularization: 0.001753\n",
      "2019-04-10 01:03:53,247 root         INFO     Train Epoch: 154 [6656/8000 (83%)]\tTotal Loss: 0.161923\n",
      "Reconstruction: 0.160332, Regularization: 0.001592\n",
      "2019-04-10 01:03:53,308 root         INFO     Train Epoch: 154 [7168/8000 (90%)]\tTotal Loss: 0.173586\n",
      "Reconstruction: 0.171331, Regularization: 0.002255\n",
      "2019-04-10 01:03:53,369 root         INFO     Train Epoch: 154 [7680/8000 (96%)]\tTotal Loss: 0.169546\n",
      "Reconstruction: 0.167746, Regularization: 0.001800\n",
      "2019-04-10 01:03:53,423 root         INFO     ====> Epoch: 154 Average loss: 0.1661\n",
      "2019-04-10 01:03:53,448 root         INFO     Train Epoch: 155 [0/8000 (0%)]\tTotal Loss: 0.163872\n",
      "Reconstruction: 0.162278, Regularization: 0.001595\n",
      "2019-04-10 01:03:53,512 root         INFO     Train Epoch: 155 [512/8000 (6%)]\tTotal Loss: 0.160944\n",
      "Reconstruction: 0.159718, Regularization: 0.001226\n",
      "2019-04-10 01:03:53,575 root         INFO     Train Epoch: 155 [1024/8000 (13%)]\tTotal Loss: 0.168897\n",
      "Reconstruction: 0.167208, Regularization: 0.001689\n",
      "2019-04-10 01:03:53,638 root         INFO     Train Epoch: 155 [1536/8000 (19%)]\tTotal Loss: 0.153212\n",
      "Reconstruction: 0.152034, Regularization: 0.001178\n",
      "2019-04-10 01:03:53,702 root         INFO     Train Epoch: 155 [2048/8000 (26%)]\tTotal Loss: 0.169461\n",
      "Reconstruction: 0.167539, Regularization: 0.001922\n",
      "2019-04-10 01:03:53,766 root         INFO     Train Epoch: 155 [2560/8000 (32%)]\tTotal Loss: 0.164330\n",
      "Reconstruction: 0.162850, Regularization: 0.001480\n",
      "2019-04-10 01:03:53,829 root         INFO     Train Epoch: 155 [3072/8000 (38%)]\tTotal Loss: 0.171860\n",
      "Reconstruction: 0.169779, Regularization: 0.002081\n",
      "2019-04-10 01:03:53,892 root         INFO     Train Epoch: 155 [3584/8000 (45%)]\tTotal Loss: 0.176909\n",
      "Reconstruction: 0.174617, Regularization: 0.002292\n",
      "2019-04-10 01:03:53,955 root         INFO     Train Epoch: 155 [4096/8000 (51%)]\tTotal Loss: 0.162150\n",
      "Reconstruction: 0.160423, Regularization: 0.001727\n",
      "2019-04-10 01:03:54,018 root         INFO     Train Epoch: 155 [4608/8000 (58%)]\tTotal Loss: 0.180578\n",
      "Reconstruction: 0.177955, Regularization: 0.002624\n",
      "2019-04-10 01:03:54,081 root         INFO     Train Epoch: 155 [5120/8000 (64%)]\tTotal Loss: 0.178984\n",
      "Reconstruction: 0.176488, Regularization: 0.002496\n",
      "2019-04-10 01:03:54,145 root         INFO     Train Epoch: 155 [5632/8000 (70%)]\tTotal Loss: 0.175821\n",
      "Reconstruction: 0.173417, Regularization: 0.002404\n",
      "2019-04-10 01:03:54,209 root         INFO     Train Epoch: 155 [6144/8000 (77%)]\tTotal Loss: 0.175802\n",
      "Reconstruction: 0.173546, Regularization: 0.002256\n",
      "2019-04-10 01:03:54,272 root         INFO     Train Epoch: 155 [6656/8000 (83%)]\tTotal Loss: 0.157577\n",
      "Reconstruction: 0.156372, Regularization: 0.001206\n",
      "2019-04-10 01:03:54,335 root         INFO     Train Epoch: 155 [7168/8000 (90%)]\tTotal Loss: 0.169288\n",
      "Reconstruction: 0.167217, Regularization: 0.002071\n",
      "2019-04-10 01:03:54,398 root         INFO     Train Epoch: 155 [7680/8000 (96%)]\tTotal Loss: 0.164765\n",
      "Reconstruction: 0.162841, Regularization: 0.001924\n",
      "2019-04-10 01:03:54,452 root         INFO     ====> Epoch: 155 Average loss: 0.1660\n",
      "2019-04-10 01:03:54,476 root         INFO     Train Epoch: 156 [0/8000 (0%)]\tTotal Loss: 0.168494\n",
      "Reconstruction: 0.166422, Regularization: 0.002071\n",
      "2019-04-10 01:03:54,540 root         INFO     Train Epoch: 156 [512/8000 (6%)]\tTotal Loss: 0.175198\n",
      "Reconstruction: 0.173313, Regularization: 0.001885\n",
      "2019-04-10 01:03:54,603 root         INFO     Train Epoch: 156 [1024/8000 (13%)]\tTotal Loss: 0.168834\n",
      "Reconstruction: 0.166847, Regularization: 0.001987\n",
      "2019-04-10 01:03:54,668 root         INFO     Train Epoch: 156 [1536/8000 (19%)]\tTotal Loss: 0.167038\n",
      "Reconstruction: 0.165323, Regularization: 0.001714\n",
      "2019-04-10 01:03:54,732 root         INFO     Train Epoch: 156 [2048/8000 (26%)]\tTotal Loss: 0.162385\n",
      "Reconstruction: 0.160727, Regularization: 0.001658\n",
      "2019-04-10 01:03:54,795 root         INFO     Train Epoch: 156 [2560/8000 (32%)]\tTotal Loss: 0.159537\n",
      "Reconstruction: 0.158255, Regularization: 0.001282\n",
      "2019-04-10 01:03:54,859 root         INFO     Train Epoch: 156 [3072/8000 (38%)]\tTotal Loss: 0.161332\n",
      "Reconstruction: 0.159868, Regularization: 0.001464\n",
      "2019-04-10 01:03:54,924 root         INFO     Train Epoch: 156 [3584/8000 (45%)]\tTotal Loss: 0.152623\n",
      "Reconstruction: 0.151583, Regularization: 0.001040\n",
      "2019-04-10 01:03:54,988 root         INFO     Train Epoch: 156 [4096/8000 (51%)]\tTotal Loss: 0.166068\n",
      "Reconstruction: 0.163943, Regularization: 0.002125\n",
      "2019-04-10 01:03:55,051 root         INFO     Train Epoch: 156 [4608/8000 (58%)]\tTotal Loss: 0.161371\n",
      "Reconstruction: 0.159652, Regularization: 0.001720\n",
      "2019-04-10 01:03:55,115 root         INFO     Train Epoch: 156 [5120/8000 (64%)]\tTotal Loss: 0.174293\n",
      "Reconstruction: 0.172029, Regularization: 0.002264\n",
      "2019-04-10 01:03:55,180 root         INFO     Train Epoch: 156 [5632/8000 (70%)]\tTotal Loss: 0.160161\n",
      "Reconstruction: 0.158632, Regularization: 0.001529\n",
      "2019-04-10 01:03:55,243 root         INFO     Train Epoch: 156 [6144/8000 (77%)]\tTotal Loss: 0.168257\n",
      "Reconstruction: 0.165658, Regularization: 0.002599\n",
      "2019-04-10 01:03:55,307 root         INFO     Train Epoch: 156 [6656/8000 (83%)]\tTotal Loss: 0.157886\n",
      "Reconstruction: 0.156624, Regularization: 0.001262\n",
      "2019-04-10 01:03:55,371 root         INFO     Train Epoch: 156 [7168/8000 (90%)]\tTotal Loss: 0.198364\n",
      "Reconstruction: 0.194982, Regularization: 0.003382\n",
      "2019-04-10 01:03:55,435 root         INFO     Train Epoch: 156 [7680/8000 (96%)]\tTotal Loss: 0.178465\n",
      "Reconstruction: 0.176026, Regularization: 0.002440\n",
      "2019-04-10 01:03:55,488 root         INFO     ====> Epoch: 156 Average loss: 0.1658\n",
      "2019-04-10 01:03:55,513 root         INFO     Train Epoch: 157 [0/8000 (0%)]\tTotal Loss: 0.159915\n",
      "Reconstruction: 0.158240, Regularization: 0.001675\n",
      "2019-04-10 01:03:55,578 root         INFO     Train Epoch: 157 [512/8000 (6%)]\tTotal Loss: 0.165612\n",
      "Reconstruction: 0.163718, Regularization: 0.001894\n",
      "2019-04-10 01:03:55,641 root         INFO     Train Epoch: 157 [1024/8000 (13%)]\tTotal Loss: 0.163238\n",
      "Reconstruction: 0.161872, Regularization: 0.001367\n",
      "2019-04-10 01:03:55,705 root         INFO     Train Epoch: 157 [1536/8000 (19%)]\tTotal Loss: 0.170178\n",
      "Reconstruction: 0.168068, Regularization: 0.002110\n",
      "2019-04-10 01:03:55,769 root         INFO     Train Epoch: 157 [2048/8000 (26%)]\tTotal Loss: 0.166469\n",
      "Reconstruction: 0.164514, Regularization: 0.001956\n",
      "2019-04-10 01:03:55,832 root         INFO     Train Epoch: 157 [2560/8000 (32%)]\tTotal Loss: 0.176672\n",
      "Reconstruction: 0.174136, Regularization: 0.002536\n",
      "2019-04-10 01:03:55,896 root         INFO     Train Epoch: 157 [3072/8000 (38%)]\tTotal Loss: 0.171028\n",
      "Reconstruction: 0.169061, Regularization: 0.001967\n",
      "2019-04-10 01:03:55,959 root         INFO     Train Epoch: 157 [3584/8000 (45%)]\tTotal Loss: 0.186478\n",
      "Reconstruction: 0.183488, Regularization: 0.002990\n",
      "2019-04-10 01:03:56,022 root         INFO     Train Epoch: 157 [4096/8000 (51%)]\tTotal Loss: 0.156891\n",
      "Reconstruction: 0.155631, Regularization: 0.001260\n",
      "2019-04-10 01:03:56,085 root         INFO     Train Epoch: 157 [4608/8000 (58%)]\tTotal Loss: 0.173759\n",
      "Reconstruction: 0.171414, Regularization: 0.002345\n",
      "2019-04-10 01:03:56,148 root         INFO     Train Epoch: 157 [5120/8000 (64%)]\tTotal Loss: 0.164644\n",
      "Reconstruction: 0.162543, Regularization: 0.002100\n",
      "2019-04-10 01:03:56,211 root         INFO     Train Epoch: 157 [5632/8000 (70%)]\tTotal Loss: 0.174714\n",
      "Reconstruction: 0.172522, Regularization: 0.002192\n",
      "2019-04-10 01:03:56,274 root         INFO     Train Epoch: 157 [6144/8000 (77%)]\tTotal Loss: 0.166182\n",
      "Reconstruction: 0.164333, Regularization: 0.001850\n",
      "2019-04-10 01:03:56,336 root         INFO     Train Epoch: 157 [6656/8000 (83%)]\tTotal Loss: 0.163868\n",
      "Reconstruction: 0.162075, Regularization: 0.001793\n",
      "2019-04-10 01:03:56,398 root         INFO     Train Epoch: 157 [7168/8000 (90%)]\tTotal Loss: 0.178743\n",
      "Reconstruction: 0.175788, Regularization: 0.002955\n",
      "2019-04-10 01:03:56,460 root         INFO     Train Epoch: 157 [7680/8000 (96%)]\tTotal Loss: 0.169669\n",
      "Reconstruction: 0.167656, Regularization: 0.002013\n",
      "2019-04-10 01:03:56,513 root         INFO     ====> Epoch: 157 Average loss: 0.1659\n",
      "2019-04-10 01:03:56,537 root         INFO     Train Epoch: 158 [0/8000 (0%)]\tTotal Loss: 0.164273\n",
      "Reconstruction: 0.162516, Regularization: 0.001757\n",
      "2019-04-10 01:03:56,601 root         INFO     Train Epoch: 158 [512/8000 (6%)]\tTotal Loss: 0.163715\n",
      "Reconstruction: 0.161868, Regularization: 0.001847\n",
      "2019-04-10 01:03:56,666 root         INFO     Train Epoch: 158 [1024/8000 (13%)]\tTotal Loss: 0.170854\n",
      "Reconstruction: 0.168595, Regularization: 0.002259\n",
      "2019-04-10 01:03:56,730 root         INFO     Train Epoch: 158 [1536/8000 (19%)]\tTotal Loss: 0.163835\n",
      "Reconstruction: 0.161896, Regularization: 0.001939\n",
      "2019-04-10 01:03:56,793 root         INFO     Train Epoch: 158 [2048/8000 (26%)]\tTotal Loss: 0.166938\n",
      "Reconstruction: 0.165044, Regularization: 0.001894\n",
      "2019-04-10 01:03:56,857 root         INFO     Train Epoch: 158 [2560/8000 (32%)]\tTotal Loss: 0.164785\n",
      "Reconstruction: 0.162929, Regularization: 0.001856\n",
      "2019-04-10 01:03:56,921 root         INFO     Train Epoch: 158 [3072/8000 (38%)]\tTotal Loss: 0.163314\n",
      "Reconstruction: 0.161276, Regularization: 0.002038\n",
      "2019-04-10 01:03:56,984 root         INFO     Train Epoch: 158 [3584/8000 (45%)]\tTotal Loss: 0.171970\n",
      "Reconstruction: 0.169614, Regularization: 0.002356\n",
      "2019-04-10 01:03:57,047 root         INFO     Train Epoch: 158 [4096/8000 (51%)]\tTotal Loss: 0.167323\n",
      "Reconstruction: 0.165268, Regularization: 0.002054\n",
      "2019-04-10 01:03:57,111 root         INFO     Train Epoch: 158 [4608/8000 (58%)]\tTotal Loss: 0.173060\n",
      "Reconstruction: 0.170787, Regularization: 0.002273\n",
      "2019-04-10 01:03:57,175 root         INFO     Train Epoch: 158 [5120/8000 (64%)]\tTotal Loss: 0.165777\n",
      "Reconstruction: 0.163780, Regularization: 0.001998\n",
      "2019-04-10 01:03:57,239 root         INFO     Train Epoch: 158 [5632/8000 (70%)]\tTotal Loss: 0.164436\n",
      "Reconstruction: 0.162289, Regularization: 0.002147\n",
      "2019-04-10 01:03:57,303 root         INFO     Train Epoch: 158 [6144/8000 (77%)]\tTotal Loss: 0.154460\n",
      "Reconstruction: 0.153417, Regularization: 0.001043\n",
      "2019-04-10 01:03:57,367 root         INFO     Train Epoch: 158 [6656/8000 (83%)]\tTotal Loss: 0.163471\n",
      "Reconstruction: 0.161705, Regularization: 0.001765\n",
      "2019-04-10 01:03:57,431 root         INFO     Train Epoch: 158 [7168/8000 (90%)]\tTotal Loss: 0.168898\n",
      "Reconstruction: 0.166633, Regularization: 0.002266\n",
      "2019-04-10 01:03:57,495 root         INFO     Train Epoch: 158 [7680/8000 (96%)]\tTotal Loss: 0.159968\n",
      "Reconstruction: 0.158226, Regularization: 0.001742\n",
      "2019-04-10 01:03:57,549 root         INFO     ====> Epoch: 158 Average loss: 0.1659\n",
      "2019-04-10 01:03:57,573 root         INFO     Train Epoch: 159 [0/8000 (0%)]\tTotal Loss: 0.170350\n",
      "Reconstruction: 0.168167, Regularization: 0.002183\n",
      "2019-04-10 01:03:57,636 root         INFO     Train Epoch: 159 [512/8000 (6%)]\tTotal Loss: 0.160172\n",
      "Reconstruction: 0.158461, Regularization: 0.001712\n",
      "2019-04-10 01:03:57,699 root         INFO     Train Epoch: 159 [1024/8000 (13%)]\tTotal Loss: 0.166172\n",
      "Reconstruction: 0.164285, Regularization: 0.001887\n",
      "2019-04-10 01:03:57,763 root         INFO     Train Epoch: 159 [1536/8000 (19%)]\tTotal Loss: 0.164334\n",
      "Reconstruction: 0.162491, Regularization: 0.001842\n",
      "2019-04-10 01:03:57,826 root         INFO     Train Epoch: 159 [2048/8000 (26%)]\tTotal Loss: 0.172979\n",
      "Reconstruction: 0.170503, Regularization: 0.002476\n",
      "2019-04-10 01:03:57,890 root         INFO     Train Epoch: 159 [2560/8000 (32%)]\tTotal Loss: 0.164332\n",
      "Reconstruction: 0.162498, Regularization: 0.001834\n",
      "2019-04-10 01:03:57,954 root         INFO     Train Epoch: 159 [3072/8000 (38%)]\tTotal Loss: 0.162742\n",
      "Reconstruction: 0.160637, Regularization: 0.002106\n",
      "2019-04-10 01:03:58,017 root         INFO     Train Epoch: 159 [3584/8000 (45%)]\tTotal Loss: 0.167019\n",
      "Reconstruction: 0.165096, Regularization: 0.001923\n",
      "2019-04-10 01:03:58,081 root         INFO     Train Epoch: 159 [4096/8000 (51%)]\tTotal Loss: 0.166342\n",
      "Reconstruction: 0.164398, Regularization: 0.001944\n",
      "2019-04-10 01:03:58,144 root         INFO     Train Epoch: 159 [4608/8000 (58%)]\tTotal Loss: 0.168805\n",
      "Reconstruction: 0.166595, Regularization: 0.002210\n",
      "2019-04-10 01:03:58,207 root         INFO     Train Epoch: 159 [5120/8000 (64%)]\tTotal Loss: 0.164231\n",
      "Reconstruction: 0.162303, Regularization: 0.001929\n",
      "2019-04-10 01:03:58,270 root         INFO     Train Epoch: 159 [5632/8000 (70%)]\tTotal Loss: 0.172610\n",
      "Reconstruction: 0.170108, Regularization: 0.002502\n",
      "2019-04-10 01:03:58,332 root         INFO     Train Epoch: 159 [6144/8000 (77%)]\tTotal Loss: 0.153951\n",
      "Reconstruction: 0.152726, Regularization: 0.001225\n",
      "2019-04-10 01:03:58,394 root         INFO     Train Epoch: 159 [6656/8000 (83%)]\tTotal Loss: 0.168226\n",
      "Reconstruction: 0.165998, Regularization: 0.002228\n",
      "2019-04-10 01:03:58,456 root         INFO     Train Epoch: 159 [7168/8000 (90%)]\tTotal Loss: 0.156317\n",
      "Reconstruction: 0.154717, Regularization: 0.001599\n",
      "2019-04-10 01:03:58,519 root         INFO     Train Epoch: 159 [7680/8000 (96%)]\tTotal Loss: 0.163742\n",
      "Reconstruction: 0.161788, Regularization: 0.001954\n",
      "2019-04-10 01:03:58,573 root         INFO     ====> Epoch: 159 Average loss: 0.1657\n",
      "2019-04-10 01:03:58,596 root         INFO     Train Epoch: 160 [0/8000 (0%)]\tTotal Loss: 0.171013\n",
      "Reconstruction: 0.168712, Regularization: 0.002301\n",
      "2019-04-10 01:03:58,661 root         INFO     Train Epoch: 160 [512/8000 (6%)]\tTotal Loss: 0.159582\n",
      "Reconstruction: 0.157883, Regularization: 0.001699\n",
      "2019-04-10 01:03:58,724 root         INFO     Train Epoch: 160 [1024/8000 (13%)]\tTotal Loss: 0.184997\n",
      "Reconstruction: 0.181471, Regularization: 0.003526\n",
      "2019-04-10 01:03:58,787 root         INFO     Train Epoch: 160 [1536/8000 (19%)]\tTotal Loss: 0.172269\n",
      "Reconstruction: 0.169945, Regularization: 0.002324\n",
      "2019-04-10 01:03:58,850 root         INFO     Train Epoch: 160 [2048/8000 (26%)]\tTotal Loss: 0.162285\n",
      "Reconstruction: 0.160630, Regularization: 0.001656\n",
      "2019-04-10 01:03:58,913 root         INFO     Train Epoch: 160 [2560/8000 (32%)]\tTotal Loss: 0.177490\n",
      "Reconstruction: 0.174710, Regularization: 0.002779\n",
      "2019-04-10 01:03:58,977 root         INFO     Train Epoch: 160 [3072/8000 (38%)]\tTotal Loss: 0.166627\n",
      "Reconstruction: 0.164461, Regularization: 0.002166\n",
      "2019-04-10 01:03:59,040 root         INFO     Train Epoch: 160 [3584/8000 (45%)]\tTotal Loss: 0.157735\n",
      "Reconstruction: 0.156367, Regularization: 0.001368\n",
      "2019-04-10 01:03:59,103 root         INFO     Train Epoch: 160 [4096/8000 (51%)]\tTotal Loss: 0.162021\n",
      "Reconstruction: 0.159844, Regularization: 0.002177\n",
      "2019-04-10 01:03:59,167 root         INFO     Train Epoch: 160 [4608/8000 (58%)]\tTotal Loss: 0.173002\n",
      "Reconstruction: 0.170619, Regularization: 0.002383\n",
      "2019-04-10 01:03:59,230 root         INFO     Train Epoch: 160 [5120/8000 (64%)]\tTotal Loss: 0.176731\n",
      "Reconstruction: 0.174133, Regularization: 0.002598\n",
      "2019-04-10 01:03:59,293 root         INFO     Train Epoch: 160 [5632/8000 (70%)]\tTotal Loss: 0.161938\n",
      "Reconstruction: 0.159656, Regularization: 0.002282\n",
      "2019-04-10 01:03:59,356 root         INFO     Train Epoch: 160 [6144/8000 (77%)]\tTotal Loss: 0.162389\n",
      "Reconstruction: 0.160485, Regularization: 0.001904\n",
      "2019-04-10 01:03:59,420 root         INFO     Train Epoch: 160 [6656/8000 (83%)]\tTotal Loss: 0.162328\n",
      "Reconstruction: 0.160430, Regularization: 0.001898\n",
      "2019-04-10 01:03:59,483 root         INFO     Train Epoch: 160 [7168/8000 (90%)]\tTotal Loss: 0.156201\n",
      "Reconstruction: 0.154797, Regularization: 0.001404\n",
      "2019-04-10 01:03:59,546 root         INFO     Train Epoch: 160 [7680/8000 (96%)]\tTotal Loss: 0.160440\n",
      "Reconstruction: 0.158415, Regularization: 0.002024\n",
      "2019-04-10 01:03:59,600 root         INFO     ====> Epoch: 160 Average loss: 0.1657\n",
      "2019-04-10 01:03:59,624 root         INFO     Train Epoch: 161 [0/8000 (0%)]\tTotal Loss: 0.174690\n",
      "Reconstruction: 0.172198, Regularization: 0.002492\n",
      "2019-04-10 01:03:59,688 root         INFO     Train Epoch: 161 [512/8000 (6%)]\tTotal Loss: 0.159763\n",
      "Reconstruction: 0.158214, Regularization: 0.001550\n",
      "2019-04-10 01:03:59,752 root         INFO     Train Epoch: 161 [1024/8000 (13%)]\tTotal Loss: 0.160899\n",
      "Reconstruction: 0.159126, Regularization: 0.001773\n",
      "2019-04-10 01:03:59,815 root         INFO     Train Epoch: 161 [1536/8000 (19%)]\tTotal Loss: 0.156639\n",
      "Reconstruction: 0.155053, Regularization: 0.001586\n",
      "2019-04-10 01:03:59,879 root         INFO     Train Epoch: 161 [2048/8000 (26%)]\tTotal Loss: 0.159745\n",
      "Reconstruction: 0.157853, Regularization: 0.001892\n",
      "2019-04-10 01:03:59,942 root         INFO     Train Epoch: 161 [2560/8000 (32%)]\tTotal Loss: 0.165711\n",
      "Reconstruction: 0.163484, Regularization: 0.002228\n",
      "2019-04-10 01:04:00,006 root         INFO     Train Epoch: 161 [3072/8000 (38%)]\tTotal Loss: 0.165738\n",
      "Reconstruction: 0.163615, Regularization: 0.002122\n",
      "2019-04-10 01:04:00,069 root         INFO     Train Epoch: 161 [3584/8000 (45%)]\tTotal Loss: 0.179658\n",
      "Reconstruction: 0.176503, Regularization: 0.003154\n",
      "2019-04-10 01:04:00,133 root         INFO     Train Epoch: 161 [4096/8000 (51%)]\tTotal Loss: 0.168764\n",
      "Reconstruction: 0.166159, Regularization: 0.002605\n",
      "2019-04-10 01:04:00,197 root         INFO     Train Epoch: 161 [4608/8000 (58%)]\tTotal Loss: 0.156052\n",
      "Reconstruction: 0.154185, Regularization: 0.001867\n",
      "2019-04-10 01:04:00,261 root         INFO     Train Epoch: 161 [5120/8000 (64%)]\tTotal Loss: 0.183277\n",
      "Reconstruction: 0.179836, Regularization: 0.003442\n",
      "2019-04-10 01:04:00,324 root         INFO     Train Epoch: 161 [5632/8000 (70%)]\tTotal Loss: 0.163020\n",
      "Reconstruction: 0.160840, Regularization: 0.002180\n",
      "2019-04-10 01:04:00,388 root         INFO     Train Epoch: 161 [6144/8000 (77%)]\tTotal Loss: 0.166148\n",
      "Reconstruction: 0.163914, Regularization: 0.002234\n",
      "2019-04-10 01:04:00,451 root         INFO     Train Epoch: 161 [6656/8000 (83%)]\tTotal Loss: 0.162821\n",
      "Reconstruction: 0.160999, Regularization: 0.001823\n",
      "2019-04-10 01:04:00,515 root         INFO     Train Epoch: 161 [7168/8000 (90%)]\tTotal Loss: 0.161727\n",
      "Reconstruction: 0.159887, Regularization: 0.001840\n",
      "2019-04-10 01:04:00,578 root         INFO     Train Epoch: 161 [7680/8000 (96%)]\tTotal Loss: 0.147064\n",
      "Reconstruction: 0.145995, Regularization: 0.001069\n",
      "2019-04-10 01:04:00,632 root         INFO     ====> Epoch: 161 Average loss: 0.1660\n",
      "2019-04-10 01:04:00,656 root         INFO     Train Epoch: 162 [0/8000 (0%)]\tTotal Loss: 0.164562\n",
      "Reconstruction: 0.162643, Regularization: 0.001919\n",
      "2019-04-10 01:04:00,720 root         INFO     Train Epoch: 162 [512/8000 (6%)]\tTotal Loss: 0.156480\n",
      "Reconstruction: 0.154986, Regularization: 0.001494\n",
      "2019-04-10 01:04:00,783 root         INFO     Train Epoch: 162 [1024/8000 (13%)]\tTotal Loss: 0.167774\n",
      "Reconstruction: 0.165524, Regularization: 0.002250\n",
      "2019-04-10 01:04:00,845 root         INFO     Train Epoch: 162 [1536/8000 (19%)]\tTotal Loss: 0.166064\n",
      "Reconstruction: 0.163605, Regularization: 0.002459\n",
      "2019-04-10 01:04:00,906 root         INFO     Train Epoch: 162 [2048/8000 (26%)]\tTotal Loss: 0.160434\n",
      "Reconstruction: 0.158613, Regularization: 0.001821\n",
      "2019-04-10 01:04:00,968 root         INFO     Train Epoch: 162 [2560/8000 (32%)]\tTotal Loss: 0.168781\n",
      "Reconstruction: 0.166311, Regularization: 0.002470\n",
      "2019-04-10 01:04:01,030 root         INFO     Train Epoch: 162 [3072/8000 (38%)]\tTotal Loss: 0.169998\n",
      "Reconstruction: 0.167414, Regularization: 0.002584\n",
      "2019-04-10 01:04:01,091 root         INFO     Train Epoch: 162 [3584/8000 (45%)]\tTotal Loss: 0.175299\n",
      "Reconstruction: 0.172569, Regularization: 0.002730\n",
      "2019-04-10 01:04:01,153 root         INFO     Train Epoch: 162 [4096/8000 (51%)]\tTotal Loss: 0.158605\n",
      "Reconstruction: 0.156662, Regularization: 0.001943\n",
      "2019-04-10 01:04:01,215 root         INFO     Train Epoch: 162 [4608/8000 (58%)]\tTotal Loss: 0.173645\n",
      "Reconstruction: 0.170748, Regularization: 0.002897\n",
      "2019-04-10 01:04:01,276 root         INFO     Train Epoch: 162 [5120/8000 (64%)]\tTotal Loss: 0.161493\n",
      "Reconstruction: 0.159645, Regularization: 0.001849\n",
      "2019-04-10 01:04:01,340 root         INFO     Train Epoch: 162 [5632/8000 (70%)]\tTotal Loss: 0.157790\n",
      "Reconstruction: 0.156054, Regularization: 0.001737\n",
      "2019-04-10 01:04:01,403 root         INFO     Train Epoch: 162 [6144/8000 (77%)]\tTotal Loss: 0.165572\n",
      "Reconstruction: 0.163337, Regularization: 0.002235\n",
      "2019-04-10 01:04:01,466 root         INFO     Train Epoch: 162 [6656/8000 (83%)]\tTotal Loss: 0.169063\n",
      "Reconstruction: 0.166734, Regularization: 0.002329\n",
      "2019-04-10 01:04:01,529 root         INFO     Train Epoch: 162 [7168/8000 (90%)]\tTotal Loss: 0.173610\n",
      "Reconstruction: 0.170893, Regularization: 0.002717\n",
      "2019-04-10 01:04:01,593 root         INFO     Train Epoch: 162 [7680/8000 (96%)]\tTotal Loss: 0.167364\n",
      "Reconstruction: 0.164885, Regularization: 0.002480\n",
      "2019-04-10 01:04:01,646 root         INFO     ====> Epoch: 162 Average loss: 0.1656\n",
      "2019-04-10 01:04:01,670 root         INFO     Train Epoch: 163 [0/8000 (0%)]\tTotal Loss: 0.182052\n",
      "Reconstruction: 0.178964, Regularization: 0.003088\n",
      "2019-04-10 01:04:01,733 root         INFO     Train Epoch: 163 [512/8000 (6%)]\tTotal Loss: 0.162006\n",
      "Reconstruction: 0.159945, Regularization: 0.002060\n",
      "2019-04-10 01:04:01,796 root         INFO     Train Epoch: 163 [1024/8000 (13%)]\tTotal Loss: 0.163130\n",
      "Reconstruction: 0.160825, Regularization: 0.002304\n",
      "2019-04-10 01:04:01,858 root         INFO     Train Epoch: 163 [1536/8000 (19%)]\tTotal Loss: 0.174858\n",
      "Reconstruction: 0.172108, Regularization: 0.002750\n",
      "2019-04-10 01:04:01,921 root         INFO     Train Epoch: 163 [2048/8000 (26%)]\tTotal Loss: 0.165227\n",
      "Reconstruction: 0.162993, Regularization: 0.002233\n",
      "2019-04-10 01:04:01,984 root         INFO     Train Epoch: 163 [2560/8000 (32%)]\tTotal Loss: 0.171508\n",
      "Reconstruction: 0.168626, Regularization: 0.002882\n",
      "2019-04-10 01:04:02,047 root         INFO     Train Epoch: 163 [3072/8000 (38%)]\tTotal Loss: 0.172774\n",
      "Reconstruction: 0.170284, Regularization: 0.002490\n",
      "2019-04-10 01:04:02,109 root         INFO     Train Epoch: 163 [3584/8000 (45%)]\tTotal Loss: 0.161171\n",
      "Reconstruction: 0.159249, Regularization: 0.001922\n",
      "2019-04-10 01:04:02,172 root         INFO     Train Epoch: 163 [4096/8000 (51%)]\tTotal Loss: 0.171741\n",
      "Reconstruction: 0.168847, Regularization: 0.002894\n",
      "2019-04-10 01:04:02,234 root         INFO     Train Epoch: 163 [4608/8000 (58%)]\tTotal Loss: 0.177818\n",
      "Reconstruction: 0.175147, Regularization: 0.002671\n",
      "2019-04-10 01:04:02,297 root         INFO     Train Epoch: 163 [5120/8000 (64%)]\tTotal Loss: 0.174275\n",
      "Reconstruction: 0.171503, Regularization: 0.002773\n",
      "2019-04-10 01:04:02,361 root         INFO     Train Epoch: 163 [5632/8000 (70%)]\tTotal Loss: 0.173121\n",
      "Reconstruction: 0.170408, Regularization: 0.002713\n",
      "2019-04-10 01:04:02,424 root         INFO     Train Epoch: 163 [6144/8000 (77%)]\tTotal Loss: 0.163390\n",
      "Reconstruction: 0.161063, Regularization: 0.002327\n",
      "2019-04-10 01:04:02,486 root         INFO     Train Epoch: 163 [6656/8000 (83%)]\tTotal Loss: 0.159462\n",
      "Reconstruction: 0.157702, Regularization: 0.001760\n",
      "2019-04-10 01:04:02,549 root         INFO     Train Epoch: 163 [7168/8000 (90%)]\tTotal Loss: 0.164867\n",
      "Reconstruction: 0.162826, Regularization: 0.002041\n",
      "2019-04-10 01:04:02,612 root         INFO     Train Epoch: 163 [7680/8000 (96%)]\tTotal Loss: 0.150672\n",
      "Reconstruction: 0.149363, Regularization: 0.001310\n",
      "2019-04-10 01:04:02,666 root         INFO     ====> Epoch: 163 Average loss: 0.1659\n",
      "2019-04-10 01:04:02,689 root         INFO     Train Epoch: 164 [0/8000 (0%)]\tTotal Loss: 0.173706\n",
      "Reconstruction: 0.170904, Regularization: 0.002803\n",
      "2019-04-10 01:04:02,753 root         INFO     Train Epoch: 164 [512/8000 (6%)]\tTotal Loss: 0.168399\n",
      "Reconstruction: 0.165956, Regularization: 0.002443\n",
      "2019-04-10 01:04:02,817 root         INFO     Train Epoch: 164 [1024/8000 (13%)]\tTotal Loss: 0.159905\n",
      "Reconstruction: 0.157840, Regularization: 0.002065\n",
      "2019-04-10 01:04:02,880 root         INFO     Train Epoch: 164 [1536/8000 (19%)]\tTotal Loss: 0.169549\n",
      "Reconstruction: 0.166744, Regularization: 0.002805\n",
      "2019-04-10 01:04:02,942 root         INFO     Train Epoch: 164 [2048/8000 (26%)]\tTotal Loss: 0.152999\n",
      "Reconstruction: 0.151495, Regularization: 0.001504\n",
      "2019-04-10 01:04:03,006 root         INFO     Train Epoch: 164 [2560/8000 (32%)]\tTotal Loss: 0.156528\n",
      "Reconstruction: 0.155012, Regularization: 0.001516\n",
      "2019-04-10 01:04:03,069 root         INFO     Train Epoch: 164 [3072/8000 (38%)]\tTotal Loss: 0.157292\n",
      "Reconstruction: 0.155127, Regularization: 0.002166\n",
      "2019-04-10 01:04:03,133 root         INFO     Train Epoch: 164 [3584/8000 (45%)]\tTotal Loss: 0.171607\n",
      "Reconstruction: 0.168524, Regularization: 0.003083\n",
      "2019-04-10 01:04:03,196 root         INFO     Train Epoch: 164 [4096/8000 (51%)]\tTotal Loss: 0.158571\n",
      "Reconstruction: 0.156548, Regularization: 0.002024\n",
      "2019-04-10 01:04:03,259 root         INFO     Train Epoch: 164 [4608/8000 (58%)]\tTotal Loss: 0.176362\n",
      "Reconstruction: 0.173412, Regularization: 0.002950\n",
      "2019-04-10 01:04:03,323 root         INFO     Train Epoch: 164 [5120/8000 (64%)]\tTotal Loss: 0.156958\n",
      "Reconstruction: 0.155118, Regularization: 0.001840\n",
      "2019-04-10 01:04:03,387 root         INFO     Train Epoch: 164 [5632/8000 (70%)]\tTotal Loss: 0.165560\n",
      "Reconstruction: 0.163051, Regularization: 0.002509\n",
      "2019-04-10 01:04:03,451 root         INFO     Train Epoch: 164 [6144/8000 (77%)]\tTotal Loss: 0.161463\n",
      "Reconstruction: 0.159183, Regularization: 0.002281\n",
      "2019-04-10 01:04:03,513 root         INFO     Train Epoch: 164 [6656/8000 (83%)]\tTotal Loss: 0.167794\n",
      "Reconstruction: 0.165038, Regularization: 0.002756\n",
      "2019-04-10 01:04:03,576 root         INFO     Train Epoch: 164 [7168/8000 (90%)]\tTotal Loss: 0.162410\n",
      "Reconstruction: 0.159991, Regularization: 0.002420\n",
      "2019-04-10 01:04:03,638 root         INFO     Train Epoch: 164 [7680/8000 (96%)]\tTotal Loss: 0.158868\n",
      "Reconstruction: 0.157121, Regularization: 0.001747\n",
      "2019-04-10 01:04:03,692 root         INFO     ====> Epoch: 164 Average loss: 0.1656\n",
      "2019-04-10 01:04:03,716 root         INFO     Train Epoch: 165 [0/8000 (0%)]\tTotal Loss: 0.159981\n",
      "Reconstruction: 0.157903, Regularization: 0.002078\n",
      "2019-04-10 01:04:03,780 root         INFO     Train Epoch: 165 [512/8000 (6%)]\tTotal Loss: 0.167831\n",
      "Reconstruction: 0.164959, Regularization: 0.002873\n",
      "2019-04-10 01:04:03,844 root         INFO     Train Epoch: 165 [1024/8000 (13%)]\tTotal Loss: 0.169178\n",
      "Reconstruction: 0.167062, Regularization: 0.002116\n",
      "2019-04-10 01:04:03,907 root         INFO     Train Epoch: 165 [1536/8000 (19%)]\tTotal Loss: 0.166808\n",
      "Reconstruction: 0.164472, Regularization: 0.002336\n",
      "2019-04-10 01:04:03,971 root         INFO     Train Epoch: 165 [2048/8000 (26%)]\tTotal Loss: 0.159280\n",
      "Reconstruction: 0.156987, Regularization: 0.002293\n",
      "2019-04-10 01:04:04,035 root         INFO     Train Epoch: 165 [2560/8000 (32%)]\tTotal Loss: 0.174939\n",
      "Reconstruction: 0.171615, Regularization: 0.003325\n",
      "2019-04-10 01:04:04,098 root         INFO     Train Epoch: 165 [3072/8000 (38%)]\tTotal Loss: 0.169184\n",
      "Reconstruction: 0.166521, Regularization: 0.002663\n",
      "2019-04-10 01:04:04,160 root         INFO     Train Epoch: 165 [3584/8000 (45%)]\tTotal Loss: 0.153517\n",
      "Reconstruction: 0.151943, Regularization: 0.001575\n",
      "2019-04-10 01:04:04,223 root         INFO     Train Epoch: 165 [4096/8000 (51%)]\tTotal Loss: 0.162658\n",
      "Reconstruction: 0.160382, Regularization: 0.002275\n",
      "2019-04-10 01:04:04,286 root         INFO     Train Epoch: 165 [4608/8000 (58%)]\tTotal Loss: 0.179979\n",
      "Reconstruction: 0.176425, Regularization: 0.003555\n",
      "2019-04-10 01:04:04,349 root         INFO     Train Epoch: 165 [5120/8000 (64%)]\tTotal Loss: 0.159709\n",
      "Reconstruction: 0.157702, Regularization: 0.002007\n",
      "2019-04-10 01:04:04,412 root         INFO     Train Epoch: 165 [5632/8000 (70%)]\tTotal Loss: 0.156959\n",
      "Reconstruction: 0.155185, Regularization: 0.001774\n",
      "2019-04-10 01:04:04,475 root         INFO     Train Epoch: 165 [6144/8000 (77%)]\tTotal Loss: 0.187940\n",
      "Reconstruction: 0.184001, Regularization: 0.003938\n",
      "2019-04-10 01:04:04,538 root         INFO     Train Epoch: 165 [6656/8000 (83%)]\tTotal Loss: 0.157811\n",
      "Reconstruction: 0.156033, Regularization: 0.001779\n",
      "2019-04-10 01:04:04,601 root         INFO     Train Epoch: 165 [7168/8000 (90%)]\tTotal Loss: 0.158009\n",
      "Reconstruction: 0.155820, Regularization: 0.002189\n",
      "2019-04-10 01:04:04,664 root         INFO     Train Epoch: 165 [7680/8000 (96%)]\tTotal Loss: 0.178619\n",
      "Reconstruction: 0.175159, Regularization: 0.003460\n",
      "2019-04-10 01:04:04,719 root         INFO     ====> Epoch: 165 Average loss: 0.1657\n",
      "2019-04-10 01:04:04,743 root         INFO     Train Epoch: 166 [0/8000 (0%)]\tTotal Loss: 0.166726\n",
      "Reconstruction: 0.164505, Regularization: 0.002221\n",
      "2019-04-10 01:04:04,807 root         INFO     Train Epoch: 166 [512/8000 (6%)]\tTotal Loss: 0.165179\n",
      "Reconstruction: 0.162557, Regularization: 0.002622\n",
      "2019-04-10 01:04:04,870 root         INFO     Train Epoch: 166 [1024/8000 (13%)]\tTotal Loss: 0.159431\n",
      "Reconstruction: 0.157197, Regularization: 0.002234\n",
      "2019-04-10 01:04:04,934 root         INFO     Train Epoch: 166 [1536/8000 (19%)]\tTotal Loss: 0.152059\n",
      "Reconstruction: 0.150642, Regularization: 0.001416\n",
      "2019-04-10 01:04:04,998 root         INFO     Train Epoch: 166 [2048/8000 (26%)]\tTotal Loss: 0.167499\n",
      "Reconstruction: 0.164641, Regularization: 0.002858\n",
      "2019-04-10 01:04:05,062 root         INFO     Train Epoch: 166 [2560/8000 (32%)]\tTotal Loss: 0.169478\n",
      "Reconstruction: 0.166872, Regularization: 0.002606\n",
      "2019-04-10 01:04:05,125 root         INFO     Train Epoch: 166 [3072/8000 (38%)]\tTotal Loss: 0.165064\n",
      "Reconstruction: 0.162985, Regularization: 0.002080\n",
      "2019-04-10 01:04:05,189 root         INFO     Train Epoch: 166 [3584/8000 (45%)]\tTotal Loss: 0.170995\n",
      "Reconstruction: 0.168013, Regularization: 0.002982\n",
      "2019-04-10 01:04:05,253 root         INFO     Train Epoch: 166 [4096/8000 (51%)]\tTotal Loss: 0.174991\n",
      "Reconstruction: 0.171715, Regularization: 0.003276\n",
      "2019-04-10 01:04:05,317 root         INFO     Train Epoch: 166 [4608/8000 (58%)]\tTotal Loss: 0.160785\n",
      "Reconstruction: 0.158725, Regularization: 0.002060\n",
      "2019-04-10 01:04:05,381 root         INFO     Train Epoch: 166 [5120/8000 (64%)]\tTotal Loss: 0.178034\n",
      "Reconstruction: 0.174860, Regularization: 0.003174\n",
      "2019-04-10 01:04:05,443 root         INFO     Train Epoch: 166 [5632/8000 (70%)]\tTotal Loss: 0.152994\n",
      "Reconstruction: 0.151394, Regularization: 0.001600\n",
      "2019-04-10 01:04:05,506 root         INFO     Train Epoch: 166 [6144/8000 (77%)]\tTotal Loss: 0.162003\n",
      "Reconstruction: 0.159811, Regularization: 0.002193\n",
      "2019-04-10 01:04:05,569 root         INFO     Train Epoch: 166 [6656/8000 (83%)]\tTotal Loss: 0.177123\n",
      "Reconstruction: 0.173791, Regularization: 0.003332\n",
      "2019-04-10 01:04:05,631 root         INFO     Train Epoch: 166 [7168/8000 (90%)]\tTotal Loss: 0.160154\n",
      "Reconstruction: 0.158077, Regularization: 0.002076\n",
      "2019-04-10 01:04:05,694 root         INFO     Train Epoch: 166 [7680/8000 (96%)]\tTotal Loss: 0.164467\n",
      "Reconstruction: 0.162040, Regularization: 0.002428\n",
      "2019-04-10 01:04:05,748 root         INFO     ====> Epoch: 166 Average loss: 0.1658\n",
      "2019-04-10 01:04:05,773 root         INFO     Train Epoch: 167 [0/8000 (0%)]\tTotal Loss: 0.160755\n",
      "Reconstruction: 0.158566, Regularization: 0.002189\n",
      "2019-04-10 01:04:05,836 root         INFO     Train Epoch: 167 [512/8000 (6%)]\tTotal Loss: 0.165430\n",
      "Reconstruction: 0.162682, Regularization: 0.002748\n",
      "2019-04-10 01:04:05,899 root         INFO     Train Epoch: 167 [1024/8000 (13%)]\tTotal Loss: 0.168572\n",
      "Reconstruction: 0.165901, Regularization: 0.002670\n",
      "2019-04-10 01:04:05,963 root         INFO     Train Epoch: 167 [1536/8000 (19%)]\tTotal Loss: 0.168347\n",
      "Reconstruction: 0.165770, Regularization: 0.002577\n",
      "2019-04-10 01:04:06,026 root         INFO     Train Epoch: 167 [2048/8000 (26%)]\tTotal Loss: 0.177913\n",
      "Reconstruction: 0.174644, Regularization: 0.003269\n",
      "2019-04-10 01:04:06,089 root         INFO     Train Epoch: 167 [2560/8000 (32%)]\tTotal Loss: 0.184420\n",
      "Reconstruction: 0.179877, Regularization: 0.004543\n",
      "2019-04-10 01:04:06,152 root         INFO     Train Epoch: 167 [3072/8000 (38%)]\tTotal Loss: 0.171102\n",
      "Reconstruction: 0.168345, Regularization: 0.002757\n",
      "2019-04-10 01:04:06,216 root         INFO     Train Epoch: 167 [3584/8000 (45%)]\tTotal Loss: 0.178200\n",
      "Reconstruction: 0.174690, Regularization: 0.003510\n",
      "2019-04-10 01:04:06,279 root         INFO     Train Epoch: 167 [4096/8000 (51%)]\tTotal Loss: 0.169512\n",
      "Reconstruction: 0.166518, Regularization: 0.002994\n",
      "2019-04-10 01:04:06,342 root         INFO     Train Epoch: 167 [4608/8000 (58%)]\tTotal Loss: 0.171424\n",
      "Reconstruction: 0.168404, Regularization: 0.003020\n",
      "2019-04-10 01:04:06,405 root         INFO     Train Epoch: 167 [5120/8000 (64%)]\tTotal Loss: 0.178782\n",
      "Reconstruction: 0.175740, Regularization: 0.003043\n",
      "2019-04-10 01:04:06,469 root         INFO     Train Epoch: 167 [5632/8000 (70%)]\tTotal Loss: 0.153168\n",
      "Reconstruction: 0.151224, Regularization: 0.001944\n",
      "2019-04-10 01:04:06,532 root         INFO     Train Epoch: 167 [6144/8000 (77%)]\tTotal Loss: 0.184625\n",
      "Reconstruction: 0.181155, Regularization: 0.003470\n",
      "2019-04-10 01:04:06,595 root         INFO     Train Epoch: 167 [6656/8000 (83%)]\tTotal Loss: 0.168534\n",
      "Reconstruction: 0.165655, Regularization: 0.002879\n",
      "2019-04-10 01:04:06,658 root         INFO     Train Epoch: 167 [7168/8000 (90%)]\tTotal Loss: 0.165233\n",
      "Reconstruction: 0.162272, Regularization: 0.002961\n",
      "2019-04-10 01:04:06,722 root         INFO     Train Epoch: 167 [7680/8000 (96%)]\tTotal Loss: 0.169848\n",
      "Reconstruction: 0.167006, Regularization: 0.002842\n",
      "2019-04-10 01:04:06,775 root         INFO     ====> Epoch: 167 Average loss: 0.1657\n",
      "2019-04-10 01:04:06,799 root         INFO     Train Epoch: 168 [0/8000 (0%)]\tTotal Loss: 0.151005\n",
      "Reconstruction: 0.149538, Regularization: 0.001467\n",
      "2019-04-10 01:04:06,862 root         INFO     Train Epoch: 168 [512/8000 (6%)]\tTotal Loss: 0.166670\n",
      "Reconstruction: 0.163729, Regularization: 0.002942\n",
      "2019-04-10 01:04:06,926 root         INFO     Train Epoch: 168 [1024/8000 (13%)]\tTotal Loss: 0.161999\n",
      "Reconstruction: 0.159773, Regularization: 0.002226\n",
      "2019-04-10 01:04:06,989 root         INFO     Train Epoch: 168 [1536/8000 (19%)]\tTotal Loss: 0.177874\n",
      "Reconstruction: 0.174735, Regularization: 0.003138\n",
      "2019-04-10 01:04:07,052 root         INFO     Train Epoch: 168 [2048/8000 (26%)]\tTotal Loss: 0.160654\n",
      "Reconstruction: 0.158235, Regularization: 0.002420\n",
      "2019-04-10 01:04:07,115 root         INFO     Train Epoch: 168 [2560/8000 (32%)]\tTotal Loss: 0.168257\n",
      "Reconstruction: 0.165518, Regularization: 0.002739\n",
      "2019-04-10 01:04:07,178 root         INFO     Train Epoch: 168 [3072/8000 (38%)]\tTotal Loss: 0.189942\n",
      "Reconstruction: 0.185985, Regularization: 0.003958\n",
      "2019-04-10 01:04:07,241 root         INFO     Train Epoch: 168 [3584/8000 (45%)]\tTotal Loss: 0.177254\n",
      "Reconstruction: 0.174159, Regularization: 0.003095\n",
      "2019-04-10 01:04:07,303 root         INFO     Train Epoch: 168 [4096/8000 (51%)]\tTotal Loss: 0.158711\n",
      "Reconstruction: 0.156611, Regularization: 0.002100\n",
      "2019-04-10 01:04:07,366 root         INFO     Train Epoch: 168 [4608/8000 (58%)]\tTotal Loss: 0.174995\n",
      "Reconstruction: 0.171685, Regularization: 0.003310\n",
      "2019-04-10 01:04:07,429 root         INFO     Train Epoch: 168 [5120/8000 (64%)]\tTotal Loss: 0.154491\n",
      "Reconstruction: 0.152774, Regularization: 0.001716\n",
      "2019-04-10 01:04:07,492 root         INFO     Train Epoch: 168 [5632/8000 (70%)]\tTotal Loss: 0.152026\n",
      "Reconstruction: 0.150331, Regularization: 0.001695\n",
      "2019-04-10 01:04:07,555 root         INFO     Train Epoch: 168 [6144/8000 (77%)]\tTotal Loss: 0.162623\n",
      "Reconstruction: 0.159747, Regularization: 0.002876\n",
      "2019-04-10 01:04:07,618 root         INFO     Train Epoch: 168 [6656/8000 (83%)]\tTotal Loss: 0.172773\n",
      "Reconstruction: 0.169315, Regularization: 0.003457\n",
      "2019-04-10 01:04:07,681 root         INFO     Train Epoch: 168 [7168/8000 (90%)]\tTotal Loss: 0.179630\n",
      "Reconstruction: 0.175763, Regularization: 0.003868\n",
      "2019-04-10 01:04:07,744 root         INFO     Train Epoch: 168 [7680/8000 (96%)]\tTotal Loss: 0.158545\n",
      "Reconstruction: 0.156751, Regularization: 0.001795\n",
      "2019-04-10 01:04:07,797 root         INFO     ====> Epoch: 168 Average loss: 0.1655\n",
      "2019-04-10 01:04:07,821 root         INFO     Train Epoch: 169 [0/8000 (0%)]\tTotal Loss: 0.173879\n",
      "Reconstruction: 0.170087, Regularization: 0.003792\n",
      "2019-04-10 01:04:07,885 root         INFO     Train Epoch: 169 [512/8000 (6%)]\tTotal Loss: 0.178261\n",
      "Reconstruction: 0.174766, Regularization: 0.003495\n",
      "2019-04-10 01:04:07,948 root         INFO     Train Epoch: 169 [1024/8000 (13%)]\tTotal Loss: 0.176559\n",
      "Reconstruction: 0.173194, Regularization: 0.003365\n",
      "2019-04-10 01:04:08,012 root         INFO     Train Epoch: 169 [1536/8000 (19%)]\tTotal Loss: 0.175662\n",
      "Reconstruction: 0.172222, Regularization: 0.003440\n",
      "2019-04-10 01:04:08,074 root         INFO     Train Epoch: 169 [2048/8000 (26%)]\tTotal Loss: 0.160346\n",
      "Reconstruction: 0.157912, Regularization: 0.002434\n",
      "2019-04-10 01:04:08,137 root         INFO     Train Epoch: 169 [2560/8000 (32%)]\tTotal Loss: 0.155122\n",
      "Reconstruction: 0.153419, Regularization: 0.001703\n",
      "2019-04-10 01:04:08,200 root         INFO     Train Epoch: 169 [3072/8000 (38%)]\tTotal Loss: 0.166629\n",
      "Reconstruction: 0.163632, Regularization: 0.002997\n",
      "2019-04-10 01:04:08,263 root         INFO     Train Epoch: 169 [3584/8000 (45%)]\tTotal Loss: 0.154118\n",
      "Reconstruction: 0.152078, Regularization: 0.002040\n",
      "2019-04-10 01:04:08,326 root         INFO     Train Epoch: 169 [4096/8000 (51%)]\tTotal Loss: 0.168162\n",
      "Reconstruction: 0.164976, Regularization: 0.003185\n",
      "2019-04-10 01:04:08,389 root         INFO     Train Epoch: 169 [4608/8000 (58%)]\tTotal Loss: 0.157711\n",
      "Reconstruction: 0.155563, Regularization: 0.002148\n",
      "2019-04-10 01:04:08,452 root         INFO     Train Epoch: 169 [5120/8000 (64%)]\tTotal Loss: 0.167631\n",
      "Reconstruction: 0.164849, Regularization: 0.002782\n",
      "2019-04-10 01:04:08,514 root         INFO     Train Epoch: 169 [5632/8000 (70%)]\tTotal Loss: 0.171983\n",
      "Reconstruction: 0.168616, Regularization: 0.003367\n",
      "2019-04-10 01:04:08,577 root         INFO     Train Epoch: 169 [6144/8000 (77%)]\tTotal Loss: 0.169133\n",
      "Reconstruction: 0.165956, Regularization: 0.003177\n",
      "2019-04-10 01:04:08,640 root         INFO     Train Epoch: 169 [6656/8000 (83%)]\tTotal Loss: 0.172558\n",
      "Reconstruction: 0.169496, Regularization: 0.003062\n",
      "2019-04-10 01:04:08,702 root         INFO     Train Epoch: 169 [7168/8000 (90%)]\tTotal Loss: 0.179936\n",
      "Reconstruction: 0.176609, Regularization: 0.003327\n",
      "2019-04-10 01:04:08,765 root         INFO     Train Epoch: 169 [7680/8000 (96%)]\tTotal Loss: 0.161856\n",
      "Reconstruction: 0.159369, Regularization: 0.002488\n",
      "2019-04-10 01:04:08,819 root         INFO     ====> Epoch: 169 Average loss: 0.1656\n",
      "2019-04-10 01:04:08,843 root         INFO     Train Epoch: 170 [0/8000 (0%)]\tTotal Loss: 0.168035\n",
      "Reconstruction: 0.165213, Regularization: 0.002822\n",
      "2019-04-10 01:04:08,906 root         INFO     Train Epoch: 170 [512/8000 (6%)]\tTotal Loss: 0.164780\n",
      "Reconstruction: 0.161850, Regularization: 0.002930\n",
      "2019-04-10 01:04:08,969 root         INFO     Train Epoch: 170 [1024/8000 (13%)]\tTotal Loss: 0.166850\n",
      "Reconstruction: 0.163545, Regularization: 0.003305\n",
      "2019-04-10 01:04:09,032 root         INFO     Train Epoch: 170 [1536/8000 (19%)]\tTotal Loss: 0.165830\n",
      "Reconstruction: 0.162919, Regularization: 0.002911\n",
      "2019-04-10 01:04:09,095 root         INFO     Train Epoch: 170 [2048/8000 (26%)]\tTotal Loss: 0.165962\n",
      "Reconstruction: 0.163023, Regularization: 0.002939\n",
      "2019-04-10 01:04:09,158 root         INFO     Train Epoch: 170 [2560/8000 (32%)]\tTotal Loss: 0.153523\n",
      "Reconstruction: 0.151055, Regularization: 0.002468\n",
      "2019-04-10 01:04:09,221 root         INFO     Train Epoch: 170 [3072/8000 (38%)]\tTotal Loss: 0.165075\n",
      "Reconstruction: 0.161869, Regularization: 0.003206\n",
      "2019-04-10 01:04:09,284 root         INFO     Train Epoch: 170 [3584/8000 (45%)]\tTotal Loss: 0.156362\n",
      "Reconstruction: 0.153978, Regularization: 0.002383\n",
      "2019-04-10 01:04:09,348 root         INFO     Train Epoch: 170 [4096/8000 (51%)]\tTotal Loss: 0.158631\n",
      "Reconstruction: 0.156402, Regularization: 0.002229\n",
      "2019-04-10 01:04:09,411 root         INFO     Train Epoch: 170 [4608/8000 (58%)]\tTotal Loss: 0.167714\n",
      "Reconstruction: 0.164559, Regularization: 0.003155\n",
      "2019-04-10 01:04:09,474 root         INFO     Train Epoch: 170 [5120/8000 (64%)]\tTotal Loss: 0.165785\n",
      "Reconstruction: 0.161990, Regularization: 0.003795\n",
      "2019-04-10 01:04:09,537 root         INFO     Train Epoch: 170 [5632/8000 (70%)]\tTotal Loss: 0.170212\n",
      "Reconstruction: 0.166619, Regularization: 0.003593\n",
      "2019-04-10 01:04:09,600 root         INFO     Train Epoch: 170 [6144/8000 (77%)]\tTotal Loss: 0.159897\n",
      "Reconstruction: 0.157018, Regularization: 0.002879\n",
      "2019-04-10 01:04:09,664 root         INFO     Train Epoch: 170 [6656/8000 (83%)]\tTotal Loss: 0.165235\n",
      "Reconstruction: 0.162943, Regularization: 0.002292\n",
      "2019-04-10 01:04:09,727 root         INFO     Train Epoch: 170 [7168/8000 (90%)]\tTotal Loss: 0.158606\n",
      "Reconstruction: 0.155910, Regularization: 0.002696\n",
      "2019-04-10 01:04:09,790 root         INFO     Train Epoch: 170 [7680/8000 (96%)]\tTotal Loss: 0.165814\n",
      "Reconstruction: 0.162739, Regularization: 0.003076\n",
      "2019-04-10 01:04:09,844 root         INFO     ====> Epoch: 170 Average loss: 0.1655\n",
      "2019-04-10 01:04:09,868 root         INFO     Train Epoch: 171 [0/8000 (0%)]\tTotal Loss: 0.170682\n",
      "Reconstruction: 0.167545, Regularization: 0.003137\n",
      "2019-04-10 01:04:09,931 root         INFO     Train Epoch: 171 [512/8000 (6%)]\tTotal Loss: 0.158572\n",
      "Reconstruction: 0.155985, Regularization: 0.002587\n",
      "2019-04-10 01:04:09,995 root         INFO     Train Epoch: 171 [1024/8000 (13%)]\tTotal Loss: 0.164359\n",
      "Reconstruction: 0.161602, Regularization: 0.002757\n",
      "2019-04-10 01:04:10,058 root         INFO     Train Epoch: 171 [1536/8000 (19%)]\tTotal Loss: 0.160071\n",
      "Reconstruction: 0.157627, Regularization: 0.002444\n",
      "2019-04-10 01:04:10,121 root         INFO     Train Epoch: 171 [2048/8000 (26%)]\tTotal Loss: 0.163866\n",
      "Reconstruction: 0.161153, Regularization: 0.002713\n",
      "2019-04-10 01:04:10,184 root         INFO     Train Epoch: 171 [2560/8000 (32%)]\tTotal Loss: 0.169629\n",
      "Reconstruction: 0.165896, Regularization: 0.003733\n",
      "2019-04-10 01:04:10,247 root         INFO     Train Epoch: 171 [3072/8000 (38%)]\tTotal Loss: 0.194822\n",
      "Reconstruction: 0.189404, Regularization: 0.005418\n",
      "2019-04-10 01:04:10,310 root         INFO     Train Epoch: 171 [3584/8000 (45%)]\tTotal Loss: 0.176598\n",
      "Reconstruction: 0.172660, Regularization: 0.003938\n",
      "2019-04-10 01:04:10,373 root         INFO     Train Epoch: 171 [4096/8000 (51%)]\tTotal Loss: 0.168939\n",
      "Reconstruction: 0.165048, Regularization: 0.003891\n",
      "2019-04-10 01:04:10,436 root         INFO     Train Epoch: 171 [4608/8000 (58%)]\tTotal Loss: 0.164703\n",
      "Reconstruction: 0.161939, Regularization: 0.002764\n",
      "2019-04-10 01:04:10,499 root         INFO     Train Epoch: 171 [5120/8000 (64%)]\tTotal Loss: 0.162156\n",
      "Reconstruction: 0.159003, Regularization: 0.003153\n",
      "2019-04-10 01:04:10,562 root         INFO     Train Epoch: 171 [5632/8000 (70%)]\tTotal Loss: 0.158239\n",
      "Reconstruction: 0.156049, Regularization: 0.002189\n",
      "2019-04-10 01:04:10,625 root         INFO     Train Epoch: 171 [6144/8000 (77%)]\tTotal Loss: 0.161002\n",
      "Reconstruction: 0.157916, Regularization: 0.003087\n",
      "2019-04-10 01:04:10,688 root         INFO     Train Epoch: 171 [6656/8000 (83%)]\tTotal Loss: 0.160195\n",
      "Reconstruction: 0.157690, Regularization: 0.002506\n",
      "2019-04-10 01:04:10,751 root         INFO     Train Epoch: 171 [7168/8000 (90%)]\tTotal Loss: 0.165411\n",
      "Reconstruction: 0.162032, Regularization: 0.003379\n",
      "2019-04-10 01:04:10,814 root         INFO     Train Epoch: 171 [7680/8000 (96%)]\tTotal Loss: 0.175896\n",
      "Reconstruction: 0.172110, Regularization: 0.003786\n",
      "2019-04-10 01:04:10,867 root         INFO     ====> Epoch: 171 Average loss: 0.1661\n",
      "2019-04-10 01:04:10,891 root         INFO     Train Epoch: 172 [0/8000 (0%)]\tTotal Loss: 0.168567\n",
      "Reconstruction: 0.165661, Regularization: 0.002906\n",
      "2019-04-10 01:04:10,955 root         INFO     Train Epoch: 172 [512/8000 (6%)]\tTotal Loss: 0.169946\n",
      "Reconstruction: 0.167137, Regularization: 0.002809\n",
      "2019-04-10 01:04:11,018 root         INFO     Train Epoch: 172 [1024/8000 (13%)]\tTotal Loss: 0.165027\n",
      "Reconstruction: 0.162024, Regularization: 0.003003\n",
      "2019-04-10 01:04:11,082 root         INFO     Train Epoch: 172 [1536/8000 (19%)]\tTotal Loss: 0.162520\n",
      "Reconstruction: 0.159628, Regularization: 0.002892\n",
      "2019-04-10 01:04:11,146 root         INFO     Train Epoch: 172 [2048/8000 (26%)]\tTotal Loss: 0.160380\n",
      "Reconstruction: 0.157891, Regularization: 0.002489\n",
      "2019-04-10 01:04:11,210 root         INFO     Train Epoch: 172 [2560/8000 (32%)]\tTotal Loss: 0.173554\n",
      "Reconstruction: 0.169896, Regularization: 0.003658\n",
      "2019-04-10 01:04:11,273 root         INFO     Train Epoch: 172 [3072/8000 (38%)]\tTotal Loss: 0.160907\n",
      "Reconstruction: 0.157940, Regularization: 0.002967\n",
      "2019-04-10 01:04:11,337 root         INFO     Train Epoch: 172 [3584/8000 (45%)]\tTotal Loss: 0.175106\n",
      "Reconstruction: 0.171227, Regularization: 0.003879\n",
      "2019-04-10 01:04:11,401 root         INFO     Train Epoch: 172 [4096/8000 (51%)]\tTotal Loss: 0.156488\n",
      "Reconstruction: 0.154393, Regularization: 0.002095\n",
      "2019-04-10 01:04:11,464 root         INFO     Train Epoch: 172 [4608/8000 (58%)]\tTotal Loss: 0.158211\n",
      "Reconstruction: 0.155760, Regularization: 0.002451\n",
      "2019-04-10 01:04:11,527 root         INFO     Train Epoch: 172 [5120/8000 (64%)]\tTotal Loss: 0.164664\n",
      "Reconstruction: 0.161529, Regularization: 0.003135\n",
      "2019-04-10 01:04:11,590 root         INFO     Train Epoch: 172 [5632/8000 (70%)]\tTotal Loss: 0.171970\n",
      "Reconstruction: 0.168395, Regularization: 0.003575\n",
      "2019-04-10 01:04:11,651 root         INFO     Train Epoch: 172 [6144/8000 (77%)]\tTotal Loss: 0.162450\n",
      "Reconstruction: 0.159553, Regularization: 0.002897\n",
      "2019-04-10 01:04:11,713 root         INFO     Train Epoch: 172 [6656/8000 (83%)]\tTotal Loss: 0.168623\n",
      "Reconstruction: 0.165123, Regularization: 0.003500\n",
      "2019-04-10 01:04:11,775 root         INFO     Train Epoch: 172 [7168/8000 (90%)]\tTotal Loss: 0.162709\n",
      "Reconstruction: 0.160001, Regularization: 0.002709\n",
      "2019-04-10 01:04:11,837 root         INFO     Train Epoch: 172 [7680/8000 (96%)]\tTotal Loss: 0.161817\n",
      "Reconstruction: 0.158862, Regularization: 0.002955\n",
      "2019-04-10 01:04:11,891 root         INFO     ====> Epoch: 172 Average loss: 0.1658\n",
      "2019-04-10 01:04:11,914 root         INFO     Train Epoch: 173 [0/8000 (0%)]\tTotal Loss: 0.175158\n",
      "Reconstruction: 0.170932, Regularization: 0.004226\n",
      "2019-04-10 01:04:11,978 root         INFO     Train Epoch: 173 [512/8000 (6%)]\tTotal Loss: 0.158472\n",
      "Reconstruction: 0.155718, Regularization: 0.002754\n",
      "2019-04-10 01:04:12,041 root         INFO     Train Epoch: 173 [1024/8000 (13%)]\tTotal Loss: 0.165221\n",
      "Reconstruction: 0.162637, Regularization: 0.002584\n",
      "2019-04-10 01:04:12,104 root         INFO     Train Epoch: 173 [1536/8000 (19%)]\tTotal Loss: 0.176562\n",
      "Reconstruction: 0.171916, Regularization: 0.004646\n",
      "2019-04-10 01:04:12,167 root         INFO     Train Epoch: 173 [2048/8000 (26%)]\tTotal Loss: 0.170512\n",
      "Reconstruction: 0.166990, Regularization: 0.003522\n",
      "2019-04-10 01:04:12,230 root         INFO     Train Epoch: 173 [2560/8000 (32%)]\tTotal Loss: 0.155375\n",
      "Reconstruction: 0.153422, Regularization: 0.001952\n",
      "2019-04-10 01:04:12,294 root         INFO     Train Epoch: 173 [3072/8000 (38%)]\tTotal Loss: 0.174355\n",
      "Reconstruction: 0.170486, Regularization: 0.003869\n",
      "2019-04-10 01:04:12,357 root         INFO     Train Epoch: 173 [3584/8000 (45%)]\tTotal Loss: 0.174461\n",
      "Reconstruction: 0.170119, Regularization: 0.004342\n",
      "2019-04-10 01:04:12,420 root         INFO     Train Epoch: 173 [4096/8000 (51%)]\tTotal Loss: 0.161835\n",
      "Reconstruction: 0.158767, Regularization: 0.003069\n",
      "2019-04-10 01:04:12,482 root         INFO     Train Epoch: 173 [4608/8000 (58%)]\tTotal Loss: 0.182334\n",
      "Reconstruction: 0.177372, Regularization: 0.004962\n",
      "2019-04-10 01:04:12,545 root         INFO     Train Epoch: 173 [5120/8000 (64%)]\tTotal Loss: 0.172933\n",
      "Reconstruction: 0.168627, Regularization: 0.004306\n",
      "2019-04-10 01:04:12,608 root         INFO     Train Epoch: 173 [5632/8000 (70%)]\tTotal Loss: 0.166851\n",
      "Reconstruction: 0.164016, Regularization: 0.002835\n",
      "2019-04-10 01:04:12,671 root         INFO     Train Epoch: 173 [6144/8000 (77%)]\tTotal Loss: 0.176074\n",
      "Reconstruction: 0.171908, Regularization: 0.004166\n",
      "2019-04-10 01:04:12,733 root         INFO     Train Epoch: 173 [6656/8000 (83%)]\tTotal Loss: 0.166019\n",
      "Reconstruction: 0.162958, Regularization: 0.003061\n",
      "2019-04-10 01:04:12,796 root         INFO     Train Epoch: 173 [7168/8000 (90%)]\tTotal Loss: 0.161839\n",
      "Reconstruction: 0.159136, Regularization: 0.002703\n",
      "2019-04-10 01:04:12,859 root         INFO     Train Epoch: 173 [7680/8000 (96%)]\tTotal Loss: 0.158172\n",
      "Reconstruction: 0.155421, Regularization: 0.002751\n",
      "2019-04-10 01:04:12,913 root         INFO     ====> Epoch: 173 Average loss: 0.1658\n",
      "2019-04-10 01:04:12,937 root         INFO     Train Epoch: 174 [0/8000 (0%)]\tTotal Loss: 0.168471\n",
      "Reconstruction: 0.165013, Regularization: 0.003459\n",
      "2019-04-10 01:04:13,000 root         INFO     Train Epoch: 174 [512/8000 (6%)]\tTotal Loss: 0.161559\n",
      "Reconstruction: 0.158459, Regularization: 0.003100\n",
      "2019-04-10 01:04:13,063 root         INFO     Train Epoch: 174 [1024/8000 (13%)]\tTotal Loss: 0.161043\n",
      "Reconstruction: 0.158143, Regularization: 0.002900\n",
      "2019-04-10 01:04:13,126 root         INFO     Train Epoch: 174 [1536/8000 (19%)]\tTotal Loss: 0.156387\n",
      "Reconstruction: 0.154419, Regularization: 0.001968\n",
      "2019-04-10 01:04:13,189 root         INFO     Train Epoch: 174 [2048/8000 (26%)]\tTotal Loss: 0.158915\n",
      "Reconstruction: 0.156454, Regularization: 0.002461\n",
      "2019-04-10 01:04:13,253 root         INFO     Train Epoch: 174 [2560/8000 (32%)]\tTotal Loss: 0.163253\n",
      "Reconstruction: 0.160170, Regularization: 0.003083\n",
      "2019-04-10 01:04:13,315 root         INFO     Train Epoch: 174 [3072/8000 (38%)]\tTotal Loss: 0.174315\n",
      "Reconstruction: 0.169916, Regularization: 0.004399\n",
      "2019-04-10 01:04:13,377 root         INFO     Train Epoch: 174 [3584/8000 (45%)]\tTotal Loss: 0.179101\n",
      "Reconstruction: 0.174880, Regularization: 0.004221\n",
      "2019-04-10 01:04:13,439 root         INFO     Train Epoch: 174 [4096/8000 (51%)]\tTotal Loss: 0.160411\n",
      "Reconstruction: 0.157815, Regularization: 0.002595\n",
      "2019-04-10 01:04:13,501 root         INFO     Train Epoch: 174 [4608/8000 (58%)]\tTotal Loss: 0.179291\n",
      "Reconstruction: 0.174670, Regularization: 0.004621\n",
      "2019-04-10 01:04:13,563 root         INFO     Train Epoch: 174 [5120/8000 (64%)]\tTotal Loss: 0.176542\n",
      "Reconstruction: 0.172858, Regularization: 0.003685\n",
      "2019-04-10 01:04:13,625 root         INFO     Train Epoch: 174 [5632/8000 (70%)]\tTotal Loss: 0.166372\n",
      "Reconstruction: 0.162832, Regularization: 0.003539\n",
      "2019-04-10 01:04:13,688 root         INFO     Train Epoch: 174 [6144/8000 (77%)]\tTotal Loss: 0.153698\n",
      "Reconstruction: 0.150927, Regularization: 0.002771\n",
      "2019-04-10 01:04:13,750 root         INFO     Train Epoch: 174 [6656/8000 (83%)]\tTotal Loss: 0.191818\n",
      "Reconstruction: 0.185606, Regularization: 0.006212\n",
      "2019-04-10 01:04:13,813 root         INFO     Train Epoch: 174 [7168/8000 (90%)]\tTotal Loss: 0.158502\n",
      "Reconstruction: 0.155947, Regularization: 0.002554\n",
      "2019-04-10 01:04:13,875 root         INFO     Train Epoch: 174 [7680/8000 (96%)]\tTotal Loss: 0.175424\n",
      "Reconstruction: 0.171939, Regularization: 0.003485\n",
      "2019-04-10 01:04:13,929 root         INFO     ====> Epoch: 174 Average loss: 0.1656\n",
      "2019-04-10 01:04:13,953 root         INFO     Train Epoch: 175 [0/8000 (0%)]\tTotal Loss: 0.162145\n",
      "Reconstruction: 0.159107, Regularization: 0.003037\n",
      "2019-04-10 01:04:14,014 root         INFO     Train Epoch: 175 [512/8000 (6%)]\tTotal Loss: 0.173557\n",
      "Reconstruction: 0.169470, Regularization: 0.004087\n",
      "2019-04-10 01:04:14,077 root         INFO     Train Epoch: 175 [1024/8000 (13%)]\tTotal Loss: 0.164040\n",
      "Reconstruction: 0.161152, Regularization: 0.002888\n",
      "2019-04-10 01:04:14,140 root         INFO     Train Epoch: 175 [1536/8000 (19%)]\tTotal Loss: 0.177337\n",
      "Reconstruction: 0.173417, Regularization: 0.003920\n",
      "2019-04-10 01:04:14,201 root         INFO     Train Epoch: 175 [2048/8000 (26%)]\tTotal Loss: 0.160948\n",
      "Reconstruction: 0.157965, Regularization: 0.002983\n",
      "2019-04-10 01:04:14,262 root         INFO     Train Epoch: 175 [2560/8000 (32%)]\tTotal Loss: 0.170808\n",
      "Reconstruction: 0.167378, Regularization: 0.003430\n",
      "2019-04-10 01:04:14,324 root         INFO     Train Epoch: 175 [3072/8000 (38%)]\tTotal Loss: 0.165605\n",
      "Reconstruction: 0.161875, Regularization: 0.003730\n",
      "2019-04-10 01:04:14,386 root         INFO     Train Epoch: 175 [3584/8000 (45%)]\tTotal Loss: 0.156846\n",
      "Reconstruction: 0.154129, Regularization: 0.002717\n",
      "2019-04-10 01:04:14,447 root         INFO     Train Epoch: 175 [4096/8000 (51%)]\tTotal Loss: 0.156400\n",
      "Reconstruction: 0.153880, Regularization: 0.002520\n",
      "2019-04-10 01:04:14,508 root         INFO     Train Epoch: 175 [4608/8000 (58%)]\tTotal Loss: 0.164900\n",
      "Reconstruction: 0.162200, Regularization: 0.002700\n",
      "2019-04-10 01:04:14,569 root         INFO     Train Epoch: 175 [5120/8000 (64%)]\tTotal Loss: 0.176695\n",
      "Reconstruction: 0.172486, Regularization: 0.004209\n",
      "2019-04-10 01:04:14,630 root         INFO     Train Epoch: 175 [5632/8000 (70%)]\tTotal Loss: 0.169541\n",
      "Reconstruction: 0.165298, Regularization: 0.004243\n",
      "2019-04-10 01:04:14,691 root         INFO     Train Epoch: 175 [6144/8000 (77%)]\tTotal Loss: 0.159971\n",
      "Reconstruction: 0.157330, Regularization: 0.002642\n",
      "2019-04-10 01:04:14,752 root         INFO     Train Epoch: 175 [6656/8000 (83%)]\tTotal Loss: 0.173764\n",
      "Reconstruction: 0.169581, Regularization: 0.004183\n",
      "2019-04-10 01:04:14,812 root         INFO     Train Epoch: 175 [7168/8000 (90%)]\tTotal Loss: 0.159819\n",
      "Reconstruction: 0.156923, Regularization: 0.002897\n",
      "2019-04-10 01:04:14,873 root         INFO     Train Epoch: 175 [7680/8000 (96%)]\tTotal Loss: 0.156153\n",
      "Reconstruction: 0.153371, Regularization: 0.002781\n",
      "2019-04-10 01:04:14,926 root         INFO     ====> Epoch: 175 Average loss: 0.1656\n",
      "2019-04-10 01:04:14,950 root         INFO     Train Epoch: 176 [0/8000 (0%)]\tTotal Loss: 0.179544\n",
      "Reconstruction: 0.175042, Regularization: 0.004502\n",
      "2019-04-10 01:04:15,013 root         INFO     Train Epoch: 176 [512/8000 (6%)]\tTotal Loss: 0.153489\n",
      "Reconstruction: 0.151038, Regularization: 0.002451\n",
      "2019-04-10 01:04:15,076 root         INFO     Train Epoch: 176 [1024/8000 (13%)]\tTotal Loss: 0.157489\n",
      "Reconstruction: 0.154959, Regularization: 0.002529\n",
      "2019-04-10 01:04:15,139 root         INFO     Train Epoch: 176 [1536/8000 (19%)]\tTotal Loss: 0.164844\n",
      "Reconstruction: 0.161381, Regularization: 0.003463\n",
      "2019-04-10 01:04:15,201 root         INFO     Train Epoch: 176 [2048/8000 (26%)]\tTotal Loss: 0.175921\n",
      "Reconstruction: 0.171407, Regularization: 0.004515\n",
      "2019-04-10 01:04:15,263 root         INFO     Train Epoch: 176 [2560/8000 (32%)]\tTotal Loss: 0.165803\n",
      "Reconstruction: 0.162511, Regularization: 0.003291\n",
      "2019-04-10 01:04:15,326 root         INFO     Train Epoch: 176 [3072/8000 (38%)]\tTotal Loss: 0.163728\n",
      "Reconstruction: 0.160385, Regularization: 0.003343\n",
      "2019-04-10 01:04:15,388 root         INFO     Train Epoch: 176 [3584/8000 (45%)]\tTotal Loss: 0.170138\n",
      "Reconstruction: 0.166190, Regularization: 0.003948\n",
      "2019-04-10 01:04:15,449 root         INFO     Train Epoch: 176 [4096/8000 (51%)]\tTotal Loss: 0.180828\n",
      "Reconstruction: 0.176120, Regularization: 0.004708\n",
      "2019-04-10 01:04:15,510 root         INFO     Train Epoch: 176 [4608/8000 (58%)]\tTotal Loss: 0.155072\n",
      "Reconstruction: 0.152228, Regularization: 0.002844\n",
      "2019-04-10 01:04:15,572 root         INFO     Train Epoch: 176 [5120/8000 (64%)]\tTotal Loss: 0.152323\n",
      "Reconstruction: 0.150058, Regularization: 0.002265\n",
      "2019-04-10 01:04:15,635 root         INFO     Train Epoch: 176 [5632/8000 (70%)]\tTotal Loss: 0.163968\n",
      "Reconstruction: 0.160348, Regularization: 0.003619\n",
      "2019-04-10 01:04:15,698 root         INFO     Train Epoch: 176 [6144/8000 (77%)]\tTotal Loss: 0.172665\n",
      "Reconstruction: 0.168812, Regularization: 0.003854\n",
      "2019-04-10 01:04:15,761 root         INFO     Train Epoch: 176 [6656/8000 (83%)]\tTotal Loss: 0.163903\n",
      "Reconstruction: 0.160188, Regularization: 0.003715\n",
      "2019-04-10 01:04:15,823 root         INFO     Train Epoch: 176 [7168/8000 (90%)]\tTotal Loss: 0.157010\n",
      "Reconstruction: 0.154056, Regularization: 0.002954\n",
      "2019-04-10 01:04:15,886 root         INFO     Train Epoch: 176 [7680/8000 (96%)]\tTotal Loss: 0.157168\n",
      "Reconstruction: 0.154802, Regularization: 0.002366\n",
      "2019-04-10 01:04:15,940 root         INFO     ====> Epoch: 176 Average loss: 0.1658\n",
      "2019-04-10 01:04:15,963 root         INFO     Train Epoch: 177 [0/8000 (0%)]\tTotal Loss: 0.170725\n",
      "Reconstruction: 0.166478, Regularization: 0.004247\n",
      "2019-04-10 01:04:16,026 root         INFO     Train Epoch: 177 [512/8000 (6%)]\tTotal Loss: 0.182186\n",
      "Reconstruction: 0.177546, Regularization: 0.004640\n",
      "2019-04-10 01:04:16,088 root         INFO     Train Epoch: 177 [1024/8000 (13%)]\tTotal Loss: 0.168066\n",
      "Reconstruction: 0.164028, Regularization: 0.004038\n",
      "2019-04-10 01:04:16,150 root         INFO     Train Epoch: 177 [1536/8000 (19%)]\tTotal Loss: 0.160008\n",
      "Reconstruction: 0.157312, Regularization: 0.002696\n",
      "2019-04-10 01:04:16,213 root         INFO     Train Epoch: 177 [2048/8000 (26%)]\tTotal Loss: 0.178152\n",
      "Reconstruction: 0.173331, Regularization: 0.004821\n",
      "2019-04-10 01:04:16,275 root         INFO     Train Epoch: 177 [2560/8000 (32%)]\tTotal Loss: 0.162593\n",
      "Reconstruction: 0.159572, Regularization: 0.003021\n",
      "2019-04-10 01:04:16,336 root         INFO     Train Epoch: 177 [3072/8000 (38%)]\tTotal Loss: 0.174473\n",
      "Reconstruction: 0.170213, Regularization: 0.004260\n",
      "2019-04-10 01:04:16,397 root         INFO     Train Epoch: 177 [3584/8000 (45%)]\tTotal Loss: 0.166575\n",
      "Reconstruction: 0.163692, Regularization: 0.002883\n",
      "2019-04-10 01:04:16,457 root         INFO     Train Epoch: 177 [4096/8000 (51%)]\tTotal Loss: 0.173478\n",
      "Reconstruction: 0.169116, Regularization: 0.004362\n",
      "2019-04-10 01:04:16,518 root         INFO     Train Epoch: 177 [4608/8000 (58%)]\tTotal Loss: 0.162057\n",
      "Reconstruction: 0.158874, Regularization: 0.003183\n",
      "2019-04-10 01:04:16,579 root         INFO     Train Epoch: 177 [5120/8000 (64%)]\tTotal Loss: 0.166358\n",
      "Reconstruction: 0.162480, Regularization: 0.003878\n",
      "2019-04-10 01:04:16,640 root         INFO     Train Epoch: 177 [5632/8000 (70%)]\tTotal Loss: 0.165046\n",
      "Reconstruction: 0.161921, Regularization: 0.003125\n",
      "2019-04-10 01:04:16,701 root         INFO     Train Epoch: 177 [6144/8000 (77%)]\tTotal Loss: 0.157009\n",
      "Reconstruction: 0.154674, Regularization: 0.002335\n",
      "2019-04-10 01:04:16,761 root         INFO     Train Epoch: 177 [6656/8000 (83%)]\tTotal Loss: 0.157496\n",
      "Reconstruction: 0.155048, Regularization: 0.002448\n",
      "2019-04-10 01:04:16,822 root         INFO     Train Epoch: 177 [7168/8000 (90%)]\tTotal Loss: 0.167848\n",
      "Reconstruction: 0.164362, Regularization: 0.003486\n",
      "2019-04-10 01:04:16,883 root         INFO     Train Epoch: 177 [7680/8000 (96%)]\tTotal Loss: 0.166139\n",
      "Reconstruction: 0.162874, Regularization: 0.003265\n",
      "2019-04-10 01:04:16,935 root         INFO     ====> Epoch: 177 Average loss: 0.1654\n",
      "2019-04-10 01:04:16,959 root         INFO     Train Epoch: 178 [0/8000 (0%)]\tTotal Loss: 0.165361\n",
      "Reconstruction: 0.161967, Regularization: 0.003394\n",
      "2019-04-10 01:04:17,022 root         INFO     Train Epoch: 178 [512/8000 (6%)]\tTotal Loss: 0.178417\n",
      "Reconstruction: 0.174294, Regularization: 0.004123\n",
      "2019-04-10 01:04:17,084 root         INFO     Train Epoch: 178 [1024/8000 (13%)]\tTotal Loss: 0.161724\n",
      "Reconstruction: 0.158097, Regularization: 0.003627\n",
      "2019-04-10 01:04:17,146 root         INFO     Train Epoch: 178 [1536/8000 (19%)]\tTotal Loss: 0.162072\n",
      "Reconstruction: 0.158570, Regularization: 0.003501\n",
      "2019-04-10 01:04:17,208 root         INFO     Train Epoch: 178 [2048/8000 (26%)]\tTotal Loss: 0.180301\n",
      "Reconstruction: 0.175978, Regularization: 0.004323\n",
      "2019-04-10 01:04:17,270 root         INFO     Train Epoch: 178 [2560/8000 (32%)]\tTotal Loss: 0.162840\n",
      "Reconstruction: 0.159649, Regularization: 0.003191\n",
      "2019-04-10 01:04:17,333 root         INFO     Train Epoch: 178 [3072/8000 (38%)]\tTotal Loss: 0.156865\n",
      "Reconstruction: 0.154254, Regularization: 0.002611\n",
      "2019-04-10 01:04:17,395 root         INFO     Train Epoch: 178 [3584/8000 (45%)]\tTotal Loss: 0.164104\n",
      "Reconstruction: 0.161082, Regularization: 0.003022\n",
      "2019-04-10 01:04:17,457 root         INFO     Train Epoch: 178 [4096/8000 (51%)]\tTotal Loss: 0.162050\n",
      "Reconstruction: 0.158808, Regularization: 0.003241\n",
      "2019-04-10 01:04:17,520 root         INFO     Train Epoch: 178 [4608/8000 (58%)]\tTotal Loss: 0.167555\n",
      "Reconstruction: 0.163771, Regularization: 0.003784\n",
      "2019-04-10 01:04:17,582 root         INFO     Train Epoch: 178 [5120/8000 (64%)]\tTotal Loss: 0.170012\n",
      "Reconstruction: 0.165980, Regularization: 0.004032\n",
      "2019-04-10 01:04:17,644 root         INFO     Train Epoch: 178 [5632/8000 (70%)]\tTotal Loss: 0.160275\n",
      "Reconstruction: 0.157142, Regularization: 0.003133\n",
      "2019-04-10 01:04:17,706 root         INFO     Train Epoch: 178 [6144/8000 (77%)]\tTotal Loss: 0.178574\n",
      "Reconstruction: 0.173558, Regularization: 0.005016\n",
      "2019-04-10 01:04:17,769 root         INFO     Train Epoch: 178 [6656/8000 (83%)]\tTotal Loss: 0.165042\n",
      "Reconstruction: 0.161440, Regularization: 0.003602\n",
      "2019-04-10 01:04:17,831 root         INFO     Train Epoch: 178 [7168/8000 (90%)]\tTotal Loss: 0.173393\n",
      "Reconstruction: 0.169590, Regularization: 0.003804\n",
      "2019-04-10 01:04:17,893 root         INFO     Train Epoch: 178 [7680/8000 (96%)]\tTotal Loss: 0.166912\n",
      "Reconstruction: 0.163768, Regularization: 0.003144\n",
      "2019-04-10 01:04:17,947 root         INFO     ====> Epoch: 178 Average loss: 0.1659\n",
      "2019-04-10 01:04:17,970 root         INFO     Train Epoch: 179 [0/8000 (0%)]\tTotal Loss: 0.172733\n",
      "Reconstruction: 0.168851, Regularization: 0.003882\n",
      "2019-04-10 01:04:18,033 root         INFO     Train Epoch: 179 [512/8000 (6%)]\tTotal Loss: 0.164280\n",
      "Reconstruction: 0.160425, Regularization: 0.003855\n",
      "2019-04-10 01:04:18,096 root         INFO     Train Epoch: 179 [1024/8000 (13%)]\tTotal Loss: 0.148067\n",
      "Reconstruction: 0.146524, Regularization: 0.001543\n",
      "2019-04-10 01:04:18,159 root         INFO     Train Epoch: 179 [1536/8000 (19%)]\tTotal Loss: 0.172393\n",
      "Reconstruction: 0.167541, Regularization: 0.004851\n",
      "2019-04-10 01:04:18,222 root         INFO     Train Epoch: 179 [2048/8000 (26%)]\tTotal Loss: 0.165345\n",
      "Reconstruction: 0.161640, Regularization: 0.003705\n",
      "2019-04-10 01:04:18,284 root         INFO     Train Epoch: 179 [2560/8000 (32%)]\tTotal Loss: 0.171207\n",
      "Reconstruction: 0.166571, Regularization: 0.004636\n",
      "2019-04-10 01:04:18,346 root         INFO     Train Epoch: 179 [3072/8000 (38%)]\tTotal Loss: 0.159593\n",
      "Reconstruction: 0.156777, Regularization: 0.002816\n",
      "2019-04-10 01:04:18,409 root         INFO     Train Epoch: 179 [3584/8000 (45%)]\tTotal Loss: 0.161167\n",
      "Reconstruction: 0.158536, Regularization: 0.002631\n",
      "2019-04-10 01:04:18,471 root         INFO     Train Epoch: 179 [4096/8000 (51%)]\tTotal Loss: 0.177955\n",
      "Reconstruction: 0.173177, Regularization: 0.004778\n",
      "2019-04-10 01:04:18,533 root         INFO     Train Epoch: 179 [4608/8000 (58%)]\tTotal Loss: 0.166249\n",
      "Reconstruction: 0.162430, Regularization: 0.003818\n",
      "2019-04-10 01:04:18,596 root         INFO     Train Epoch: 179 [5120/8000 (64%)]\tTotal Loss: 0.167704\n",
      "Reconstruction: 0.163791, Regularization: 0.003913\n",
      "2019-04-10 01:04:18,659 root         INFO     Train Epoch: 179 [5632/8000 (70%)]\tTotal Loss: 0.167480\n",
      "Reconstruction: 0.164299, Regularization: 0.003181\n",
      "2019-04-10 01:04:18,721 root         INFO     Train Epoch: 179 [6144/8000 (77%)]\tTotal Loss: 0.176349\n",
      "Reconstruction: 0.172022, Regularization: 0.004327\n",
      "2019-04-10 01:04:18,784 root         INFO     Train Epoch: 179 [6656/8000 (83%)]\tTotal Loss: 0.168508\n",
      "Reconstruction: 0.164997, Regularization: 0.003511\n",
      "2019-04-10 01:04:18,846 root         INFO     Train Epoch: 179 [7168/8000 (90%)]\tTotal Loss: 0.162803\n",
      "Reconstruction: 0.159615, Regularization: 0.003188\n",
      "2019-04-10 01:04:18,909 root         INFO     Train Epoch: 179 [7680/8000 (96%)]\tTotal Loss: 0.173869\n",
      "Reconstruction: 0.169833, Regularization: 0.004037\n",
      "2019-04-10 01:04:18,962 root         INFO     ====> Epoch: 179 Average loss: 0.1656\n",
      "2019-04-10 01:04:18,986 root         INFO     Train Epoch: 180 [0/8000 (0%)]\tTotal Loss: 0.166744\n",
      "Reconstruction: 0.162718, Regularization: 0.004026\n",
      "2019-04-10 01:04:19,048 root         INFO     Train Epoch: 180 [512/8000 (6%)]\tTotal Loss: 0.161323\n",
      "Reconstruction: 0.157932, Regularization: 0.003391\n",
      "2019-04-10 01:04:19,110 root         INFO     Train Epoch: 180 [1024/8000 (13%)]\tTotal Loss: 0.169270\n",
      "Reconstruction: 0.165687, Regularization: 0.003582\n",
      "2019-04-10 01:04:19,171 root         INFO     Train Epoch: 180 [1536/8000 (19%)]\tTotal Loss: 0.183737\n",
      "Reconstruction: 0.177406, Regularization: 0.006332\n",
      "2019-04-10 01:04:19,232 root         INFO     Train Epoch: 180 [2048/8000 (26%)]\tTotal Loss: 0.169249\n",
      "Reconstruction: 0.165515, Regularization: 0.003733\n",
      "2019-04-10 01:04:19,294 root         INFO     Train Epoch: 180 [2560/8000 (32%)]\tTotal Loss: 0.172084\n",
      "Reconstruction: 0.167428, Regularization: 0.004655\n",
      "2019-04-10 01:04:19,355 root         INFO     Train Epoch: 180 [3072/8000 (38%)]\tTotal Loss: 0.164681\n",
      "Reconstruction: 0.161433, Regularization: 0.003248\n",
      "2019-04-10 01:04:19,417 root         INFO     Train Epoch: 180 [3584/8000 (45%)]\tTotal Loss: 0.160503\n",
      "Reconstruction: 0.157275, Regularization: 0.003228\n",
      "2019-04-10 01:04:19,478 root         INFO     Train Epoch: 180 [4096/8000 (51%)]\tTotal Loss: 0.163893\n",
      "Reconstruction: 0.160758, Regularization: 0.003135\n",
      "2019-04-10 01:04:19,540 root         INFO     Train Epoch: 180 [4608/8000 (58%)]\tTotal Loss: 0.168523\n",
      "Reconstruction: 0.164238, Regularization: 0.004285\n",
      "2019-04-10 01:04:19,601 root         INFO     Train Epoch: 180 [5120/8000 (64%)]\tTotal Loss: 0.164475\n",
      "Reconstruction: 0.161508, Regularization: 0.002967\n",
      "2019-04-10 01:04:19,663 root         INFO     Train Epoch: 180 [5632/8000 (70%)]\tTotal Loss: 0.159662\n",
      "Reconstruction: 0.156401, Regularization: 0.003261\n",
      "2019-04-10 01:04:19,724 root         INFO     Train Epoch: 180 [6144/8000 (77%)]\tTotal Loss: 0.152733\n",
      "Reconstruction: 0.150679, Regularization: 0.002054\n",
      "2019-04-10 01:04:19,786 root         INFO     Train Epoch: 180 [6656/8000 (83%)]\tTotal Loss: 0.163429\n",
      "Reconstruction: 0.160027, Regularization: 0.003403\n",
      "2019-04-10 01:04:19,846 root         INFO     Train Epoch: 180 [7168/8000 (90%)]\tTotal Loss: 0.154995\n",
      "Reconstruction: 0.151946, Regularization: 0.003049\n",
      "2019-04-10 01:04:19,907 root         INFO     Train Epoch: 180 [7680/8000 (96%)]\tTotal Loss: 0.153838\n",
      "Reconstruction: 0.150622, Regularization: 0.003217\n",
      "2019-04-10 01:04:19,960 root         INFO     ====> Epoch: 180 Average loss: 0.1653\n",
      "2019-04-10 01:04:19,984 root         INFO     Train Epoch: 181 [0/8000 (0%)]\tTotal Loss: 0.158477\n",
      "Reconstruction: 0.155320, Regularization: 0.003158\n",
      "2019-04-10 01:04:20,046 root         INFO     Train Epoch: 181 [512/8000 (6%)]\tTotal Loss: 0.172953\n",
      "Reconstruction: 0.168294, Regularization: 0.004660\n",
      "2019-04-10 01:04:20,108 root         INFO     Train Epoch: 181 [1024/8000 (13%)]\tTotal Loss: 0.158154\n",
      "Reconstruction: 0.154609, Regularization: 0.003545\n",
      "2019-04-10 01:04:20,170 root         INFO     Train Epoch: 181 [1536/8000 (19%)]\tTotal Loss: 0.168432\n",
      "Reconstruction: 0.164347, Regularization: 0.004086\n",
      "2019-04-10 01:04:20,233 root         INFO     Train Epoch: 181 [2048/8000 (26%)]\tTotal Loss: 0.163101\n",
      "Reconstruction: 0.159870, Regularization: 0.003231\n",
      "2019-04-10 01:04:20,295 root         INFO     Train Epoch: 181 [2560/8000 (32%)]\tTotal Loss: 0.163418\n",
      "Reconstruction: 0.160266, Regularization: 0.003152\n",
      "2019-04-10 01:04:20,358 root         INFO     Train Epoch: 181 [3072/8000 (38%)]\tTotal Loss: 0.167350\n",
      "Reconstruction: 0.163883, Regularization: 0.003467\n",
      "2019-04-10 01:04:20,420 root         INFO     Train Epoch: 181 [3584/8000 (45%)]\tTotal Loss: 0.162099\n",
      "Reconstruction: 0.159152, Regularization: 0.002947\n",
      "2019-04-10 01:04:20,482 root         INFO     Train Epoch: 181 [4096/8000 (51%)]\tTotal Loss: 0.163065\n",
      "Reconstruction: 0.159965, Regularization: 0.003100\n",
      "2019-04-10 01:04:20,544 root         INFO     Train Epoch: 181 [4608/8000 (58%)]\tTotal Loss: 0.161903\n",
      "Reconstruction: 0.158459, Regularization: 0.003443\n",
      "2019-04-10 01:04:20,606 root         INFO     Train Epoch: 181 [5120/8000 (64%)]\tTotal Loss: 0.168336\n",
      "Reconstruction: 0.164613, Regularization: 0.003723\n",
      "2019-04-10 01:04:20,668 root         INFO     Train Epoch: 181 [5632/8000 (70%)]\tTotal Loss: 0.175230\n",
      "Reconstruction: 0.170428, Regularization: 0.004801\n",
      "2019-04-10 01:04:20,731 root         INFO     Train Epoch: 181 [6144/8000 (77%)]\tTotal Loss: 0.162718\n",
      "Reconstruction: 0.159224, Regularization: 0.003494\n",
      "2019-04-10 01:04:20,794 root         INFO     Train Epoch: 181 [6656/8000 (83%)]\tTotal Loss: 0.168194\n",
      "Reconstruction: 0.163707, Regularization: 0.004486\n",
      "2019-04-10 01:04:20,855 root         INFO     Train Epoch: 181 [7168/8000 (90%)]\tTotal Loss: 0.163025\n",
      "Reconstruction: 0.159215, Regularization: 0.003810\n",
      "2019-04-10 01:04:20,917 root         INFO     Train Epoch: 181 [7680/8000 (96%)]\tTotal Loss: 0.159274\n",
      "Reconstruction: 0.155682, Regularization: 0.003593\n",
      "2019-04-10 01:04:20,970 root         INFO     ====> Epoch: 181 Average loss: 0.1657\n",
      "2019-04-10 01:04:20,994 root         INFO     Train Epoch: 182 [0/8000 (0%)]\tTotal Loss: 0.164188\n",
      "Reconstruction: 0.160628, Regularization: 0.003561\n",
      "2019-04-10 01:04:21,057 root         INFO     Train Epoch: 182 [512/8000 (6%)]\tTotal Loss: 0.174331\n",
      "Reconstruction: 0.169807, Regularization: 0.004524\n",
      "2019-04-10 01:04:21,121 root         INFO     Train Epoch: 182 [1024/8000 (13%)]\tTotal Loss: 0.179135\n",
      "Reconstruction: 0.173701, Regularization: 0.005434\n",
      "2019-04-10 01:04:21,184 root         INFO     Train Epoch: 182 [1536/8000 (19%)]\tTotal Loss: 0.154201\n",
      "Reconstruction: 0.150781, Regularization: 0.003420\n",
      "2019-04-10 01:04:21,247 root         INFO     Train Epoch: 182 [2048/8000 (26%)]\tTotal Loss: 0.163745\n",
      "Reconstruction: 0.160473, Regularization: 0.003272\n",
      "2019-04-10 01:04:21,311 root         INFO     Train Epoch: 182 [2560/8000 (32%)]\tTotal Loss: 0.159186\n",
      "Reconstruction: 0.156047, Regularization: 0.003139\n",
      "2019-04-10 01:04:21,375 root         INFO     Train Epoch: 182 [3072/8000 (38%)]\tTotal Loss: 0.162669\n",
      "Reconstruction: 0.159016, Regularization: 0.003653\n",
      "2019-04-10 01:04:21,438 root         INFO     Train Epoch: 182 [3584/8000 (45%)]\tTotal Loss: 0.162123\n",
      "Reconstruction: 0.158471, Regularization: 0.003652\n",
      "2019-04-10 01:04:21,501 root         INFO     Train Epoch: 182 [4096/8000 (51%)]\tTotal Loss: 0.153568\n",
      "Reconstruction: 0.150867, Regularization: 0.002701\n",
      "2019-04-10 01:04:21,565 root         INFO     Train Epoch: 182 [4608/8000 (58%)]\tTotal Loss: 0.169623\n",
      "Reconstruction: 0.164654, Regularization: 0.004970\n",
      "2019-04-10 01:04:21,628 root         INFO     Train Epoch: 182 [5120/8000 (64%)]\tTotal Loss: 0.165007\n",
      "Reconstruction: 0.161319, Regularization: 0.003688\n",
      "2019-04-10 01:04:21,692 root         INFO     Train Epoch: 182 [5632/8000 (70%)]\tTotal Loss: 0.170399\n",
      "Reconstruction: 0.166055, Regularization: 0.004344\n",
      "2019-04-10 01:04:21,755 root         INFO     Train Epoch: 182 [6144/8000 (77%)]\tTotal Loss: 0.162403\n",
      "Reconstruction: 0.159245, Regularization: 0.003159\n",
      "2019-04-10 01:04:21,819 root         INFO     Train Epoch: 182 [6656/8000 (83%)]\tTotal Loss: 0.150984\n",
      "Reconstruction: 0.148531, Regularization: 0.002452\n",
      "2019-04-10 01:04:21,882 root         INFO     Train Epoch: 182 [7168/8000 (90%)]\tTotal Loss: 0.169341\n",
      "Reconstruction: 0.164699, Regularization: 0.004643\n",
      "2019-04-10 01:04:21,945 root         INFO     Train Epoch: 182 [7680/8000 (96%)]\tTotal Loss: 0.171884\n",
      "Reconstruction: 0.166633, Regularization: 0.005251\n",
      "2019-04-10 01:04:21,999 root         INFO     ====> Epoch: 182 Average loss: 0.1656\n",
      "2019-04-10 01:04:22,023 root         INFO     Train Epoch: 183 [0/8000 (0%)]\tTotal Loss: 0.162173\n",
      "Reconstruction: 0.159044, Regularization: 0.003129\n",
      "2019-04-10 01:04:22,087 root         INFO     Train Epoch: 183 [512/8000 (6%)]\tTotal Loss: 0.182728\n",
      "Reconstruction: 0.175546, Regularization: 0.007182\n",
      "2019-04-10 01:04:22,151 root         INFO     Train Epoch: 183 [1024/8000 (13%)]\tTotal Loss: 0.165456\n",
      "Reconstruction: 0.161633, Regularization: 0.003824\n",
      "2019-04-10 01:04:22,214 root         INFO     Train Epoch: 183 [1536/8000 (19%)]\tTotal Loss: 0.159622\n",
      "Reconstruction: 0.156295, Regularization: 0.003327\n",
      "2019-04-10 01:04:22,278 root         INFO     Train Epoch: 183 [2048/8000 (26%)]\tTotal Loss: 0.153067\n",
      "Reconstruction: 0.150284, Regularization: 0.002782\n",
      "2019-04-10 01:04:22,341 root         INFO     Train Epoch: 183 [2560/8000 (32%)]\tTotal Loss: 0.170127\n",
      "Reconstruction: 0.166081, Regularization: 0.004046\n",
      "2019-04-10 01:04:22,404 root         INFO     Train Epoch: 183 [3072/8000 (38%)]\tTotal Loss: 0.155804\n",
      "Reconstruction: 0.152976, Regularization: 0.002828\n",
      "2019-04-10 01:04:22,468 root         INFO     Train Epoch: 183 [3584/8000 (45%)]\tTotal Loss: 0.166068\n",
      "Reconstruction: 0.161482, Regularization: 0.004586\n",
      "2019-04-10 01:04:22,531 root         INFO     Train Epoch: 183 [4096/8000 (51%)]\tTotal Loss: 0.166565\n",
      "Reconstruction: 0.162687, Regularization: 0.003878\n",
      "2019-04-10 01:04:22,593 root         INFO     Train Epoch: 183 [4608/8000 (58%)]\tTotal Loss: 0.157665\n",
      "Reconstruction: 0.154496, Regularization: 0.003170\n",
      "2019-04-10 01:04:22,656 root         INFO     Train Epoch: 183 [5120/8000 (64%)]\tTotal Loss: 0.154991\n",
      "Reconstruction: 0.152167, Regularization: 0.002824\n",
      "2019-04-10 01:04:22,719 root         INFO     Train Epoch: 183 [5632/8000 (70%)]\tTotal Loss: 0.168588\n",
      "Reconstruction: 0.164550, Regularization: 0.004038\n",
      "2019-04-10 01:04:22,781 root         INFO     Train Epoch: 183 [6144/8000 (77%)]\tTotal Loss: 0.160705\n",
      "Reconstruction: 0.157420, Regularization: 0.003285\n",
      "2019-04-10 01:04:22,844 root         INFO     Train Epoch: 183 [6656/8000 (83%)]\tTotal Loss: 0.160292\n",
      "Reconstruction: 0.156693, Regularization: 0.003599\n",
      "2019-04-10 01:04:22,907 root         INFO     Train Epoch: 183 [7168/8000 (90%)]\tTotal Loss: 0.159899\n",
      "Reconstruction: 0.156369, Regularization: 0.003530\n",
      "2019-04-10 01:04:22,970 root         INFO     Train Epoch: 183 [7680/8000 (96%)]\tTotal Loss: 0.157843\n",
      "Reconstruction: 0.154061, Regularization: 0.003782\n",
      "2019-04-10 01:04:23,024 root         INFO     ====> Epoch: 183 Average loss: 0.1654\n",
      "2019-04-10 01:04:23,048 root         INFO     Train Epoch: 184 [0/8000 (0%)]\tTotal Loss: 0.177439\n",
      "Reconstruction: 0.172521, Regularization: 0.004918\n",
      "2019-04-10 01:04:23,112 root         INFO     Train Epoch: 184 [512/8000 (6%)]\tTotal Loss: 0.158425\n",
      "Reconstruction: 0.154919, Regularization: 0.003506\n",
      "2019-04-10 01:04:23,175 root         INFO     Train Epoch: 184 [1024/8000 (13%)]\tTotal Loss: 0.164011\n",
      "Reconstruction: 0.160099, Regularization: 0.003912\n",
      "2019-04-10 01:04:23,238 root         INFO     Train Epoch: 184 [1536/8000 (19%)]\tTotal Loss: 0.169109\n",
      "Reconstruction: 0.165749, Regularization: 0.003359\n",
      "2019-04-10 01:04:23,302 root         INFO     Train Epoch: 184 [2048/8000 (26%)]\tTotal Loss: 0.159389\n",
      "Reconstruction: 0.156487, Regularization: 0.002902\n",
      "2019-04-10 01:04:23,366 root         INFO     Train Epoch: 184 [2560/8000 (32%)]\tTotal Loss: 0.170719\n",
      "Reconstruction: 0.166871, Regularization: 0.003849\n",
      "2019-04-10 01:04:23,429 root         INFO     Train Epoch: 184 [3072/8000 (38%)]\tTotal Loss: 0.171230\n",
      "Reconstruction: 0.166068, Regularization: 0.005162\n",
      "2019-04-10 01:04:23,492 root         INFO     Train Epoch: 184 [3584/8000 (45%)]\tTotal Loss: 0.172242\n",
      "Reconstruction: 0.167088, Regularization: 0.005154\n",
      "2019-04-10 01:04:23,555 root         INFO     Train Epoch: 184 [4096/8000 (51%)]\tTotal Loss: 0.163124\n",
      "Reconstruction: 0.159011, Regularization: 0.004112\n",
      "2019-04-10 01:04:23,619 root         INFO     Train Epoch: 184 [4608/8000 (58%)]\tTotal Loss: 0.191681\n",
      "Reconstruction: 0.184037, Regularization: 0.007644\n",
      "2019-04-10 01:04:23,682 root         INFO     Train Epoch: 184 [5120/8000 (64%)]\tTotal Loss: 0.173239\n",
      "Reconstruction: 0.168602, Regularization: 0.004637\n",
      "2019-04-10 01:04:23,746 root         INFO     Train Epoch: 184 [5632/8000 (70%)]\tTotal Loss: 0.173048\n",
      "Reconstruction: 0.168190, Regularization: 0.004858\n",
      "2019-04-10 01:04:23,809 root         INFO     Train Epoch: 184 [6144/8000 (77%)]\tTotal Loss: 0.161062\n",
      "Reconstruction: 0.157409, Regularization: 0.003653\n",
      "2019-04-10 01:04:23,873 root         INFO     Train Epoch: 184 [6656/8000 (83%)]\tTotal Loss: 0.157764\n",
      "Reconstruction: 0.154611, Regularization: 0.003153\n",
      "2019-04-10 01:04:23,936 root         INFO     Train Epoch: 184 [7168/8000 (90%)]\tTotal Loss: 0.169226\n",
      "Reconstruction: 0.165487, Regularization: 0.003739\n",
      "2019-04-10 01:04:24,000 root         INFO     Train Epoch: 184 [7680/8000 (96%)]\tTotal Loss: 0.157371\n",
      "Reconstruction: 0.153924, Regularization: 0.003447\n",
      "2019-04-10 01:04:24,054 root         INFO     ====> Epoch: 184 Average loss: 0.1651\n",
      "2019-04-10 01:04:24,078 root         INFO     Train Epoch: 185 [0/8000 (0%)]\tTotal Loss: 0.165670\n",
      "Reconstruction: 0.161719, Regularization: 0.003952\n",
      "2019-04-10 01:04:24,142 root         INFO     Train Epoch: 185 [512/8000 (6%)]\tTotal Loss: 0.162593\n",
      "Reconstruction: 0.159334, Regularization: 0.003260\n",
      "2019-04-10 01:04:24,207 root         INFO     Train Epoch: 185 [1024/8000 (13%)]\tTotal Loss: 0.155263\n",
      "Reconstruction: 0.152151, Regularization: 0.003112\n",
      "2019-04-10 01:04:24,271 root         INFO     Train Epoch: 185 [1536/8000 (19%)]\tTotal Loss: 0.165725\n",
      "Reconstruction: 0.161151, Regularization: 0.004574\n",
      "2019-04-10 01:04:24,334 root         INFO     Train Epoch: 185 [2048/8000 (26%)]\tTotal Loss: 0.165384\n",
      "Reconstruction: 0.161182, Regularization: 0.004202\n",
      "2019-04-10 01:04:24,399 root         INFO     Train Epoch: 185 [2560/8000 (32%)]\tTotal Loss: 0.162415\n",
      "Reconstruction: 0.158989, Regularization: 0.003426\n",
      "2019-04-10 01:04:24,463 root         INFO     Train Epoch: 185 [3072/8000 (38%)]\tTotal Loss: 0.159813\n",
      "Reconstruction: 0.156396, Regularization: 0.003417\n",
      "2019-04-10 01:04:24,527 root         INFO     Train Epoch: 185 [3584/8000 (45%)]\tTotal Loss: 0.172649\n",
      "Reconstruction: 0.166934, Regularization: 0.005715\n",
      "2019-04-10 01:04:24,590 root         INFO     Train Epoch: 185 [4096/8000 (51%)]\tTotal Loss: 0.159457\n",
      "Reconstruction: 0.156376, Regularization: 0.003080\n",
      "2019-04-10 01:04:24,655 root         INFO     Train Epoch: 185 [4608/8000 (58%)]\tTotal Loss: 0.162149\n",
      "Reconstruction: 0.158341, Regularization: 0.003808\n",
      "2019-04-10 01:04:24,719 root         INFO     Train Epoch: 185 [5120/8000 (64%)]\tTotal Loss: 0.171704\n",
      "Reconstruction: 0.165687, Regularization: 0.006016\n",
      "2019-04-10 01:04:24,783 root         INFO     Train Epoch: 185 [5632/8000 (70%)]\tTotal Loss: 0.148542\n",
      "Reconstruction: 0.146026, Regularization: 0.002515\n",
      "2019-04-10 01:04:24,846 root         INFO     Train Epoch: 185 [6144/8000 (77%)]\tTotal Loss: 0.161897\n",
      "Reconstruction: 0.157701, Regularization: 0.004195\n",
      "2019-04-10 01:04:24,910 root         INFO     Train Epoch: 185 [6656/8000 (83%)]\tTotal Loss: 0.165665\n",
      "Reconstruction: 0.160795, Regularization: 0.004870\n",
      "2019-04-10 01:04:24,972 root         INFO     Train Epoch: 185 [7168/8000 (90%)]\tTotal Loss: 0.163869\n",
      "Reconstruction: 0.159672, Regularization: 0.004197\n",
      "2019-04-10 01:04:25,034 root         INFO     Train Epoch: 185 [7680/8000 (96%)]\tTotal Loss: 0.169257\n",
      "Reconstruction: 0.165519, Regularization: 0.003738\n",
      "2019-04-10 01:04:25,087 root         INFO     ====> Epoch: 185 Average loss: 0.1653\n",
      "2019-04-10 01:04:25,110 root         INFO     Train Epoch: 186 [0/8000 (0%)]\tTotal Loss: 0.170325\n",
      "Reconstruction: 0.165858, Regularization: 0.004467\n",
      "2019-04-10 01:04:25,173 root         INFO     Train Epoch: 186 [512/8000 (6%)]\tTotal Loss: 0.174945\n",
      "Reconstruction: 0.168704, Regularization: 0.006240\n",
      "2019-04-10 01:04:25,237 root         INFO     Train Epoch: 186 [1024/8000 (13%)]\tTotal Loss: 0.170319\n",
      "Reconstruction: 0.165454, Regularization: 0.004865\n",
      "2019-04-10 01:04:25,300 root         INFO     Train Epoch: 186 [1536/8000 (19%)]\tTotal Loss: 0.175702\n",
      "Reconstruction: 0.169675, Regularization: 0.006026\n",
      "2019-04-10 01:04:25,364 root         INFO     Train Epoch: 186 [2048/8000 (26%)]\tTotal Loss: 0.155171\n",
      "Reconstruction: 0.151838, Regularization: 0.003334\n",
      "2019-04-10 01:04:25,427 root         INFO     Train Epoch: 186 [2560/8000 (32%)]\tTotal Loss: 0.169333\n",
      "Reconstruction: 0.164545, Regularization: 0.004788\n",
      "2019-04-10 01:04:25,488 root         INFO     Train Epoch: 186 [3072/8000 (38%)]\tTotal Loss: 0.179468\n",
      "Reconstruction: 0.174064, Regularization: 0.005404\n",
      "2019-04-10 01:04:25,552 root         INFO     Train Epoch: 186 [3584/8000 (45%)]\tTotal Loss: 0.174777\n",
      "Reconstruction: 0.168940, Regularization: 0.005838\n",
      "2019-04-10 01:04:25,615 root         INFO     Train Epoch: 186 [4096/8000 (51%)]\tTotal Loss: 0.159141\n",
      "Reconstruction: 0.155472, Regularization: 0.003669\n",
      "2019-04-10 01:04:25,678 root         INFO     Train Epoch: 186 [4608/8000 (58%)]\tTotal Loss: 0.163485\n",
      "Reconstruction: 0.159838, Regularization: 0.003647\n",
      "2019-04-10 01:04:25,741 root         INFO     Train Epoch: 186 [5120/8000 (64%)]\tTotal Loss: 0.167083\n",
      "Reconstruction: 0.162959, Regularization: 0.004124\n",
      "2019-04-10 01:04:25,805 root         INFO     Train Epoch: 186 [5632/8000 (70%)]\tTotal Loss: 0.149309\n",
      "Reconstruction: 0.146772, Regularization: 0.002537\n",
      "2019-04-10 01:04:25,868 root         INFO     Train Epoch: 186 [6144/8000 (77%)]\tTotal Loss: 0.158365\n",
      "Reconstruction: 0.154772, Regularization: 0.003593\n",
      "2019-04-10 01:04:25,932 root         INFO     Train Epoch: 186 [6656/8000 (83%)]\tTotal Loss: 0.160938\n",
      "Reconstruction: 0.156822, Regularization: 0.004116\n",
      "2019-04-10 01:04:25,995 root         INFO     Train Epoch: 186 [7168/8000 (90%)]\tTotal Loss: 0.167086\n",
      "Reconstruction: 0.163325, Regularization: 0.003761\n",
      "2019-04-10 01:04:26,058 root         INFO     Train Epoch: 186 [7680/8000 (96%)]\tTotal Loss: 0.161211\n",
      "Reconstruction: 0.157346, Regularization: 0.003865\n",
      "2019-04-10 01:04:26,112 root         INFO     ====> Epoch: 186 Average loss: 0.1653\n",
      "2019-04-10 01:04:26,135 root         INFO     Train Epoch: 187 [0/8000 (0%)]\tTotal Loss: 0.176074\n",
      "Reconstruction: 0.170541, Regularization: 0.005534\n",
      "2019-04-10 01:04:26,199 root         INFO     Train Epoch: 187 [512/8000 (6%)]\tTotal Loss: 0.152781\n",
      "Reconstruction: 0.150059, Regularization: 0.002722\n",
      "2019-04-10 01:04:26,263 root         INFO     Train Epoch: 187 [1024/8000 (13%)]\tTotal Loss: 0.181599\n",
      "Reconstruction: 0.175745, Regularization: 0.005853\n",
      "2019-04-10 01:04:26,327 root         INFO     Train Epoch: 187 [1536/8000 (19%)]\tTotal Loss: 0.164467\n",
      "Reconstruction: 0.159972, Regularization: 0.004495\n",
      "2019-04-10 01:04:26,391 root         INFO     Train Epoch: 187 [2048/8000 (26%)]\tTotal Loss: 0.157161\n",
      "Reconstruction: 0.153434, Regularization: 0.003727\n",
      "2019-04-10 01:04:26,455 root         INFO     Train Epoch: 187 [2560/8000 (32%)]\tTotal Loss: 0.154245\n",
      "Reconstruction: 0.150941, Regularization: 0.003304\n",
      "2019-04-10 01:04:26,519 root         INFO     Train Epoch: 187 [3072/8000 (38%)]\tTotal Loss: 0.165972\n",
      "Reconstruction: 0.161987, Regularization: 0.003985\n",
      "2019-04-10 01:04:26,582 root         INFO     Train Epoch: 187 [3584/8000 (45%)]\tTotal Loss: 0.183053\n",
      "Reconstruction: 0.176787, Regularization: 0.006266\n",
      "2019-04-10 01:04:26,647 root         INFO     Train Epoch: 187 [4096/8000 (51%)]\tTotal Loss: 0.161959\n",
      "Reconstruction: 0.157766, Regularization: 0.004193\n",
      "2019-04-10 01:04:26,711 root         INFO     Train Epoch: 187 [4608/8000 (58%)]\tTotal Loss: 0.160687\n",
      "Reconstruction: 0.157058, Regularization: 0.003628\n",
      "2019-04-10 01:04:26,775 root         INFO     Train Epoch: 187 [5120/8000 (64%)]\tTotal Loss: 0.167655\n",
      "Reconstruction: 0.163172, Regularization: 0.004483\n",
      "2019-04-10 01:04:26,839 root         INFO     Train Epoch: 187 [5632/8000 (70%)]\tTotal Loss: 0.165028\n",
      "Reconstruction: 0.160550, Regularization: 0.004477\n",
      "2019-04-10 01:04:26,902 root         INFO     Train Epoch: 187 [6144/8000 (77%)]\tTotal Loss: 0.172463\n",
      "Reconstruction: 0.167593, Regularization: 0.004870\n",
      "2019-04-10 01:04:26,966 root         INFO     Train Epoch: 187 [6656/8000 (83%)]\tTotal Loss: 0.173577\n",
      "Reconstruction: 0.168265, Regularization: 0.005312\n",
      "2019-04-10 01:04:27,030 root         INFO     Train Epoch: 187 [7168/8000 (90%)]\tTotal Loss: 0.165970\n",
      "Reconstruction: 0.161010, Regularization: 0.004960\n",
      "2019-04-10 01:04:27,094 root         INFO     Train Epoch: 187 [7680/8000 (96%)]\tTotal Loss: 0.169484\n",
      "Reconstruction: 0.164321, Regularization: 0.005163\n",
      "2019-04-10 01:04:27,148 root         INFO     ====> Epoch: 187 Average loss: 0.1652\n",
      "2019-04-10 01:04:27,172 root         INFO     Train Epoch: 188 [0/8000 (0%)]\tTotal Loss: 0.161920\n",
      "Reconstruction: 0.157999, Regularization: 0.003921\n",
      "2019-04-10 01:04:27,236 root         INFO     Train Epoch: 188 [512/8000 (6%)]\tTotal Loss: 0.161872\n",
      "Reconstruction: 0.157840, Regularization: 0.004033\n",
      "2019-04-10 01:04:27,299 root         INFO     Train Epoch: 188 [1024/8000 (13%)]\tTotal Loss: 0.155891\n",
      "Reconstruction: 0.152644, Regularization: 0.003247\n",
      "2019-04-10 01:04:27,362 root         INFO     Train Epoch: 188 [1536/8000 (19%)]\tTotal Loss: 0.164889\n",
      "Reconstruction: 0.160309, Regularization: 0.004580\n",
      "2019-04-10 01:04:27,426 root         INFO     Train Epoch: 188 [2048/8000 (26%)]\tTotal Loss: 0.163693\n",
      "Reconstruction: 0.159529, Regularization: 0.004164\n",
      "2019-04-10 01:04:27,489 root         INFO     Train Epoch: 188 [2560/8000 (32%)]\tTotal Loss: 0.161818\n",
      "Reconstruction: 0.156513, Regularization: 0.005305\n",
      "2019-04-10 01:04:27,552 root         INFO     Train Epoch: 188 [3072/8000 (38%)]\tTotal Loss: 0.185806\n",
      "Reconstruction: 0.179872, Regularization: 0.005934\n",
      "2019-04-10 01:04:27,616 root         INFO     Train Epoch: 188 [3584/8000 (45%)]\tTotal Loss: 0.152865\n",
      "Reconstruction: 0.149877, Regularization: 0.002988\n",
      "2019-04-10 01:04:27,679 root         INFO     Train Epoch: 188 [4096/8000 (51%)]\tTotal Loss: 0.178072\n",
      "Reconstruction: 0.173220, Regularization: 0.004851\n",
      "2019-04-10 01:04:27,742 root         INFO     Train Epoch: 188 [4608/8000 (58%)]\tTotal Loss: 0.150242\n",
      "Reconstruction: 0.146932, Regularization: 0.003310\n",
      "2019-04-10 01:04:27,806 root         INFO     Train Epoch: 188 [5120/8000 (64%)]\tTotal Loss: 0.159235\n",
      "Reconstruction: 0.155753, Regularization: 0.003482\n",
      "2019-04-10 01:04:27,869 root         INFO     Train Epoch: 188 [5632/8000 (70%)]\tTotal Loss: 0.173724\n",
      "Reconstruction: 0.168987, Regularization: 0.004736\n",
      "2019-04-10 01:04:27,933 root         INFO     Train Epoch: 188 [6144/8000 (77%)]\tTotal Loss: 0.175978\n",
      "Reconstruction: 0.170917, Regularization: 0.005061\n",
      "2019-04-10 01:04:27,996 root         INFO     Train Epoch: 188 [6656/8000 (83%)]\tTotal Loss: 0.165436\n",
      "Reconstruction: 0.160823, Regularization: 0.004613\n",
      "2019-04-10 01:04:28,059 root         INFO     Train Epoch: 188 [7168/8000 (90%)]\tTotal Loss: 0.172104\n",
      "Reconstruction: 0.166487, Regularization: 0.005617\n",
      "2019-04-10 01:04:28,123 root         INFO     Train Epoch: 188 [7680/8000 (96%)]\tTotal Loss: 0.172094\n",
      "Reconstruction: 0.167521, Regularization: 0.004573\n",
      "2019-04-10 01:04:28,177 root         INFO     ====> Epoch: 188 Average loss: 0.1651\n",
      "2019-04-10 01:04:28,200 root         INFO     Train Epoch: 189 [0/8000 (0%)]\tTotal Loss: 0.160426\n",
      "Reconstruction: 0.156340, Regularization: 0.004087\n",
      "2019-04-10 01:04:28,266 root         INFO     Train Epoch: 189 [512/8000 (6%)]\tTotal Loss: 0.160142\n",
      "Reconstruction: 0.156680, Regularization: 0.003462\n",
      "2019-04-10 01:04:28,329 root         INFO     Train Epoch: 189 [1024/8000 (13%)]\tTotal Loss: 0.177171\n",
      "Reconstruction: 0.172400, Regularization: 0.004771\n",
      "2019-04-10 01:04:28,393 root         INFO     Train Epoch: 189 [1536/8000 (19%)]\tTotal Loss: 0.161152\n",
      "Reconstruction: 0.157650, Regularization: 0.003502\n",
      "2019-04-10 01:04:28,457 root         INFO     Train Epoch: 189 [2048/8000 (26%)]\tTotal Loss: 0.161003\n",
      "Reconstruction: 0.156944, Regularization: 0.004059\n",
      "2019-04-10 01:04:28,521 root         INFO     Train Epoch: 189 [2560/8000 (32%)]\tTotal Loss: 0.169322\n",
      "Reconstruction: 0.164185, Regularization: 0.005138\n",
      "2019-04-10 01:04:28,585 root         INFO     Train Epoch: 189 [3072/8000 (38%)]\tTotal Loss: 0.170110\n",
      "Reconstruction: 0.165096, Regularization: 0.005014\n",
      "2019-04-10 01:04:28,649 root         INFO     Train Epoch: 189 [3584/8000 (45%)]\tTotal Loss: 0.178837\n",
      "Reconstruction: 0.172233, Regularization: 0.006604\n",
      "2019-04-10 01:04:28,713 root         INFO     Train Epoch: 189 [4096/8000 (51%)]\tTotal Loss: 0.157169\n",
      "Reconstruction: 0.153111, Regularization: 0.004058\n",
      "2019-04-10 01:04:28,777 root         INFO     Train Epoch: 189 [4608/8000 (58%)]\tTotal Loss: 0.161219\n",
      "Reconstruction: 0.157068, Regularization: 0.004151\n",
      "2019-04-10 01:04:28,841 root         INFO     Train Epoch: 189 [5120/8000 (64%)]\tTotal Loss: 0.167980\n",
      "Reconstruction: 0.162785, Regularization: 0.005194\n",
      "2019-04-10 01:04:28,905 root         INFO     Train Epoch: 189 [5632/8000 (70%)]\tTotal Loss: 0.152820\n",
      "Reconstruction: 0.149641, Regularization: 0.003179\n",
      "2019-04-10 01:04:28,969 root         INFO     Train Epoch: 189 [6144/8000 (77%)]\tTotal Loss: 0.162947\n",
      "Reconstruction: 0.158162, Regularization: 0.004786\n",
      "2019-04-10 01:04:29,033 root         INFO     Train Epoch: 189 [6656/8000 (83%)]\tTotal Loss: 0.163825\n",
      "Reconstruction: 0.159785, Regularization: 0.004040\n",
      "2019-04-10 01:04:29,097 root         INFO     Train Epoch: 189 [7168/8000 (90%)]\tTotal Loss: 0.164480\n",
      "Reconstruction: 0.159913, Regularization: 0.004567\n",
      "2019-04-10 01:04:29,160 root         INFO     Train Epoch: 189 [7680/8000 (96%)]\tTotal Loss: 0.163525\n",
      "Reconstruction: 0.159225, Regularization: 0.004300\n",
      "2019-04-10 01:04:29,214 root         INFO     ====> Epoch: 189 Average loss: 0.1651\n",
      "2019-04-10 01:04:29,239 root         INFO     Train Epoch: 190 [0/8000 (0%)]\tTotal Loss: 0.174129\n",
      "Reconstruction: 0.169256, Regularization: 0.004873\n",
      "2019-04-10 01:04:29,303 root         INFO     Train Epoch: 190 [512/8000 (6%)]\tTotal Loss: 0.170550\n",
      "Reconstruction: 0.164853, Regularization: 0.005697\n",
      "2019-04-10 01:04:29,367 root         INFO     Train Epoch: 190 [1024/8000 (13%)]\tTotal Loss: 0.168508\n",
      "Reconstruction: 0.162948, Regularization: 0.005560\n",
      "2019-04-10 01:04:29,430 root         INFO     Train Epoch: 190 [1536/8000 (19%)]\tTotal Loss: 0.178498\n",
      "Reconstruction: 0.171955, Regularization: 0.006543\n",
      "2019-04-10 01:04:29,494 root         INFO     Train Epoch: 190 [2048/8000 (26%)]\tTotal Loss: 0.167311\n",
      "Reconstruction: 0.162514, Regularization: 0.004797\n",
      "2019-04-10 01:04:29,556 root         INFO     Train Epoch: 190 [2560/8000 (32%)]\tTotal Loss: 0.169990\n",
      "Reconstruction: 0.164569, Regularization: 0.005422\n",
      "2019-04-10 01:04:29,618 root         INFO     Train Epoch: 190 [3072/8000 (38%)]\tTotal Loss: 0.177720\n",
      "Reconstruction: 0.171680, Regularization: 0.006040\n",
      "2019-04-10 01:04:29,680 root         INFO     Train Epoch: 190 [3584/8000 (45%)]\tTotal Loss: 0.174332\n",
      "Reconstruction: 0.169408, Regularization: 0.004923\n",
      "2019-04-10 01:04:29,743 root         INFO     Train Epoch: 190 [4096/8000 (51%)]\tTotal Loss: 0.153633\n",
      "Reconstruction: 0.150771, Regularization: 0.002862\n",
      "2019-04-10 01:04:29,805 root         INFO     Train Epoch: 190 [4608/8000 (58%)]\tTotal Loss: 0.155046\n",
      "Reconstruction: 0.152059, Regularization: 0.002987\n",
      "2019-04-10 01:04:29,867 root         INFO     Train Epoch: 190 [5120/8000 (64%)]\tTotal Loss: 0.159288\n",
      "Reconstruction: 0.154369, Regularization: 0.004919\n",
      "2019-04-10 01:04:29,930 root         INFO     Train Epoch: 190 [5632/8000 (70%)]\tTotal Loss: 0.153089\n",
      "Reconstruction: 0.149849, Regularization: 0.003240\n",
      "2019-04-10 01:04:29,993 root         INFO     Train Epoch: 190 [6144/8000 (77%)]\tTotal Loss: 0.163417\n",
      "Reconstruction: 0.158491, Regularization: 0.004926\n",
      "2019-04-10 01:04:30,055 root         INFO     Train Epoch: 190 [6656/8000 (83%)]\tTotal Loss: 0.177181\n",
      "Reconstruction: 0.171468, Regularization: 0.005713\n",
      "2019-04-10 01:04:30,117 root         INFO     Train Epoch: 190 [7168/8000 (90%)]\tTotal Loss: 0.159722\n",
      "Reconstruction: 0.156675, Regularization: 0.003047\n",
      "2019-04-10 01:04:30,178 root         INFO     Train Epoch: 190 [7680/8000 (96%)]\tTotal Loss: 0.166173\n",
      "Reconstruction: 0.161241, Regularization: 0.004932\n",
      "2019-04-10 01:04:30,231 root         INFO     ====> Epoch: 190 Average loss: 0.1655\n",
      "2019-04-10 01:04:30,255 root         INFO     Train Epoch: 191 [0/8000 (0%)]\tTotal Loss: 0.156454\n",
      "Reconstruction: 0.152606, Regularization: 0.003849\n",
      "2019-04-10 01:04:30,317 root         INFO     Train Epoch: 191 [512/8000 (6%)]\tTotal Loss: 0.159585\n",
      "Reconstruction: 0.155589, Regularization: 0.003996\n",
      "2019-04-10 01:04:30,379 root         INFO     Train Epoch: 191 [1024/8000 (13%)]\tTotal Loss: 0.169503\n",
      "Reconstruction: 0.164241, Regularization: 0.005262\n",
      "2019-04-10 01:04:30,441 root         INFO     Train Epoch: 191 [1536/8000 (19%)]\tTotal Loss: 0.170233\n",
      "Reconstruction: 0.164881, Regularization: 0.005352\n",
      "2019-04-10 01:04:30,502 root         INFO     Train Epoch: 191 [2048/8000 (26%)]\tTotal Loss: 0.164289\n",
      "Reconstruction: 0.159890, Regularization: 0.004399\n",
      "2019-04-10 01:04:30,563 root         INFO     Train Epoch: 191 [2560/8000 (32%)]\tTotal Loss: 0.152492\n",
      "Reconstruction: 0.149253, Regularization: 0.003239\n",
      "2019-04-10 01:04:30,624 root         INFO     Train Epoch: 191 [3072/8000 (38%)]\tTotal Loss: 0.161260\n",
      "Reconstruction: 0.156608, Regularization: 0.004652\n",
      "2019-04-10 01:04:30,686 root         INFO     Train Epoch: 191 [3584/8000 (45%)]\tTotal Loss: 0.163198\n",
      "Reconstruction: 0.159314, Regularization: 0.003884\n",
      "2019-04-10 01:04:30,747 root         INFO     Train Epoch: 191 [4096/8000 (51%)]\tTotal Loss: 0.150305\n",
      "Reconstruction: 0.147314, Regularization: 0.002990\n",
      "2019-04-10 01:04:30,808 root         INFO     Train Epoch: 191 [4608/8000 (58%)]\tTotal Loss: 0.167258\n",
      "Reconstruction: 0.161815, Regularization: 0.005443\n",
      "2019-04-10 01:04:30,869 root         INFO     Train Epoch: 191 [5120/8000 (64%)]\tTotal Loss: 0.160378\n",
      "Reconstruction: 0.156175, Regularization: 0.004203\n",
      "2019-04-10 01:04:30,931 root         INFO     Train Epoch: 191 [5632/8000 (70%)]\tTotal Loss: 0.167182\n",
      "Reconstruction: 0.162069, Regularization: 0.005113\n",
      "2019-04-10 01:04:30,993 root         INFO     Train Epoch: 191 [6144/8000 (77%)]\tTotal Loss: 0.146180\n",
      "Reconstruction: 0.143184, Regularization: 0.002997\n",
      "2019-04-10 01:04:31,055 root         INFO     Train Epoch: 191 [6656/8000 (83%)]\tTotal Loss: 0.170458\n",
      "Reconstruction: 0.165013, Regularization: 0.005445\n",
      "2019-04-10 01:04:31,118 root         INFO     Train Epoch: 191 [7168/8000 (90%)]\tTotal Loss: 0.187967\n",
      "Reconstruction: 0.179950, Regularization: 0.008017\n",
      "2019-04-10 01:04:31,179 root         INFO     Train Epoch: 191 [7680/8000 (96%)]\tTotal Loss: 0.160180\n",
      "Reconstruction: 0.156043, Regularization: 0.004136\n",
      "2019-04-10 01:04:31,233 root         INFO     ====> Epoch: 191 Average loss: 0.1654\n",
      "2019-04-10 01:04:31,256 root         INFO     Train Epoch: 192 [0/8000 (0%)]\tTotal Loss: 0.162171\n",
      "Reconstruction: 0.157622, Regularization: 0.004549\n",
      "2019-04-10 01:04:31,319 root         INFO     Train Epoch: 192 [512/8000 (6%)]\tTotal Loss: 0.164322\n",
      "Reconstruction: 0.159036, Regularization: 0.005286\n",
      "2019-04-10 01:04:31,381 root         INFO     Train Epoch: 192 [1024/8000 (13%)]\tTotal Loss: 0.160085\n",
      "Reconstruction: 0.155583, Regularization: 0.004502\n",
      "2019-04-10 01:04:31,444 root         INFO     Train Epoch: 192 [1536/8000 (19%)]\tTotal Loss: 0.160692\n",
      "Reconstruction: 0.156192, Regularization: 0.004500\n",
      "2019-04-10 01:04:31,506 root         INFO     Train Epoch: 192 [2048/8000 (26%)]\tTotal Loss: 0.155572\n",
      "Reconstruction: 0.152166, Regularization: 0.003407\n",
      "2019-04-10 01:04:31,568 root         INFO     Train Epoch: 192 [2560/8000 (32%)]\tTotal Loss: 0.168059\n",
      "Reconstruction: 0.162782, Regularization: 0.005277\n",
      "2019-04-10 01:04:31,630 root         INFO     Train Epoch: 192 [3072/8000 (38%)]\tTotal Loss: 0.162441\n",
      "Reconstruction: 0.157405, Regularization: 0.005036\n",
      "2019-04-10 01:04:31,693 root         INFO     Train Epoch: 192 [3584/8000 (45%)]\tTotal Loss: 0.165047\n",
      "Reconstruction: 0.161403, Regularization: 0.003644\n",
      "2019-04-10 01:04:31,755 root         INFO     Train Epoch: 192 [4096/8000 (51%)]\tTotal Loss: 0.163600\n",
      "Reconstruction: 0.158927, Regularization: 0.004673\n",
      "2019-04-10 01:04:31,818 root         INFO     Train Epoch: 192 [4608/8000 (58%)]\tTotal Loss: 0.159300\n",
      "Reconstruction: 0.154260, Regularization: 0.005041\n",
      "2019-04-10 01:04:31,880 root         INFO     Train Epoch: 192 [5120/8000 (64%)]\tTotal Loss: 0.166616\n",
      "Reconstruction: 0.163020, Regularization: 0.003596\n",
      "2019-04-10 01:04:31,942 root         INFO     Train Epoch: 192 [5632/8000 (70%)]\tTotal Loss: 0.148817\n",
      "Reconstruction: 0.146131, Regularization: 0.002686\n",
      "2019-04-10 01:04:32,005 root         INFO     Train Epoch: 192 [6144/8000 (77%)]\tTotal Loss: 0.163044\n",
      "Reconstruction: 0.157576, Regularization: 0.005469\n",
      "2019-04-10 01:04:32,067 root         INFO     Train Epoch: 192 [6656/8000 (83%)]\tTotal Loss: 0.148268\n",
      "Reconstruction: 0.145633, Regularization: 0.002635\n",
      "2019-04-10 01:04:32,129 root         INFO     Train Epoch: 192 [7168/8000 (90%)]\tTotal Loss: 0.159865\n",
      "Reconstruction: 0.154848, Regularization: 0.005017\n",
      "2019-04-10 01:04:32,192 root         INFO     Train Epoch: 192 [7680/8000 (96%)]\tTotal Loss: 0.168712\n",
      "Reconstruction: 0.162545, Regularization: 0.006168\n",
      "2019-04-10 01:04:32,245 root         INFO     ====> Epoch: 192 Average loss: 0.1653\n",
      "2019-04-10 01:04:32,269 root         INFO     Train Epoch: 193 [0/8000 (0%)]\tTotal Loss: 0.161865\n",
      "Reconstruction: 0.156806, Regularization: 0.005059\n",
      "2019-04-10 01:04:32,331 root         INFO     Train Epoch: 193 [512/8000 (6%)]\tTotal Loss: 0.167060\n",
      "Reconstruction: 0.162136, Regularization: 0.004923\n",
      "2019-04-10 01:04:32,393 root         INFO     Train Epoch: 193 [1024/8000 (13%)]\tTotal Loss: 0.157258\n",
      "Reconstruction: 0.153895, Regularization: 0.003363\n",
      "2019-04-10 01:04:32,455 root         INFO     Train Epoch: 193 [1536/8000 (19%)]\tTotal Loss: 0.169826\n",
      "Reconstruction: 0.165115, Regularization: 0.004711\n",
      "2019-04-10 01:04:32,516 root         INFO     Train Epoch: 193 [2048/8000 (26%)]\tTotal Loss: 0.167323\n",
      "Reconstruction: 0.161871, Regularization: 0.005453\n",
      "2019-04-10 01:04:32,579 root         INFO     Train Epoch: 193 [2560/8000 (32%)]\tTotal Loss: 0.151363\n",
      "Reconstruction: 0.148062, Regularization: 0.003301\n",
      "2019-04-10 01:04:32,641 root         INFO     Train Epoch: 193 [3072/8000 (38%)]\tTotal Loss: 0.170216\n",
      "Reconstruction: 0.165228, Regularization: 0.004988\n",
      "2019-04-10 01:04:32,703 root         INFO     Train Epoch: 193 [3584/8000 (45%)]\tTotal Loss: 0.156993\n",
      "Reconstruction: 0.152583, Regularization: 0.004411\n",
      "2019-04-10 01:04:32,765 root         INFO     Train Epoch: 193 [4096/8000 (51%)]\tTotal Loss: 0.164955\n",
      "Reconstruction: 0.160010, Regularization: 0.004946\n",
      "2019-04-10 01:04:32,826 root         INFO     Train Epoch: 193 [4608/8000 (58%)]\tTotal Loss: 0.164589\n",
      "Reconstruction: 0.158817, Regularization: 0.005772\n",
      "2019-04-10 01:04:32,888 root         INFO     Train Epoch: 193 [5120/8000 (64%)]\tTotal Loss: 0.159651\n",
      "Reconstruction: 0.155015, Regularization: 0.004636\n",
      "2019-04-10 01:04:32,950 root         INFO     Train Epoch: 193 [5632/8000 (70%)]\tTotal Loss: 0.179201\n",
      "Reconstruction: 0.172017, Regularization: 0.007184\n",
      "2019-04-10 01:04:33,012 root         INFO     Train Epoch: 193 [6144/8000 (77%)]\tTotal Loss: 0.159937\n",
      "Reconstruction: 0.155019, Regularization: 0.004919\n",
      "2019-04-10 01:04:33,073 root         INFO     Train Epoch: 193 [6656/8000 (83%)]\tTotal Loss: 0.164437\n",
      "Reconstruction: 0.159976, Regularization: 0.004460\n",
      "2019-04-10 01:04:33,135 root         INFO     Train Epoch: 193 [7168/8000 (90%)]\tTotal Loss: 0.164456\n",
      "Reconstruction: 0.160353, Regularization: 0.004103\n",
      "2019-04-10 01:04:33,197 root         INFO     Train Epoch: 193 [7680/8000 (96%)]\tTotal Loss: 0.164743\n",
      "Reconstruction: 0.158827, Regularization: 0.005917\n",
      "2019-04-10 01:04:33,250 root         INFO     ====> Epoch: 193 Average loss: 0.1651\n",
      "2019-04-10 01:04:33,274 root         INFO     Train Epoch: 194 [0/8000 (0%)]\tTotal Loss: 0.164222\n",
      "Reconstruction: 0.160126, Regularization: 0.004097\n",
      "2019-04-10 01:04:33,339 root         INFO     Train Epoch: 194 [512/8000 (6%)]\tTotal Loss: 0.169188\n",
      "Reconstruction: 0.164455, Regularization: 0.004733\n",
      "2019-04-10 01:04:33,403 root         INFO     Train Epoch: 194 [1024/8000 (13%)]\tTotal Loss: 0.159910\n",
      "Reconstruction: 0.154638, Regularization: 0.005272\n",
      "2019-04-10 01:04:33,465 root         INFO     Train Epoch: 194 [1536/8000 (19%)]\tTotal Loss: 0.170057\n",
      "Reconstruction: 0.164912, Regularization: 0.005145\n",
      "2019-04-10 01:04:33,528 root         INFO     Train Epoch: 194 [2048/8000 (26%)]\tTotal Loss: 0.168549\n",
      "Reconstruction: 0.162026, Regularization: 0.006522\n",
      "2019-04-10 01:04:33,591 root         INFO     Train Epoch: 194 [2560/8000 (32%)]\tTotal Loss: 0.153159\n",
      "Reconstruction: 0.149143, Regularization: 0.004017\n",
      "2019-04-10 01:04:33,653 root         INFO     Train Epoch: 194 [3072/8000 (38%)]\tTotal Loss: 0.161840\n",
      "Reconstruction: 0.157484, Regularization: 0.004355\n",
      "2019-04-10 01:04:33,716 root         INFO     Train Epoch: 194 [3584/8000 (45%)]\tTotal Loss: 0.166098\n",
      "Reconstruction: 0.161352, Regularization: 0.004747\n",
      "2019-04-10 01:04:33,778 root         INFO     Train Epoch: 194 [4096/8000 (51%)]\tTotal Loss: 0.175576\n",
      "Reconstruction: 0.169217, Regularization: 0.006359\n",
      "2019-04-10 01:04:33,841 root         INFO     Train Epoch: 194 [4608/8000 (58%)]\tTotal Loss: 0.175094\n",
      "Reconstruction: 0.169625, Regularization: 0.005469\n",
      "2019-04-10 01:04:33,905 root         INFO     Train Epoch: 194 [5120/8000 (64%)]\tTotal Loss: 0.162748\n",
      "Reconstruction: 0.159002, Regularization: 0.003746\n",
      "2019-04-10 01:04:33,969 root         INFO     Train Epoch: 194 [5632/8000 (70%)]\tTotal Loss: 0.169408\n",
      "Reconstruction: 0.163692, Regularization: 0.005716\n",
      "2019-04-10 01:04:34,033 root         INFO     Train Epoch: 194 [6144/8000 (77%)]\tTotal Loss: 0.166466\n",
      "Reconstruction: 0.160341, Regularization: 0.006126\n",
      "2019-04-10 01:04:34,097 root         INFO     Train Epoch: 194 [6656/8000 (83%)]\tTotal Loss: 0.171390\n",
      "Reconstruction: 0.165001, Regularization: 0.006388\n",
      "2019-04-10 01:04:34,159 root         INFO     Train Epoch: 194 [7168/8000 (90%)]\tTotal Loss: 0.165365\n",
      "Reconstruction: 0.161315, Regularization: 0.004049\n",
      "2019-04-10 01:04:34,222 root         INFO     Train Epoch: 194 [7680/8000 (96%)]\tTotal Loss: 0.170794\n",
      "Reconstruction: 0.165144, Regularization: 0.005650\n",
      "2019-04-10 01:04:34,275 root         INFO     ====> Epoch: 194 Average loss: 0.1651\n",
      "2019-04-10 01:04:34,299 root         INFO     Train Epoch: 195 [0/8000 (0%)]\tTotal Loss: 0.158780\n",
      "Reconstruction: 0.153895, Regularization: 0.004885\n",
      "2019-04-10 01:04:34,363 root         INFO     Train Epoch: 195 [512/8000 (6%)]\tTotal Loss: 0.150742\n",
      "Reconstruction: 0.147745, Regularization: 0.002997\n",
      "2019-04-10 01:04:34,427 root         INFO     Train Epoch: 195 [1024/8000 (13%)]\tTotal Loss: 0.153857\n",
      "Reconstruction: 0.150665, Regularization: 0.003192\n",
      "2019-04-10 01:04:34,491 root         INFO     Train Epoch: 195 [1536/8000 (19%)]\tTotal Loss: 0.164963\n",
      "Reconstruction: 0.159871, Regularization: 0.005092\n",
      "2019-04-10 01:04:34,554 root         INFO     Train Epoch: 195 [2048/8000 (26%)]\tTotal Loss: 0.165204\n",
      "Reconstruction: 0.160087, Regularization: 0.005117\n",
      "2019-04-10 01:04:34,616 root         INFO     Train Epoch: 195 [2560/8000 (32%)]\tTotal Loss: 0.185296\n",
      "Reconstruction: 0.176942, Regularization: 0.008353\n",
      "2019-04-10 01:04:34,678 root         INFO     Train Epoch: 195 [3072/8000 (38%)]\tTotal Loss: 0.181849\n",
      "Reconstruction: 0.175040, Regularization: 0.006810\n",
      "2019-04-10 01:04:34,741 root         INFO     Train Epoch: 195 [3584/8000 (45%)]\tTotal Loss: 0.160213\n",
      "Reconstruction: 0.155700, Regularization: 0.004513\n",
      "2019-04-10 01:04:34,803 root         INFO     Train Epoch: 195 [4096/8000 (51%)]\tTotal Loss: 0.164597\n",
      "Reconstruction: 0.159253, Regularization: 0.005344\n",
      "2019-04-10 01:04:34,865 root         INFO     Train Epoch: 195 [4608/8000 (58%)]\tTotal Loss: 0.157532\n",
      "Reconstruction: 0.153429, Regularization: 0.004104\n",
      "2019-04-10 01:04:34,928 root         INFO     Train Epoch: 195 [5120/8000 (64%)]\tTotal Loss: 0.164498\n",
      "Reconstruction: 0.159518, Regularization: 0.004980\n",
      "2019-04-10 01:04:34,990 root         INFO     Train Epoch: 195 [5632/8000 (70%)]\tTotal Loss: 0.178387\n",
      "Reconstruction: 0.171510, Regularization: 0.006878\n",
      "2019-04-10 01:04:35,053 root         INFO     Train Epoch: 195 [6144/8000 (77%)]\tTotal Loss: 0.177821\n",
      "Reconstruction: 0.171114, Regularization: 0.006707\n",
      "2019-04-10 01:04:35,115 root         INFO     Train Epoch: 195 [6656/8000 (83%)]\tTotal Loss: 0.160022\n",
      "Reconstruction: 0.155350, Regularization: 0.004672\n",
      "2019-04-10 01:04:35,177 root         INFO     Train Epoch: 195 [7168/8000 (90%)]\tTotal Loss: 0.164380\n",
      "Reconstruction: 0.159079, Regularization: 0.005301\n",
      "2019-04-10 01:04:35,239 root         INFO     Train Epoch: 195 [7680/8000 (96%)]\tTotal Loss: 0.152942\n",
      "Reconstruction: 0.148906, Regularization: 0.004035\n",
      "2019-04-10 01:04:35,294 root         INFO     ====> Epoch: 195 Average loss: 0.1651\n",
      "2019-04-10 01:04:35,318 root         INFO     Train Epoch: 196 [0/8000 (0%)]\tTotal Loss: 0.164924\n",
      "Reconstruction: 0.159593, Regularization: 0.005331\n",
      "2019-04-10 01:04:35,381 root         INFO     Train Epoch: 196 [512/8000 (6%)]\tTotal Loss: 0.167195\n",
      "Reconstruction: 0.161291, Regularization: 0.005904\n",
      "2019-04-10 01:04:35,444 root         INFO     Train Epoch: 196 [1024/8000 (13%)]\tTotal Loss: 0.159439\n",
      "Reconstruction: 0.155230, Regularization: 0.004209\n",
      "2019-04-10 01:04:35,507 root         INFO     Train Epoch: 196 [1536/8000 (19%)]\tTotal Loss: 0.151834\n",
      "Reconstruction: 0.148208, Regularization: 0.003626\n",
      "2019-04-10 01:04:35,568 root         INFO     Train Epoch: 196 [2048/8000 (26%)]\tTotal Loss: 0.172270\n",
      "Reconstruction: 0.166097, Regularization: 0.006173\n",
      "2019-04-10 01:04:35,629 root         INFO     Train Epoch: 196 [2560/8000 (32%)]\tTotal Loss: 0.166176\n",
      "Reconstruction: 0.160043, Regularization: 0.006133\n",
      "2019-04-10 01:04:35,690 root         INFO     Train Epoch: 196 [3072/8000 (38%)]\tTotal Loss: 0.160342\n",
      "Reconstruction: 0.155892, Regularization: 0.004450\n",
      "2019-04-10 01:04:35,752 root         INFO     Train Epoch: 196 [3584/8000 (45%)]\tTotal Loss: 0.163155\n",
      "Reconstruction: 0.157458, Regularization: 0.005697\n",
      "2019-04-10 01:04:35,813 root         INFO     Train Epoch: 196 [4096/8000 (51%)]\tTotal Loss: 0.173632\n",
      "Reconstruction: 0.167695, Regularization: 0.005937\n",
      "2019-04-10 01:04:35,874 root         INFO     Train Epoch: 196 [4608/8000 (58%)]\tTotal Loss: 0.162955\n",
      "Reconstruction: 0.157675, Regularization: 0.005280\n",
      "2019-04-10 01:04:35,935 root         INFO     Train Epoch: 196 [5120/8000 (64%)]\tTotal Loss: 0.175552\n",
      "Reconstruction: 0.170447, Regularization: 0.005105\n",
      "2019-04-10 01:04:35,996 root         INFO     Train Epoch: 196 [5632/8000 (70%)]\tTotal Loss: 0.162099\n",
      "Reconstruction: 0.158360, Regularization: 0.003739\n",
      "2019-04-10 01:04:36,057 root         INFO     Train Epoch: 196 [6144/8000 (77%)]\tTotal Loss: 0.167176\n",
      "Reconstruction: 0.161676, Regularization: 0.005500\n",
      "2019-04-10 01:04:36,118 root         INFO     Train Epoch: 196 [6656/8000 (83%)]\tTotal Loss: 0.170026\n",
      "Reconstruction: 0.164887, Regularization: 0.005140\n",
      "2019-04-10 01:04:36,179 root         INFO     Train Epoch: 196 [7168/8000 (90%)]\tTotal Loss: 0.162749\n",
      "Reconstruction: 0.158049, Regularization: 0.004701\n",
      "2019-04-10 01:04:36,239 root         INFO     Train Epoch: 196 [7680/8000 (96%)]\tTotal Loss: 0.163577\n",
      "Reconstruction: 0.158371, Regularization: 0.005206\n",
      "2019-04-10 01:04:36,293 root         INFO     ====> Epoch: 196 Average loss: 0.1645\n",
      "2019-04-10 01:04:36,317 root         INFO     Train Epoch: 197 [0/8000 (0%)]\tTotal Loss: 0.155766\n",
      "Reconstruction: 0.151480, Regularization: 0.004286\n",
      "2019-04-10 01:04:36,379 root         INFO     Train Epoch: 197 [512/8000 (6%)]\tTotal Loss: 0.165139\n",
      "Reconstruction: 0.160654, Regularization: 0.004485\n",
      "2019-04-10 01:04:36,440 root         INFO     Train Epoch: 197 [1024/8000 (13%)]\tTotal Loss: 0.154451\n",
      "Reconstruction: 0.150284, Regularization: 0.004167\n",
      "2019-04-10 01:04:36,501 root         INFO     Train Epoch: 197 [1536/8000 (19%)]\tTotal Loss: 0.171838\n",
      "Reconstruction: 0.165302, Regularization: 0.006536\n",
      "2019-04-10 01:04:36,562 root         INFO     Train Epoch: 197 [2048/8000 (26%)]\tTotal Loss: 0.189948\n",
      "Reconstruction: 0.182750, Regularization: 0.007198\n",
      "2019-04-10 01:04:36,623 root         INFO     Train Epoch: 197 [2560/8000 (32%)]\tTotal Loss: 0.163480\n",
      "Reconstruction: 0.158144, Regularization: 0.005336\n",
      "2019-04-10 01:04:36,683 root         INFO     Train Epoch: 197 [3072/8000 (38%)]\tTotal Loss: 0.160743\n",
      "Reconstruction: 0.155936, Regularization: 0.004807\n",
      "2019-04-10 01:04:36,744 root         INFO     Train Epoch: 197 [3584/8000 (45%)]\tTotal Loss: 0.152999\n",
      "Reconstruction: 0.148984, Regularization: 0.004015\n",
      "2019-04-10 01:04:36,805 root         INFO     Train Epoch: 197 [4096/8000 (51%)]\tTotal Loss: 0.163837\n",
      "Reconstruction: 0.157294, Regularization: 0.006543\n",
      "2019-04-10 01:04:36,866 root         INFO     Train Epoch: 197 [4608/8000 (58%)]\tTotal Loss: 0.178404\n",
      "Reconstruction: 0.171986, Regularization: 0.006418\n",
      "2019-04-10 01:04:36,927 root         INFO     Train Epoch: 197 [5120/8000 (64%)]\tTotal Loss: 0.174585\n",
      "Reconstruction: 0.168406, Regularization: 0.006179\n",
      "2019-04-10 01:04:36,988 root         INFO     Train Epoch: 197 [5632/8000 (70%)]\tTotal Loss: 0.169957\n",
      "Reconstruction: 0.164840, Regularization: 0.005117\n",
      "2019-04-10 01:04:37,049 root         INFO     Train Epoch: 197 [6144/8000 (77%)]\tTotal Loss: 0.174026\n",
      "Reconstruction: 0.167706, Regularization: 0.006320\n",
      "2019-04-10 01:04:37,110 root         INFO     Train Epoch: 197 [6656/8000 (83%)]\tTotal Loss: 0.168827\n",
      "Reconstruction: 0.162699, Regularization: 0.006128\n",
      "2019-04-10 01:04:37,171 root         INFO     Train Epoch: 197 [7168/8000 (90%)]\tTotal Loss: 0.159146\n",
      "Reconstruction: 0.153649, Regularization: 0.005497\n",
      "2019-04-10 01:04:37,232 root         INFO     Train Epoch: 197 [7680/8000 (96%)]\tTotal Loss: 0.161565\n",
      "Reconstruction: 0.156454, Regularization: 0.005111\n",
      "2019-04-10 01:04:37,284 root         INFO     ====> Epoch: 197 Average loss: 0.1653\n",
      "2019-04-10 01:04:37,308 root         INFO     Train Epoch: 198 [0/8000 (0%)]\tTotal Loss: 0.155163\n",
      "Reconstruction: 0.151329, Regularization: 0.003834\n",
      "2019-04-10 01:04:37,371 root         INFO     Train Epoch: 198 [512/8000 (6%)]\tTotal Loss: 0.178288\n",
      "Reconstruction: 0.170497, Regularization: 0.007792\n",
      "2019-04-10 01:04:37,433 root         INFO     Train Epoch: 198 [1024/8000 (13%)]\tTotal Loss: 0.165098\n",
      "Reconstruction: 0.158832, Regularization: 0.006266\n",
      "2019-04-10 01:04:37,495 root         INFO     Train Epoch: 198 [1536/8000 (19%)]\tTotal Loss: 0.179100\n",
      "Reconstruction: 0.170744, Regularization: 0.008356\n",
      "2019-04-10 01:04:37,558 root         INFO     Train Epoch: 198 [2048/8000 (26%)]\tTotal Loss: 0.162743\n",
      "Reconstruction: 0.157717, Regularization: 0.005026\n",
      "2019-04-10 01:04:37,620 root         INFO     Train Epoch: 198 [2560/8000 (32%)]\tTotal Loss: 0.155373\n",
      "Reconstruction: 0.151289, Regularization: 0.004084\n",
      "2019-04-10 01:04:37,682 root         INFO     Train Epoch: 198 [3072/8000 (38%)]\tTotal Loss: 0.160195\n",
      "Reconstruction: 0.155535, Regularization: 0.004661\n",
      "2019-04-10 01:04:37,744 root         INFO     Train Epoch: 198 [3584/8000 (45%)]\tTotal Loss: 0.151595\n",
      "Reconstruction: 0.148472, Regularization: 0.003123\n",
      "2019-04-10 01:04:37,806 root         INFO     Train Epoch: 198 [4096/8000 (51%)]\tTotal Loss: 0.158357\n",
      "Reconstruction: 0.153980, Regularization: 0.004377\n",
      "2019-04-10 01:04:37,868 root         INFO     Train Epoch: 198 [4608/8000 (58%)]\tTotal Loss: 0.169071\n",
      "Reconstruction: 0.164171, Regularization: 0.004899\n",
      "2019-04-10 01:04:37,930 root         INFO     Train Epoch: 198 [5120/8000 (64%)]\tTotal Loss: 0.160791\n",
      "Reconstruction: 0.155397, Regularization: 0.005394\n",
      "2019-04-10 01:04:37,992 root         INFO     Train Epoch: 198 [5632/8000 (70%)]\tTotal Loss: 0.163805\n",
      "Reconstruction: 0.158171, Regularization: 0.005634\n",
      "2019-04-10 01:04:38,054 root         INFO     Train Epoch: 198 [6144/8000 (77%)]\tTotal Loss: 0.165662\n",
      "Reconstruction: 0.159411, Regularization: 0.006251\n",
      "2019-04-10 01:04:38,116 root         INFO     Train Epoch: 198 [6656/8000 (83%)]\tTotal Loss: 0.163834\n",
      "Reconstruction: 0.158267, Regularization: 0.005567\n",
      "2019-04-10 01:04:38,178 root         INFO     Train Epoch: 198 [7168/8000 (90%)]\tTotal Loss: 0.159562\n",
      "Reconstruction: 0.154366, Regularization: 0.005196\n",
      "2019-04-10 01:04:38,240 root         INFO     Train Epoch: 198 [7680/8000 (96%)]\tTotal Loss: 0.168909\n",
      "Reconstruction: 0.163588, Regularization: 0.005321\n",
      "2019-04-10 01:04:38,294 root         INFO     ====> Epoch: 198 Average loss: 0.1648\n",
      "2019-04-10 01:04:38,318 root         INFO     Train Epoch: 199 [0/8000 (0%)]\tTotal Loss: 0.161376\n",
      "Reconstruction: 0.156285, Regularization: 0.005091\n",
      "2019-04-10 01:04:38,382 root         INFO     Train Epoch: 199 [512/8000 (6%)]\tTotal Loss: 0.155582\n",
      "Reconstruction: 0.151259, Regularization: 0.004323\n",
      "2019-04-10 01:04:38,444 root         INFO     Train Epoch: 199 [1024/8000 (13%)]\tTotal Loss: 0.168156\n",
      "Reconstruction: 0.163479, Regularization: 0.004677\n",
      "2019-04-10 01:04:38,506 root         INFO     Train Epoch: 199 [1536/8000 (19%)]\tTotal Loss: 0.156807\n",
      "Reconstruction: 0.150862, Regularization: 0.005945\n",
      "2019-04-10 01:04:38,568 root         INFO     Train Epoch: 199 [2048/8000 (26%)]\tTotal Loss: 0.172182\n",
      "Reconstruction: 0.166232, Regularization: 0.005950\n",
      "2019-04-10 01:04:38,631 root         INFO     Train Epoch: 199 [2560/8000 (32%)]\tTotal Loss: 0.160439\n",
      "Reconstruction: 0.154446, Regularization: 0.005993\n",
      "2019-04-10 01:04:38,693 root         INFO     Train Epoch: 199 [3072/8000 (38%)]\tTotal Loss: 0.166677\n",
      "Reconstruction: 0.160925, Regularization: 0.005752\n",
      "2019-04-10 01:04:38,756 root         INFO     Train Epoch: 199 [3584/8000 (45%)]\tTotal Loss: 0.168685\n",
      "Reconstruction: 0.163071, Regularization: 0.005614\n",
      "2019-04-10 01:04:38,818 root         INFO     Train Epoch: 199 [4096/8000 (51%)]\tTotal Loss: 0.171150\n",
      "Reconstruction: 0.164361, Regularization: 0.006789\n",
      "2019-04-10 01:04:38,881 root         INFO     Train Epoch: 199 [4608/8000 (58%)]\tTotal Loss: 0.172771\n",
      "Reconstruction: 0.167208, Regularization: 0.005563\n",
      "2019-04-10 01:04:38,944 root         INFO     Train Epoch: 199 [5120/8000 (64%)]\tTotal Loss: 0.166585\n",
      "Reconstruction: 0.160556, Regularization: 0.006029\n",
      "2019-04-10 01:04:39,007 root         INFO     Train Epoch: 199 [5632/8000 (70%)]\tTotal Loss: 0.160459\n",
      "Reconstruction: 0.155750, Regularization: 0.004708\n",
      "2019-04-10 01:04:39,069 root         INFO     Train Epoch: 199 [6144/8000 (77%)]\tTotal Loss: 0.166241\n",
      "Reconstruction: 0.160561, Regularization: 0.005680\n",
      "2019-04-10 01:04:39,132 root         INFO     Train Epoch: 199 [6656/8000 (83%)]\tTotal Loss: 0.158760\n",
      "Reconstruction: 0.154381, Regularization: 0.004379\n",
      "2019-04-10 01:04:39,195 root         INFO     Train Epoch: 199 [7168/8000 (90%)]\tTotal Loss: 0.157624\n",
      "Reconstruction: 0.152652, Regularization: 0.004973\n",
      "2019-04-10 01:04:39,257 root         INFO     Train Epoch: 199 [7680/8000 (96%)]\tTotal Loss: 0.171986\n",
      "Reconstruction: 0.166482, Regularization: 0.005504\n",
      "2019-04-10 01:04:39,311 root         INFO     ====> Epoch: 199 Average loss: 0.1647\n",
      "2019-04-10 01:04:39,319 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) done      TrainVAE()\n",
      "2019-04-10 01:04:39,320 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 01:04:39,320 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   DONE\n",
      "2019-04-10 01:04:39,320 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:04:39,320 luigi-interface DEBUG    Pending tasks: 1\n",
      "2019-04-10 01:04:39,321 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) running   RunAll()\n",
      "2019-04-10 01:04:39,321 luigi-interface INFO     [pid 1925] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) done      RunAll()\n",
      "2019-04-10 01:04:39,321 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 01:04:39,322 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   DONE\n",
      "2019-04-10 01:04:39,322 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 01:04:39,322 luigi-interface DEBUG    Done\n",
      "2019-04-10 01:04:39,322 luigi-interface DEBUG    There are no more tasks to run at this time\n",
      "2019-04-10 01:04:39,322 luigi-interface INFO     Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=1925) was stopped. Shutting down Keep-Alive thread\n",
      "2019-04-10 01:04:39,323 luigi-interface INFO     \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 4 ran successfully:\n",
      "    - 1 MakeDataSet()\n",
      "    - 1 RunAll()\n",
      "    - 1 TrainVAE()\n",
      "    - 1 TrainVEM()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = 'logs'\n",
    "logs = []\n",
    "for filename in os.listdir(OUTPUT):\n",
    "    if re.search(pattern, filename):\n",
    "        logs.append(filename)\n",
    "\n",
    "print('Found %d log files.' % len(logs))\n",
    "        \n",
    "for filename in logs:\n",
    "    path = os.path.join(OUTPUT, filename)\n",
    "    print('\\n-- Log file: %s\\n' % filename)\n",
    "    with open(path, 'r') as f:\n",
    "        message = f.read()\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
