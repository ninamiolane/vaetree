{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import importlib\n",
    "import toylosses\n",
    "importlib.reload(toylosses)\n",
    "import toynn\n",
    "importlib.reload(toynn)\n",
    "import toyvis\n",
    "importlib.reload(toyvis)\n",
    "\n",
    "import torch\n",
    "sns.set()\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "OUTPUT = '/scratch/users/nmiolane/toyoutput'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on experiment's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.3.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.3.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.4.weight tensor([[ 50.,   0.],\n",
      "        [  0., -40.]], device='cuda:0') \n",
      "\n",
      "layers.4.bias tensor([2.0000, 2.4000], device='cuda:0') \n",
      "\n",
      "layers.5.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.5.bias tensor([0., 0.], device='cuda:0') \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3937482238905453,\n",
       " 14.280675375129617,\n",
       " -3.4734722143018555,\n",
       " 8.787599927100272)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_DIM = 2\n",
    "LATENT_DIM = 1\n",
    "N_DECODER_LAYERS = 5\n",
    "NONLINEARITY = False\n",
    "N_SAMPLES = 10000\n",
    "WITH_BIASX = True\n",
    "WITH_LOGVARX = True\n",
    "\n",
    "W_TRUE = {}\n",
    "B_TRUE = {}\n",
    "\n",
    "W_TRUE[0] = [[0.6], [-0.7]]\n",
    "B_TRUE[0] = [0., -0.1]\n",
    "\n",
    "W_TRUE[1] = [[0.1, -0.1], [-0.1, .1]]\n",
    "B_TRUE[1] = [0.1, 0.]\n",
    "\n",
    "W_TRUE[2] = [[0.1, -0.1], [-0.1, .1]]\n",
    "B_TRUE[2] = [0.1, 0.]\n",
    "\n",
    "W_TRUE[3] = [[0.1, -0.1], [-0.1, .1]]\n",
    "B_TRUE[3] = [0.1, 0.]\n",
    "\n",
    "# For the reconstruction\n",
    "W_TRUE[4] = [[50., 0.], [0., -40.]]\n",
    "B_TRUE[4] = [2., 2.4]\n",
    "\n",
    "# For the logvarx\n",
    "W_TRUE[5] = [[0., 0.], [0., 0.]]\n",
    "B_TRUE[5] = [0., 0.]\n",
    "\n",
    "if WITH_LOGVARX:\n",
    "    assert len(W_TRUE) == N_DECODER_LAYERS + 1, len(W_TRUE)\n",
    "else:\n",
    "    assert len(W_TRUE) == N_DECODER_LAYERS\n",
    "\n",
    "WITH_BIASZ = True\n",
    "WITH_LOGVARZ = True\n",
    "\n",
    "decoder_true = toynn.make_decoder_true(\n",
    "    w_true=W_TRUE, b_true=B_TRUE, latent_dim=LATENT_DIM, \n",
    "    data_dim=DATA_DIM, n_layers=N_DECODER_LAYERS,\n",
    "    nonlinearity=NONLINEARITY, with_biasx=WITH_BIASX, with_logvarx=WITH_LOGVARX)\n",
    "\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, N_SAMPLES)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = toyvis.plot_data(generated_true_x, color='green', label='from decoder true', ax=ax)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect generation of synthetic data from decoder_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.3.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.3.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.4.weight tensor([[ 50.,   0.],\n",
      "        [  0., -40.]], device='cuda:0') \n",
      "\n",
      "layers.4.bias tensor([2.0000, 2.4000], device='cuda:0') \n",
      "\n",
      "layers.5.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.5.bias tensor([0., 0.], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(nina): Add a comparison to a FA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results from standard VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- True values of parameters\n",
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.3.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.3.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.4.weight tensor([[ 50.,   0.],\n",
      "        [  0., -40.]], device='cuda:0') \n",
      "\n",
      "layers.4.bias tensor([2.0000, 2.4000], device='cuda:0') \n",
      "\n",
      "layers.5.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.5.bias tensor([0., 0.], device='cuda:0') \n",
      "\n",
      "\n",
      "-- Learnt values of parameters\n",
      "layers.0.weight tensor([[0.0584],\n",
      "        [0.8375]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 1.8072, -0.0108], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[-0.2373,  0.5176],\n",
      "        [ 0.9711,  0.6603]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([1.0104, 0.6324], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[-0.4301, -0.6963],\n",
      "        [-0.0855, -1.3836]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([-0.1852, -1.0327], device='cuda:0') \n",
      "\n",
      "layers.3.weight tensor([[1.0630, 0.0750],\n",
      "        [0.3455, 0.6086]], device='cuda:0') \n",
      "\n",
      "layers.3.bias tensor([-0.0108,  0.3020], device='cuda:0') \n",
      "\n",
      "layers.4.weight tensor([[-0.5556, -1.1383],\n",
      "        [-0.5462, -0.3796]], device='cuda:0') \n",
      "\n",
      "layers.4.bias tensor([1.3920, 0.1249], device='cuda:0') \n",
      "\n",
      "layers.5.weight tensor([[-0.5563,  0.3343],\n",
      "        [ 1.1140, -0.6836]], device='cuda:0') \n",
      "\n",
      "layers.5.bias tensor([ 0.6084, -0.3300], device='cuda:0') \n",
      "\n",
      "Last losses:\n",
      "[0.11127853170037269, 0.11163388028740882, 0.11142358934879303, 0.11147166752815246, 0.11139166802167892]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "decoder_path = glob.glob(f'{OUTPUT}/train_vae/models/decoder.pth')[0]\n",
    "decoder = torch.load(decoder_path, map_location=DEVICE)\n",
    "\n",
    "print('-- True values of parameters')\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "print('\\n-- Learnt values of parameters')\n",
    "for name, param in decoder.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "    \n",
    "losses_vae_path = glob.glob(f'{OUTPUT}/train_vae/train_losses.pkl')[0]\n",
    "train_losses_all_epochs = pickle.load(open(losses_vae_path, 'rb'))\n",
    "\n",
    "train_losses_total = [loss['total'] for loss in train_losses_all_epochs]\n",
    "n_epochs = len(train_losses_total)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(range(n_epochs), train_losses_total)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(range(90, n_epochs), train_losses_total[90:])\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(range(160, n_epochs), train_losses_total[160:])\n",
    "\n",
    "print('Last losses:')\n",
    "print(train_losses_total[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1825155656412, 14.64683101096897, -2.551395948319436, 8.082488809839951)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, n_samples)\n",
    "generated_x = toynn.generate_from_decoder(decoder, n_samples)\n",
    "\n",
    "# For 1D\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(16, 4))\n",
    "axis_side = 20\n",
    "\n",
    "ax = axes[0]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[1]\n",
    "toyvis.plot_data(generated_x, color='red', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[2]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "toyvis.plot_data(generated_x, color='red', ax=ax)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results from VEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- True values of parameters\n",
      "layers.0.weight tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([ 0.0000, -0.1000], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.3.weight tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0') \n",
      "\n",
      "layers.3.bias tensor([0.1000, 0.0000], device='cuda:0') \n",
      "\n",
      "layers.4.weight tensor([[ 50.,   0.],\n",
      "        [  0., -40.]], device='cuda:0') \n",
      "\n",
      "layers.4.bias tensor([2.0000, 2.4000], device='cuda:0') \n",
      "\n",
      "layers.5.weight tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0') \n",
      "\n",
      "layers.5.bias tensor([0., 0.], device='cuda:0') \n",
      "\n",
      "\n",
      "-- Learnt values of parameters\n",
      "layers.0.weight tensor([[ 0.0399],\n",
      "        [-0.2628]], device='cuda:0') \n",
      "\n",
      "layers.0.bias tensor([-1.0571, -0.9511], device='cuda:0') \n",
      "\n",
      "layers.1.weight tensor([[-1.1546, -0.5607],\n",
      "        [ 0.3218,  1.5424]], device='cuda:0') \n",
      "\n",
      "layers.1.bias tensor([ 0.3030, -0.2123], device='cuda:0') \n",
      "\n",
      "layers.2.weight tensor([[ 0.3065, -1.5522],\n",
      "        [-1.9339,  0.7801]], device='cuda:0') \n",
      "\n",
      "layers.2.bias tensor([-0.1119, -0.8363], device='cuda:0') \n",
      "\n",
      "layers.3.weight tensor([[ 1.1608, -1.4384],\n",
      "        [-0.3987,  0.5692]], device='cuda:0') \n",
      "\n",
      "layers.3.bias tensor([-0.3539, -0.5961], device='cuda:0') \n",
      "\n",
      "layers.4.weight tensor([[ 0.6711, -0.7382],\n",
      "        [ 1.2313,  0.1679]], device='cuda:0') \n",
      "\n",
      "layers.4.bias tensor([ 0.3881, -0.4239], device='cuda:0') \n",
      "\n",
      "layers.5.weight tensor([[-0.1607,  0.4618],\n",
      "        [-0.7144, -0.5952]], device='cuda:0') \n",
      "\n",
      "layers.5.bias tensor([-0.3345,  0.6009], device='cuda:0') \n",
      "\n",
      "Last losses:\n",
      "[2.793389576911926, 2.7520885729789732, 2.7116498560905455, 2.7250452308654785, 2.648495577812195]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_true_path = glob.glob(f'{OUTPUT}/synthetic/decoder_true.pth')[0]\n",
    "decoder_true = torch.load(decoder_true_path, map_location=DEVICE)\n",
    "\n",
    "decoder_path = glob.glob(f'{OUTPUT}/train_vem/models/decoder.pth')[0]\n",
    "decoder = torch.load(decoder_path, map_location=DEVICE)\n",
    "\n",
    "print('-- True values of parameters')\n",
    "for name, param in decoder_true.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "\n",
    "print('\\n-- Learnt values of parameters')\n",
    "for name, param in decoder.named_parameters():\n",
    "    print(name, param.data, '\\n')\n",
    "    \n",
    "losses_vae_path = glob.glob(f'{OUTPUT}/train_vem/train_losses.pkl')[0]\n",
    "train_losses_all_epochs = pickle.load(open(losses_vae_path, 'rb'))\n",
    "\n",
    "plt.figure()\n",
    "train_losses_total = [loss['total'] for loss in train_losses_all_epochs]\n",
    "n_epochs = len(train_losses_total)\n",
    "plt.plot(range(n_epochs), train_losses_total)\n",
    "print('Last losses:')\n",
    "print(train_losses_total[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.277376258156189, 13.65253670175495, -7.122444828654034, 11.75983484224179)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "generated_true_x = toynn.generate_from_decoder(decoder_true, n_samples)\n",
    "generated_x = toynn.generate_from_decoder(decoder, n_samples)\n",
    "\n",
    "# For 1D\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(16, 4))\n",
    "axis_side = 20\n",
    "\n",
    "ax = axes[0]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[1]\n",
    "toyvis.plot_data(generated_x, color='orange', ax=ax)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[2]\n",
    "toyvis.plot_data(generated_true_x, color='darkgreen', ax=ax)\n",
    "toyvis.plot_data(generated_x, color='orange', ax=ax)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print pipeline logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 log files.\n",
      "\n",
      "-- Log file: logs2019-04-09 23:57:27.267992.txt\n",
      "\n",
      "2019-04-09 23:57:27,268 root         INFO     start\n",
      "2019-04-09 23:57:27,282 luigi        INFO     logging configured by default settings\n",
      "2019-04-09 23:57:27,307 luigi-interface DEBUG    Checking if RunAll() is complete\n",
      "2019-04-09 23:57:27,308 luigi-interface DEBUG    Checking if TrainVAE() is complete\n",
      "2019-04-09 23:57:27,308 luigi-interface DEBUG    Checking if TrainVEM() is complete\n",
      "2019-04-09 23:57:27,309 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   PENDING\n",
      "2019-04-09 23:57:27,310 luigi-interface DEBUG    Checking if MakeDataSet() is complete\n",
      "2019-04-09 23:57:27,310 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   PENDING\n",
      "2019-04-09 23:57:27,310 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   DONE\n",
      "2019-04-09 23:57:27,311 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   PENDING\n",
      "2019-04-09 23:57:27,311 luigi-interface INFO     Done scheduling tasks\n",
      "2019-04-09 23:57:27,311 luigi-interface INFO     Running Worker with 1 processes\n",
      "2019-04-09 23:57:27,311 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-09 23:57:27,312 luigi-interface DEBUG    Pending tasks: 3\n",
      "2019-04-09 23:57:27,312 luigi-interface INFO     [pid 26441] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26441) running   TrainVEM()\n",
      "2019-04-09 23:57:27,328 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-09 23:57:27,329 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-09 23:57:31,137 root         INFO     Values of VEM's decoder parameters before training:\n",
      "2019-04-09 23:57:31,137 root         INFO     layers.0.weight\n",
      "2019-04-09 23:57:31,155 root         INFO     tensor([[-0.2651],\n",
      "        [ 0.1353]], device='cuda:0')\n",
      "2019-04-09 23:57:31,178 root         INFO     layers.0.bias\n",
      "2019-04-09 23:57:31,178 root         INFO     tensor([-0.3248, -0.5761], device='cuda:0')\n",
      "2019-04-09 23:57:31,179 root         INFO     layers.1.weight\n",
      "2019-04-09 23:57:31,179 root         INFO     tensor([[-0.0574,  0.4460],\n",
      "        [ 0.5879, -0.3491]], device='cuda:0')\n",
      "2019-04-09 23:57:31,181 root         INFO     layers.1.bias\n",
      "2019-04-09 23:57:31,181 root         INFO     tensor([-0.4054, -0.0326], device='cuda:0')\n",
      "2019-04-09 23:57:31,182 root         INFO     layers.2.weight\n",
      "2019-04-09 23:57:31,182 root         INFO     tensor([[ 0.3113,  0.3165],\n",
      "        [-0.2632,  0.2449]], device='cuda:0')\n",
      "2019-04-09 23:57:31,183 root         INFO     layers.2.bias\n",
      "2019-04-09 23:57:31,183 root         INFO     tensor([-0.1202, -0.0849], device='cuda:0')\n",
      "2019-04-09 23:57:31,185 root         INFO     layers.3.weight\n",
      "2019-04-09 23:57:31,185 root         INFO     tensor([[-0.5734,  0.5065],\n",
      "        [-0.4986,  0.1762]], device='cuda:0')\n",
      "2019-04-09 23:57:31,186 root         INFO     layers.3.bias\n",
      "2019-04-09 23:57:31,186 root         INFO     tensor([-0.3539, -0.5961], device='cuda:0')\n",
      "2019-04-09 23:57:31,187 root         INFO     layers.4.weight\n",
      "2019-04-09 23:57:31,187 root         INFO     tensor([[-0.0450,  0.2346],\n",
      "        [-0.6488,  0.0511]], device='cuda:0')\n",
      "2019-04-09 23:57:31,189 root         INFO     layers.4.bias\n",
      "2019-04-09 23:57:31,189 root         INFO     tensor([-0.2659, -0.1007], device='cuda:0')\n",
      "2019-04-09 23:57:31,190 root         INFO     layers.5.weight\n",
      "2019-04-09 23:57:31,190 root         INFO     tensor([[-0.3153, -0.0881],\n",
      "        [ 0.6361, -0.6798]], device='cuda:0')\n",
      "2019-04-09 23:57:31,191 root         INFO     layers.5.bias\n",
      "2019-04-09 23:57:31,191 root         INFO     tensor([-0.3345,  0.6009], device='cuda:0')\n",
      "2019-04-09 23:57:31,265 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 2.339250\n",
      "Reconstruction: 2.177301, Regularization: 0.111175, Discriminator: 0.017666; Generator: 0.033108,\n",
      "D(x): 0.880, D(G(z)): 0.351\n",
      "2019-04-09 23:57:31,388 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: 2.052754\n",
      "Reconstruction: 1.901497, Regularization: 0.099750, Discriminator: 0.018216; Generator: 0.033291,\n",
      "D(x): 0.863, D(G(z)): 0.349\n",
      "2019-04-09 23:57:31,499 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: 1.954011\n",
      "Reconstruction: 1.808930, Regularization: 0.092825, Discriminator: 0.018993; Generator: 0.033263,\n",
      "D(x): 0.841, D(G(z)): 0.348\n",
      "2019-04-09 23:57:31,610 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: 2.074988\n",
      "Reconstruction: 1.925562, Regularization: 0.098658, Discriminator: 0.018742; Generator: 0.032026,\n",
      "D(x): 0.871, D(G(z)): 0.364\n",
      "2019-04-09 23:57:31,722 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: 2.168085\n",
      "Reconstruction: 2.020264, Regularization: 0.097229, Discriminator: 0.018956; Generator: 0.031637,\n",
      "D(x): 0.869, D(G(z)): 0.368\n",
      "2019-04-09 23:57:31,832 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: 2.347726\n",
      "Reconstruction: 2.194109, Regularization: 0.102719, Discriminator: 0.017484; Generator: 0.033414,\n",
      "D(x): 0.885, D(G(z)): 0.347\n",
      "2019-04-09 23:57:31,943 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: 2.038956\n",
      "Reconstruction: 1.896982, Regularization: 0.091553, Discriminator: 0.019040; Generator: 0.031381,\n",
      "D(x): 0.870, D(G(z)): 0.370\n",
      "2019-04-09 23:57:32,053 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: 2.251890\n",
      "Reconstruction: 2.107852, Regularization: 0.092952, Discriminator: 0.017914; Generator: 0.033173,\n",
      "D(x): 0.874, D(G(z)): 0.350\n",
      "2019-04-09 23:57:32,164 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: 2.231246\n",
      "Reconstruction: 2.088248, Regularization: 0.093360, Discriminator: 0.018107; Generator: 0.031531,\n",
      "D(x): 0.891, D(G(z)): 0.368\n",
      "2019-04-09 23:57:32,275 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: 2.266084\n",
      "Reconstruction: 2.121504, Regularization: 0.093788, Discriminator: 0.016474; Generator: 0.034318,\n",
      "D(x): 0.897, D(G(z)): 0.338\n",
      "2019-04-09 23:57:32,387 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: 2.121203\n",
      "Reconstruction: 1.983350, Regularization: 0.087459, Discriminator: 0.017927; Generator: 0.032468,\n",
      "D(x): 0.883, D(G(z)): 0.358\n",
      "2019-04-09 23:57:32,499 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: 2.193747\n",
      "Reconstruction: 2.055149, Regularization: 0.088259, Discriminator: 0.017940; Generator: 0.032400,\n",
      "D(x): 0.888, D(G(z)): 0.360\n",
      "2019-04-09 23:57:32,611 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: 2.088677\n",
      "Reconstruction: 1.954070, Regularization: 0.083767, Discriminator: 0.017553; Generator: 0.033287,\n",
      "D(x): 0.882, D(G(z)): 0.348\n",
      "2019-04-09 23:57:32,723 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: 2.261750\n",
      "Reconstruction: 2.122800, Regularization: 0.089575, Discriminator: 0.017029; Generator: 0.032347,\n",
      "D(x): 0.916, D(G(z)): 0.362\n",
      "2019-04-09 23:57:32,835 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: 2.030193\n",
      "Reconstruction: 1.899597, Regularization: 0.080140, Discriminator: 0.018067; Generator: 0.032389,\n",
      "D(x): 0.886, D(G(z)): 0.361\n",
      "2019-04-09 23:57:32,946 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: 2.134882\n",
      "Reconstruction: 2.003892, Regularization: 0.081122, Discriminator: 0.017716; Generator: 0.032152,\n",
      "D(x): 0.894, D(G(z)): 0.361\n",
      "2019-04-09 23:57:33,027 root         INFO     ====> Epoch: 0 Average loss: 2.1390\n",
      "2019-04-09 23:57:33,054 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: 2.119722\n",
      "Reconstruction: 1.988796, Regularization: 0.080518, Discriminator: 0.017381; Generator: 0.033027,\n",
      "D(x): 0.894, D(G(z)): 0.353\n",
      "2019-04-09 23:57:33,165 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: 2.409056\n",
      "Reconstruction: 2.273874, Regularization: 0.085348, Discriminator: 0.017311; Generator: 0.032522,\n",
      "D(x): 0.906, D(G(z)): 0.359\n",
      "2019-04-09 23:57:33,276 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: 2.017982\n",
      "Reconstruction: 1.891628, Regularization: 0.075367, Discriminator: 0.017315; Generator: 0.033673,\n",
      "D(x): 0.889, D(G(z)): 0.347\n",
      "2019-04-09 23:57:33,386 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: 2.247534\n",
      "Reconstruction: 2.116794, Regularization: 0.081155, Discriminator: 0.015855; Generator: 0.033730,\n",
      "D(x): 0.922, D(G(z)): 0.344\n",
      "2019-04-09 23:57:33,497 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: 2.230585\n",
      "Reconstruction: 2.101933, Regularization: 0.077755, Discriminator: 0.016430; Generator: 0.034467,\n",
      "D(x): 0.908, D(G(z)): 0.342\n",
      "2019-04-09 23:57:33,607 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: 2.212348\n",
      "Reconstruction: 2.085066, Regularization: 0.078204, Discriminator: 0.016811; Generator: 0.032267,\n",
      "D(x): 0.921, D(G(z)): 0.361\n",
      "2019-04-09 23:57:33,718 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: 2.093416\n",
      "Reconstruction: 1.969444, Regularization: 0.074206, Discriminator: 0.016886; Generator: 0.032881,\n",
      "D(x): 0.910, D(G(z)): 0.355\n",
      "2019-04-09 23:57:33,828 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: 2.002418\n",
      "Reconstruction: 1.882487, Regularization: 0.069547, Discriminator: 0.017876; Generator: 0.032508,\n",
      "D(x): 0.894, D(G(z)): 0.361\n",
      "2019-04-09 23:57:33,939 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: 2.299951\n",
      "Reconstruction: 2.174201, Regularization: 0.075508, Discriminator: 0.015295; Generator: 0.034947,\n",
      "D(x): 0.919, D(G(z)): 0.330\n",
      "2019-04-09 23:57:34,049 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: 2.249851\n",
      "Reconstruction: 2.126372, Regularization: 0.073593, Discriminator: 0.016245; Generator: 0.033641,\n",
      "D(x): 0.922, D(G(z)): 0.349\n",
      "2019-04-09 23:57:34,160 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: 2.145958\n",
      "Reconstruction: 2.025111, Regularization: 0.071406, Discriminator: 0.015922; Generator: 0.033520,\n",
      "D(x): 0.927, D(G(z)): 0.348\n",
      "2019-04-09 23:57:34,270 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: 2.299435\n",
      "Reconstruction: 2.175941, Regularization: 0.074060, Discriminator: 0.014630; Generator: 0.034804,\n",
      "D(x): 0.943, D(G(z)): 0.333\n",
      "2019-04-09 23:57:34,381 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: 2.126229\n",
      "Reconstruction: 2.009538, Regularization: 0.067172, Discriminator: 0.016763; Generator: 0.032756,\n",
      "D(x): 0.920, D(G(z)): 0.359\n",
      "2019-04-09 23:57:34,491 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: 2.099696\n",
      "Reconstruction: 1.984396, Regularization: 0.064415, Discriminator: 0.015946; Generator: 0.034939,\n",
      "D(x): 0.901, D(G(z)): 0.329\n",
      "2019-04-09 23:57:34,599 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: 2.183778\n",
      "Reconstruction: 2.066909, Regularization: 0.066381, Discriminator: 0.015551; Generator: 0.034938,\n",
      "D(x): 0.922, D(G(z)): 0.335\n",
      "2019-04-09 23:57:34,707 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: 2.407690\n",
      "Reconstruction: 2.289035, Regularization: 0.068736, Discriminator: 0.015454; Generator: 0.034465,\n",
      "D(x): 0.928, D(G(z)): 0.338\n",
      "2019-04-09 23:57:34,787 root         INFO     ====> Epoch: 1 Average loss: 2.2327\n",
      "2019-04-09 23:57:34,815 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: 1.979838\n",
      "Reconstruction: 1.867239, Regularization: 0.062068, Discriminator: 0.015310; Generator: 0.035221,\n",
      "D(x): 0.917, D(G(z)): 0.328\n",
      "2019-04-09 23:57:34,928 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: 2.087983\n",
      "Reconstruction: 1.974399, Regularization: 0.062494, Discriminator: 0.014608; Generator: 0.036481,\n",
      "D(x): 0.922, D(G(z)): 0.316\n",
      "2019-04-09 23:57:35,041 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: 2.360594\n",
      "Reconstruction: 2.245857, Regularization: 0.064247, Discriminator: 0.015510; Generator: 0.034980,\n",
      "D(x): 0.918, D(G(z)): 0.332\n",
      "2019-04-09 23:57:35,153 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: 2.430417\n",
      "Reconstruction: 2.315115, Regularization: 0.065600, Discriminator: 0.014872; Generator: 0.034830,\n",
      "D(x): 0.936, D(G(z)): 0.333\n",
      "2019-04-09 23:57:35,263 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: 2.201802\n",
      "Reconstruction: 2.089407, Regularization: 0.062110, Discriminator: 0.014846; Generator: 0.035439,\n",
      "D(x): 0.932, D(G(z)): 0.329\n",
      "2019-04-09 23:57:35,375 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: 2.205503\n",
      "Reconstruction: 2.095824, Regularization: 0.058778, Discriminator: 0.016360; Generator: 0.034542,\n",
      "D(x): 0.906, D(G(z)): 0.340\n",
      "2019-04-09 23:57:35,485 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: 2.337512\n",
      "Reconstruction: 2.225943, Regularization: 0.062791, Discriminator: 0.015547; Generator: 0.033231,\n",
      "D(x): 0.945, D(G(z)): 0.352\n",
      "2019-04-09 23:57:35,597 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: 2.167865\n",
      "Reconstruction: 2.059854, Regularization: 0.056852, Discriminator: 0.015080; Generator: 0.036079,\n",
      "D(x): 0.913, D(G(z)): 0.319\n",
      "2019-04-09 23:57:35,707 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: 2.667099\n",
      "Reconstruction: 2.552332, Regularization: 0.064163, Discriminator: 0.013871; Generator: 0.036733,\n",
      "D(x): 0.945, D(G(z)): 0.316\n",
      "2019-04-09 23:57:35,818 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: 2.375542\n",
      "Reconstruction: 2.265671, Regularization: 0.059660, Discriminator: 0.015076; Generator: 0.035135,\n",
      "D(x): 0.939, D(G(z)): 0.336\n",
      "2019-04-09 23:57:35,929 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: 2.221538\n",
      "Reconstruction: 2.114705, Regularization: 0.056370, Discriminator: 0.015475; Generator: 0.034987,\n",
      "D(x): 0.925, D(G(z)): 0.334\n",
      "2019-04-09 23:57:36,039 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: 2.290626\n",
      "Reconstruction: 2.184582, Regularization: 0.056172, Discriminator: 0.015027; Generator: 0.034845,\n",
      "D(x): 0.933, D(G(z)): 0.334\n",
      "2019-04-09 23:57:36,150 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: 2.516831\n",
      "Reconstruction: 2.408415, Regularization: 0.059014, Discriminator: 0.014894; Generator: 0.034507,\n",
      "D(x): 0.948, D(G(z)): 0.340\n",
      "2019-04-09 23:57:36,260 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: 2.079470\n",
      "Reconstruction: 1.975160, Regularization: 0.051777, Discriminator: 0.013893; Generator: 0.038640,\n",
      "D(x): 0.922, D(G(z)): 0.299\n",
      "2019-04-09 23:57:36,370 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: 2.330820\n",
      "Reconstruction: 2.226541, Regularization: 0.054163, Discriminator: 0.015481; Generator: 0.034635,\n",
      "D(x): 0.935, D(G(z)): 0.341\n",
      "2019-04-09 23:57:36,481 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: 2.388390\n",
      "Reconstruction: 2.281892, Regularization: 0.055923, Discriminator: 0.014037; Generator: 0.036538,\n",
      "D(x): 0.944, D(G(z)): 0.320\n",
      "2019-04-09 23:57:36,562 root         INFO     ====> Epoch: 2 Average loss: 2.3068\n",
      "2019-04-09 23:57:36,589 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: 2.243677\n",
      "Reconstruction: 2.140292, Regularization: 0.052699, Discriminator: 0.014813; Generator: 0.035874,\n",
      "D(x): 0.929, D(G(z)): 0.325\n",
      "2019-04-09 23:57:36,700 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: 2.681935\n",
      "Reconstruction: 2.574698, Regularization: 0.057657, Discriminator: 0.014454; Generator: 0.035125,\n",
      "D(x): 0.952, D(G(z)): 0.334\n",
      "2019-04-09 23:57:36,809 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: 2.220768\n",
      "Reconstruction: 2.116834, Regularization: 0.052106, Discriminator: 0.013822; Generator: 0.038006,\n",
      "D(x): 0.937, D(G(z)): 0.305\n",
      "2019-04-09 23:57:36,918 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: 2.416802\n",
      "Reconstruction: 2.312694, Regularization: 0.052411, Discriminator: 0.013200; Generator: 0.038497,\n",
      "D(x): 0.943, D(G(z)): 0.300\n",
      "2019-04-09 23:57:37,026 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: 2.348833\n",
      "Reconstruction: 2.246229, Regularization: 0.051849, Discriminator: 0.013861; Generator: 0.036893,\n",
      "D(x): 0.939, D(G(z)): 0.312\n",
      "2019-04-09 23:57:37,135 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: 2.132521\n",
      "Reconstruction: 2.033087, Regularization: 0.048000, Discriminator: 0.013199; Generator: 0.038234,\n",
      "D(x): 0.938, D(G(z)): 0.299\n",
      "2019-04-09 23:57:37,244 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: 2.267686\n",
      "Reconstruction: 2.166661, Regularization: 0.049406, Discriminator: 0.013225; Generator: 0.038394,\n",
      "D(x): 0.939, D(G(z)): 0.299\n",
      "2019-04-09 23:57:37,352 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: 2.324150\n",
      "Reconstruction: 2.223301, Regularization: 0.049146, Discriminator: 0.013322; Generator: 0.038381,\n",
      "D(x): 0.941, D(G(z)): 0.301\n",
      "2019-04-09 23:57:37,462 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: 2.418316\n",
      "Reconstruction: 2.317925, Regularization: 0.049575, Discriminator: 0.013510; Generator: 0.037306,\n",
      "D(x): 0.948, D(G(z)): 0.311\n",
      "2019-04-09 23:57:37,571 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: 2.477314\n",
      "Reconstruction: 2.375293, Regularization: 0.050057, Discriminator: 0.012239; Generator: 0.039725,\n",
      "D(x): 0.956, D(G(z)): 0.289\n",
      "2019-04-09 23:57:37,680 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: 2.230078\n",
      "Reconstruction: 2.133633, Regularization: 0.044725, Discriminator: 0.014335; Generator: 0.037385,\n",
      "D(x): 0.925, D(G(z)): 0.310\n",
      "2019-04-09 23:57:37,790 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: 2.426176\n",
      "Reconstruction: 2.325673, Regularization: 0.048899, Discriminator: 0.012479; Generator: 0.039125,\n",
      "D(x): 0.953, D(G(z)): 0.292\n",
      "2019-04-09 23:57:37,900 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: 2.536123\n",
      "Reconstruction: 2.436363, Regularization: 0.048663, Discriminator: 0.013106; Generator: 0.037991,\n",
      "D(x): 0.949, D(G(z)): 0.302\n",
      "2019-04-09 23:57:38,010 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: 2.479166\n",
      "Reconstruction: 2.379875, Regularization: 0.048243, Discriminator: 0.012175; Generator: 0.038872,\n",
      "D(x): 0.961, D(G(z)): 0.293\n",
      "2019-04-09 23:57:38,120 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: 2.125485\n",
      "Reconstruction: 2.029929, Regularization: 0.043306, Discriminator: 0.012771; Generator: 0.039480,\n",
      "D(x): 0.943, D(G(z)): 0.291\n",
      "2019-04-09 23:57:38,229 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: 2.467419\n",
      "Reconstruction: 2.368466, Regularization: 0.046683, Discriminator: 0.011498; Generator: 0.040772,\n",
      "D(x): 0.961, D(G(z)): 0.277\n",
      "2019-04-09 23:57:38,309 root         INFO     ====> Epoch: 3 Average loss: 2.3666\n",
      "2019-04-09 23:57:38,337 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: 2.546032\n",
      "Reconstruction: 2.446517, Regularization: 0.047433, Discriminator: 0.012589; Generator: 0.039494,\n",
      "D(x): 0.945, D(G(z)): 0.288\n",
      "2019-04-09 23:57:38,449 root         INFO     Train Epoch: 4 [512/8000 (6%)]\tTotal Loss: 2.431896\n",
      "Reconstruction: 2.334892, Regularization: 0.045636, Discriminator: 0.012770; Generator: 0.038598,\n",
      "D(x): 0.949, D(G(z)): 0.297\n",
      "2019-04-09 23:57:38,559 root         INFO     Train Epoch: 4 [1024/8000 (13%)]\tTotal Loss: 2.483373\n",
      "Reconstruction: 2.386361, Regularization: 0.046201, Discriminator: 0.012221; Generator: 0.038590,\n",
      "D(x): 0.965, D(G(z)): 0.297\n",
      "2019-04-09 23:57:38,670 root         INFO     Train Epoch: 4 [1536/8000 (19%)]\tTotal Loss: 2.288322\n",
      "Reconstruction: 2.191615, Regularization: 0.043738, Discriminator: 0.012436; Generator: 0.040534,\n",
      "D(x): 0.941, D(G(z)): 0.281\n",
      "2019-04-09 23:57:38,777 root         INFO     Train Epoch: 4 [2048/8000 (26%)]\tTotal Loss: 2.247029\n",
      "Reconstruction: 2.152482, Regularization: 0.039807, Discriminator: 0.012766; Generator: 0.041974,\n",
      "D(x): 0.915, D(G(z)): 0.267\n",
      "2019-04-09 23:57:38,886 root         INFO     Train Epoch: 4 [2560/8000 (32%)]\tTotal Loss: 2.319559\n",
      "Reconstruction: 2.224002, Regularization: 0.042036, Discriminator: 0.012706; Generator: 0.040815,\n",
      "D(x): 0.936, D(G(z)): 0.279\n",
      "2019-04-09 23:57:38,994 root         INFO     Train Epoch: 4 [3072/8000 (38%)]\tTotal Loss: 2.642929\n",
      "Reconstruction: 2.545756, Regularization: 0.046720, Discriminator: 0.012595; Generator: 0.037858,\n",
      "D(x): 0.967, D(G(z)): 0.306\n",
      "2019-04-09 23:57:39,102 root         INFO     Train Epoch: 4 [3584/8000 (45%)]\tTotal Loss: 2.733521\n",
      "Reconstruction: 2.635514, Regularization: 0.045756, Discriminator: 0.012126; Generator: 0.040124,\n",
      "D(x): 0.956, D(G(z)): 0.285\n",
      "2019-04-09 23:57:39,210 root         INFO     Train Epoch: 4 [4096/8000 (51%)]\tTotal Loss: 2.446182\n",
      "Reconstruction: 2.349242, Regularization: 0.043003, Discriminator: 0.011005; Generator: 0.042932,\n",
      "D(x): 0.950, D(G(z)): 0.257\n",
      "2019-04-09 23:57:39,318 root         INFO     Train Epoch: 4 [4608/8000 (58%)]\tTotal Loss: 2.381518\n",
      "Reconstruction: 2.286836, Regularization: 0.042450, Discriminator: 0.011445; Generator: 0.040788,\n",
      "D(x): 0.960, D(G(z)): 0.276\n",
      "2019-04-09 23:57:39,425 root         INFO     Train Epoch: 4 [5120/8000 (64%)]\tTotal Loss: 2.372978\n",
      "Reconstruction: 2.279647, Regularization: 0.039972, Discriminator: 0.012698; Generator: 0.040660,\n",
      "D(x): 0.931, D(G(z)): 0.279\n",
      "2019-04-09 23:57:39,532 root         INFO     Train Epoch: 4 [5632/8000 (70%)]\tTotal Loss: 2.429797\n",
      "Reconstruction: 2.336118, Regularization: 0.041421, Discriminator: 0.011646; Generator: 0.040612,\n",
      "D(x): 0.956, D(G(z)): 0.277\n",
      "2019-04-09 23:57:39,639 root         INFO     Train Epoch: 4 [6144/8000 (77%)]\tTotal Loss: 2.363950\n",
      "Reconstruction: 2.269010, Regularization: 0.040834, Discriminator: 0.011281; Generator: 0.042825,\n",
      "D(x): 0.947, D(G(z)): 0.260\n",
      "2019-04-09 23:57:39,747 root         INFO     Train Epoch: 4 [6656/8000 (83%)]\tTotal Loss: 2.238274\n",
      "Reconstruction: 2.146042, Regularization: 0.038643, Discriminator: 0.012180; Generator: 0.041409,\n",
      "D(x): 0.935, D(G(z)): 0.271\n",
      "2019-04-09 23:57:39,855 root         INFO     Train Epoch: 4 [7168/8000 (90%)]\tTotal Loss: 2.515895\n",
      "Reconstruction: 2.419841, Regularization: 0.041383, Discriminator: 0.010781; Generator: 0.043890,\n",
      "D(x): 0.954, D(G(z)): 0.254\n",
      "2019-04-09 23:57:39,963 root         INFO     Train Epoch: 4 [7680/8000 (96%)]\tTotal Loss: 2.544545\n",
      "Reconstruction: 2.450127, Regularization: 0.041280, Discriminator: 0.011115; Generator: 0.042023,\n",
      "D(x): 0.957, D(G(z)): 0.265\n",
      "2019-04-09 23:57:40,044 root         INFO     ====> Epoch: 4 Average loss: 2.4179\n",
      "2019-04-09 23:57:40,071 root         INFO     Train Epoch: 5 [0/8000 (0%)]\tTotal Loss: 2.716971\n",
      "Reconstruction: 2.619020, Regularization: 0.044332, Discriminator: 0.011091; Generator: 0.042527,\n",
      "D(x): 0.956, D(G(z)): 0.263\n",
      "2019-04-09 23:57:40,182 root         INFO     Train Epoch: 5 [512/8000 (6%)]\tTotal Loss: 2.525099\n",
      "Reconstruction: 2.428954, Regularization: 0.042122, Discriminator: 0.011206; Generator: 0.042817,\n",
      "D(x): 0.961, D(G(z)): 0.267\n",
      "2019-04-09 23:57:40,292 root         INFO     Train Epoch: 5 [1024/8000 (13%)]\tTotal Loss: 2.470682\n",
      "Reconstruction: 2.376340, Regularization: 0.040640, Discriminator: 0.011755; Generator: 0.041946,\n",
      "D(x): 0.946, D(G(z)): 0.268\n",
      "2019-04-09 23:57:40,403 root         INFO     Train Epoch: 5 [1536/8000 (19%)]\tTotal Loss: 2.334043\n",
      "Reconstruction: 2.240508, Regularization: 0.039101, Discriminator: 0.011019; Generator: 0.043416,\n",
      "D(x): 0.949, D(G(z)): 0.256\n",
      "2019-04-09 23:57:40,514 root         INFO     Train Epoch: 5 [2048/8000 (26%)]\tTotal Loss: 2.355677\n",
      "Reconstruction: 2.263200, Regularization: 0.037712, Discriminator: 0.011412; Generator: 0.043352,\n",
      "D(x): 0.943, D(G(z)): 0.258\n",
      "2019-04-09 23:57:40,624 root         INFO     Train Epoch: 5 [2560/8000 (32%)]\tTotal Loss: 2.459597\n",
      "Reconstruction: 2.366151, Regularization: 0.039280, Discriminator: 0.011023; Generator: 0.043143,\n",
      "D(x): 0.947, D(G(z)): 0.255\n",
      "2019-04-09 23:57:40,734 root         INFO     Train Epoch: 5 [3072/8000 (38%)]\tTotal Loss: 2.554432\n",
      "Reconstruction: 2.461082, Regularization: 0.039453, Discriminator: 0.011110; Generator: 0.042787,\n",
      "D(x): 0.951, D(G(z)): 0.260\n",
      "2019-04-09 23:57:40,844 root         INFO     Train Epoch: 5 [3584/8000 (45%)]\tTotal Loss: 2.522971\n",
      "Reconstruction: 2.428673, Regularization: 0.039127, Discriminator: 0.009784; Generator: 0.045388,\n",
      "D(x): 0.965, D(G(z)): 0.240\n",
      "2019-04-09 23:57:40,955 root         INFO     Train Epoch: 5 [4096/8000 (51%)]\tTotal Loss: 2.475683\n",
      "Reconstruction: 2.381872, Regularization: 0.039208, Discriminator: 0.010531; Generator: 0.044071,\n",
      "D(x): 0.959, D(G(z)): 0.252\n",
      "2019-04-09 23:57:41,064 root         INFO     Train Epoch: 5 [4608/8000 (58%)]\tTotal Loss: 2.525269\n",
      "Reconstruction: 2.430689, Regularization: 0.037617, Discriminator: 0.010808; Generator: 0.046155,\n",
      "D(x): 0.935, D(G(z)): 0.237\n",
      "2019-04-09 23:57:41,174 root         INFO     Train Epoch: 5 [5120/8000 (64%)]\tTotal Loss: 2.590308\n",
      "Reconstruction: 2.494628, Regularization: 0.039208, Discriminator: 0.011933; Generator: 0.044540,\n",
      "D(x): 0.930, D(G(z)): 0.249\n",
      "2019-04-09 23:57:41,284 root         INFO     Train Epoch: 5 [5632/8000 (70%)]\tTotal Loss: 2.390768\n",
      "Reconstruction: 2.299664, Regularization: 0.036293, Discriminator: 0.011213; Generator: 0.043598,\n",
      "D(x): 0.945, D(G(z)): 0.256\n",
      "2019-04-09 23:57:41,395 root         INFO     Train Epoch: 5 [6144/8000 (77%)]\tTotal Loss: 2.477177\n",
      "Reconstruction: 2.382846, Regularization: 0.038308, Discriminator: 0.009758; Generator: 0.046264,\n",
      "D(x): 0.957, D(G(z)): 0.233\n",
      "2019-04-09 23:57:41,505 root         INFO     Train Epoch: 5 [6656/8000 (83%)]\tTotal Loss: 2.339853\n",
      "Reconstruction: 2.247826, Regularization: 0.037150, Discriminator: 0.011327; Generator: 0.043550,\n",
      "D(x): 0.941, D(G(z)): 0.256\n",
      "2019-04-09 23:57:41,617 root         INFO     Train Epoch: 5 [7168/8000 (90%)]\tTotal Loss: 2.304023\n",
      "Reconstruction: 2.209170, Regularization: 0.035750, Discriminator: 0.011326; Generator: 0.047777,\n",
      "D(x): 0.910, D(G(z)): 0.225\n",
      "2019-04-09 23:57:41,729 root         INFO     Train Epoch: 5 [7680/8000 (96%)]\tTotal Loss: 2.624019\n",
      "Reconstruction: 2.529009, Regularization: 0.038379, Discriminator: 0.010478; Generator: 0.046153,\n",
      "D(x): 0.941, D(G(z)): 0.234\n",
      "2019-04-09 23:57:41,809 root         INFO     ====> Epoch: 5 Average loss: 2.4806\n",
      "2019-04-09 23:57:41,837 root         INFO     Train Epoch: 6 [0/8000 (0%)]\tTotal Loss: 2.518466\n",
      "Reconstruction: 2.424773, Regularization: 0.038817, Discriminator: 0.009954; Generator: 0.044921,\n",
      "D(x): 0.964, D(G(z)): 0.243\n",
      "2019-04-09 23:57:41,949 root         INFO     Train Epoch: 6 [512/8000 (6%)]\tTotal Loss: 2.689888\n",
      "Reconstruction: 2.593985, Regularization: 0.040552, Discriminator: 0.009445; Generator: 0.045907,\n",
      "D(x): 0.969, D(G(z)): 0.236\n",
      "2019-04-09 23:57:42,059 root         INFO     Train Epoch: 6 [1024/8000 (13%)]\tTotal Loss: 2.835569\n",
      "Reconstruction: 2.738463, Regularization: 0.041730, Discriminator: 0.009543; Generator: 0.045833,\n",
      "D(x): 0.970, D(G(z)): 0.238\n",
      "2019-04-09 23:57:42,170 root         INFO     Train Epoch: 6 [1536/8000 (19%)]\tTotal Loss: 2.727098\n",
      "Reconstruction: 2.630121, Regularization: 0.040211, Discriminator: 0.009291; Generator: 0.047475,\n",
      "D(x): 0.962, D(G(z)): 0.224\n",
      "2019-04-09 23:57:42,280 root         INFO     Train Epoch: 6 [2048/8000 (26%)]\tTotal Loss: 2.532433\n",
      "Reconstruction: 2.438075, Regularization: 0.038409, Discriminator: 0.010559; Generator: 0.045390,\n",
      "D(x): 0.944, D(G(z)): 0.238\n",
      "2019-04-09 23:57:42,390 root         INFO     Train Epoch: 6 [2560/8000 (32%)]\tTotal Loss: 2.413865\n",
      "Reconstruction: 2.318575, Regularization: 0.036862, Discriminator: 0.009718; Generator: 0.048709,\n",
      "D(x): 0.942, D(G(z)): 0.219\n",
      "2019-04-09 23:57:42,499 root         INFO     Train Epoch: 6 [3072/8000 (38%)]\tTotal Loss: 2.563608\n",
      "Reconstruction: 2.467556, Regularization: 0.039689, Discriminator: 0.009461; Generator: 0.046902,\n",
      "D(x): 0.966, D(G(z)): 0.232\n",
      "2019-04-09 23:57:42,608 root         INFO     Train Epoch: 6 [3584/8000 (45%)]\tTotal Loss: 2.707845\n",
      "Reconstruction: 2.610651, Regularization: 0.040173, Discriminator: 0.009234; Generator: 0.047787,\n",
      "D(x): 0.961, D(G(z)): 0.223\n",
      "2019-04-09 23:57:42,718 root         INFO     Train Epoch: 6 [4096/8000 (51%)]\tTotal Loss: 2.509057\n",
      "Reconstruction: 2.411966, Regularization: 0.038599, Discriminator: 0.009139; Generator: 0.049352,\n",
      "D(x): 0.948, D(G(z)): 0.209\n",
      "2019-04-09 23:57:42,829 root         INFO     Train Epoch: 6 [4608/8000 (58%)]\tTotal Loss: 2.399684\n",
      "Reconstruction: 2.305218, Regularization: 0.036274, Discriminator: 0.009451; Generator: 0.048741,\n",
      "D(x): 0.950, D(G(z)): 0.218\n",
      "2019-04-09 23:57:42,938 root         INFO     Train Epoch: 6 [5120/8000 (64%)]\tTotal Loss: 2.535214\n",
      "Reconstruction: 2.440318, Regularization: 0.037979, Discriminator: 0.009879; Generator: 0.047039,\n",
      "D(x): 0.947, D(G(z)): 0.227\n",
      "2019-04-09 23:57:43,047 root         INFO     Train Epoch: 6 [5632/8000 (70%)]\tTotal Loss: 2.764369\n",
      "Reconstruction: 2.666314, Regularization: 0.040895, Discriminator: 0.009348; Generator: 0.047813,\n",
      "D(x): 0.959, D(G(z)): 0.224\n",
      "2019-04-09 23:57:43,157 root         INFO     Train Epoch: 6 [6144/8000 (77%)]\tTotal Loss: 2.680448\n",
      "Reconstruction: 2.581782, Regularization: 0.040402, Discriminator: 0.008300; Generator: 0.049964,\n",
      "D(x): 0.971, D(G(z)): 0.209\n",
      "2019-04-09 23:57:43,266 root         INFO     Train Epoch: 6 [6656/8000 (83%)]\tTotal Loss: 2.420471\n",
      "Reconstruction: 2.324496, Regularization: 0.037784, Discriminator: 0.009083; Generator: 0.049108,\n",
      "D(x): 0.955, D(G(z)): 0.214\n",
      "2019-04-09 23:57:43,376 root         INFO     Train Epoch: 6 [7168/8000 (90%)]\tTotal Loss: 2.606848\n",
      "Reconstruction: 2.508412, Regularization: 0.039099, Discriminator: 0.008767; Generator: 0.050571,\n",
      "D(x): 0.953, D(G(z)): 0.205\n",
      "2019-04-09 23:57:43,486 root         INFO     Train Epoch: 6 [7680/8000 (96%)]\tTotal Loss: 2.593854\n",
      "Reconstruction: 2.496763, Regularization: 0.039644, Discriminator: 0.009472; Generator: 0.047975,\n",
      "D(x): 0.954, D(G(z)): 0.223\n",
      "2019-04-09 23:57:43,567 root         INFO     ====> Epoch: 6 Average loss: 2.5738\n",
      "2019-04-09 23:57:43,593 root         INFO     Train Epoch: 7 [0/8000 (0%)]\tTotal Loss: 2.582782\n",
      "Reconstruction: 2.484458, Regularization: 0.038890, Discriminator: 0.009119; Generator: 0.050315,\n",
      "D(x): 0.950, D(G(z)): 0.207\n",
      "2019-04-09 23:57:43,707 root         INFO     Train Epoch: 7 [512/8000 (6%)]\tTotal Loss: 2.686829\n",
      "Reconstruction: 2.585726, Regularization: 0.039869, Discriminator: 0.008243; Generator: 0.052991,\n",
      "D(x): 0.954, D(G(z)): 0.191\n",
      "2019-04-09 23:57:43,819 root         INFO     Train Epoch: 7 [1024/8000 (13%)]\tTotal Loss: 2.782117\n",
      "Reconstruction: 2.679967, Regularization: 0.042930, Discriminator: 0.008675; Generator: 0.050545,\n",
      "D(x): 0.958, D(G(z)): 0.206\n",
      "2019-04-09 23:57:43,930 root         INFO     Train Epoch: 7 [1536/8000 (19%)]\tTotal Loss: 2.624012\n",
      "Reconstruction: 2.522754, Regularization: 0.039707, Discriminator: 0.008081; Generator: 0.053469,\n",
      "D(x): 0.954, D(G(z)): 0.187\n",
      "2019-04-09 23:57:44,042 root         INFO     Train Epoch: 7 [2048/8000 (26%)]\tTotal Loss: 2.469860\n",
      "Reconstruction: 2.369578, Regularization: 0.039950, Discriminator: 0.007953; Generator: 0.052379,\n",
      "D(x): 0.966, D(G(z)): 0.195\n",
      "2019-04-09 23:57:44,153 root         INFO     Train Epoch: 7 [2560/8000 (32%)]\tTotal Loss: 2.545157\n",
      "Reconstruction: 2.444808, Regularization: 0.039975, Discriminator: 0.008552; Generator: 0.051822,\n",
      "D(x): 0.952, D(G(z)): 0.197\n",
      "2019-04-09 23:57:44,265 root         INFO     Train Epoch: 7 [3072/8000 (38%)]\tTotal Loss: 2.572594\n",
      "Reconstruction: 2.474159, Regularization: 0.039630, Discriminator: 0.009021; Generator: 0.049785,\n",
      "D(x): 0.955, D(G(z)): 0.212\n",
      "2019-04-09 23:57:44,374 root         INFO     Train Epoch: 7 [3584/8000 (45%)]\tTotal Loss: 2.559930\n",
      "Reconstruction: 2.459153, Regularization: 0.040330, Discriminator: 0.008460; Generator: 0.051987,\n",
      "D(x): 0.955, D(G(z)): 0.198\n",
      "2019-04-09 23:57:44,483 root         INFO     Train Epoch: 7 [4096/8000 (51%)]\tTotal Loss: 2.857029\n",
      "Reconstruction: 2.753008, Regularization: 0.043706, Discriminator: 0.008189; Generator: 0.052126,\n",
      "D(x): 0.961, D(G(z)): 0.196\n",
      "2019-04-09 23:57:44,593 root         INFO     Train Epoch: 7 [4608/8000 (58%)]\tTotal Loss: 2.741398\n",
      "Reconstruction: 2.637182, Regularization: 0.043594, Discriminator: 0.008405; Generator: 0.052217,\n",
      "D(x): 0.955, D(G(z)): 0.196\n",
      "2019-04-09 23:57:44,702 root         INFO     Train Epoch: 7 [5120/8000 (64%)]\tTotal Loss: 2.511128\n",
      "Reconstruction: 2.411119, Regularization: 0.039568, Discriminator: 0.008761; Generator: 0.051679,\n",
      "D(x): 0.946, D(G(z)): 0.197\n",
      "2019-04-09 23:57:44,812 root         INFO     Train Epoch: 7 [5632/8000 (70%)]\tTotal Loss: 2.516608\n",
      "Reconstruction: 2.416537, Regularization: 0.040767, Discriminator: 0.009209; Generator: 0.050094,\n",
      "D(x): 0.945, D(G(z)): 0.208\n",
      "2019-04-09 23:57:44,922 root         INFO     Train Epoch: 7 [6144/8000 (77%)]\tTotal Loss: 2.909624\n",
      "Reconstruction: 2.799075, Regularization: 0.047075, Discriminator: 0.006914; Generator: 0.056559,\n",
      "D(x): 0.968, D(G(z)): 0.169\n",
      "2019-04-09 23:57:45,031 root         INFO     Train Epoch: 7 [6656/8000 (83%)]\tTotal Loss: 2.720916\n",
      "Reconstruction: 2.615023, Regularization: 0.044900, Discriminator: 0.008005; Generator: 0.052988,\n",
      "D(x): 0.958, D(G(z)): 0.189\n",
      "2019-04-09 23:57:45,140 root         INFO     Train Epoch: 7 [7168/8000 (90%)]\tTotal Loss: 2.669194\n",
      "Reconstruction: 2.563677, Regularization: 0.042476, Discriminator: 0.009446; Generator: 0.053595,\n",
      "D(x): 0.921, D(G(z)): 0.187\n",
      "2019-04-09 23:57:45,249 root         INFO     Train Epoch: 7 [7680/8000 (96%)]\tTotal Loss: 2.898531\n",
      "Reconstruction: 2.790880, Regularization: 0.044905, Discriminator: 0.008369; Generator: 0.054378,\n",
      "D(x): 0.944, D(G(z)): 0.182\n",
      "2019-04-09 23:57:45,329 root         INFO     ====> Epoch: 7 Average loss: 2.6871\n",
      "2019-04-09 23:57:45,356 root         INFO     Train Epoch: 8 [0/8000 (0%)]\tTotal Loss: 2.398392\n",
      "Reconstruction: 2.293049, Regularization: 0.039803, Discriminator: 0.007266; Generator: 0.058274,\n",
      "D(x): 0.949, D(G(z)): 0.162\n",
      "2019-04-09 23:57:45,469 root         INFO     Train Epoch: 8 [512/8000 (6%)]\tTotal Loss: 2.946515\n",
      "Reconstruction: 2.834199, Regularization: 0.049985, Discriminator: 0.007348; Generator: 0.054982,\n",
      "D(x): 0.965, D(G(z)): 0.178\n",
      "2019-04-09 23:57:45,580 root         INFO     Train Epoch: 8 [1024/8000 (13%)]\tTotal Loss: 3.003541\n",
      "Reconstruction: 2.891445, Regularization: 0.049408, Discriminator: 0.007508; Generator: 0.055181,\n",
      "D(x): 0.959, D(G(z)): 0.177\n",
      "2019-04-09 23:57:45,692 root         INFO     Train Epoch: 8 [1536/8000 (19%)]\tTotal Loss: 2.669897\n",
      "Reconstruction: 2.562297, Regularization: 0.046361, Discriminator: 0.008341; Generator: 0.052898,\n",
      "D(x): 0.951, D(G(z)): 0.191\n",
      "2019-04-09 23:57:45,805 root         INFO     Train Epoch: 8 [2048/8000 (26%)]\tTotal Loss: 2.719884\n",
      "Reconstruction: 2.608761, Regularization: 0.046621, Discriminator: 0.008082; Generator: 0.056420,\n",
      "D(x): 0.942, D(G(z)): 0.170\n",
      "2019-04-09 23:57:45,915 root         INFO     Train Epoch: 8 [2560/8000 (32%)]\tTotal Loss: 2.766762\n",
      "Reconstruction: 2.655828, Regularization: 0.047096, Discriminator: 0.007903; Generator: 0.055936,\n",
      "D(x): 0.948, D(G(z)): 0.173\n",
      "2019-04-09 23:57:46,024 root         INFO     Train Epoch: 8 [3072/8000 (38%)]\tTotal Loss: 2.972008\n",
      "Reconstruction: 2.855198, Regularization: 0.052412, Discriminator: 0.006524; Generator: 0.057874,\n",
      "D(x): 0.970, D(G(z)): 0.162\n",
      "2019-04-09 23:57:46,134 root         INFO     Train Epoch: 8 [3584/8000 (45%)]\tTotal Loss: 2.942870\n",
      "Reconstruction: 2.827428, Regularization: 0.051576, Discriminator: 0.006939; Generator: 0.056927,\n",
      "D(x): 0.966, D(G(z)): 0.168\n",
      "2019-04-09 23:57:46,243 root         INFO     Train Epoch: 8 [4096/8000 (51%)]\tTotal Loss: 2.641622\n",
      "Reconstruction: 2.533222, Regularization: 0.045475, Discriminator: 0.008034; Generator: 0.054891,\n",
      "D(x): 0.948, D(G(z)): 0.181\n",
      "2019-04-09 23:57:46,354 root         INFO     Train Epoch: 8 [4608/8000 (58%)]\tTotal Loss: 2.858557\n",
      "Reconstruction: 2.743115, Regularization: 0.051201, Discriminator: 0.007599; Generator: 0.056642,\n",
      "D(x): 0.949, D(G(z)): 0.167\n",
      "2019-04-09 23:57:46,464 root         INFO     Train Epoch: 8 [5120/8000 (64%)]\tTotal Loss: 3.049378\n",
      "Reconstruction: 2.930481, Regularization: 0.055484, Discriminator: 0.007653; Generator: 0.055760,\n",
      "D(x): 0.953, D(G(z)): 0.172\n",
      "2019-04-09 23:57:46,575 root         INFO     Train Epoch: 8 [5632/8000 (70%)]\tTotal Loss: 3.092506\n",
      "Reconstruction: 2.974740, Regularization: 0.055603, Discriminator: 0.007230; Generator: 0.054933,\n",
      "D(x): 0.970, D(G(z)): 0.180\n",
      "2019-04-09 23:57:46,684 root         INFO     Train Epoch: 8 [6144/8000 (77%)]\tTotal Loss: 2.861867\n",
      "Reconstruction: 2.746739, Regularization: 0.053112, Discriminator: 0.007874; Generator: 0.054141,\n",
      "D(x): 0.958, D(G(z)): 0.185\n",
      "2019-04-09 23:57:46,792 root         INFO     Train Epoch: 8 [6656/8000 (83%)]\tTotal Loss: 2.795170\n",
      "Reconstruction: 2.677562, Regularization: 0.051260, Discriminator: 0.006512; Generator: 0.059836,\n",
      "D(x): 0.962, D(G(z)): 0.153\n",
      "2019-04-09 23:57:46,900 root         INFO     Train Epoch: 8 [7168/8000 (90%)]\tTotal Loss: 2.820825\n",
      "Reconstruction: 2.704323, Regularization: 0.053200, Discriminator: 0.007786; Generator: 0.055516,\n",
      "D(x): 0.951, D(G(z)): 0.174\n",
      "2019-04-09 23:57:47,008 root         INFO     Train Epoch: 8 [7680/8000 (96%)]\tTotal Loss: 3.202289\n",
      "Reconstruction: 3.078268, Regularization: 0.059577, Discriminator: 0.006569; Generator: 0.057874,\n",
      "D(x): 0.969, D(G(z)): 0.161\n",
      "2019-04-09 23:57:47,089 root         INFO     ====> Epoch: 8 Average loss: 2.8077\n",
      "2019-04-09 23:57:47,116 root         INFO     Train Epoch: 9 [0/8000 (0%)]\tTotal Loss: 3.037700\n",
      "Reconstruction: 2.916634, Regularization: 0.055388, Discriminator: 0.006544; Generator: 0.059135,\n",
      "D(x): 0.964, D(G(z)): 0.156\n",
      "2019-04-09 23:57:47,228 root         INFO     Train Epoch: 9 [512/8000 (6%)]\tTotal Loss: 2.695919\n",
      "Reconstruction: 2.578808, Regularization: 0.050734, Discriminator: 0.006899; Generator: 0.059477,\n",
      "D(x): 0.950, D(G(z)): 0.153\n",
      "2019-04-09 23:57:47,340 root         INFO     Train Epoch: 9 [1024/8000 (13%)]\tTotal Loss: 2.929866\n",
      "Reconstruction: 2.809342, Regularization: 0.058041, Discriminator: 0.007755; Generator: 0.054728,\n",
      "D(x): 0.959, D(G(z)): 0.180\n",
      "2019-04-09 23:57:47,449 root         INFO     Train Epoch: 9 [1536/8000 (19%)]\tTotal Loss: 2.664168\n",
      "Reconstruction: 2.549760, Regularization: 0.049701, Discriminator: 0.007363; Generator: 0.057344,\n",
      "D(x): 0.946, D(G(z)): 0.163\n",
      "2019-04-09 23:57:47,559 root         INFO     Train Epoch: 9 [2048/8000 (26%)]\tTotal Loss: 3.034592\n",
      "Reconstruction: 2.906030, Regularization: 0.062080, Discriminator: 0.005790; Generator: 0.060691,\n",
      "D(x): 0.978, D(G(z)): 0.149\n",
      "2019-04-09 23:57:47,668 root         INFO     Train Epoch: 9 [2560/8000 (32%)]\tTotal Loss: 2.946498\n",
      "Reconstruction: 2.822728, Regularization: 0.057662, Discriminator: 0.006715; Generator: 0.059392,\n",
      "D(x): 0.958, D(G(z)): 0.155\n",
      "2019-04-09 23:57:47,779 root         INFO     Train Epoch: 9 [3072/8000 (38%)]\tTotal Loss: 3.040473\n",
      "Reconstruction: 2.914406, Regularization: 0.061261, Discriminator: 0.006890; Generator: 0.057916,\n",
      "D(x): 0.959, D(G(z)): 0.160\n",
      "2019-04-09 23:57:47,888 root         INFO     Train Epoch: 9 [3584/8000 (45%)]\tTotal Loss: 2.764902\n",
      "Reconstruction: 2.641056, Regularization: 0.056144, Discriminator: 0.006941; Generator: 0.060762,\n",
      "D(x): 0.947, D(G(z)): 0.150\n",
      "2019-04-09 23:57:47,996 root         INFO     Train Epoch: 9 [4096/8000 (51%)]\tTotal Loss: 3.159222\n",
      "Reconstruction: 3.025852, Regularization: 0.067344, Discriminator: 0.006764; Generator: 0.059263,\n",
      "D(x): 0.962, D(G(z)): 0.158\n",
      "2019-04-09 23:57:48,106 root         INFO     Train Epoch: 9 [4608/8000 (58%)]\tTotal Loss: 2.852827\n",
      "Reconstruction: 2.727541, Regularization: 0.057292, Discriminator: 0.006316; Generator: 0.061678,\n",
      "D(x): 0.958, D(G(z)): 0.145\n",
      "2019-04-09 23:57:48,215 root         INFO     Train Epoch: 9 [5120/8000 (64%)]\tTotal Loss: 2.461398\n",
      "Reconstruction: 2.339643, Regularization: 0.052511, Discriminator: 0.006276; Generator: 0.062969,\n",
      "D(x): 0.950, D(G(z)): 0.137\n",
      "2019-04-09 23:57:48,324 root         INFO     Train Epoch: 9 [5632/8000 (70%)]\tTotal Loss: 2.886088\n",
      "Reconstruction: 2.755825, Regularization: 0.060888, Discriminator: 0.006622; Generator: 0.062753,\n",
      "D(x): 0.944, D(G(z)): 0.139\n",
      "2019-04-09 23:57:48,432 root         INFO     Train Epoch: 9 [6144/8000 (77%)]\tTotal Loss: 2.926095\n",
      "Reconstruction: 2.789399, Regularization: 0.065624, Discriminator: 0.006035; Generator: 0.065038,\n",
      "D(x): 0.953, D(G(z)): 0.131\n",
      "2019-04-09 23:57:48,541 root         INFO     Train Epoch: 9 [6656/8000 (83%)]\tTotal Loss: 2.913698\n",
      "Reconstruction: 2.782078, Regularization: 0.065166, Discriminator: 0.006463; Generator: 0.059991,\n",
      "D(x): 0.959, D(G(z)): 0.150\n",
      "2019-04-09 23:57:48,650 root         INFO     Train Epoch: 9 [7168/8000 (90%)]\tTotal Loss: 2.921562\n",
      "Reconstruction: 2.788799, Regularization: 0.065185, Discriminator: 0.006900; Generator: 0.060677,\n",
      "D(x): 0.954, D(G(z)): 0.155\n",
      "2019-04-09 23:57:48,760 root         INFO     Train Epoch: 9 [7680/8000 (96%)]\tTotal Loss: 2.724580\n",
      "Reconstruction: 2.593179, Regularization: 0.063985, Discriminator: 0.006667; Generator: 0.060749,\n",
      "D(x): 0.952, D(G(z)): 0.148\n",
      "2019-04-09 23:57:48,841 root         INFO     ====> Epoch: 9 Average loss: 2.9226\n",
      "2019-04-09 23:57:48,868 root         INFO     Train Epoch: 10 [0/8000 (0%)]\tTotal Loss: 3.163290\n",
      "Reconstruction: 3.023406, Regularization: 0.072356, Discriminator: 0.005909; Generator: 0.061620,\n",
      "D(x): 0.968, D(G(z)): 0.143\n",
      "2019-04-09 23:57:48,981 root         INFO     Train Epoch: 10 [512/8000 (6%)]\tTotal Loss: 3.208916\n",
      "Reconstruction: 3.069094, Regularization: 0.069370, Discriminator: 0.006091; Generator: 0.064362,\n",
      "D(x): 0.956, D(G(z)): 0.134\n",
      "2019-04-09 23:57:49,092 root         INFO     Train Epoch: 10 [1024/8000 (13%)]\tTotal Loss: 2.698849\n",
      "Reconstruction: 2.565971, Regularization: 0.064811, Discriminator: 0.006791; Generator: 0.061276,\n",
      "D(x): 0.950, D(G(z)): 0.149\n",
      "2019-04-09 23:57:49,204 root         INFO     Train Epoch: 10 [1536/8000 (19%)]\tTotal Loss: 3.250508\n",
      "Reconstruction: 3.106590, Regularization: 0.074720, Discriminator: 0.005461; Generator: 0.063737,\n",
      "D(x): 0.972, D(G(z)): 0.135\n",
      "2019-04-09 23:57:49,316 root         INFO     Train Epoch: 10 [2048/8000 (26%)]\tTotal Loss: 3.127661\n",
      "Reconstruction: 2.985883, Regularization: 0.070431, Discriminator: 0.005824; Generator: 0.065524,\n",
      "D(x): 0.955, D(G(z)): 0.127\n",
      "2019-04-09 23:57:49,427 root         INFO     Train Epoch: 10 [2560/8000 (32%)]\tTotal Loss: 2.993953\n",
      "Reconstruction: 2.852433, Regularization: 0.073276, Discriminator: 0.005788; Generator: 0.062456,\n",
      "D(x): 0.970, D(G(z)): 0.141\n",
      "2019-04-09 23:57:49,539 root         INFO     Train Epoch: 10 [3072/8000 (38%)]\tTotal Loss: 3.070201\n",
      "Reconstruction: 2.932155, Regularization: 0.069552, Discriminator: 0.005966; Generator: 0.062527,\n",
      "D(x): 0.960, D(G(z)): 0.138\n",
      "2019-04-09 23:57:49,651 root         INFO     Train Epoch: 10 [3584/8000 (45%)]\tTotal Loss: 2.967317\n",
      "Reconstruction: 2.813628, Regularization: 0.082468, Discriminator: 0.005059; Generator: 0.066163,\n",
      "D(x): 0.976, D(G(z)): 0.127\n",
      "2019-04-09 23:57:49,762 root         INFO     Train Epoch: 10 [4096/8000 (51%)]\tTotal Loss: 2.927368\n",
      "Reconstruction: 2.778325, Regularization: 0.078436, Discriminator: 0.005326; Generator: 0.065282,\n",
      "D(x): 0.970, D(G(z)): 0.129\n",
      "2019-04-09 23:57:49,873 root         INFO     Train Epoch: 10 [4608/8000 (58%)]\tTotal Loss: 3.241155\n",
      "Reconstruction: 3.085502, Regularization: 0.083903, Discriminator: 0.004748; Generator: 0.067003,\n",
      "D(x): 0.982, D(G(z)): 0.124\n",
      "2019-04-09 23:57:49,984 root         INFO     Train Epoch: 10 [5120/8000 (64%)]\tTotal Loss: 2.845888\n",
      "Reconstruction: 2.702682, Regularization: 0.071619, Discriminator: 0.006103; Generator: 0.065484,\n",
      "D(x): 0.947, D(G(z)): 0.127\n",
      "2019-04-09 23:57:50,096 root         INFO     Train Epoch: 10 [5632/8000 (70%)]\tTotal Loss: 2.799600\n",
      "Reconstruction: 2.654857, Regularization: 0.073123, Discriminator: 0.005992; Generator: 0.065628,\n",
      "D(x): 0.950, D(G(z)): 0.126\n",
      "2019-04-09 23:57:50,207 root         INFO     Train Epoch: 10 [6144/8000 (77%)]\tTotal Loss: 2.778109\n",
      "Reconstruction: 2.628543, Regularization: 0.078245, Discriminator: 0.005241; Generator: 0.066080,\n",
      "D(x): 0.969, D(G(z)): 0.126\n",
      "2019-04-09 23:57:50,319 root         INFO     Train Epoch: 10 [6656/8000 (83%)]\tTotal Loss: 3.137809\n",
      "Reconstruction: 2.987719, Regularization: 0.076372, Discriminator: 0.007779; Generator: 0.065939,\n",
      "D(x): 0.927, D(G(z)): 0.126\n",
      "2019-04-09 23:57:50,430 root         INFO     Train Epoch: 10 [7168/8000 (90%)]\tTotal Loss: 3.181791\n",
      "Reconstruction: 3.024662, Regularization: 0.082612, Discriminator: 0.006253; Generator: 0.068264,\n",
      "D(x): 0.944, D(G(z)): 0.117\n",
      "2019-04-09 23:57:50,540 root         INFO     Train Epoch: 10 [7680/8000 (96%)]\tTotal Loss: 3.063052\n",
      "Reconstruction: 2.905887, Regularization: 0.086292, Discriminator: 0.005445; Generator: 0.065428,\n",
      "D(x): 0.970, D(G(z)): 0.132\n",
      "2019-04-09 23:57:50,621 root         INFO     ====> Epoch: 10 Average loss: 3.0122\n",
      "2019-04-09 23:57:50,648 root         INFO     Train Epoch: 11 [0/8000 (0%)]\tTotal Loss: 3.221933\n",
      "Reconstruction: 3.061165, Regularization: 0.087660, Discriminator: 0.004547; Generator: 0.068561,\n",
      "D(x): 0.979, D(G(z)): 0.116\n",
      "2019-04-09 23:57:50,758 root         INFO     Train Epoch: 11 [512/8000 (6%)]\tTotal Loss: 2.987289\n",
      "Reconstruction: 2.829602, Regularization: 0.087917, Discriminator: 0.006174; Generator: 0.063596,\n",
      "D(x): 0.958, D(G(z)): 0.139\n",
      "2019-04-09 23:57:50,869 root         INFO     Train Epoch: 11 [1024/8000 (13%)]\tTotal Loss: 2.923467\n",
      "Reconstruction: 2.774608, Regularization: 0.075952, Discriminator: 0.005566; Generator: 0.067341,\n",
      "D(x): 0.954, D(G(z)): 0.121\n",
      "2019-04-09 23:57:50,981 root         INFO     Train Epoch: 11 [1536/8000 (19%)]\tTotal Loss: 3.308960\n",
      "Reconstruction: 3.145760, Regularization: 0.089552, Discriminator: 0.006515; Generator: 0.067134,\n",
      "D(x): 0.944, D(G(z)): 0.124\n",
      "2019-04-09 23:57:51,093 root         INFO     Train Epoch: 11 [2048/8000 (26%)]\tTotal Loss: 2.932134\n",
      "Reconstruction: 2.763280, Regularization: 0.096217, Discriminator: 0.005300; Generator: 0.067336,\n",
      "D(x): 0.963, D(G(z)): 0.120\n",
      "2019-04-09 23:57:51,205 root         INFO     Train Epoch: 11 [2560/8000 (32%)]\tTotal Loss: 3.111924\n",
      "Reconstruction: 2.953729, Regularization: 0.086567, Discriminator: 0.005424; Generator: 0.066204,\n",
      "D(x): 0.965, D(G(z)): 0.126\n",
      "2019-04-09 23:57:51,317 root         INFO     Train Epoch: 11 [3072/8000 (38%)]\tTotal Loss: 3.217960\n",
      "Reconstruction: 3.050196, Regularization: 0.093802, Discriminator: 0.006512; Generator: 0.067450,\n",
      "D(x): 0.934, D(G(z)): 0.120\n",
      "2019-04-09 23:57:51,429 root         INFO     Train Epoch: 11 [3584/8000 (45%)]\tTotal Loss: 3.116481\n",
      "Reconstruction: 2.958578, Regularization: 0.085116, Discriminator: 0.005327; Generator: 0.067459,\n",
      "D(x): 0.962, D(G(z)): 0.121\n",
      "2019-04-09 23:57:51,541 root         INFO     Train Epoch: 11 [4096/8000 (51%)]\tTotal Loss: 3.374131\n",
      "Reconstruction: 3.206933, Regularization: 0.094149, Discriminator: 0.004775; Generator: 0.068273,\n",
      "D(x): 0.975, D(G(z)): 0.117\n",
      "2019-04-09 23:57:51,653 root         INFO     Train Epoch: 11 [4608/8000 (58%)]\tTotal Loss: 2.985998\n",
      "Reconstruction: 2.820935, Regularization: 0.092536, Discriminator: 0.004655; Generator: 0.067872,\n",
      "D(x): 0.978, D(G(z)): 0.118\n",
      "2019-04-09 23:57:51,763 root         INFO     Train Epoch: 11 [5120/8000 (64%)]\tTotal Loss: 2.995039\n",
      "Reconstruction: 2.830172, Regularization: 0.091316, Discriminator: 0.005627; Generator: 0.067922,\n",
      "D(x): 0.949, D(G(z)): 0.117\n",
      "2019-04-09 23:57:51,872 root         INFO     Train Epoch: 11 [5632/8000 (70%)]\tTotal Loss: 2.893215\n",
      "Reconstruction: 2.736130, Regularization: 0.082930, Discriminator: 0.005960; Generator: 0.068195,\n",
      "D(x): 0.944, D(G(z)): 0.119\n",
      "2019-04-09 23:57:51,983 root         INFO     Train Epoch: 11 [6144/8000 (77%)]\tTotal Loss: 2.980920\n",
      "Reconstruction: 2.819638, Regularization: 0.089046, Discriminator: 0.006177; Generator: 0.066059,\n",
      "D(x): 0.945, D(G(z)): 0.126\n",
      "2019-04-09 23:57:52,094 root         INFO     Train Epoch: 11 [6656/8000 (83%)]\tTotal Loss: 3.119643\n",
      "Reconstruction: 2.953029, Regularization: 0.089735, Discriminator: 0.004924; Generator: 0.071955,\n",
      "D(x): 0.959, D(G(z)): 0.106\n",
      "2019-04-09 23:57:52,205 root         INFO     Train Epoch: 11 [7168/8000 (90%)]\tTotal Loss: 3.057750\n",
      "Reconstruction: 2.893859, Regularization: 0.091599, Discriminator: 0.006309; Generator: 0.065982,\n",
      "D(x): 0.941, D(G(z)): 0.126\n",
      "2019-04-09 23:57:52,316 root         INFO     Train Epoch: 11 [7680/8000 (96%)]\tTotal Loss: 3.127793\n",
      "Reconstruction: 2.945591, Regularization: 0.106818, Discriminator: 0.005057; Generator: 0.070326,\n",
      "D(x): 0.957, D(G(z)): 0.108\n",
      "2019-04-09 23:57:52,396 root         INFO     ====> Epoch: 11 Average loss: 3.1089\n",
      "2019-04-09 23:57:52,423 root         INFO     Train Epoch: 12 [0/8000 (0%)]\tTotal Loss: 3.451962\n",
      "Reconstruction: 3.279974, Regularization: 0.100785, Discriminator: 0.005122; Generator: 0.066081,\n",
      "D(x): 0.972, D(G(z)): 0.125\n",
      "2019-04-09 23:57:52,537 root         INFO     Train Epoch: 12 [512/8000 (6%)]\tTotal Loss: 2.963519\n",
      "Reconstruction: 2.796014, Regularization: 0.092103, Discriminator: 0.006212; Generator: 0.069191,\n",
      "D(x): 0.937, D(G(z)): 0.114\n",
      "2019-04-09 23:57:52,649 root         INFO     Train Epoch: 12 [1024/8000 (13%)]\tTotal Loss: 2.817754\n",
      "Reconstruction: 2.655537, Regularization: 0.086868, Discriminator: 0.005699; Generator: 0.069649,\n",
      "D(x): 0.942, D(G(z)): 0.111\n",
      "2019-04-09 23:57:52,761 root         INFO     Train Epoch: 12 [1536/8000 (19%)]\tTotal Loss: 2.986716\n",
      "Reconstruction: 2.813560, Regularization: 0.098297, Discriminator: 0.005173; Generator: 0.069686,\n",
      "D(x): 0.957, D(G(z)): 0.112\n",
      "2019-04-09 23:57:52,873 root         INFO     Train Epoch: 12 [2048/8000 (26%)]\tTotal Loss: 3.012506\n",
      "Reconstruction: 2.831266, Regularization: 0.105434, Discriminator: 0.006986; Generator: 0.068820,\n",
      "D(x): 0.920, D(G(z)): 0.115\n",
      "2019-04-09 23:57:52,985 root         INFO     Train Epoch: 12 [2560/8000 (32%)]\tTotal Loss: 3.128466\n",
      "Reconstruction: 2.957357, Regularization: 0.098437, Discriminator: 0.006045; Generator: 0.066627,\n",
      "D(x): 0.944, D(G(z)): 0.123\n",
      "2019-04-09 23:57:53,097 root         INFO     Train Epoch: 12 [3072/8000 (38%)]\tTotal Loss: 2.963109\n",
      "Reconstruction: 2.780555, Regularization: 0.108308, Discriminator: 0.004729; Generator: 0.069517,\n",
      "D(x): 0.971, D(G(z)): 0.113\n",
      "2019-04-09 23:57:53,209 root         INFO     Train Epoch: 12 [3584/8000 (45%)]\tTotal Loss: 3.091827\n",
      "Reconstruction: 2.915190, Regularization: 0.101737, Discriminator: 0.005615; Generator: 0.069286,\n",
      "D(x): 0.951, D(G(z)): 0.117\n",
      "2019-04-09 23:57:53,321 root         INFO     Train Epoch: 12 [4096/8000 (51%)]\tTotal Loss: 2.846906\n",
      "Reconstruction: 2.678588, Regularization: 0.091981, Discriminator: 0.005471; Generator: 0.070865,\n",
      "D(x): 0.944, D(G(z)): 0.108\n",
      "2019-04-09 23:57:53,433 root         INFO     Train Epoch: 12 [4608/8000 (58%)]\tTotal Loss: 3.106673\n",
      "Reconstruction: 2.919716, Regularization: 0.109003, Discriminator: 0.005186; Generator: 0.072768,\n",
      "D(x): 0.951, D(G(z)): 0.102\n",
      "2019-04-09 23:57:53,545 root         INFO     Train Epoch: 12 [5120/8000 (64%)]\tTotal Loss: 3.008981\n",
      "Reconstruction: 2.835649, Regularization: 0.097541, Discriminator: 0.005798; Generator: 0.069993,\n",
      "D(x): 0.945, D(G(z)): 0.114\n",
      "2019-04-09 23:57:53,657 root         INFO     Train Epoch: 12 [5632/8000 (70%)]\tTotal Loss: 3.231179\n",
      "Reconstruction: 3.048149, Regularization: 0.110342, Discriminator: 0.005197; Generator: 0.067491,\n",
      "D(x): 0.967, D(G(z)): 0.122\n",
      "2019-04-09 23:57:53,769 root         INFO     Train Epoch: 12 [6144/8000 (77%)]\tTotal Loss: 3.104384\n",
      "Reconstruction: 2.922809, Regularization: 0.105218, Discriminator: 0.005219; Generator: 0.071138,\n",
      "D(x): 0.957, D(G(z)): 0.107\n",
      "2019-04-09 23:57:53,881 root         INFO     Train Epoch: 12 [6656/8000 (83%)]\tTotal Loss: 3.256231\n",
      "Reconstruction: 3.065214, Regularization: 0.111891, Discriminator: 0.004343; Generator: 0.074782,\n",
      "D(x): 0.965, D(G(z)): 0.096\n",
      "2019-04-09 23:57:53,993 root         INFO     Train Epoch: 12 [7168/8000 (90%)]\tTotal Loss: 3.027323\n",
      "Reconstruction: 2.847441, Regularization: 0.099955, Discriminator: 0.007605; Generator: 0.072321,\n",
      "D(x): 0.916, D(G(z)): 0.103\n",
      "2019-04-09 23:57:54,105 root         INFO     Train Epoch: 12 [7680/8000 (96%)]\tTotal Loss: 3.528762\n",
      "Reconstruction: 3.318900, Regularization: 0.131929, Discriminator: 0.004442; Generator: 0.073491,\n",
      "D(x): 0.966, D(G(z)): 0.099\n",
      "2019-04-09 23:57:54,187 root         INFO     ====> Epoch: 12 Average loss: 3.1544\n",
      "2019-04-09 23:57:54,213 root         INFO     Train Epoch: 13 [0/8000 (0%)]\tTotal Loss: 3.182495\n",
      "Reconstruction: 2.985934, Regularization: 0.116657, Discriminator: 0.006271; Generator: 0.073633,\n",
      "D(x): 0.941, D(G(z)): 0.099\n",
      "2019-04-09 23:57:54,327 root         INFO     Train Epoch: 13 [512/8000 (6%)]\tTotal Loss: 3.400183\n",
      "Reconstruction: 3.191107, Regularization: 0.131130, Discriminator: 0.004053; Generator: 0.073893,\n",
      "D(x): 0.974, D(G(z)): 0.097\n",
      "2019-04-09 23:57:54,439 root         INFO     Train Epoch: 13 [1024/8000 (13%)]\tTotal Loss: 3.123812\n",
      "Reconstruction: 2.929820, Regularization: 0.115119, Discriminator: 0.005371; Generator: 0.073501,\n",
      "D(x): 0.942, D(G(z)): 0.101\n",
      "2019-04-09 23:57:54,552 root         INFO     Train Epoch: 13 [1536/8000 (19%)]\tTotal Loss: 3.253053\n",
      "Reconstruction: 3.050878, Regularization: 0.124773, Discriminator: 0.005083; Generator: 0.072320,\n",
      "D(x): 0.955, D(G(z)): 0.105\n",
      "2019-04-09 23:57:54,665 root         INFO     Train Epoch: 13 [2048/8000 (26%)]\tTotal Loss: 3.218649\n",
      "Reconstruction: 3.016254, Regularization: 0.126258, Discriminator: 0.004453; Generator: 0.071684,\n",
      "D(x): 0.973, D(G(z)): 0.107\n",
      "2019-04-09 23:57:54,777 root         INFO     Train Epoch: 13 [2560/8000 (32%)]\tTotal Loss: 3.178833\n",
      "Reconstruction: 2.976766, Regularization: 0.122967, Discriminator: 0.004117; Generator: 0.074984,\n",
      "D(x): 0.970, D(G(z)): 0.095\n",
      "2019-04-09 23:57:54,890 root         INFO     Train Epoch: 13 [3072/8000 (38%)]\tTotal Loss: 3.558739\n",
      "Reconstruction: 3.334510, Regularization: 0.147110, Discriminator: 0.003870; Generator: 0.073249,\n",
      "D(x): 0.983, D(G(z)): 0.100\n",
      "2019-04-09 23:57:55,002 root         INFO     Train Epoch: 13 [3584/8000 (45%)]\tTotal Loss: 3.092005\n",
      "Reconstruction: 2.890481, Regularization: 0.123379, Discriminator: 0.004225; Generator: 0.073920,\n",
      "D(x): 0.971, D(G(z)): 0.099\n",
      "2019-04-09 23:57:55,114 root         INFO     Train Epoch: 13 [4096/8000 (51%)]\tTotal Loss: 3.602055\n",
      "Reconstruction: 3.375195, Regularization: 0.149649, Discriminator: 0.004606; Generator: 0.072606,\n",
      "D(x): 0.969, D(G(z)): 0.104\n",
      "2019-04-09 23:57:55,226 root         INFO     Train Epoch: 13 [4608/8000 (58%)]\tTotal Loss: 3.262257\n",
      "Reconstruction: 3.047179, Regularization: 0.135130, Discriminator: 0.003954; Generator: 0.075995,\n",
      "D(x): 0.972, D(G(z)): 0.092\n",
      "2019-04-09 23:57:55,337 root         INFO     Train Epoch: 13 [5120/8000 (64%)]\tTotal Loss: 2.994265\n",
      "Reconstruction: 2.799249, Regularization: 0.117731, Discriminator: 0.004893; Generator: 0.072392,\n",
      "D(x): 0.956, D(G(z)): 0.103\n",
      "2019-04-09 23:57:55,448 root         INFO     Train Epoch: 13 [5632/8000 (70%)]\tTotal Loss: 3.643207\n",
      "Reconstruction: 3.407776, Regularization: 0.158105, Discriminator: 0.003898; Generator: 0.073429,\n",
      "D(x): 0.981, D(G(z)): 0.099\n",
      "2019-04-09 23:57:55,560 root         INFO     Train Epoch: 13 [6144/8000 (77%)]\tTotal Loss: 3.269257\n",
      "Reconstruction: 3.064080, Regularization: 0.126124, Discriminator: 0.005149; Generator: 0.073904,\n",
      "D(x): 0.953, D(G(z)): 0.098\n",
      "2019-04-09 23:57:55,672 root         INFO     Train Epoch: 13 [6656/8000 (83%)]\tTotal Loss: 3.167594\n",
      "Reconstruction: 2.957576, Regularization: 0.130971, Discriminator: 0.005490; Generator: 0.073557,\n",
      "D(x): 0.946, D(G(z)): 0.100\n",
      "2019-04-09 23:57:55,783 root         INFO     Train Epoch: 13 [7168/8000 (90%)]\tTotal Loss: 3.165270\n",
      "Reconstruction: 2.957766, Regularization: 0.124477, Discriminator: 0.005681; Generator: 0.077345,\n",
      "D(x): 0.935, D(G(z)): 0.089\n",
      "2019-04-09 23:57:55,895 root         INFO     Train Epoch: 13 [7680/8000 (96%)]\tTotal Loss: 2.858975\n",
      "Reconstruction: 2.660987, Regularization: 0.117358, Discriminator: 0.006653; Generator: 0.073977,\n",
      "D(x): 0.913, D(G(z)): 0.097\n",
      "2019-04-09 23:57:55,977 root         INFO     ====> Epoch: 13 Average loss: 3.1930\n",
      "2019-04-09 23:57:56,004 root         INFO     Train Epoch: 14 [0/8000 (0%)]\tTotal Loss: 3.281957\n",
      "Reconstruction: 3.059277, Regularization: 0.145721, Discriminator: 0.004368; Generator: 0.072591,\n",
      "D(x): 0.971, D(G(z)): 0.102\n",
      "2019-04-09 23:57:56,116 root         INFO     Train Epoch: 14 [512/8000 (6%)]\tTotal Loss: 3.413845\n",
      "Reconstruction: 3.182929, Regularization: 0.151618, Discriminator: 0.004312; Generator: 0.074986,\n",
      "D(x): 0.965, D(G(z)): 0.095\n",
      "2019-04-09 23:57:56,228 root         INFO     Train Epoch: 14 [1024/8000 (13%)]\tTotal Loss: 3.059886\n",
      "Reconstruction: 2.848878, Regularization: 0.130595, Discriminator: 0.004958; Generator: 0.075454,\n",
      "D(x): 0.948, D(G(z)): 0.096\n",
      "2019-04-09 23:57:56,340 root         INFO     Train Epoch: 14 [1536/8000 (19%)]\tTotal Loss: 3.021562\n",
      "Reconstruction: 2.805627, Regularization: 0.136651, Discriminator: 0.004806; Generator: 0.074478,\n",
      "D(x): 0.953, D(G(z)): 0.096\n",
      "2019-04-09 23:57:56,451 root         INFO     Train Epoch: 14 [2048/8000 (26%)]\tTotal Loss: 3.373429\n",
      "Reconstruction: 3.139387, Regularization: 0.152635, Discriminator: 0.003279; Generator: 0.078129,\n",
      "D(x): 0.983, D(G(z)): 0.084\n",
      "2019-04-09 23:57:56,562 root         INFO     Train Epoch: 14 [2560/8000 (32%)]\tTotal Loss: 2.886945\n",
      "Reconstruction: 2.661677, Regularization: 0.142760, Discriminator: 0.004632; Generator: 0.077876,\n",
      "D(x): 0.949, D(G(z)): 0.088\n",
      "2019-04-09 23:57:56,672 root         INFO     Train Epoch: 14 [3072/8000 (38%)]\tTotal Loss: 3.390654\n",
      "Reconstruction: 3.160545, Regularization: 0.146601, Discriminator: 0.005519; Generator: 0.077989,\n",
      "D(x): 0.936, D(G(z)): 0.086\n",
      "2019-04-09 23:57:56,781 root         INFO     Train Epoch: 14 [3584/8000 (45%)]\tTotal Loss: 3.323571\n",
      "Reconstruction: 3.104542, Regularization: 0.139943, Discriminator: 0.004873; Generator: 0.074213,\n",
      "D(x): 0.962, D(G(z)): 0.096\n",
      "2019-04-09 23:57:56,892 root         INFO     Train Epoch: 14 [4096/8000 (51%)]\tTotal Loss: 3.172298\n",
      "Reconstruction: 2.948764, Regularization: 0.144913, Discriminator: 0.004156; Generator: 0.074466,\n",
      "D(x): 0.971, D(G(z)): 0.097\n",
      "2019-04-09 23:57:57,003 root         INFO     Train Epoch: 14 [4608/8000 (58%)]\tTotal Loss: 2.992516\n",
      "Reconstruction: 2.773535, Regularization: 0.135789, Discriminator: 0.004744; Generator: 0.078448,\n",
      "D(x): 0.943, D(G(z)): 0.085\n",
      "2019-04-09 23:57:57,113 root         INFO     Train Epoch: 14 [5120/8000 (64%)]\tTotal Loss: 2.786785\n",
      "Reconstruction: 2.569156, Regularization: 0.137219, Discriminator: 0.004281; Generator: 0.076130,\n",
      "D(x): 0.960, D(G(z)): 0.090\n",
      "2019-04-09 23:57:57,224 root         INFO     Train Epoch: 14 [5632/8000 (70%)]\tTotal Loss: 3.110117\n",
      "Reconstruction: 2.868860, Regularization: 0.158511, Discriminator: 0.005570; Generator: 0.077176,\n",
      "D(x): 0.933, D(G(z)): 0.088\n",
      "2019-04-09 23:57:57,334 root         INFO     Train Epoch: 14 [6144/8000 (77%)]\tTotal Loss: 3.517406\n",
      "Reconstruction: 3.277077, Regularization: 0.160921, Discriminator: 0.003796; Generator: 0.075612,\n",
      "D(x): 0.978, D(G(z)): 0.093\n",
      "2019-04-09 23:57:57,444 root         INFO     Train Epoch: 14 [6656/8000 (83%)]\tTotal Loss: 3.098778\n",
      "Reconstruction: 2.859922, Regularization: 0.155640, Discriminator: 0.006218; Generator: 0.076998,\n",
      "D(x): 0.937, D(G(z)): 0.089\n",
      "2019-04-09 23:57:57,555 root         INFO     Train Epoch: 14 [7168/8000 (90%)]\tTotal Loss: 3.227515\n",
      "Reconstruction: 2.988951, Regularization: 0.156360, Discriminator: 0.005228; Generator: 0.076976,\n",
      "D(x): 0.947, D(G(z)): 0.089\n",
      "2019-04-09 23:57:57,666 root         INFO     Train Epoch: 14 [7680/8000 (96%)]\tTotal Loss: 2.928011\n",
      "Reconstruction: 2.696330, Regularization: 0.150712, Discriminator: 0.004598; Generator: 0.076371,\n",
      "D(x): 0.953, D(G(z)): 0.090\n",
      "2019-04-09 23:57:57,747 root         INFO     ====> Epoch: 14 Average loss: 3.2129\n",
      "2019-04-09 23:57:57,775 root         INFO     Train Epoch: 15 [0/8000 (0%)]\tTotal Loss: 3.410897\n",
      "Reconstruction: 3.182223, Regularization: 0.149581, Discriminator: 0.004374; Generator: 0.074719,\n",
      "D(x): 0.967, D(G(z)): 0.096\n",
      "2019-04-09 23:57:57,886 root         INFO     Train Epoch: 15 [512/8000 (6%)]\tTotal Loss: 3.208578\n",
      "Reconstruction: 2.973142, Regularization: 0.153562, Discriminator: 0.005636; Generator: 0.076237,\n",
      "D(x): 0.940, D(G(z)): 0.091\n",
      "2019-04-09 23:57:57,997 root         INFO     Train Epoch: 15 [1024/8000 (13%)]\tTotal Loss: 3.192437\n",
      "Reconstruction: 2.945327, Regularization: 0.164622, Discriminator: 0.004601; Generator: 0.077887,\n",
      "D(x): 0.949, D(G(z)): 0.085\n",
      "2019-04-09 23:57:58,108 root         INFO     Train Epoch: 15 [1536/8000 (19%)]\tTotal Loss: 3.293513\n",
      "Reconstruction: 3.052861, Regularization: 0.157972, Discriminator: 0.004007; Generator: 0.078673,\n",
      "D(x): 0.963, D(G(z)): 0.083\n",
      "2019-04-09 23:57:58,218 root         INFO     Train Epoch: 15 [2048/8000 (26%)]\tTotal Loss: 3.124331\n",
      "Reconstruction: 2.885011, Regularization: 0.157145, Discriminator: 0.003794; Generator: 0.078381,\n",
      "D(x): 0.969, D(G(z)): 0.084\n",
      "2019-04-09 23:57:58,328 root         INFO     Train Epoch: 15 [2560/8000 (32%)]\tTotal Loss: 3.097527\n",
      "Reconstruction: 2.846988, Regularization: 0.165997, Discriminator: 0.006028; Generator: 0.078513,\n",
      "D(x): 0.932, D(G(z)): 0.086\n",
      "2019-04-09 23:57:58,438 root         INFO     Train Epoch: 15 [3072/8000 (38%)]\tTotal Loss: 3.196577\n",
      "Reconstruction: 2.961701, Regularization: 0.150543, Discriminator: 0.009477; Generator: 0.074856,\n",
      "D(x): 0.894, D(G(z)): 0.095\n",
      "2019-04-09 23:57:58,548 root         INFO     Train Epoch: 15 [3584/8000 (45%)]\tTotal Loss: 3.153176\n",
      "Reconstruction: 2.893569, Regularization: 0.177979, Discriminator: 0.004134; Generator: 0.077493,\n",
      "D(x): 0.961, D(G(z)): 0.085\n",
      "2019-04-09 23:57:58,658 root         INFO     Train Epoch: 15 [4096/8000 (51%)]\tTotal Loss: 3.466816\n",
      "Reconstruction: 3.227704, Regularization: 0.153250, Discriminator: 0.007344; Generator: 0.078518,\n",
      "D(x): 0.893, D(G(z)): 0.084\n",
      "2019-04-09 23:57:58,769 root         INFO     Train Epoch: 15 [4608/8000 (58%)]\tTotal Loss: 2.875218\n",
      "Reconstruction: 2.634131, Regularization: 0.154349, Discriminator: 0.007265; Generator: 0.079473,\n",
      "D(x): 0.927, D(G(z)): 0.081\n",
      "2019-04-09 23:57:58,879 root         INFO     Train Epoch: 15 [5120/8000 (64%)]\tTotal Loss: 3.927926\n",
      "Reconstruction: 3.653815, Regularization: 0.188900, Discriminator: 0.004772; Generator: 0.080439,\n",
      "D(x): 0.947, D(G(z)): 0.079\n",
      "2019-04-09 23:57:58,990 root         INFO     Train Epoch: 15 [5632/8000 (70%)]\tTotal Loss: 3.263106\n",
      "Reconstruction: 2.999807, Regularization: 0.178200, Discriminator: 0.004090; Generator: 0.081009,\n",
      "D(x): 0.958, D(G(z)): 0.078\n",
      "2019-04-09 23:57:59,100 root         INFO     Train Epoch: 15 [6144/8000 (77%)]\tTotal Loss: 3.340903\n",
      "Reconstruction: 3.089034, Regularization: 0.167171, Discriminator: 0.005864; Generator: 0.078833,\n",
      "D(x): 0.926, D(G(z)): 0.083\n",
      "2019-04-09 23:57:59,210 root         INFO     Train Epoch: 15 [6656/8000 (83%)]\tTotal Loss: 2.787169\n",
      "Reconstruction: 2.565405, Regularization: 0.138923, Discriminator: 0.004505; Generator: 0.078337,\n",
      "D(x): 0.949, D(G(z)): 0.084\n",
      "2019-04-09 23:57:59,321 root         INFO     Train Epoch: 15 [7168/8000 (90%)]\tTotal Loss: 3.131292\n",
      "Reconstruction: 2.880223, Regularization: 0.166298, Discriminator: 0.005050; Generator: 0.079721,\n",
      "D(x): 0.934, D(G(z)): 0.082\n",
      "2019-04-09 23:57:59,431 root         INFO     Train Epoch: 15 [7680/8000 (96%)]\tTotal Loss: 3.410470\n",
      "Reconstruction: 3.143965, Regularization: 0.183146, Discriminator: 0.005929; Generator: 0.077430,\n",
      "D(x): 0.938, D(G(z)): 0.087\n",
      "2019-04-09 23:57:59,512 root         INFO     ====> Epoch: 15 Average loss: 3.1389\n",
      "2019-04-09 23:57:59,539 root         INFO     Train Epoch: 16 [0/8000 (0%)]\tTotal Loss: 3.412353\n",
      "Reconstruction: 3.153059, Regularization: 0.173026, Discriminator: 0.005932; Generator: 0.080336,\n",
      "D(x): 0.919, D(G(z)): 0.079\n",
      "2019-04-09 23:57:59,650 root         INFO     Train Epoch: 16 [512/8000 (6%)]\tTotal Loss: 3.067302\n",
      "Reconstruction: 2.806280, Regularization: 0.173164, Discriminator: 0.008920; Generator: 0.078938,\n",
      "D(x): 0.886, D(G(z)): 0.084\n",
      "2019-04-09 23:57:59,760 root         INFO     Train Epoch: 16 [1024/8000 (13%)]\tTotal Loss: 3.284743\n",
      "Reconstruction: 3.025826, Regularization: 0.171630, Discriminator: 0.007667; Generator: 0.079620,\n",
      "D(x): 0.879, D(G(z)): 0.081\n",
      "2019-04-09 23:57:59,871 root         INFO     Train Epoch: 16 [1536/8000 (19%)]\tTotal Loss: 3.310433\n",
      "Reconstruction: 3.065035, Regularization: 0.162416, Discriminator: 0.005731; Generator: 0.077252,\n",
      "D(x): 0.930, D(G(z)): 0.088\n",
      "2019-04-09 23:57:59,982 root         INFO     Train Epoch: 16 [2048/8000 (26%)]\tTotal Loss: 2.941196\n",
      "Reconstruction: 2.695147, Regularization: 0.161808, Discriminator: 0.006847; Generator: 0.077395,\n",
      "D(x): 0.911, D(G(z)): 0.087\n",
      "2019-04-09 23:58:00,092 root         INFO     Train Epoch: 16 [2560/8000 (32%)]\tTotal Loss: 2.772647\n",
      "Reconstruction: 2.519389, Regularization: 0.170224, Discriminator: 0.005022; Generator: 0.078012,\n",
      "D(x): 0.938, D(G(z)): 0.085\n",
      "2019-04-09 23:58:00,202 root         INFO     Train Epoch: 16 [3072/8000 (38%)]\tTotal Loss: 2.845695\n",
      "Reconstruction: 2.587412, Regularization: 0.175050, Discriminator: 0.004888; Generator: 0.078345,\n",
      "D(x): 0.942, D(G(z)): 0.084\n",
      "2019-04-09 23:58:00,313 root         INFO     Train Epoch: 16 [3584/8000 (45%)]\tTotal Loss: 2.961197\n",
      "Reconstruction: 2.695775, Regularization: 0.181408, Discriminator: 0.004366; Generator: 0.079648,\n",
      "D(x): 0.953, D(G(z)): 0.081\n",
      "2019-04-09 23:58:00,423 root         INFO     Train Epoch: 16 [4096/8000 (51%)]\tTotal Loss: 2.901020\n",
      "Reconstruction: 2.638585, Regularization: 0.178746, Discriminator: 0.004561; Generator: 0.079128,\n",
      "D(x): 0.947, D(G(z)): 0.081\n",
      "2019-04-09 23:58:00,533 root         INFO     Train Epoch: 16 [4608/8000 (58%)]\tTotal Loss: 3.206621\n",
      "Reconstruction: 2.919723, Regularization: 0.200457, Discriminator: 0.004467; Generator: 0.081973,\n",
      "D(x): 0.945, D(G(z)): 0.075\n",
      "2019-04-09 23:58:00,644 root         INFO     Train Epoch: 16 [5120/8000 (64%)]\tTotal Loss: 3.471912\n",
      "Reconstruction: 3.190677, Regularization: 0.190681, Discriminator: 0.008988; Generator: 0.081567,\n",
      "D(x): 0.894, D(G(z)): 0.076\n",
      "2019-04-09 23:58:00,754 root         INFO     Train Epoch: 16 [5632/8000 (70%)]\tTotal Loss: 3.152682\n",
      "Reconstruction: 2.875123, Regularization: 0.190949, Discriminator: 0.005003; Generator: 0.081606,\n",
      "D(x): 0.939, D(G(z)): 0.077\n",
      "2019-04-09 23:58:00,864 root         INFO     Train Epoch: 16 [6144/8000 (77%)]\tTotal Loss: 2.631003\n",
      "Reconstruction: 2.376731, Regularization: 0.168493, Discriminator: 0.003969; Generator: 0.081810,\n",
      "D(x): 0.954, D(G(z)): 0.075\n",
      "2019-04-09 23:58:00,974 root         INFO     Train Epoch: 16 [6656/8000 (83%)]\tTotal Loss: 3.308974\n",
      "Reconstruction: 3.029581, Regularization: 0.192987, Discriminator: 0.006878; Generator: 0.079528,\n",
      "D(x): 0.925, D(G(z)): 0.081\n",
      "2019-04-09 23:58:01,083 root         INFO     Train Epoch: 16 [7168/8000 (90%)]\tTotal Loss: 3.268176\n",
      "Reconstruction: 2.981587, Regularization: 0.200942, Discriminator: 0.004557; Generator: 0.081090,\n",
      "D(x): 0.946, D(G(z)): 0.076\n",
      "2019-04-09 23:58:01,193 root         INFO     Train Epoch: 16 [7680/8000 (96%)]\tTotal Loss: 2.866579\n",
      "Reconstruction: 2.596219, Regularization: 0.184523, Discriminator: 0.005470; Generator: 0.080367,\n",
      "D(x): 0.924, D(G(z)): 0.079\n",
      "2019-04-09 23:58:01,273 root         INFO     ====> Epoch: 16 Average loss: 3.0296\n",
      "2019-04-09 23:58:01,300 root         INFO     Train Epoch: 17 [0/8000 (0%)]\tTotal Loss: 2.966086\n",
      "Reconstruction: 2.686434, Regularization: 0.193682, Discriminator: 0.006119; Generator: 0.079852,\n",
      "D(x): 0.918, D(G(z)): 0.080\n",
      "2019-04-09 23:58:01,408 root         INFO     Train Epoch: 17 [512/8000 (6%)]\tTotal Loss: 2.450706\n",
      "Reconstruction: 2.194851, Regularization: 0.168454, Discriminator: 0.006467; Generator: 0.080934,\n",
      "D(x): 0.905, D(G(z)): 0.077\n",
      "2019-04-09 23:58:01,516 root         INFO     Train Epoch: 17 [1024/8000 (13%)]\tTotal Loss: 2.642094\n",
      "Reconstruction: 2.385672, Regularization: 0.171666, Discriminator: 0.005342; Generator: 0.079414,\n",
      "D(x): 0.925, D(G(z)): 0.081\n",
      "2019-04-09 23:58:01,624 root         INFO     Train Epoch: 17 [1536/8000 (19%)]\tTotal Loss: 2.878553\n",
      "Reconstruction: 2.607451, Regularization: 0.185046, Discriminator: 0.004726; Generator: 0.081329,\n",
      "D(x): 0.938, D(G(z)): 0.076\n",
      "2019-04-09 23:58:01,732 root         INFO     Train Epoch: 17 [2048/8000 (26%)]\tTotal Loss: 3.085770\n",
      "Reconstruction: 2.805074, Regularization: 0.196808, Discriminator: 0.006600; Generator: 0.077288,\n",
      "D(x): 0.926, D(G(z)): 0.087\n",
      "2019-04-09 23:58:01,841 root         INFO     Train Epoch: 17 [2560/8000 (32%)]\tTotal Loss: 3.060577\n",
      "Reconstruction: 2.784275, Regularization: 0.191639, Discriminator: 0.005240; Generator: 0.079424,\n",
      "D(x): 0.934, D(G(z)): 0.081\n",
      "2019-04-09 23:58:01,950 root         INFO     Train Epoch: 17 [3072/8000 (38%)]\tTotal Loss: 2.741216\n",
      "Reconstruction: 2.489887, Regularization: 0.166211, Discriminator: 0.006339; Generator: 0.078779,\n",
      "D(x): 0.913, D(G(z)): 0.083\n",
      "2019-04-09 23:58:02,058 root         INFO     Train Epoch: 17 [3584/8000 (45%)]\tTotal Loss: 2.503623\n",
      "Reconstruction: 2.252474, Regularization: 0.163688, Discriminator: 0.007424; Generator: 0.080037,\n",
      "D(x): 0.905, D(G(z)): 0.079\n",
      "2019-04-09 23:58:02,167 root         INFO     Train Epoch: 17 [4096/8000 (51%)]\tTotal Loss: 4.124803\n",
      "Reconstruction: 3.847789, Regularization: 0.188308, Discriminator: 0.008398; Generator: 0.080308,\n",
      "D(x): 0.906, D(G(z)): 0.080\n",
      "2019-04-09 23:58:02,276 root         INFO     Train Epoch: 17 [4608/8000 (58%)]\tTotal Loss: 3.484942\n",
      "Reconstruction: 3.190513, Regularization: 0.212069, Discriminator: 0.004677; Generator: 0.077682,\n",
      "D(x): 0.954, D(G(z)): 0.085\n",
      "2019-04-09 23:58:02,384 root         INFO     Train Epoch: 17 [5120/8000 (64%)]\tTotal Loss: 2.444050\n",
      "Reconstruction: 2.188659, Regularization: 0.172363, Discriminator: 0.007218; Generator: 0.075809,\n",
      "D(x): 0.911, D(G(z)): 0.091\n",
      "2019-04-09 23:58:02,493 root         INFO     Train Epoch: 17 [5632/8000 (70%)]\tTotal Loss: 2.493238\n",
      "Reconstruction: 2.248362, Regularization: 0.160517, Discriminator: 0.005065; Generator: 0.079295,\n",
      "D(x): 0.931, D(G(z)): 0.081\n",
      "2019-04-09 23:58:02,601 root         INFO     Train Epoch: 17 [6144/8000 (77%)]\tTotal Loss: 2.560448\n",
      "Reconstruction: 2.285138, Regularization: 0.191542, Discriminator: 0.004464; Generator: 0.079304,\n",
      "D(x): 0.947, D(G(z)): 0.081\n",
      "2019-04-09 23:58:02,709 root         INFO     Train Epoch: 17 [6656/8000 (83%)]\tTotal Loss: 2.674919\n",
      "Reconstruction: 2.421043, Regularization: 0.167050, Discriminator: 0.009042; Generator: 0.077783,\n",
      "D(x): 0.872, D(G(z)): 0.086\n",
      "2019-04-09 23:58:02,818 root         INFO     Train Epoch: 17 [7168/8000 (90%)]\tTotal Loss: 3.144746\n",
      "Reconstruction: 2.864712, Regularization: 0.195186, Discriminator: 0.008677; Generator: 0.076172,\n",
      "D(x): 0.877, D(G(z)): 0.090\n",
      "2019-04-09 23:58:02,926 root         INFO     Train Epoch: 17 [7680/8000 (96%)]\tTotal Loss: 3.504958\n",
      "Reconstruction: 3.196126, Regularization: 0.224871, Discriminator: 0.005604; Generator: 0.078357,\n",
      "D(x): 0.929, D(G(z)): 0.083\n",
      "2019-04-09 23:58:03,006 root         INFO     ====> Epoch: 17 Average loss: 2.8818\n",
      "2019-04-09 23:58:03,032 root         INFO     Train Epoch: 18 [0/8000 (0%)]\tTotal Loss: 3.043009\n",
      "Reconstruction: 2.748252, Regularization: 0.208729, Discriminator: 0.007332; Generator: 0.078696,\n",
      "D(x): 0.920, D(G(z)): 0.082\n",
      "2019-04-09 23:58:03,141 root         INFO     Train Epoch: 18 [512/8000 (6%)]\tTotal Loss: 2.348608\n",
      "Reconstruction: 2.098954, Regularization: 0.165047, Discriminator: 0.005274; Generator: 0.079332,\n",
      "D(x): 0.925, D(G(z)): 0.080\n",
      "2019-04-09 23:58:03,249 root         INFO     Train Epoch: 18 [1024/8000 (13%)]\tTotal Loss: 2.508655\n",
      "Reconstruction: 2.242527, Regularization: 0.182388, Discriminator: 0.005473; Generator: 0.078267,\n",
      "D(x): 0.925, D(G(z)): 0.084\n",
      "2019-04-09 23:58:03,358 root         INFO     Train Epoch: 18 [1536/8000 (19%)]\tTotal Loss: 2.536900\n",
      "Reconstruction: 2.278894, Regularization: 0.172713, Discriminator: 0.007784; Generator: 0.077509,\n",
      "D(x): 0.888, D(G(z)): 0.085\n",
      "2019-04-09 23:58:03,466 root         INFO     Train Epoch: 18 [2048/8000 (26%)]\tTotal Loss: 2.449589\n",
      "Reconstruction: 2.184374, Regularization: 0.183637, Discriminator: 0.006020; Generator: 0.075558,\n",
      "D(x): 0.924, D(G(z)): 0.090\n",
      "2019-04-09 23:58:03,574 root         INFO     Train Epoch: 18 [2560/8000 (32%)]\tTotal Loss: 2.423056\n",
      "Reconstruction: 2.152909, Regularization: 0.184444, Discriminator: 0.010493; Generator: 0.075211,\n",
      "D(x): 0.849, D(G(z)): 0.091\n",
      "2019-04-09 23:58:03,683 root         INFO     Train Epoch: 18 [3072/8000 (38%)]\tTotal Loss: 2.998028\n",
      "Reconstruction: 2.704193, Regularization: 0.210585, Discriminator: 0.007083; Generator: 0.076167,\n",
      "D(x): 0.915, D(G(z)): 0.088\n",
      "2019-04-09 23:58:03,791 root         INFO     Train Epoch: 18 [3584/8000 (45%)]\tTotal Loss: 2.603893\n",
      "Reconstruction: 2.355998, Regularization: 0.167327, Discriminator: 0.007867; Generator: 0.072701,\n",
      "D(x): 0.898, D(G(z)): 0.099\n",
      "2019-04-09 23:58:03,899 root         INFO     Train Epoch: 18 [4096/8000 (51%)]\tTotal Loss: 2.488209\n",
      "Reconstruction: 2.215606, Regularization: 0.188518, Discriminator: 0.009158; Generator: 0.074928,\n",
      "D(x): 0.858, D(G(z)): 0.092\n",
      "2019-04-09 23:58:04,008 root         INFO     Train Epoch: 18 [4608/8000 (58%)]\tTotal Loss: 2.871849\n",
      "Reconstruction: 2.591951, Regularization: 0.199391, Discriminator: 0.006291; Generator: 0.074216,\n",
      "D(x): 0.927, D(G(z)): 0.094\n",
      "2019-04-09 23:58:04,116 root         INFO     Train Epoch: 18 [5120/8000 (64%)]\tTotal Loss: 2.644751\n",
      "Reconstruction: 2.379064, Regularization: 0.181241, Discriminator: 0.011449; Generator: 0.072997,\n",
      "D(x): 0.865, D(G(z)): 0.098\n",
      "2019-04-09 23:58:04,224 root         INFO     Train Epoch: 18 [5632/8000 (70%)]\tTotal Loss: 2.849802\n",
      "Reconstruction: 2.563869, Regularization: 0.204958, Discriminator: 0.006728; Generator: 0.074247,\n",
      "D(x): 0.910, D(G(z)): 0.094\n",
      "2019-04-09 23:58:04,333 root         INFO     Train Epoch: 18 [6144/8000 (77%)]\tTotal Loss: 2.863138\n",
      "Reconstruction: 2.591934, Regularization: 0.189394, Discriminator: 0.006405; Generator: 0.075405,\n",
      "D(x): 0.910, D(G(z)): 0.091\n",
      "2019-04-09 23:58:04,441 root         INFO     Train Epoch: 18 [6656/8000 (83%)]\tTotal Loss: 2.285443\n",
      "Reconstruction: 2.031893, Regularization: 0.174491, Discriminator: 0.006746; Generator: 0.072313,\n",
      "D(x): 0.911, D(G(z)): 0.100\n",
      "2019-04-09 23:58:04,549 root         INFO     Train Epoch: 18 [7168/8000 (90%)]\tTotal Loss: 2.036770\n",
      "Reconstruction: 1.783742, Regularization: 0.171463, Discriminator: 0.009473; Generator: 0.072092,\n",
      "D(x): 0.854, D(G(z)): 0.101\n",
      "2019-04-09 23:58:04,658 root         INFO     Train Epoch: 18 [7680/8000 (96%)]\tTotal Loss: 2.941970\n",
      "Reconstruction: 2.659004, Regularization: 0.204245, Discriminator: 0.007120; Generator: 0.071601,\n",
      "D(x): 0.913, D(G(z)): 0.102\n",
      "2019-04-09 23:58:04,737 root         INFO     ====> Epoch: 18 Average loss: 2.7967\n",
      "2019-04-09 23:58:04,763 root         INFO     Train Epoch: 19 [0/8000 (0%)]\tTotal Loss: 2.862666\n",
      "Reconstruction: 2.597261, Regularization: 0.188536, Discriminator: 0.006388; Generator: 0.070481,\n",
      "D(x): 0.921, D(G(z)): 0.106\n",
      "2019-04-09 23:58:04,874 root         INFO     Train Epoch: 19 [512/8000 (6%)]\tTotal Loss: 2.238018\n",
      "Reconstruction: 1.988760, Regularization: 0.169348, Discriminator: 0.008636; Generator: 0.071274,\n",
      "D(x): 0.869, D(G(z)): 0.103\n",
      "2019-04-09 23:58:04,982 root         INFO     Train Epoch: 19 [1024/8000 (13%)]\tTotal Loss: 3.253593\n",
      "Reconstruction: 2.960114, Regularization: 0.217538, Discriminator: 0.006488; Generator: 0.069452,\n",
      "D(x): 0.928, D(G(z)): 0.109\n",
      "2019-04-09 23:58:05,090 root         INFO     Train Epoch: 19 [1536/8000 (19%)]\tTotal Loss: 3.131636\n",
      "Reconstruction: 2.847646, Regularization: 0.207926, Discriminator: 0.007862; Generator: 0.068202,\n",
      "D(x): 0.922, D(G(z)): 0.114\n",
      "2019-04-09 23:58:05,197 root         INFO     Train Epoch: 19 [2048/8000 (26%)]\tTotal Loss: 2.808546\n",
      "Reconstruction: 2.535809, Regularization: 0.194245, Discriminator: 0.009918; Generator: 0.068573,\n",
      "D(x): 0.869, D(G(z)): 0.112\n",
      "2019-04-09 23:58:05,304 root         INFO     Train Epoch: 19 [2560/8000 (32%)]\tTotal Loss: 2.915580\n",
      "Reconstruction: 2.612348, Regularization: 0.225140, Discriminator: 0.008663; Generator: 0.069429,\n",
      "D(x): 0.881, D(G(z)): 0.109\n",
      "2019-04-09 23:58:05,414 root         INFO     Train Epoch: 19 [3072/8000 (38%)]\tTotal Loss: 2.909472\n",
      "Reconstruction: 2.627318, Regularization: 0.205820, Discriminator: 0.009192; Generator: 0.067141,\n",
      "D(x): 0.891, D(G(z)): 0.118\n",
      "2019-04-09 23:58:05,521 root         INFO     Train Epoch: 19 [3584/8000 (45%)]\tTotal Loss: 2.764638\n",
      "Reconstruction: 2.498362, Regularization: 0.192743, Discriminator: 0.007246; Generator: 0.066287,\n",
      "D(x): 0.915, D(G(z)): 0.120\n",
      "2019-04-09 23:58:05,629 root         INFO     Train Epoch: 19 [4096/8000 (51%)]\tTotal Loss: 2.945044\n",
      "Reconstruction: 2.659016, Regularization: 0.211049, Discriminator: 0.008559; Generator: 0.066419,\n",
      "D(x): 0.897, D(G(z)): 0.120\n",
      "2019-04-09 23:58:05,737 root         INFO     Train Epoch: 19 [4608/8000 (58%)]\tTotal Loss: 3.034719\n",
      "Reconstruction: 2.774302, Regularization: 0.185115, Discriminator: 0.010572; Generator: 0.064729,\n",
      "D(x): 0.902, D(G(z)): 0.127\n",
      "2019-04-09 23:58:05,845 root         INFO     Train Epoch: 19 [5120/8000 (64%)]\tTotal Loss: 2.607139\n",
      "Reconstruction: 2.359117, Regularization: 0.168578, Discriminator: 0.012011; Generator: 0.067433,\n",
      "D(x): 0.879, D(G(z)): 0.116\n",
      "2019-04-09 23:58:05,953 root         INFO     Train Epoch: 19 [5632/8000 (70%)]\tTotal Loss: 2.777084\n",
      "Reconstruction: 2.509565, Regularization: 0.194483, Discriminator: 0.008984; Generator: 0.064053,\n",
      "D(x): 0.890, D(G(z)): 0.130\n",
      "2019-04-09 23:58:06,062 root         INFO     Train Epoch: 19 [6144/8000 (77%)]\tTotal Loss: 2.547949\n",
      "Reconstruction: 2.284250, Regularization: 0.188889, Discriminator: 0.009464; Generator: 0.065346,\n",
      "D(x): 0.879, D(G(z)): 0.124\n",
      "2019-04-09 23:58:06,170 root         INFO     Train Epoch: 19 [6656/8000 (83%)]\tTotal Loss: 1.938427\n",
      "Reconstruction: 1.703252, Regularization: 0.159518, Discriminator: 0.011445; Generator: 0.064213,\n",
      "D(x): 0.861, D(G(z)): 0.129\n",
      "2019-04-09 23:58:06,278 root         INFO     Train Epoch: 19 [7168/8000 (90%)]\tTotal Loss: 2.534018\n",
      "Reconstruction: 2.267076, Regularization: 0.196069, Discriminator: 0.007559; Generator: 0.063313,\n",
      "D(x): 0.924, D(G(z)): 0.132\n",
      "2019-04-09 23:58:06,386 root         INFO     Train Epoch: 19 [7680/8000 (96%)]\tTotal Loss: 3.294018\n",
      "Reconstruction: 3.012461, Regularization: 0.210523, Discriminator: 0.008555; Generator: 0.062478,\n",
      "D(x): 0.912, D(G(z)): 0.136\n",
      "2019-04-09 23:58:06,465 root         INFO     ====> Epoch: 19 Average loss: 2.7491\n",
      "2019-04-09 23:58:06,492 root         INFO     Train Epoch: 20 [0/8000 (0%)]\tTotal Loss: 2.568822\n",
      "Reconstruction: 2.319946, Regularization: 0.177662, Discriminator: 0.007796; Generator: 0.063418,\n",
      "D(x): 0.908, D(G(z)): 0.132\n",
      "2019-04-09 23:58:06,600 root         INFO     Train Epoch: 20 [512/8000 (6%)]\tTotal Loss: 2.750042\n",
      "Reconstruction: 2.492906, Regularization: 0.189589, Discriminator: 0.007066; Generator: 0.060482,\n",
      "D(x): 0.937, D(G(z)): 0.145\n",
      "2019-04-09 23:58:06,708 root         INFO     Train Epoch: 20 [1024/8000 (13%)]\tTotal Loss: 2.532296\n",
      "Reconstruction: 2.273113, Regularization: 0.186614, Discriminator: 0.012249; Generator: 0.060320,\n",
      "D(x): 0.889, D(G(z)): 0.145\n",
      "2019-04-09 23:58:06,816 root         INFO     Train Epoch: 20 [1536/8000 (19%)]\tTotal Loss: 3.522307\n",
      "Reconstruction: 3.217725, Regularization: 0.235730, Discriminator: 0.009620; Generator: 0.059232,\n",
      "D(x): 0.912, D(G(z)): 0.151\n",
      "2019-04-09 23:58:06,924 root         INFO     Train Epoch: 20 [2048/8000 (26%)]\tTotal Loss: 2.661809\n",
      "Reconstruction: 2.404530, Regularization: 0.180346, Discriminator: 0.018919; Generator: 0.058014,\n",
      "D(x): 0.793, D(G(z)): 0.156\n",
      "2019-04-09 23:58:07,032 root         INFO     Train Epoch: 20 [2560/8000 (32%)]\tTotal Loss: 2.184802\n",
      "Reconstruction: 1.935120, Regularization: 0.175751, Discriminator: 0.015425; Generator: 0.058507,\n",
      "D(x): 0.778, D(G(z)): 0.154\n",
      "2019-04-09 23:58:07,140 root         INFO     Train Epoch: 20 [3072/8000 (38%)]\tTotal Loss: 2.402582\n",
      "Reconstruction: 2.152422, Regularization: 0.179561, Discriminator: 0.012808; Generator: 0.057792,\n",
      "D(x): 0.834, D(G(z)): 0.158\n",
      "2019-04-09 23:58:07,249 root         INFO     Train Epoch: 20 [3584/8000 (45%)]\tTotal Loss: 2.934350\n",
      "Reconstruction: 2.667198, Regularization: 0.197654, Discriminator: 0.013312; Generator: 0.056186,\n",
      "D(x): 0.830, D(G(z)): 0.166\n",
      "2019-04-09 23:58:07,357 root         INFO     Train Epoch: 20 [4096/8000 (51%)]\tTotal Loss: 4.372934\n",
      "Reconstruction: 4.110852, Regularization: 0.195514, Discriminator: 0.012344; Generator: 0.054224,\n",
      "D(x): 0.874, D(G(z)): 0.176\n",
      "2019-04-09 23:58:07,465 root         INFO     Train Epoch: 20 [4608/8000 (58%)]\tTotal Loss: 2.720968\n",
      "Reconstruction: 2.457802, Regularization: 0.190965, Discriminator: 0.018514; Generator: 0.053687,\n",
      "D(x): 0.802, D(G(z)): 0.180\n",
      "2019-04-09 23:58:07,573 root         INFO     Train Epoch: 20 [5120/8000 (64%)]\tTotal Loss: 3.582181\n",
      "Reconstruction: 3.307055, Regularization: 0.206010, Discriminator: 0.015560; Generator: 0.053556,\n",
      "D(x): 0.833, D(G(z)): 0.180\n",
      "2019-04-09 23:58:07,681 root         INFO     Train Epoch: 20 [5632/8000 (70%)]\tTotal Loss: 3.618161\n",
      "Reconstruction: 3.354521, Regularization: 0.193636, Discriminator: 0.017378; Generator: 0.052626,\n",
      "D(x): 0.807, D(G(z)): 0.186\n",
      "2019-04-09 23:58:07,789 root         INFO     Train Epoch: 20 [6144/8000 (77%)]\tTotal Loss: 2.777753\n",
      "Reconstruction: 2.523002, Regularization: 0.185826, Discriminator: 0.018179; Generator: 0.050746,\n",
      "D(x): 0.843, D(G(z)): 0.197\n",
      "2019-04-09 23:58:07,897 root         INFO     Train Epoch: 20 [6656/8000 (83%)]\tTotal Loss: 2.254029\n",
      "Reconstruction: 1.989114, Regularization: 0.197066, Discriminator: 0.016985; Generator: 0.050864,\n",
      "D(x): 0.778, D(G(z)): 0.196\n",
      "2019-04-09 23:58:08,005 root         INFO     Train Epoch: 20 [7168/8000 (90%)]\tTotal Loss: 2.191183\n",
      "Reconstruction: 1.936492, Regularization: 0.188060, Discriminator: 0.017632; Generator: 0.049000,\n",
      "D(x): 0.797, D(G(z)): 0.208\n",
      "2019-04-09 23:58:08,113 root         INFO     Train Epoch: 20 [7680/8000 (96%)]\tTotal Loss: 3.569408\n",
      "Reconstruction: 3.319082, Regularization: 0.186171, Discriminator: 0.015509; Generator: 0.048647,\n",
      "D(x): 0.851, D(G(z)): 0.211\n",
      "2019-04-09 23:58:08,193 root         INFO     ====> Epoch: 20 Average loss: 2.7759\n",
      "2019-04-09 23:58:08,220 root         INFO     Train Epoch: 21 [0/8000 (0%)]\tTotal Loss: 2.751873\n",
      "Reconstruction: 2.491648, Regularization: 0.193558, Discriminator: 0.017307; Generator: 0.049360,\n",
      "D(x): 0.797, D(G(z)): 0.206\n",
      "2019-04-09 23:58:08,328 root         INFO     Train Epoch: 21 [512/8000 (6%)]\tTotal Loss: 4.747453\n",
      "Reconstruction: 4.480381, Regularization: 0.193268, Discriminator: 0.024980; Generator: 0.048824,\n",
      "D(x): 0.725, D(G(z)): 0.210\n",
      "2019-04-09 23:58:08,437 root         INFO     Train Epoch: 21 [1024/8000 (13%)]\tTotal Loss: 2.839303\n",
      "Reconstruction: 2.557220, Regularization: 0.206521, Discriminator: 0.029054; Generator: 0.046507,\n",
      "D(x): 0.703, D(G(z)): 0.226\n",
      "2019-04-09 23:58:08,545 root         INFO     Train Epoch: 21 [1536/8000 (19%)]\tTotal Loss: 2.490640\n",
      "Reconstruction: 2.242665, Regularization: 0.172370, Discriminator: 0.030222; Generator: 0.045384,\n",
      "D(x): 0.680, D(G(z)): 0.234\n",
      "2019-04-09 23:58:08,654 root         INFO     Train Epoch: 21 [2048/8000 (26%)]\tTotal Loss: 2.726024\n",
      "Reconstruction: 2.455893, Regularization: 0.192209, Discriminator: 0.033612; Generator: 0.044309,\n",
      "D(x): 0.677, D(G(z)): 0.242\n",
      "2019-04-09 23:58:08,763 root         INFO     Train Epoch: 21 [2560/8000 (32%)]\tTotal Loss: 4.080369\n",
      "Reconstruction: 3.808140, Regularization: 0.211085, Discriminator: 0.019040; Generator: 0.042103,\n",
      "D(x): 0.811, D(G(z)): 0.260\n",
      "2019-04-09 23:58:08,872 root         INFO     Train Epoch: 21 [3072/8000 (38%)]\tTotal Loss: 2.592953\n",
      "Reconstruction: 2.336748, Regularization: 0.195193, Discriminator: 0.018747; Generator: 0.042266,\n",
      "D(x): 0.813, D(G(z)): 0.259\n",
      "2019-04-09 23:58:08,981 root         INFO     Train Epoch: 21 [3584/8000 (45%)]\tTotal Loss: 3.099509\n",
      "Reconstruction: 2.804975, Regularization: 0.221741, Discriminator: 0.030630; Generator: 0.042164,\n",
      "D(x): 0.725, D(G(z)): 0.260\n",
      "2019-04-09 23:58:09,089 root         INFO     Train Epoch: 21 [4096/8000 (51%)]\tTotal Loss: 2.695408\n",
      "Reconstruction: 2.414218, Regularization: 0.192808, Discriminator: 0.045740; Generator: 0.042642,\n",
      "D(x): 0.637, D(G(z)): 0.256\n",
      "2019-04-09 23:58:09,198 root         INFO     Train Epoch: 21 [4608/8000 (58%)]\tTotal Loss: 2.932286\n",
      "Reconstruction: 2.661949, Regularization: 0.199768, Discriminator: 0.029371; Generator: 0.041198,\n",
      "D(x): 0.739, D(G(z)): 0.268\n",
      "2019-04-09 23:58:09,307 root         INFO     Train Epoch: 21 [5120/8000 (64%)]\tTotal Loss: 7.544900\n",
      "Reconstruction: 7.304347, Regularization: 0.166226, Discriminator: 0.034755; Generator: 0.039572,\n",
      "D(x): 0.637, D(G(z)): 0.282\n",
      "2019-04-09 23:58:09,415 root         INFO     Train Epoch: 21 [5632/8000 (70%)]\tTotal Loss: 2.572344\n",
      "Reconstruction: 2.304618, Regularization: 0.201343, Discriminator: 0.027774; Generator: 0.038609,\n",
      "D(x): 0.699, D(G(z)): 0.291\n",
      "2019-04-09 23:58:09,524 root         INFO     Train Epoch: 21 [6144/8000 (77%)]\tTotal Loss: 2.512634\n",
      "Reconstruction: 2.235383, Regularization: 0.200435, Discriminator: 0.039025; Generator: 0.037791,\n",
      "D(x): 0.637, D(G(z)): 0.299\n",
      "2019-04-09 23:58:09,633 root         INFO     Train Epoch: 21 [6656/8000 (83%)]\tTotal Loss: 2.764509\n",
      "Reconstruction: 2.501376, Regularization: 0.189104, Discriminator: 0.037221; Generator: 0.036808,\n",
      "D(x): 0.651, D(G(z)): 0.309\n",
      "2019-04-09 23:58:09,741 root         INFO     Train Epoch: 21 [7168/8000 (90%)]\tTotal Loss: 3.579646\n",
      "Reconstruction: 3.334424, Regularization: 0.161635, Discriminator: 0.046339; Generator: 0.037248,\n",
      "D(x): 0.518, D(G(z)): 0.304\n",
      "2019-04-09 23:58:09,849 root         INFO     Train Epoch: 21 [7680/8000 (96%)]\tTotal Loss: 2.043197\n",
      "Reconstruction: 1.781172, Regularization: 0.186225, Discriminator: 0.041006; Generator: 0.034794,\n",
      "D(x): 0.609, D(G(z)): 0.329\n",
      "2019-04-09 23:58:09,929 root         INFO     ====> Epoch: 21 Average loss: 2.8449\n",
      "2019-04-09 23:58:09,955 root         INFO     Train Epoch: 22 [0/8000 (0%)]\tTotal Loss: 1.288774\n",
      "Reconstruction: 1.042735, Regularization: 0.170675, Discriminator: 0.041854; Generator: 0.033510,\n",
      "D(x): 0.588, D(G(z)): 0.343\n",
      "2019-04-09 23:58:10,064 root         INFO     Train Epoch: 22 [512/8000 (6%)]\tTotal Loss: 3.309451\n",
      "Reconstruction: 3.043729, Regularization: 0.180091, Discriminator: 0.051761; Generator: 0.033870,\n",
      "D(x): 0.528, D(G(z)): 0.339\n",
      "2019-04-09 23:58:10,172 root         INFO     Train Epoch: 22 [1024/8000 (13%)]\tTotal Loss: 2.328396\n",
      "Reconstruction: 2.033601, Regularization: 0.211834, Discriminator: 0.048369; Generator: 0.034592,\n",
      "D(x): 0.607, D(G(z)): 0.331\n",
      "2019-04-09 23:58:10,280 root         INFO     Train Epoch: 22 [1536/8000 (19%)]\tTotal Loss: 3.254202\n",
      "Reconstruction: 3.004137, Regularization: 0.174099, Discriminator: 0.041086; Generator: 0.034880,\n",
      "D(x): 0.630, D(G(z)): 0.329\n",
      "2019-04-09 23:58:10,389 root         INFO     Train Epoch: 22 [2048/8000 (26%)]\tTotal Loss: 3.249754\n",
      "Reconstruction: 2.988457, Regularization: 0.190385, Discriminator: 0.037569; Generator: 0.033342,\n",
      "D(x): 0.647, D(G(z)): 0.345\n",
      "2019-04-09 23:58:10,496 root         INFO     Train Epoch: 22 [2560/8000 (32%)]\tTotal Loss: 3.019929\n",
      "Reconstruction: 2.749893, Regularization: 0.190187, Discriminator: 0.048051; Generator: 0.031797,\n",
      "D(x): 0.588, D(G(z)): 0.362\n",
      "2019-04-09 23:58:10,604 root         INFO     Train Epoch: 22 [3072/8000 (38%)]\tTotal Loss: 1.393407\n",
      "Reconstruction: 1.158570, Regularization: 0.158864, Discriminator: 0.044515; Generator: 0.031457,\n",
      "D(x): 0.574, D(G(z)): 0.367\n",
      "2019-04-09 23:58:10,713 root         INFO     Train Epoch: 22 [3584/8000 (45%)]\tTotal Loss: 3.241042\n",
      "Reconstruction: 2.935029, Regularization: 0.218374, Discriminator: 0.056912; Generator: 0.030727,\n",
      "D(x): 0.519, D(G(z)): 0.375\n",
      "2019-04-09 23:58:10,820 root         INFO     Train Epoch: 22 [4096/8000 (51%)]\tTotal Loss: 2.082648\n",
      "Reconstruction: 1.819978, Regularization: 0.179429, Discriminator: 0.053672; Generator: 0.029569,\n",
      "D(x): 0.525, D(G(z)): 0.389\n",
      "2019-04-09 23:58:10,928 root         INFO     Train Epoch: 22 [4608/8000 (58%)]\tTotal Loss: 3.074462\n",
      "Reconstruction: 2.766487, Regularization: 0.236187, Discriminator: 0.043144; Generator: 0.028645,\n",
      "D(x): 0.644, D(G(z)): 0.401\n",
      "2019-04-09 23:58:11,036 root         INFO     Train Epoch: 22 [5120/8000 (64%)]\tTotal Loss: 2.692900\n",
      "Reconstruction: 2.396518, Regularization: 0.220275, Discriminator: 0.046816; Generator: 0.029292,\n",
      "D(x): 0.470, D(G(z)): 0.393\n",
      "2019-04-09 23:58:11,144 root         INFO     Train Epoch: 22 [5632/8000 (70%)]\tTotal Loss: 2.226129\n",
      "Reconstruction: 1.955843, Regularization: 0.176444, Discriminator: 0.065936; Generator: 0.027905,\n",
      "D(x): 0.429, D(G(z)): 0.410\n",
      "2019-04-09 23:58:11,252 root         INFO     Train Epoch: 22 [6144/8000 (77%)]\tTotal Loss: 2.160625\n",
      "Reconstruction: 1.852187, Regularization: 0.215494, Discriminator: 0.066821; Generator: 0.026123,\n",
      "D(x): 0.327, D(G(z)): 0.434\n",
      "2019-04-09 23:58:11,360 root         INFO     Train Epoch: 22 [6656/8000 (83%)]\tTotal Loss: 2.434978\n",
      "Reconstruction: 2.139991, Regularization: 0.216271, Discriminator: 0.052023; Generator: 0.026692,\n",
      "D(x): 0.565, D(G(z)): 0.427\n",
      "2019-04-09 23:58:11,469 root         INFO     Train Epoch: 22 [7168/8000 (90%)]\tTotal Loss: 3.313210\n",
      "Reconstruction: 3.017279, Regularization: 0.198105, Discriminator: 0.071563; Generator: 0.026263,\n",
      "D(x): 0.382, D(G(z)): 0.432\n",
      "2019-04-09 23:58:11,577 root         INFO     Train Epoch: 22 [7680/8000 (96%)]\tTotal Loss: 2.170855\n",
      "Reconstruction: 1.884388, Regularization: 0.196171, Discriminator: 0.064751; Generator: 0.025546,\n",
      "D(x): 0.440, D(G(z)): 0.442\n",
      "2019-04-09 23:58:11,656 root         INFO     ====> Epoch: 22 Average loss: 2.9094\n",
      "2019-04-09 23:58:11,683 root         INFO     Train Epoch: 23 [0/8000 (0%)]\tTotal Loss: 4.070137\n",
      "Reconstruction: 3.778576, Regularization: 0.201853, Discriminator: 0.063898; Generator: 0.025810,\n",
      "D(x): 0.490, D(G(z)): 0.439\n",
      "2019-04-09 23:58:11,792 root         INFO     Train Epoch: 23 [512/8000 (6%)]\tTotal Loss: 2.668577\n",
      "Reconstruction: 2.377886, Regularization: 0.210978, Discriminator: 0.052980; Generator: 0.026733,\n",
      "D(x): 0.529, D(G(z)): 0.426\n",
      "2019-04-09 23:58:11,899 root         INFO     Train Epoch: 23 [1024/8000 (13%)]\tTotal Loss: 2.817436\n",
      "Reconstruction: 2.516621, Regularization: 0.210275, Discriminator: 0.065182; Generator: 0.025358,\n",
      "D(x): 0.493, D(G(z)): 0.445\n",
      "2019-04-09 23:58:12,007 root         INFO     Train Epoch: 23 [1536/8000 (19%)]\tTotal Loss: 2.774183\n",
      "Reconstruction: 2.460996, Regularization: 0.233750, Discriminator: 0.055235; Generator: 0.024202,\n",
      "D(x): 0.527, D(G(z)): 0.462\n",
      "2019-04-09 23:58:12,114 root         INFO     Train Epoch: 23 [2048/8000 (26%)]\tTotal Loss: 3.010172\n",
      "Reconstruction: 2.713314, Regularization: 0.225464, Discriminator: 0.047283; Generator: 0.024110,\n",
      "D(x): 0.568, D(G(z)): 0.463\n",
      "2019-04-09 23:58:12,221 root         INFO     Train Epoch: 23 [2560/8000 (32%)]\tTotal Loss: 4.717438\n",
      "Reconstruction: 4.414197, Regularization: 0.203239, Discriminator: 0.076109; Generator: 0.023893,\n",
      "D(x): 0.336, D(G(z)): 0.466\n",
      "2019-04-09 23:58:12,329 root         INFO     Train Epoch: 23 [3072/8000 (38%)]\tTotal Loss: 2.186213\n",
      "Reconstruction: 1.866885, Regularization: 0.212607, Discriminator: 0.083165; Generator: 0.023556,\n",
      "D(x): 0.286, D(G(z)): 0.471\n",
      "2019-04-09 23:58:12,437 root         INFO     Train Epoch: 23 [3584/8000 (45%)]\tTotal Loss: 2.192492\n",
      "Reconstruction: 1.883195, Regularization: 0.211705, Discriminator: 0.074548; Generator: 0.023045,\n",
      "D(x): 0.405, D(G(z)): 0.479\n",
      "2019-04-09 23:58:12,544 root         INFO     Train Epoch: 23 [4096/8000 (51%)]\tTotal Loss: 2.308214\n",
      "Reconstruction: 2.014105, Regularization: 0.199158, Discriminator: 0.072217; Generator: 0.022735,\n",
      "D(x): 0.360, D(G(z)): 0.484\n",
      "2019-04-09 23:58:12,652 root         INFO     Train Epoch: 23 [4608/8000 (58%)]\tTotal Loss: 2.534428\n",
      "Reconstruction: 2.233120, Regularization: 0.207953, Discriminator: 0.070414; Generator: 0.022941,\n",
      "D(x): 0.357, D(G(z)): 0.480\n",
      "2019-04-09 23:58:12,760 root         INFO     Train Epoch: 23 [5120/8000 (64%)]\tTotal Loss: 2.782727\n",
      "Reconstruction: 2.477952, Regularization: 0.198540, Discriminator: 0.083376; Generator: 0.022859,\n",
      "D(x): 0.341, D(G(z)): 0.482\n",
      "2019-04-09 23:58:12,867 root         INFO     Train Epoch: 23 [5632/8000 (70%)]\tTotal Loss: 3.639742\n",
      "Reconstruction: 3.344027, Regularization: 0.205010, Discriminator: 0.068619; Generator: 0.022086,\n",
      "D(x): 0.437, D(G(z)): 0.494\n",
      "2019-04-09 23:58:12,975 root         INFO     Train Epoch: 23 [6144/8000 (77%)]\tTotal Loss: 3.005554\n",
      "Reconstruction: 2.686633, Regularization: 0.231639, Discriminator: 0.065913; Generator: 0.021369,\n",
      "D(x): 0.391, D(G(z)): 0.505\n",
      "2019-04-09 23:58:13,083 root         INFO     Train Epoch: 23 [6656/8000 (83%)]\tTotal Loss: 2.455627\n",
      "Reconstruction: 2.126169, Regularization: 0.230868, Discriminator: 0.076749; Generator: 0.021841,\n",
      "D(x): 0.315, D(G(z)): 0.497\n",
      "2019-04-09 23:58:13,190 root         INFO     Train Epoch: 23 [7168/8000 (90%)]\tTotal Loss: 4.612752\n",
      "Reconstruction: 4.292957, Regularization: 0.222092, Discriminator: 0.076176; Generator: 0.021528,\n",
      "D(x): 0.339, D(G(z)): 0.502\n",
      "2019-04-09 23:58:13,297 root         INFO     Train Epoch: 23 [7680/8000 (96%)]\tTotal Loss: 2.908357\n",
      "Reconstruction: 2.591522, Regularization: 0.235180, Discriminator: 0.060444; Generator: 0.021211,\n",
      "D(x): 0.403, D(G(z)): 0.507\n",
      "2019-04-09 23:58:13,376 root         INFO     ====> Epoch: 23 Average loss: 2.9191\n",
      "2019-04-09 23:58:13,403 root         INFO     Train Epoch: 24 [0/8000 (0%)]\tTotal Loss: 4.038965\n",
      "Reconstruction: 3.742576, Regularization: 0.207393, Discriminator: 0.068469; Generator: 0.020526,\n",
      "D(x): 0.339, D(G(z)): 0.519\n",
      "2019-04-09 23:58:13,512 root         INFO     Train Epoch: 24 [512/8000 (6%)]\tTotal Loss: 3.335732\n",
      "Reconstruction: 3.026341, Regularization: 0.227456, Discriminator: 0.061218; Generator: 0.020717,\n",
      "D(x): 0.469, D(G(z)): 0.516\n",
      "2019-04-09 23:58:13,621 root         INFO     Train Epoch: 24 [1024/8000 (13%)]\tTotal Loss: 3.837348\n",
      "Reconstruction: 3.503642, Regularization: 0.242743, Discriminator: 0.070374; Generator: 0.020590,\n",
      "D(x): 0.319, D(G(z)): 0.518\n",
      "2019-04-09 23:58:13,731 root         INFO     Train Epoch: 24 [1536/8000 (19%)]\tTotal Loss: 2.296941\n",
      "Reconstruction: 1.939392, Regularization: 0.247682, Discriminator: 0.089912; Generator: 0.019956,\n",
      "D(x): 0.221, D(G(z)): 0.528\n",
      "2019-04-09 23:58:13,842 root         INFO     Train Epoch: 24 [2048/8000 (26%)]\tTotal Loss: 4.384710\n",
      "Reconstruction: 4.082958, Regularization: 0.214012, Discriminator: 0.067977; Generator: 0.019763,\n",
      "D(x): 0.354, D(G(z)): 0.531\n",
      "2019-04-09 23:58:13,948 root         INFO     Train Epoch: 24 [2560/8000 (32%)]\tTotal Loss: 2.410951\n",
      "Reconstruction: 2.096868, Regularization: 0.223728, Discriminator: 0.070548; Generator: 0.019807,\n",
      "D(x): 0.377, D(G(z)): 0.531\n",
      "2019-04-09 23:58:14,053 root         INFO     Train Epoch: 24 [3072/8000 (38%)]\tTotal Loss: 3.392028\n",
      "Reconstruction: 3.072176, Regularization: 0.230889, Discriminator: 0.069246; Generator: 0.019718,\n",
      "D(x): 0.360, D(G(z)): 0.532\n",
      "2019-04-09 23:58:14,159 root         INFO     Train Epoch: 24 [3584/8000 (45%)]\tTotal Loss: 3.249089\n",
      "Reconstruction: 2.950565, Regularization: 0.210009, Discriminator: 0.068795; Generator: 0.019720,\n",
      "D(x): 0.371, D(G(z)): 0.532\n",
      "2019-04-09 23:58:14,264 root         INFO     Train Epoch: 24 [4096/8000 (51%)]\tTotal Loss: 2.320510\n",
      "Reconstruction: 1.992301, Regularization: 0.232401, Discriminator: 0.076425; Generator: 0.019384,\n",
      "D(x): 0.318, D(G(z)): 0.538\n",
      "2019-04-09 23:58:14,369 root         INFO     Train Epoch: 24 [4608/8000 (58%)]\tTotal Loss: 2.612051\n",
      "Reconstruction: 2.291020, Regularization: 0.233550, Discriminator: 0.067878; Generator: 0.019603,\n",
      "D(x): 0.351, D(G(z)): 0.534\n",
      "2019-04-09 23:58:14,477 root         INFO     Train Epoch: 24 [5120/8000 (64%)]\tTotal Loss: 3.573670\n",
      "Reconstruction: 3.265381, Regularization: 0.221260, Discriminator: 0.067620; Generator: 0.019409,\n",
      "D(x): 0.351, D(G(z)): 0.537\n",
      "2019-04-09 23:58:14,587 root         INFO     Train Epoch: 24 [5632/8000 (70%)]\tTotal Loss: 2.191929\n",
      "Reconstruction: 1.871388, Regularization: 0.222159, Discriminator: 0.078886; Generator: 0.019495,\n",
      "D(x): 0.317, D(G(z)): 0.536\n",
      "2019-04-09 23:58:14,697 root         INFO     Train Epoch: 24 [6144/8000 (77%)]\tTotal Loss: 2.759403\n",
      "Reconstruction: 2.480562, Regularization: 0.188004, Discriminator: 0.071533; Generator: 0.019304,\n",
      "D(x): 0.325, D(G(z)): 0.539\n",
      "2019-04-09 23:58:14,805 root         INFO     Train Epoch: 24 [6656/8000 (83%)]\tTotal Loss: 3.293581\n",
      "Reconstruction: 2.953343, Regularization: 0.252857, Discriminator: 0.068176; Generator: 0.019204,\n",
      "D(x): 0.344, D(G(z)): 0.541\n",
      "2019-04-09 23:58:14,913 root         INFO     Train Epoch: 24 [7168/8000 (90%)]\tTotal Loss: 2.384545\n",
      "Reconstruction: 2.061805, Regularization: 0.240874, Discriminator: 0.062723; Generator: 0.019142,\n",
      "D(x): 0.379, D(G(z)): 0.542\n",
      "2019-04-09 23:58:15,021 root         INFO     Train Epoch: 24 [7680/8000 (96%)]\tTotal Loss: 2.613786\n",
      "Reconstruction: 2.273767, Regularization: 0.253092, Discriminator: 0.067868; Generator: 0.019059,\n",
      "D(x): 0.339, D(G(z)): 0.543\n",
      "2019-04-09 23:58:15,101 root         INFO     ====> Epoch: 24 Average loss: 2.9309\n",
      "2019-04-09 23:58:15,128 root         INFO     Train Epoch: 25 [0/8000 (0%)]\tTotal Loss: 3.090189\n",
      "Reconstruction: 2.783679, Regularization: 0.211269, Discriminator: 0.076012; Generator: 0.019229,\n",
      "D(x): 0.258, D(G(z)): 0.540\n",
      "2019-04-09 23:58:15,238 root         INFO     Train Epoch: 25 [512/8000 (6%)]\tTotal Loss: 3.527741\n",
      "Reconstruction: 3.213513, Regularization: 0.232577, Discriminator: 0.062614; Generator: 0.019038,\n",
      "D(x): 0.426, D(G(z)): 0.544\n",
      "2019-04-09 23:58:15,349 root         INFO     Train Epoch: 25 [1024/8000 (13%)]\tTotal Loss: 3.261812\n",
      "Reconstruction: 2.892195, Regularization: 0.275466, Discriminator: 0.075364; Generator: 0.018787,\n",
      "D(x): 0.317, D(G(z)): 0.548\n",
      "2019-04-09 23:58:15,460 root         INFO     Train Epoch: 25 [1536/8000 (19%)]\tTotal Loss: 3.193587\n",
      "Reconstruction: 2.823125, Regularization: 0.263038, Discriminator: 0.088771; Generator: 0.018653,\n",
      "D(x): 0.194, D(G(z)): 0.551\n",
      "2019-04-09 23:58:15,571 root         INFO     Train Epoch: 25 [2048/8000 (26%)]\tTotal Loss: 2.946018\n",
      "Reconstruction: 2.632034, Regularization: 0.224651, Discriminator: 0.070915; Generator: 0.018418,\n",
      "D(x): 0.358, D(G(z)): 0.555\n",
      "2019-04-09 23:58:15,682 root         INFO     Train Epoch: 25 [2560/8000 (32%)]\tTotal Loss: 2.658642\n",
      "Reconstruction: 2.302248, Regularization: 0.256927, Discriminator: 0.080962; Generator: 0.018504,\n",
      "D(x): 0.265, D(G(z)): 0.553\n",
      "2019-04-09 23:58:15,792 root         INFO     Train Epoch: 25 [3072/8000 (38%)]\tTotal Loss: 4.053611\n",
      "Reconstruction: 3.688541, Regularization: 0.264395, Discriminator: 0.082253; Generator: 0.018423,\n",
      "D(x): 0.253, D(G(z)): 0.555\n",
      "2019-04-09 23:58:15,903 root         INFO     Train Epoch: 25 [3584/8000 (45%)]\tTotal Loss: 3.265106\n",
      "Reconstruction: 2.944505, Regularization: 0.227527, Discriminator: 0.074561; Generator: 0.018513,\n",
      "D(x): 0.280, D(G(z)): 0.553\n",
      "2019-04-09 23:58:16,014 root         INFO     Train Epoch: 25 [4096/8000 (51%)]\tTotal Loss: 2.261831\n",
      "Reconstruction: 1.945338, Regularization: 0.226487, Discriminator: 0.071500; Generator: 0.018505,\n",
      "D(x): 0.285, D(G(z)): 0.553\n",
      "2019-04-09 23:58:16,125 root         INFO     Train Epoch: 25 [4608/8000 (58%)]\tTotal Loss: 3.241102\n",
      "Reconstruction: 2.882993, Regularization: 0.263129, Discriminator: 0.076514; Generator: 0.018467,\n",
      "D(x): 0.293, D(G(z)): 0.554\n",
      "2019-04-09 23:58:16,236 root         INFO     Train Epoch: 25 [5120/8000 (64%)]\tTotal Loss: 2.584785\n",
      "Reconstruction: 2.287943, Regularization: 0.211312, Discriminator: 0.067080; Generator: 0.018450,\n",
      "D(x): 0.313, D(G(z)): 0.554\n",
      "2019-04-09 23:58:16,347 root         INFO     Train Epoch: 25 [5632/8000 (70%)]\tTotal Loss: 4.191050\n",
      "Reconstruction: 3.833623, Regularization: 0.267922, Discriminator: 0.070975; Generator: 0.018530,\n",
      "D(x): 0.278, D(G(z)): 0.553\n",
      "2019-04-09 23:58:16,457 root         INFO     Train Epoch: 25 [6144/8000 (77%)]\tTotal Loss: 3.598651\n",
      "Reconstruction: 3.269373, Regularization: 0.232608, Discriminator: 0.078185; Generator: 0.018484,\n",
      "D(x): 0.269, D(G(z)): 0.554\n",
      "2019-04-09 23:58:16,568 root         INFO     Train Epoch: 25 [6656/8000 (83%)]\tTotal Loss: 2.161938\n",
      "Reconstruction: 1.858165, Regularization: 0.215440, Discriminator: 0.069917; Generator: 0.018416,\n",
      "D(x): 0.295, D(G(z)): 0.555\n",
      "2019-04-09 23:58:16,679 root         INFO     Train Epoch: 25 [7168/8000 (90%)]\tTotal Loss: 2.277367\n",
      "Reconstruction: 1.953725, Regularization: 0.228793, Discriminator: 0.076479; Generator: 0.018371,\n",
      "D(x): 0.284, D(G(z)): 0.556\n",
      "2019-04-09 23:58:16,791 root         INFO     Train Epoch: 25 [7680/8000 (96%)]\tTotal Loss: 2.630736\n",
      "Reconstruction: 2.301803, Regularization: 0.240570, Discriminator: 0.070046; Generator: 0.018318,\n",
      "D(x): 0.312, D(G(z)): 0.556\n",
      "2019-04-09 23:58:16,872 root         INFO     ====> Epoch: 25 Average loss: 2.9746\n",
      "2019-04-09 23:58:16,898 root         INFO     Train Epoch: 26 [0/8000 (0%)]\tTotal Loss: 2.412807\n",
      "Reconstruction: 2.103484, Regularization: 0.221037, Discriminator: 0.069998; Generator: 0.018287,\n",
      "D(x): 0.301, D(G(z)): 0.557\n",
      "2019-04-09 23:58:17,010 root         INFO     Train Epoch: 26 [512/8000 (6%)]\tTotal Loss: 3.333387\n",
      "Reconstruction: 2.990750, Regularization: 0.252181, Discriminator: 0.072201; Generator: 0.018256,\n",
      "D(x): 0.290, D(G(z)): 0.558\n",
      "2019-04-09 23:58:17,119 root         INFO     Train Epoch: 26 [1024/8000 (13%)]\tTotal Loss: 2.510571\n",
      "Reconstruction: 2.166680, Regularization: 0.257048, Discriminator: 0.068522; Generator: 0.018323,\n",
      "D(x): 0.303, D(G(z)): 0.556\n",
      "2019-04-09 23:58:17,227 root         INFO     Train Epoch: 26 [1536/8000 (19%)]\tTotal Loss: 5.152012\n",
      "Reconstruction: 4.821375, Regularization: 0.243733, Discriminator: 0.068647; Generator: 0.018259,\n",
      "D(x): 0.317, D(G(z)): 0.558\n",
      "2019-04-09 23:58:17,336 root         INFO     Train Epoch: 26 [2048/8000 (26%)]\tTotal Loss: 3.492981\n",
      "Reconstruction: 3.159351, Regularization: 0.240497, Discriminator: 0.075005; Generator: 0.018127,\n",
      "D(x): 0.279, D(G(z)): 0.560\n",
      "2019-04-09 23:58:17,446 root         INFO     Train Epoch: 26 [2560/8000 (32%)]\tTotal Loss: 2.478883\n",
      "Reconstruction: 2.132897, Regularization: 0.257739, Discriminator: 0.070258; Generator: 0.017988,\n",
      "D(x): 0.279, D(G(z)): 0.562\n",
      "2019-04-09 23:58:17,555 root         INFO     Train Epoch: 26 [3072/8000 (38%)]\tTotal Loss: 5.165498\n",
      "Reconstruction: 4.871408, Regularization: 0.213492, Discriminator: 0.062606; Generator: 0.017992,\n",
      "D(x): 0.347, D(G(z)): 0.562\n",
      "2019-04-09 23:58:17,664 root         INFO     Train Epoch: 26 [3584/8000 (45%)]\tTotal Loss: 2.012860\n",
      "Reconstruction: 1.676328, Regularization: 0.244230, Discriminator: 0.074269; Generator: 0.018034,\n",
      "D(x): 0.269, D(G(z)): 0.562\n",
      "2019-04-09 23:58:17,773 root         INFO     Train Epoch: 26 [4096/8000 (51%)]\tTotal Loss: 3.040088\n",
      "Reconstruction: 2.688791, Regularization: 0.257260, Discriminator: 0.075916; Generator: 0.018121,\n",
      "D(x): 0.258, D(G(z)): 0.560\n",
      "2019-04-09 23:58:17,881 root         INFO     Train Epoch: 26 [4608/8000 (58%)]\tTotal Loss: 1.883808\n",
      "Reconstruction: 1.574714, Regularization: 0.216051, Discriminator: 0.074881; Generator: 0.018161,\n",
      "D(x): 0.276, D(G(z)): 0.559\n",
      "2019-04-09 23:58:17,991 root         INFO     Train Epoch: 26 [5120/8000 (64%)]\tTotal Loss: 2.769821\n",
      "Reconstruction: 2.434900, Regularization: 0.248283, Discriminator: 0.068529; Generator: 0.018109,\n",
      "D(x): 0.321, D(G(z)): 0.560\n",
      "2019-04-09 23:58:18,100 root         INFO     Train Epoch: 26 [5632/8000 (70%)]\tTotal Loss: 2.021786\n",
      "Reconstruction: 1.679290, Regularization: 0.250835, Discriminator: 0.073439; Generator: 0.018222,\n",
      "D(x): 0.296, D(G(z)): 0.558\n",
      "2019-04-09 23:58:18,210 root         INFO     Train Epoch: 26 [6144/8000 (77%)]\tTotal Loss: 2.926050\n",
      "Reconstruction: 2.572187, Regularization: 0.265051, Discriminator: 0.070710; Generator: 0.018102,\n",
      "D(x): 0.295, D(G(z)): 0.560\n",
      "2019-04-09 23:58:18,319 root         INFO     Train Epoch: 26 [6656/8000 (83%)]\tTotal Loss: 2.908658\n",
      "Reconstruction: 2.564746, Regularization: 0.249246, Discriminator: 0.076260; Generator: 0.018405,\n",
      "D(x): 0.258, D(G(z)): 0.555\n",
      "2019-04-09 23:58:18,427 root         INFO     Train Epoch: 26 [7168/8000 (90%)]\tTotal Loss: 2.617888\n",
      "Reconstruction: 2.272988, Regularization: 0.253707, Discriminator: 0.072949; Generator: 0.018244,\n",
      "D(x): 0.267, D(G(z)): 0.558\n",
      "2019-04-09 23:58:18,536 root         INFO     Train Epoch: 26 [7680/8000 (96%)]\tTotal Loss: 2.527470\n",
      "Reconstruction: 2.180218, Regularization: 0.259437, Discriminator: 0.069426; Generator: 0.018388,\n",
      "D(x): 0.288, D(G(z)): 0.555\n",
      "2019-04-09 23:58:18,616 root         INFO     ====> Epoch: 26 Average loss: 3.0255\n",
      "2019-04-09 23:58:18,643 root         INFO     Train Epoch: 27 [0/8000 (0%)]\tTotal Loss: 2.635856\n",
      "Reconstruction: 2.275035, Regularization: 0.269112, Discriminator: 0.073324; Generator: 0.018385,\n",
      "D(x): 0.275, D(G(z)): 0.555\n",
      "2019-04-09 23:58:18,755 root         INFO     Train Epoch: 27 [512/8000 (6%)]\tTotal Loss: 3.071921\n",
      "Reconstruction: 2.753207, Regularization: 0.233795, Discriminator: 0.066802; Generator: 0.018117,\n",
      "D(x): 0.318, D(G(z)): 0.560\n",
      "2019-04-09 23:58:18,867 root         INFO     Train Epoch: 27 [1024/8000 (13%)]\tTotal Loss: 2.415771\n",
      "Reconstruction: 2.064531, Regularization: 0.257783, Discriminator: 0.075101; Generator: 0.018356,\n",
      "D(x): 0.249, D(G(z)): 0.556\n",
      "2019-04-09 23:58:18,978 root         INFO     Train Epoch: 27 [1536/8000 (19%)]\tTotal Loss: 5.498510\n",
      "Reconstruction: 5.140890, Regularization: 0.267615, Discriminator: 0.071869; Generator: 0.018136,\n",
      "D(x): 0.280, D(G(z)): 0.560\n",
      "2019-04-09 23:58:19,088 root         INFO     Train Epoch: 27 [2048/8000 (26%)]\tTotal Loss: 4.264401\n",
      "Reconstruction: 3.892520, Regularization: 0.285180, Discriminator: 0.068403; Generator: 0.018297,\n",
      "D(x): 0.290, D(G(z)): 0.557\n",
      "2019-04-09 23:58:19,199 root         INFO     Train Epoch: 27 [2560/8000 (32%)]\tTotal Loss: 2.331124\n",
      "Reconstruction: 2.000445, Regularization: 0.245663, Discriminator: 0.066575; Generator: 0.018440,\n",
      "D(x): 0.313, D(G(z)): 0.554\n",
      "2019-04-09 23:58:19,310 root         INFO     Train Epoch: 27 [3072/8000 (38%)]\tTotal Loss: 2.661386\n",
      "Reconstruction: 2.286265, Regularization: 0.285183, Discriminator: 0.071391; Generator: 0.018547,\n",
      "D(x): 0.275, D(G(z)): 0.553\n",
      "2019-04-09 23:58:19,421 root         INFO     Train Epoch: 27 [3584/8000 (45%)]\tTotal Loss: 2.805664\n",
      "Reconstruction: 2.467359, Regularization: 0.252113, Discriminator: 0.067688; Generator: 0.018505,\n",
      "D(x): 0.289, D(G(z)): 0.553\n",
      "2019-04-09 23:58:19,532 root         INFO     Train Epoch: 27 [4096/8000 (51%)]\tTotal Loss: 3.423608\n",
      "Reconstruction: 3.072603, Regularization: 0.264339, Discriminator: 0.068078; Generator: 0.018588,\n",
      "D(x): 0.282, D(G(z)): 0.552\n",
      "2019-04-09 23:58:19,643 root         INFO     Train Epoch: 27 [4608/8000 (58%)]\tTotal Loss: 3.119854\n",
      "Reconstruction: 2.733381, Regularization: 0.298055, Discriminator: 0.069855; Generator: 0.018562,\n",
      "D(x): 0.276, D(G(z)): 0.552\n",
      "2019-04-09 23:58:19,754 root         INFO     Train Epoch: 27 [5120/8000 (64%)]\tTotal Loss: 2.850448\n",
      "Reconstruction: 2.472984, Regularization: 0.290066, Discriminator: 0.069169; Generator: 0.018228,\n",
      "D(x): 0.292, D(G(z)): 0.558\n",
      "2019-04-09 23:58:19,865 root         INFO     Train Epoch: 27 [5632/8000 (70%)]\tTotal Loss: 1.877791\n",
      "Reconstruction: 1.552853, Regularization: 0.238792, Discriminator: 0.067347; Generator: 0.018798,\n",
      "D(x): 0.285, D(G(z)): 0.548\n",
      "2019-04-09 23:58:19,976 root         INFO     Train Epoch: 27 [6144/8000 (77%)]\tTotal Loss: 2.402604\n",
      "Reconstruction: 2.043771, Regularization: 0.275478, Discriminator: 0.064693; Generator: 0.018662,\n",
      "D(x): 0.312, D(G(z)): 0.551\n",
      "2019-04-09 23:58:20,087 root         INFO     Train Epoch: 27 [6656/8000 (83%)]\tTotal Loss: 2.418845\n",
      "Reconstruction: 2.064371, Regularization: 0.271182, Discriminator: 0.064260; Generator: 0.019031,\n",
      "D(x): 0.311, D(G(z)): 0.544\n",
      "2019-04-09 23:58:20,197 root         INFO     Train Epoch: 27 [7168/8000 (90%)]\tTotal Loss: 2.917779\n",
      "Reconstruction: 2.559376, Regularization: 0.272625, Discriminator: 0.066947; Generator: 0.018830,\n",
      "D(x): 0.291, D(G(z)): 0.548\n",
      "2019-04-09 23:58:20,308 root         INFO     Train Epoch: 27 [7680/8000 (96%)]\tTotal Loss: 3.731297\n",
      "Reconstruction: 3.387228, Regularization: 0.256968, Discriminator: 0.068208; Generator: 0.018892,\n",
      "D(x): 0.288, D(G(z)): 0.547\n",
      "2019-04-09 23:58:20,388 root         INFO     ====> Epoch: 27 Average loss: 3.0347\n",
      "2019-04-09 23:58:20,415 root         INFO     Train Epoch: 28 [0/8000 (0%)]\tTotal Loss: 2.922836\n",
      "Reconstruction: 2.551243, Regularization: 0.289206, Discriminator: 0.063647; Generator: 0.018740,\n",
      "D(x): 0.319, D(G(z)): 0.549\n",
      "2019-04-09 23:58:20,525 root         INFO     Train Epoch: 28 [512/8000 (6%)]\tTotal Loss: 2.585304\n",
      "Reconstruction: 2.268278, Regularization: 0.234815, Discriminator: 0.063266; Generator: 0.018945,\n",
      "D(x): 0.330, D(G(z)): 0.546\n",
      "2019-04-09 23:58:20,635 root         INFO     Train Epoch: 28 [1024/8000 (13%)]\tTotal Loss: 2.395170\n",
      "Reconstruction: 2.037919, Regularization: 0.272551, Discriminator: 0.065853; Generator: 0.018847,\n",
      "D(x): 0.295, D(G(z)): 0.547\n",
      "2019-04-09 23:58:20,745 root         INFO     Train Epoch: 28 [1536/8000 (19%)]\tTotal Loss: 2.680921\n",
      "Reconstruction: 2.341240, Regularization: 0.254960, Discriminator: 0.065495; Generator: 0.019225,\n",
      "D(x): 0.305, D(G(z)): 0.541\n",
      "2019-04-09 23:58:20,856 root         INFO     Train Epoch: 28 [2048/8000 (26%)]\tTotal Loss: 2.585100\n",
      "Reconstruction: 2.213396, Regularization: 0.284162, Discriminator: 0.068541; Generator: 0.019001,\n",
      "D(x): 0.262, D(G(z)): 0.545\n",
      "2019-04-09 23:58:20,966 root         INFO     Train Epoch: 28 [2560/8000 (32%)]\tTotal Loss: 2.673726\n",
      "Reconstruction: 2.282813, Regularization: 0.304901, Discriminator: 0.066792; Generator: 0.019220,\n",
      "D(x): 0.299, D(G(z)): 0.541\n",
      "2019-04-09 23:58:21,076 root         INFO     Train Epoch: 28 [3072/8000 (38%)]\tTotal Loss: 2.649020\n",
      "Reconstruction: 2.313431, Regularization: 0.249176, Discriminator: 0.066999; Generator: 0.019415,\n",
      "D(x): 0.272, D(G(z)): 0.537\n",
      "2019-04-09 23:58:21,186 root         INFO     Train Epoch: 28 [3584/8000 (45%)]\tTotal Loss: 2.820322\n",
      "Reconstruction: 2.419448, Regularization: 0.312762, Discriminator: 0.068720; Generator: 0.019392,\n",
      "D(x): 0.265, D(G(z)): 0.538\n",
      "2019-04-09 23:58:21,296 root         INFO     Train Epoch: 28 [4096/8000 (51%)]\tTotal Loss: 4.049686\n",
      "Reconstruction: 3.694713, Regularization: 0.264710, Discriminator: 0.070838; Generator: 0.019426,\n",
      "D(x): 0.246, D(G(z)): 0.537\n",
      "2019-04-09 23:58:21,407 root         INFO     Train Epoch: 28 [4608/8000 (58%)]\tTotal Loss: 2.638268\n",
      "Reconstruction: 2.273119, Regularization: 0.282153, Discriminator: 0.063498; Generator: 0.019499,\n",
      "D(x): 0.292, D(G(z)): 0.536\n",
      "2019-04-09 23:58:21,517 root         INFO     Train Epoch: 28 [5120/8000 (64%)]\tTotal Loss: 2.800823\n",
      "Reconstruction: 2.392928, Regularization: 0.321683, Discriminator: 0.066959; Generator: 0.019253,\n",
      "D(x): 0.282, D(G(z)): 0.540\n",
      "2019-04-09 23:58:21,627 root         INFO     Train Epoch: 28 [5632/8000 (70%)]\tTotal Loss: 3.491853\n",
      "Reconstruction: 3.099699, Regularization: 0.307793, Discriminator: 0.064730; Generator: 0.019632,\n",
      "D(x): 0.288, D(G(z)): 0.534\n",
      "2019-04-09 23:58:21,737 root         INFO     Train Epoch: 28 [6144/8000 (77%)]\tTotal Loss: 3.246708\n",
      "Reconstruction: 2.836284, Regularization: 0.325297, Discriminator: 0.065651; Generator: 0.019476,\n",
      "D(x): 0.282, D(G(z)): 0.536\n",
      "2019-04-09 23:58:21,847 root         INFO     Train Epoch: 28 [6656/8000 (83%)]\tTotal Loss: 3.568193\n",
      "Reconstruction: 3.185024, Regularization: 0.296822, Discriminator: 0.066540; Generator: 0.019806,\n",
      "D(x): 0.266, D(G(z)): 0.531\n",
      "2019-04-09 23:58:21,958 root         INFO     Train Epoch: 28 [7168/8000 (90%)]\tTotal Loss: 3.842975\n",
      "Reconstruction: 3.507191, Regularization: 0.254179, Discriminator: 0.062140; Generator: 0.019464,\n",
      "D(x): 0.319, D(G(z)): 0.537\n",
      "2019-04-09 23:58:22,068 root         INFO     Train Epoch: 28 [7680/8000 (96%)]\tTotal Loss: 2.529504\n",
      "Reconstruction: 2.137827, Regularization: 0.306498, Discriminator: 0.065489; Generator: 0.019690,\n",
      "D(x): 0.290, D(G(z)): 0.533\n",
      "2019-04-09 23:58:22,149 root         INFO     ====> Epoch: 28 Average loss: 3.0875\n",
      "2019-04-09 23:58:22,176 root         INFO     Train Epoch: 29 [0/8000 (0%)]\tTotal Loss: 3.019459\n",
      "Reconstruction: 2.594102, Regularization: 0.342373, Discriminator: 0.063053; Generator: 0.019931,\n",
      "D(x): 0.299, D(G(z)): 0.529\n",
      "2019-04-09 23:58:22,288 root         INFO     Train Epoch: 29 [512/8000 (6%)]\tTotal Loss: 3.201869\n",
      "Reconstruction: 2.812723, Regularization: 0.306136, Discriminator: 0.063338; Generator: 0.019672,\n",
      "D(x): 0.305, D(G(z)): 0.533\n",
      "2019-04-09 23:58:22,401 root         INFO     Train Epoch: 29 [1024/8000 (13%)]\tTotal Loss: 2.546861\n",
      "Reconstruction: 2.147081, Regularization: 0.316689, Discriminator: 0.062849; Generator: 0.020242,\n",
      "D(x): 0.297, D(G(z)): 0.523\n",
      "2019-04-09 23:58:22,510 root         INFO     Train Epoch: 29 [1536/8000 (19%)]\tTotal Loss: 2.900249\n",
      "Reconstruction: 2.502755, Regularization: 0.309755, Discriminator: 0.067791; Generator: 0.019947,\n",
      "D(x): 0.277, D(G(z)): 0.528\n",
      "2019-04-09 23:58:22,619 root         INFO     Train Epoch: 29 [2048/8000 (26%)]\tTotal Loss: 4.701391\n",
      "Reconstruction: 4.340459, Regularization: 0.280919, Discriminator: 0.060041; Generator: 0.019972,\n",
      "D(x): 0.322, D(G(z)): 0.528\n",
      "2019-04-09 23:58:22,728 root         INFO     Train Epoch: 29 [2560/8000 (32%)]\tTotal Loss: 3.324614\n",
      "Reconstruction: 2.965868, Regularization: 0.275290, Discriminator: 0.063621; Generator: 0.019836,\n",
      "D(x): 0.292, D(G(z)): 0.530\n",
      "2019-04-09 23:58:22,836 root         INFO     Train Epoch: 29 [3072/8000 (38%)]\tTotal Loss: 3.909509\n",
      "Reconstruction: 3.512169, Regularization: 0.312588, Discriminator: 0.064674; Generator: 0.020079,\n",
      "D(x): 0.287, D(G(z)): 0.526\n",
      "2019-04-09 23:58:22,945 root         INFO     Train Epoch: 29 [3584/8000 (45%)]\tTotal Loss: 3.234359\n",
      "Reconstruction: 2.809559, Regularization: 0.341036, Discriminator: 0.063621; Generator: 0.020143,\n",
      "D(x): 0.289, D(G(z)): 0.525\n",
      "2019-04-09 23:58:23,053 root         INFO     Train Epoch: 29 [4096/8000 (51%)]\tTotal Loss: 7.094780\n",
      "Reconstruction: 6.743371, Regularization: 0.267704, Discriminator: 0.063692; Generator: 0.020013,\n",
      "D(x): 0.297, D(G(z)): 0.527\n",
      "2019-04-09 23:58:23,162 root         INFO     Train Epoch: 29 [4608/8000 (58%)]\tTotal Loss: 4.875148\n",
      "Reconstruction: 4.486709, Regularization: 0.305165, Discriminator: 0.063586; Generator: 0.019687,\n",
      "D(x): 0.291, D(G(z)): 0.533\n",
      "2019-04-09 23:58:23,271 root         INFO     Train Epoch: 29 [5120/8000 (64%)]\tTotal Loss: 3.345919\n",
      "Reconstruction: 2.943542, Regularization: 0.315695, Discriminator: 0.066227; Generator: 0.020455,\n",
      "D(x): 0.266, D(G(z)): 0.520\n",
      "2019-04-09 23:58:23,381 root         INFO     Train Epoch: 29 [5632/8000 (70%)]\tTotal Loss: 4.015838\n",
      "Reconstruction: 3.640418, Regularization: 0.289994, Discriminator: 0.065142; Generator: 0.020284,\n",
      "D(x): 0.269, D(G(z)): 0.523\n",
      "2019-04-09 23:58:23,490 root         INFO     Train Epoch: 29 [6144/8000 (77%)]\tTotal Loss: 2.127634\n",
      "Reconstruction: 1.751475, Regularization: 0.292745, Discriminator: 0.063132; Generator: 0.020282,\n",
      "D(x): 0.289, D(G(z)): 0.523\n",
      "2019-04-09 23:58:23,599 root         INFO     Train Epoch: 29 [6656/8000 (83%)]\tTotal Loss: 3.938069\n",
      "Reconstruction: 3.500388, Regularization: 0.356300, Discriminator: 0.061114; Generator: 0.020267,\n",
      "D(x): 0.310, D(G(z)): 0.523\n",
      "2019-04-09 23:58:23,708 root         INFO     Train Epoch: 29 [7168/8000 (90%)]\tTotal Loss: 2.566253\n",
      "Reconstruction: 2.166314, Regularization: 0.318620, Discriminator: 0.060647; Generator: 0.020672,\n",
      "D(x): 0.302, D(G(z)): 0.516\n",
      "2019-04-09 23:58:23,817 root         INFO     Train Epoch: 29 [7680/8000 (96%)]\tTotal Loss: 2.535768\n",
      "Reconstruction: 2.143017, Regularization: 0.309039, Discriminator: 0.063429; Generator: 0.020283,\n",
      "D(x): 0.285, D(G(z)): 0.523\n",
      "2019-04-09 23:58:23,896 root         INFO     ====> Epoch: 29 Average loss: 3.1882\n",
      "2019-04-09 23:58:23,923 root         INFO     Train Epoch: 30 [0/8000 (0%)]\tTotal Loss: 2.166458\n",
      "Reconstruction: 1.806379, Regularization: 0.280729, Discriminator: 0.058977; Generator: 0.020373,\n",
      "D(x): 0.327, D(G(z)): 0.521\n",
      "2019-04-09 23:58:24,034 root         INFO     Train Epoch: 30 [512/8000 (6%)]\tTotal Loss: 4.080323\n",
      "Reconstruction: 3.685966, Regularization: 0.309681, Discriminator: 0.064076; Generator: 0.020601,\n",
      "D(x): 0.281, D(G(z)): 0.517\n",
      "2019-04-09 23:58:24,145 root         INFO     Train Epoch: 30 [1024/8000 (13%)]\tTotal Loss: 2.817369\n",
      "Reconstruction: 2.414173, Regularization: 0.320185, Discriminator: 0.062229; Generator: 0.020783,\n",
      "D(x): 0.294, D(G(z)): 0.514\n",
      "2019-04-09 23:58:24,255 root         INFO     Train Epoch: 30 [1536/8000 (19%)]\tTotal Loss: 3.079162\n",
      "Reconstruction: 2.664805, Regularization: 0.332210, Discriminator: 0.061536; Generator: 0.020612,\n",
      "D(x): 0.300, D(G(z)): 0.517\n",
      "2019-04-09 23:58:24,364 root         INFO     Train Epoch: 30 [2048/8000 (26%)]\tTotal Loss: 2.728998\n",
      "Reconstruction: 2.339403, Regularization: 0.306982, Discriminator: 0.061556; Generator: 0.021058,\n",
      "D(x): 0.295, D(G(z)): 0.510\n",
      "2019-04-09 23:58:24,473 root         INFO     Train Epoch: 30 [2560/8000 (32%)]\tTotal Loss: 4.186234\n",
      "Reconstruction: 3.811472, Regularization: 0.294320, Discriminator: 0.059567; Generator: 0.020875,\n",
      "D(x): 0.318, D(G(z)): 0.513\n",
      "2019-04-09 23:58:24,582 root         INFO     Train Epoch: 30 [3072/8000 (38%)]\tTotal Loss: 2.584912\n",
      "Reconstruction: 2.171342, Regularization: 0.330963, Discriminator: 0.061775; Generator: 0.020833,\n",
      "D(x): 0.292, D(G(z)): 0.514\n",
      "2019-04-09 23:58:24,691 root         INFO     Train Epoch: 30 [3584/8000 (45%)]\tTotal Loss: 3.054898\n",
      "Reconstruction: 2.663327, Regularization: 0.310109, Discriminator: 0.060278; Generator: 0.021183,\n",
      "D(x): 0.302, D(G(z)): 0.508\n",
      "2019-04-09 23:58:24,800 root         INFO     Train Epoch: 30 [4096/8000 (51%)]\tTotal Loss: 3.036436\n",
      "Reconstruction: 2.618433, Regularization: 0.337209, Discriminator: 0.059628; Generator: 0.021167,\n",
      "D(x): 0.312, D(G(z)): 0.508\n",
      "2019-04-09 23:58:24,909 root         INFO     Train Epoch: 30 [4608/8000 (58%)]\tTotal Loss: 2.608366\n",
      "Reconstruction: 2.223749, Regularization: 0.304056, Discriminator: 0.059543; Generator: 0.021019,\n",
      "D(x): 0.310, D(G(z)): 0.511\n",
      "2019-04-09 23:58:25,018 root         INFO     Train Epoch: 30 [5120/8000 (64%)]\tTotal Loss: 3.468737\n",
      "Reconstruction: 3.065724, Regularization: 0.322913, Discriminator: 0.059101; Generator: 0.020999,\n",
      "D(x): 0.317, D(G(z)): 0.511\n",
      "2019-04-09 23:58:25,128 root         INFO     Train Epoch: 30 [5632/8000 (70%)]\tTotal Loss: 3.807576\n",
      "Reconstruction: 3.421658, Regularization: 0.305986, Discriminator: 0.058616; Generator: 0.021315,\n",
      "D(x): 0.318, D(G(z)): 0.506\n",
      "2019-04-09 23:58:25,241 root         INFO     Train Epoch: 30 [6144/8000 (77%)]\tTotal Loss: 3.341177\n",
      "Reconstruction: 2.945914, Regularization: 0.316654, Discriminator: 0.057438; Generator: 0.021172,\n",
      "D(x): 0.332, D(G(z)): 0.508\n",
      "2019-04-09 23:58:25,353 root         INFO     Train Epoch: 30 [6656/8000 (83%)]\tTotal Loss: 2.687107\n",
      "Reconstruction: 2.307895, Regularization: 0.298594, Discriminator: 0.059506; Generator: 0.021111,\n",
      "D(x): 0.315, D(G(z)): 0.509\n",
      "2019-04-09 23:58:25,465 root         INFO     Train Epoch: 30 [7168/8000 (90%)]\tTotal Loss: 2.638204\n",
      "Reconstruction: 2.260239, Regularization: 0.298192, Discriminator: 0.058189; Generator: 0.021584,\n",
      "D(x): 0.322, D(G(z)): 0.501\n",
      "2019-04-09 23:58:25,578 root         INFO     Train Epoch: 30 [7680/8000 (96%)]\tTotal Loss: 5.428149\n",
      "Reconstruction: 5.022745, Regularization: 0.322789, Discriminator: 0.061489; Generator: 0.021127,\n",
      "D(x): 0.297, D(G(z)): 0.509\n",
      "2019-04-09 23:58:25,660 root         INFO     ====> Epoch: 30 Average loss: 3.2237\n",
      "2019-04-09 23:58:25,687 root         INFO     Train Epoch: 31 [0/8000 (0%)]\tTotal Loss: 2.984577\n",
      "Reconstruction: 2.529086, Regularization: 0.376528, Discriminator: 0.057467; Generator: 0.021497,\n",
      "D(x): 0.326, D(G(z)): 0.503\n",
      "2019-04-09 23:58:25,799 root         INFO     Train Epoch: 31 [512/8000 (6%)]\tTotal Loss: 2.579633\n",
      "Reconstruction: 2.148091, Regularization: 0.351726, Discriminator: 0.058189; Generator: 0.021627,\n",
      "D(x): 0.318, D(G(z)): 0.501\n",
      "2019-04-09 23:58:25,911 root         INFO     Train Epoch: 31 [1024/8000 (13%)]\tTotal Loss: 2.658983\n",
      "Reconstruction: 2.227479, Regularization: 0.350116, Discriminator: 0.059795; Generator: 0.021593,\n",
      "D(x): 0.302, D(G(z)): 0.501\n",
      "2019-04-09 23:58:26,023 root         INFO     Train Epoch: 31 [1536/8000 (19%)]\tTotal Loss: 2.767312\n",
      "Reconstruction: 2.336684, Regularization: 0.349020, Discriminator: 0.059930; Generator: 0.021678,\n",
      "D(x): 0.301, D(G(z)): 0.500\n",
      "2019-04-09 23:58:26,134 root         INFO     Train Epoch: 31 [2048/8000 (26%)]\tTotal Loss: 2.618619\n",
      "Reconstruction: 2.250557, Regularization: 0.286015, Discriminator: 0.060244; Generator: 0.021802,\n",
      "D(x): 0.299, D(G(z)): 0.498\n",
      "2019-04-09 23:58:26,247 root         INFO     Train Epoch: 31 [2560/8000 (32%)]\tTotal Loss: 3.411015\n",
      "Reconstruction: 2.999988, Regularization: 0.330475, Discriminator: 0.058458; Generator: 0.022095,\n",
      "D(x): 0.311, D(G(z)): 0.493\n",
      "2019-04-09 23:58:26,359 root         INFO     Train Epoch: 31 [3072/8000 (38%)]\tTotal Loss: 4.147310\n",
      "Reconstruction: 3.720692, Regularization: 0.348797, Discriminator: 0.056111; Generator: 0.021710,\n",
      "D(x): 0.341, D(G(z)): 0.499\n",
      "2019-04-09 23:58:26,467 root         INFO     Train Epoch: 31 [3584/8000 (45%)]\tTotal Loss: 2.809148\n",
      "Reconstruction: 2.402799, Regularization: 0.326755, Discriminator: 0.057672; Generator: 0.021922,\n",
      "D(x): 0.320, D(G(z)): 0.496\n",
      "2019-04-09 23:58:26,576 root         INFO     Train Epoch: 31 [4096/8000 (51%)]\tTotal Loss: 3.917387\n",
      "Reconstruction: 3.450908, Regularization: 0.388294, Discriminator: 0.056046; Generator: 0.022138,\n",
      "D(x): 0.335, D(G(z)): 0.493\n",
      "2019-04-09 23:58:26,684 root         INFO     Train Epoch: 31 [4608/8000 (58%)]\tTotal Loss: 6.728331\n",
      "Reconstruction: 6.284153, Regularization: 0.365343, Discriminator: 0.056949; Generator: 0.021886,\n",
      "D(x): 0.326, D(G(z)): 0.497\n",
      "2019-04-09 23:58:26,792 root         INFO     Train Epoch: 31 [5120/8000 (64%)]\tTotal Loss: 3.139903\n",
      "Reconstruction: 2.693802, Regularization: 0.368158, Discriminator: 0.055868; Generator: 0.022075,\n",
      "D(x): 0.337, D(G(z)): 0.494\n",
      "2019-04-09 23:58:26,902 root         INFO     Train Epoch: 31 [5632/8000 (70%)]\tTotal Loss: 3.264046\n",
      "Reconstruction: 2.859766, Regularization: 0.324322, Discriminator: 0.057755; Generator: 0.022202,\n",
      "D(x): 0.314, D(G(z)): 0.492\n",
      "2019-04-09 23:58:27,010 root         INFO     Train Epoch: 31 [6144/8000 (77%)]\tTotal Loss: 2.230617\n",
      "Reconstruction: 1.829018, Regularization: 0.323954, Discriminator: 0.055127; Generator: 0.022517,\n",
      "D(x): 0.340, D(G(z)): 0.487\n",
      "2019-04-09 23:58:27,119 root         INFO     Train Epoch: 31 [6656/8000 (83%)]\tTotal Loss: 6.648844\n",
      "Reconstruction: 6.221930, Regularization: 0.347475, Discriminator: 0.057135; Generator: 0.022304,\n",
      "D(x): 0.319, D(G(z)): 0.490\n",
      "2019-04-09 23:58:27,228 root         INFO     Train Epoch: 31 [7168/8000 (90%)]\tTotal Loss: 2.493817\n",
      "Reconstruction: 2.045674, Regularization: 0.369340, Discriminator: 0.056409; Generator: 0.022394,\n",
      "D(x): 0.331, D(G(z)): 0.489\n",
      "2019-04-09 23:58:27,336 root         INFO     Train Epoch: 31 [7680/8000 (96%)]\tTotal Loss: 4.356323\n",
      "Reconstruction: 3.939212, Regularization: 0.337987, Discriminator: 0.056622; Generator: 0.022502,\n",
      "D(x): 0.323, D(G(z)): 0.487\n",
      "2019-04-09 23:58:27,416 root         INFO     ====> Epoch: 31 Average loss: 3.3128\n",
      "2019-04-09 23:58:27,443 root         INFO     Train Epoch: 32 [0/8000 (0%)]\tTotal Loss: 3.729981\n",
      "Reconstruction: 3.313243, Regularization: 0.336175, Discriminator: 0.058169; Generator: 0.022394,\n",
      "D(x): 0.309, D(G(z)): 0.489\n",
      "2019-04-09 23:58:27,556 root         INFO     Train Epoch: 32 [512/8000 (6%)]\tTotal Loss: 1.975086\n",
      "Reconstruction: 1.601584, Regularization: 0.294900, Discriminator: 0.056110; Generator: 0.022492,\n",
      "D(x): 0.328, D(G(z)): 0.487\n",
      "2019-04-09 23:58:27,667 root         INFO     Train Epoch: 32 [1024/8000 (13%)]\tTotal Loss: 3.139322\n",
      "Reconstruction: 2.706184, Regularization: 0.354570, Discriminator: 0.056184; Generator: 0.022384,\n",
      "D(x): 0.328, D(G(z)): 0.489\n",
      "2019-04-09 23:58:27,778 root         INFO     Train Epoch: 32 [1536/8000 (19%)]\tTotal Loss: 2.670217\n",
      "Reconstruction: 2.221704, Regularization: 0.369107, Discriminator: 0.056532; Generator: 0.022873,\n",
      "D(x): 0.319, D(G(z)): 0.481\n",
      "2019-04-09 23:58:27,890 root         INFO     Train Epoch: 32 [2048/8000 (26%)]\tTotal Loss: 4.341093\n",
      "Reconstruction: 3.907809, Regularization: 0.355269, Discriminator: 0.055398; Generator: 0.022616,\n",
      "D(x): 0.334, D(G(z)): 0.485\n",
      "2019-04-09 23:58:28,000 root         INFO     Train Epoch: 32 [2560/8000 (32%)]\tTotal Loss: 3.316489\n",
      "Reconstruction: 2.906204, Regularization: 0.331609, Discriminator: 0.055791; Generator: 0.022885,\n",
      "D(x): 0.326, D(G(z)): 0.481\n",
      "2019-04-09 23:58:28,111 root         INFO     Train Epoch: 32 [3072/8000 (38%)]\tTotal Loss: 2.944391\n",
      "Reconstruction: 2.531101, Regularization: 0.335112, Discriminator: 0.055482; Generator: 0.022695,\n",
      "D(x): 0.333, D(G(z)): 0.484\n",
      "2019-04-09 23:58:28,222 root         INFO     Train Epoch: 32 [3584/8000 (45%)]\tTotal Loss: 3.281305\n",
      "Reconstruction: 2.838693, Regularization: 0.364896, Discriminator: 0.054659; Generator: 0.023057,\n",
      "D(x): 0.339, D(G(z)): 0.478\n",
      "2019-04-09 23:58:28,334 root         INFO     Train Epoch: 32 [4096/8000 (51%)]\tTotal Loss: 2.239543\n",
      "Reconstruction: 1.822766, Regularization: 0.337407, Discriminator: 0.056157; Generator: 0.023213,\n",
      "D(x): 0.319, D(G(z)): 0.476\n",
      "2019-04-09 23:58:28,443 root         INFO     Train Epoch: 32 [4608/8000 (58%)]\tTotal Loss: 2.813949\n",
      "Reconstruction: 2.321338, Regularization: 0.413875, Discriminator: 0.055908; Generator: 0.022828,\n",
      "D(x): 0.325, D(G(z)): 0.482\n",
      "2019-04-09 23:58:28,551 root         INFO     Train Epoch: 32 [5120/8000 (64%)]\tTotal Loss: 2.986526\n",
      "Reconstruction: 2.546267, Regularization: 0.361544, Discriminator: 0.055886; Generator: 0.022829,\n",
      "D(x): 0.327, D(G(z)): 0.482\n",
      "2019-04-09 23:58:28,660 root         INFO     Train Epoch: 32 [5632/8000 (70%)]\tTotal Loss: 2.767696\n",
      "Reconstruction: 2.309312, Regularization: 0.379878, Discriminator: 0.055064; Generator: 0.023441,\n",
      "D(x): 0.329, D(G(z)): 0.472\n",
      "2019-04-09 23:58:28,770 root         INFO     Train Epoch: 32 [6144/8000 (77%)]\tTotal Loss: 3.274482\n",
      "Reconstruction: 2.832482, Regularization: 0.363327, Discriminator: 0.055283; Generator: 0.023390,\n",
      "D(x): 0.327, D(G(z)): 0.473\n",
      "2019-04-09 23:58:28,880 root         INFO     Train Epoch: 32 [6656/8000 (83%)]\tTotal Loss: 5.395881\n",
      "Reconstruction: 4.946019, Regularization: 0.371791, Discriminator: 0.054695; Generator: 0.023376,\n",
      "D(x): 0.334, D(G(z)): 0.473\n",
      "2019-04-09 23:58:28,989 root         INFO     Train Epoch: 32 [7168/8000 (90%)]\tTotal Loss: 3.545363\n",
      "Reconstruction: 3.003967, Regularization: 0.462521, Discriminator: 0.055394; Generator: 0.023481,\n",
      "D(x): 0.324, D(G(z)): 0.472\n",
      "2019-04-09 23:58:29,098 root         INFO     Train Epoch: 32 [7680/8000 (96%)]\tTotal Loss: 2.646491\n",
      "Reconstruction: 2.204365, Regularization: 0.363931, Discriminator: 0.054633; Generator: 0.023562,\n",
      "D(x): 0.333, D(G(z)): 0.471\n",
      "2019-04-09 23:58:29,179 root         INFO     ====> Epoch: 32 Average loss: 3.4219\n",
      "2019-04-09 23:58:29,206 root         INFO     Train Epoch: 33 [0/8000 (0%)]\tTotal Loss: 5.489367\n",
      "Reconstruction: 5.063775, Regularization: 0.347410, Discriminator: 0.054736; Generator: 0.023446,\n",
      "D(x): 0.331, D(G(z)): 0.472\n",
      "2019-04-09 23:58:29,317 root         INFO     Train Epoch: 33 [512/8000 (6%)]\tTotal Loss: 3.045761\n",
      "Reconstruction: 2.547998, Regularization: 0.420428, Discriminator: 0.053817; Generator: 0.023518,\n",
      "D(x): 0.342, D(G(z)): 0.471\n",
      "2019-04-09 23:58:29,429 root         INFO     Train Epoch: 33 [1024/8000 (13%)]\tTotal Loss: 3.119875\n",
      "Reconstruction: 2.645849, Regularization: 0.395395, Discriminator: 0.055284; Generator: 0.023348,\n",
      "D(x): 0.326, D(G(z)): 0.474\n",
      "2019-04-09 23:58:29,539 root         INFO     Train Epoch: 33 [1536/8000 (19%)]\tTotal Loss: 4.210721\n",
      "Reconstruction: 3.753608, Regularization: 0.378981, Discriminator: 0.054455; Generator: 0.023678,\n",
      "D(x): 0.333, D(G(z)): 0.469\n",
      "2019-04-09 23:58:29,648 root         INFO     Train Epoch: 33 [2048/8000 (26%)]\tTotal Loss: 3.149867\n",
      "Reconstruction: 2.649251, Regularization: 0.422599, Discriminator: 0.054585; Generator: 0.023432,\n",
      "D(x): 0.333, D(G(z)): 0.473\n",
      "2019-04-09 23:58:29,755 root         INFO     Train Epoch: 33 [2560/8000 (32%)]\tTotal Loss: 3.361256\n",
      "Reconstruction: 2.929832, Regularization: 0.353533, Discriminator: 0.054279; Generator: 0.023613,\n",
      "D(x): 0.335, D(G(z)): 0.470\n",
      "2019-04-09 23:58:29,863 root         INFO     Train Epoch: 33 [3072/8000 (38%)]\tTotal Loss: 2.937488\n",
      "Reconstruction: 2.464597, Regularization: 0.395329, Discriminator: 0.053807; Generator: 0.023756,\n",
      "D(x): 0.338, D(G(z)): 0.468\n",
      "2019-04-09 23:58:29,971 root         INFO     Train Epoch: 33 [3584/8000 (45%)]\tTotal Loss: 1.987955\n",
      "Reconstruction: 1.578797, Regularization: 0.331998, Discriminator: 0.053439; Generator: 0.023720,\n",
      "D(x): 0.342, D(G(z)): 0.468\n",
      "2019-04-09 23:58:30,079 root         INFO     Train Epoch: 33 [4096/8000 (51%)]\tTotal Loss: 4.342260\n",
      "Reconstruction: 3.852950, Regularization: 0.412173, Discriminator: 0.053442; Generator: 0.023695,\n",
      "D(x): 0.342, D(G(z)): 0.469\n",
      "2019-04-09 23:58:30,187 root         INFO     Train Epoch: 33 [4608/8000 (58%)]\tTotal Loss: 6.381032\n",
      "Reconstruction: 5.945243, Regularization: 0.358104, Discriminator: 0.053678; Generator: 0.024007,\n",
      "D(x): 0.336, D(G(z)): 0.464\n",
      "2019-04-09 23:58:30,295 root         INFO     Train Epoch: 33 [5120/8000 (64%)]\tTotal Loss: 4.501690\n",
      "Reconstruction: 4.055000, Regularization: 0.370434, Discriminator: 0.052355; Generator: 0.023902,\n",
      "D(x): 0.352, D(G(z)): 0.466\n",
      "2019-04-09 23:58:30,403 root         INFO     Train Epoch: 33 [5632/8000 (70%)]\tTotal Loss: 3.630370\n",
      "Reconstruction: 3.229341, Regularization: 0.322799, Discriminator: 0.054283; Generator: 0.023948,\n",
      "D(x): 0.330, D(G(z)): 0.465\n",
      "2019-04-09 23:58:30,511 root         INFO     Train Epoch: 33 [6144/8000 (77%)]\tTotal Loss: 6.132204\n",
      "Reconstruction: 5.680965, Regularization: 0.374016, Discriminator: 0.053248; Generator: 0.023974,\n",
      "D(x): 0.343, D(G(z)): 0.464\n",
      "2019-04-09 23:58:30,618 root         INFO     Train Epoch: 33 [6656/8000 (83%)]\tTotal Loss: 2.919271\n",
      "Reconstruction: 2.393922, Regularization: 0.447350, Discriminator: 0.053773; Generator: 0.024226,\n",
      "D(x): 0.334, D(G(z)): 0.461\n",
      "2019-04-09 23:58:30,726 root         INFO     Train Epoch: 33 [7168/8000 (90%)]\tTotal Loss: 3.566243\n",
      "Reconstruction: 3.084731, Regularization: 0.404236, Discriminator: 0.053205; Generator: 0.024071,\n",
      "D(x): 0.341, D(G(z)): 0.463\n",
      "2019-04-09 23:58:30,833 root         INFO     Train Epoch: 33 [7680/8000 (96%)]\tTotal Loss: 6.802641\n",
      "Reconstruction: 6.370233, Regularization: 0.355020, Discriminator: 0.053255; Generator: 0.024133,\n",
      "D(x): 0.340, D(G(z)): 0.462\n",
      "2019-04-09 23:58:30,914 root         INFO     ====> Epoch: 33 Average loss: 3.4641\n",
      "2019-04-09 23:58:30,941 root         INFO     Train Epoch: 34 [0/8000 (0%)]\tTotal Loss: 2.696442\n",
      "Reconstruction: 2.199326, Regularization: 0.418873, Discriminator: 0.054067; Generator: 0.024176,\n",
      "D(x): 0.331, D(G(z)): 0.461\n",
      "2019-04-09 23:58:31,051 root         INFO     Train Epoch: 34 [512/8000 (6%)]\tTotal Loss: 3.333436\n",
      "Reconstruction: 2.843943, Regularization: 0.412893, Discriminator: 0.052318; Generator: 0.024282,\n",
      "D(x): 0.349, D(G(z)): 0.460\n",
      "2019-04-09 23:58:31,161 root         INFO     Train Epoch: 34 [1024/8000 (13%)]\tTotal Loss: 2.872329\n",
      "Reconstruction: 2.352964, Regularization: 0.442846, Discriminator: 0.052087; Generator: 0.024431,\n",
      "D(x): 0.349, D(G(z)): 0.458\n",
      "2019-04-09 23:58:31,271 root         INFO     Train Epoch: 34 [1536/8000 (19%)]\tTotal Loss: 2.873212\n",
      "Reconstruction: 2.402133, Regularization: 0.394246, Discriminator: 0.052178; Generator: 0.024655,\n",
      "D(x): 0.347, D(G(z)): 0.454\n",
      "2019-04-09 23:58:31,381 root         INFO     Train Epoch: 34 [2048/8000 (26%)]\tTotal Loss: 6.223105\n",
      "Reconstruction: 5.787760, Regularization: 0.359221, Discriminator: 0.051665; Generator: 0.024459,\n",
      "D(x): 0.354, D(G(z)): 0.457\n",
      "2019-04-09 23:58:31,492 root         INFO     Train Epoch: 34 [2560/8000 (32%)]\tTotal Loss: 2.893759\n",
      "Reconstruction: 2.442819, Regularization: 0.374019, Discriminator: 0.052334; Generator: 0.024587,\n",
      "D(x): 0.346, D(G(z)): 0.455\n",
      "2019-04-09 23:58:31,602 root         INFO     Train Epoch: 34 [3072/8000 (38%)]\tTotal Loss: 2.849213\n",
      "Reconstruction: 2.335467, Regularization: 0.436754, Discriminator: 0.052629; Generator: 0.024363,\n",
      "D(x): 0.344, D(G(z)): 0.459\n",
      "2019-04-09 23:58:31,712 root         INFO     Train Epoch: 34 [3584/8000 (45%)]\tTotal Loss: 3.809343\n",
      "Reconstruction: 3.282554, Regularization: 0.450272, Discriminator: 0.051932; Generator: 0.024584,\n",
      "D(x): 0.350, D(G(z)): 0.455\n",
      "2019-04-09 23:58:31,822 root         INFO     Train Epoch: 34 [4096/8000 (51%)]\tTotal Loss: 2.720719\n",
      "Reconstruction: 2.209929, Regularization: 0.433800, Discriminator: 0.052402; Generator: 0.024588,\n",
      "D(x): 0.345, D(G(z)): 0.455\n",
      "2019-04-09 23:58:31,932 root         INFO     Train Epoch: 34 [4608/8000 (58%)]\tTotal Loss: 3.131730\n",
      "Reconstruction: 2.638549, Regularization: 0.416964, Discriminator: 0.051567; Generator: 0.024650,\n",
      "D(x): 0.353, D(G(z)): 0.454\n",
      "2019-04-09 23:58:32,042 root         INFO     Train Epoch: 34 [5120/8000 (64%)]\tTotal Loss: 2.483490\n",
      "Reconstruction: 1.984536, Regularization: 0.423178, Discriminator: 0.051186; Generator: 0.024590,\n",
      "D(x): 0.358, D(G(z)): 0.455\n",
      "2019-04-09 23:58:32,151 root         INFO     Train Epoch: 34 [5632/8000 (70%)]\tTotal Loss: 3.315455\n",
      "Reconstruction: 2.843217, Regularization: 0.396485, Discriminator: 0.051035; Generator: 0.024718,\n",
      "D(x): 0.358, D(G(z)): 0.453\n",
      "2019-04-09 23:58:32,261 root         INFO     Train Epoch: 34 [6144/8000 (77%)]\tTotal Loss: 5.334615\n",
      "Reconstruction: 4.860369, Regularization: 0.398068, Discriminator: 0.051350; Generator: 0.024829,\n",
      "D(x): 0.354, D(G(z)): 0.452\n",
      "2019-04-09 23:58:32,371 root         INFO     Train Epoch: 34 [6656/8000 (83%)]\tTotal Loss: 3.639933\n",
      "Reconstruction: 3.077137, Regularization: 0.486332, Discriminator: 0.051649; Generator: 0.024815,\n",
      "D(x): 0.350, D(G(z)): 0.452\n",
      "2019-04-09 23:58:32,481 root         INFO     Train Epoch: 34 [7168/8000 (90%)]\tTotal Loss: 3.494158\n",
      "Reconstruction: 2.965940, Regularization: 0.452020, Discriminator: 0.051126; Generator: 0.025072,\n",
      "D(x): 0.354, D(G(z)): 0.448\n",
      "2019-04-09 23:58:32,590 root         INFO     Train Epoch: 34 [7680/8000 (96%)]\tTotal Loss: 2.136003\n",
      "Reconstruction: 1.665792, Regularization: 0.394709, Discriminator: 0.050356; Generator: 0.025147,\n",
      "D(x): 0.362, D(G(z)): 0.447\n",
      "2019-04-09 23:58:32,670 root         INFO     ====> Epoch: 34 Average loss: 3.5532\n",
      "2019-04-09 23:58:32,698 root         INFO     Train Epoch: 35 [0/8000 (0%)]\tTotal Loss: 2.609232\n",
      "Reconstruction: 2.069080, Regularization: 0.463619, Discriminator: 0.051252; Generator: 0.025281,\n",
      "D(x): 0.351, D(G(z)): 0.445\n",
      "2019-04-09 23:58:32,810 root         INFO     Train Epoch: 35 [512/8000 (6%)]\tTotal Loss: 2.866514\n",
      "Reconstruction: 2.381559, Regularization: 0.408695, Discriminator: 0.051160; Generator: 0.025100,\n",
      "D(x): 0.353, D(G(z)): 0.448\n",
      "2019-04-09 23:58:32,921 root         INFO     Train Epoch: 35 [1024/8000 (13%)]\tTotal Loss: 3.593615\n",
      "Reconstruction: 3.166336, Regularization: 0.351258, Discriminator: 0.050846; Generator: 0.025175,\n",
      "D(x): 0.356, D(G(z)): 0.447\n",
      "2019-04-09 23:58:33,032 root         INFO     Train Epoch: 35 [1536/8000 (19%)]\tTotal Loss: 3.245611\n",
      "Reconstruction: 2.701038, Regularization: 0.468816, Discriminator: 0.050390; Generator: 0.025368,\n",
      "D(x): 0.360, D(G(z)): 0.444\n",
      "2019-04-09 23:58:33,143 root         INFO     Train Epoch: 35 [2048/8000 (26%)]\tTotal Loss: 3.166317\n",
      "Reconstruction: 2.670601, Regularization: 0.419773, Discriminator: 0.050600; Generator: 0.025343,\n",
      "D(x): 0.357, D(G(z)): 0.445\n",
      "2019-04-09 23:58:33,252 root         INFO     Train Epoch: 35 [2560/8000 (32%)]\tTotal Loss: 3.828116\n",
      "Reconstruction: 3.267859, Regularization: 0.484434, Discriminator: 0.050351; Generator: 0.025472,\n",
      "D(x): 0.359, D(G(z)): 0.443\n",
      "2019-04-09 23:58:33,362 root         INFO     Train Epoch: 35 [3072/8000 (38%)]\tTotal Loss: 3.734684\n",
      "Reconstruction: 3.186623, Regularization: 0.472190, Discriminator: 0.050317; Generator: 0.025554,\n",
      "D(x): 0.359, D(G(z)): 0.442\n",
      "2019-04-09 23:58:33,472 root         INFO     Train Epoch: 35 [3584/8000 (45%)]\tTotal Loss: 2.848591\n",
      "Reconstruction: 2.361318, Regularization: 0.411335, Discriminator: 0.050486; Generator: 0.025453,\n",
      "D(x): 0.358, D(G(z)): 0.443\n",
      "2019-04-09 23:58:33,582 root         INFO     Train Epoch: 35 [4096/8000 (51%)]\tTotal Loss: 3.345273\n",
      "Reconstruction: 2.838101, Regularization: 0.431779, Discriminator: 0.050009; Generator: 0.025385,\n",
      "D(x): 0.364, D(G(z)): 0.444\n",
      "2019-04-09 23:58:33,691 root         INFO     Train Epoch: 35 [4608/8000 (58%)]\tTotal Loss: 3.215234\n",
      "Reconstruction: 2.667187, Regularization: 0.472359, Discriminator: 0.050243; Generator: 0.025445,\n",
      "D(x): 0.360, D(G(z)): 0.443\n",
      "2019-04-09 23:58:33,801 root         INFO     Train Epoch: 35 [5120/8000 (64%)]\tTotal Loss: 5.047001\n",
      "Reconstruction: 4.505982, Regularization: 0.465372, Discriminator: 0.050136; Generator: 0.025511,\n",
      "D(x): 0.361, D(G(z)): 0.442\n",
      "2019-04-09 23:58:33,914 root         INFO     Train Epoch: 35 [5632/8000 (70%)]\tTotal Loss: 2.637657\n",
      "Reconstruction: 2.104760, Regularization: 0.457401, Discriminator: 0.049819; Generator: 0.025677,\n",
      "D(x): 0.363, D(G(z)): 0.440\n",
      "2019-04-09 23:58:34,026 root         INFO     Train Epoch: 35 [6144/8000 (77%)]\tTotal Loss: 3.589928\n",
      "Reconstruction: 3.022572, Regularization: 0.491844, Discriminator: 0.049915; Generator: 0.025597,\n",
      "D(x): 0.363, D(G(z)): 0.441\n",
      "2019-04-09 23:58:34,138 root         INFO     Train Epoch: 35 [6656/8000 (83%)]\tTotal Loss: 2.423461\n",
      "Reconstruction: 1.919097, Regularization: 0.428770, Discriminator: 0.049819; Generator: 0.025775,\n",
      "D(x): 0.362, D(G(z)): 0.438\n",
      "2019-04-09 23:58:34,250 root         INFO     Train Epoch: 35 [7168/8000 (90%)]\tTotal Loss: 3.087678\n",
      "Reconstruction: 2.578385, Regularization: 0.433904, Discriminator: 0.049635; Generator: 0.025754,\n",
      "D(x): 0.364, D(G(z)): 0.439\n",
      "2019-04-09 23:58:34,362 root         INFO     Train Epoch: 35 [7680/8000 (96%)]\tTotal Loss: 2.820321\n",
      "Reconstruction: 2.250563, Regularization: 0.494395, Discriminator: 0.049584; Generator: 0.025779,\n",
      "D(x): 0.365, D(G(z)): 0.438\n",
      "2019-04-09 23:58:34,442 root         INFO     ====> Epoch: 35 Average loss: 3.6120\n",
      "2019-04-09 23:58:34,469 root         INFO     Train Epoch: 36 [0/8000 (0%)]\tTotal Loss: 2.746616\n",
      "Reconstruction: 2.204601, Regularization: 0.466497, Discriminator: 0.049660; Generator: 0.025858,\n",
      "D(x): 0.363, D(G(z)): 0.437\n",
      "2019-04-09 23:58:34,582 root         INFO     Train Epoch: 36 [512/8000 (6%)]\tTotal Loss: 2.701786\n",
      "Reconstruction: 2.147325, Regularization: 0.479032, Discriminator: 0.049552; Generator: 0.025877,\n",
      "D(x): 0.365, D(G(z)): 0.437\n",
      "2019-04-09 23:58:34,694 root         INFO     Train Epoch: 36 [1024/8000 (13%)]\tTotal Loss: 3.194492\n",
      "Reconstruction: 2.665847, Regularization: 0.453639, Discriminator: 0.049039; Generator: 0.025966,\n",
      "D(x): 0.369, D(G(z)): 0.436\n",
      "2019-04-09 23:58:34,805 root         INFO     Train Epoch: 36 [1536/8000 (19%)]\tTotal Loss: 6.647748\n",
      "Reconstruction: 6.102068, Regularization: 0.470834, Discriminator: 0.048894; Generator: 0.025952,\n",
      "D(x): 0.371, D(G(z)): 0.436\n",
      "2019-04-09 23:58:34,917 root         INFO     Train Epoch: 36 [2048/8000 (26%)]\tTotal Loss: 3.755466\n",
      "Reconstruction: 3.142163, Regularization: 0.538445, Discriminator: 0.048979; Generator: 0.025878,\n",
      "D(x): 0.371, D(G(z)): 0.437\n",
      "2019-04-09 23:58:35,028 root         INFO     Train Epoch: 36 [2560/8000 (32%)]\tTotal Loss: 2.457284\n",
      "Reconstruction: 1.961708, Regularization: 0.420386, Discriminator: 0.049127; Generator: 0.026064,\n",
      "D(x): 0.367, D(G(z)): 0.434\n",
      "2019-04-09 23:58:35,140 root         INFO     Train Epoch: 36 [3072/8000 (38%)]\tTotal Loss: 2.389359\n",
      "Reconstruction: 1.851696, Regularization: 0.462290, Discriminator: 0.049334; Generator: 0.026039,\n",
      "D(x): 0.365, D(G(z)): 0.435\n",
      "2019-04-09 23:58:35,251 root         INFO     Train Epoch: 36 [3584/8000 (45%)]\tTotal Loss: 4.438457\n",
      "Reconstruction: 3.842440, Regularization: 0.521183, Discriminator: 0.048567; Generator: 0.026266,\n",
      "D(x): 0.372, D(G(z)): 0.432\n",
      "2019-04-09 23:58:35,363 root         INFO     Train Epoch: 36 [4096/8000 (51%)]\tTotal Loss: 5.887378\n",
      "Reconstruction: 5.357509, Regularization: 0.455052, Discriminator: 0.048660; Generator: 0.026157,\n",
      "D(x): 0.372, D(G(z)): 0.433\n",
      "2019-04-09 23:58:35,474 root         INFO     Train Epoch: 36 [4608/8000 (58%)]\tTotal Loss: 3.698332\n",
      "Reconstruction: 3.088128, Regularization: 0.535201, Discriminator: 0.048917; Generator: 0.026086,\n",
      "D(x): 0.370, D(G(z)): 0.434\n",
      "2019-04-09 23:58:35,586 root         INFO     Train Epoch: 36 [5120/8000 (64%)]\tTotal Loss: 5.221322\n",
      "Reconstruction: 4.674007, Regularization: 0.472855, Discriminator: 0.048144; Generator: 0.026317,\n",
      "D(x): 0.377, D(G(z)): 0.431\n",
      "2019-04-09 23:58:35,697 root         INFO     Train Epoch: 36 [5632/8000 (70%)]\tTotal Loss: 2.142415\n",
      "Reconstruction: 1.650732, Regularization: 0.417060, Discriminator: 0.048312; Generator: 0.026311,\n",
      "D(x): 0.375, D(G(z)): 0.431\n",
      "2019-04-09 23:58:35,809 root         INFO     Train Epoch: 36 [6144/8000 (77%)]\tTotal Loss: 3.118906\n",
      "Reconstruction: 2.504235, Regularization: 0.539940, Discriminator: 0.048632; Generator: 0.026099,\n",
      "D(x): 0.373, D(G(z)): 0.434\n",
      "2019-04-09 23:58:35,921 root         INFO     Train Epoch: 36 [6656/8000 (83%)]\tTotal Loss: 3.083067\n",
      "Reconstruction: 2.506140, Regularization: 0.502269, Discriminator: 0.048487; Generator: 0.026170,\n",
      "D(x): 0.374, D(G(z)): 0.433\n",
      "2019-04-09 23:58:36,032 root         INFO     Train Epoch: 36 [7168/8000 (90%)]\tTotal Loss: 2.942738\n",
      "Reconstruction: 2.354410, Regularization: 0.513486, Discriminator: 0.048504; Generator: 0.026338,\n",
      "D(x): 0.372, D(G(z)): 0.431\n",
      "2019-04-09 23:58:36,142 root         INFO     Train Epoch: 36 [7680/8000 (96%)]\tTotal Loss: 3.364280\n",
      "Reconstruction: 2.720864, Regularization: 0.568732, Discriminator: 0.048365; Generator: 0.026319,\n",
      "D(x): 0.374, D(G(z)): 0.431\n",
      "2019-04-09 23:58:36,223 root         INFO     ====> Epoch: 36 Average loss: 3.7109\n",
      "2019-04-09 23:58:36,250 root         INFO     Train Epoch: 37 [0/8000 (0%)]\tTotal Loss: 5.476861\n",
      "Reconstruction: 4.942688, Regularization: 0.459504, Discriminator: 0.048218; Generator: 0.026450,\n",
      "D(x): 0.375, D(G(z)): 0.429\n",
      "2019-04-09 23:58:36,362 root         INFO     Train Epoch: 37 [512/8000 (6%)]\tTotal Loss: 4.333489\n",
      "Reconstruction: 3.793842, Regularization: 0.464991, Discriminator: 0.048344; Generator: 0.026313,\n",
      "D(x): 0.374, D(G(z)): 0.431\n",
      "2019-04-09 23:58:36,473 root         INFO     Train Epoch: 37 [1024/8000 (13%)]\tTotal Loss: 11.070521\n",
      "Reconstruction: 10.498346, Regularization: 0.497532, Discriminator: 0.048194; Generator: 0.026449,\n",
      "D(x): 0.375, D(G(z)): 0.429\n",
      "2019-04-09 23:58:36,584 root         INFO     Train Epoch: 37 [1536/8000 (19%)]\tTotal Loss: 3.616559\n",
      "Reconstruction: 3.032367, Regularization: 0.509712, Discriminator: 0.047968; Generator: 0.026512,\n",
      "D(x): 0.377, D(G(z)): 0.428\n",
      "2019-04-09 23:58:36,696 root         INFO     Train Epoch: 37 [2048/8000 (26%)]\tTotal Loss: 3.101865\n",
      "Reconstruction: 2.526966, Regularization: 0.500594, Discriminator: 0.047764; Generator: 0.026540,\n",
      "D(x): 0.379, D(G(z)): 0.428\n",
      "2019-04-09 23:58:36,807 root         INFO     Train Epoch: 37 [2560/8000 (32%)]\tTotal Loss: 3.516494\n",
      "Reconstruction: 3.025694, Regularization: 0.416152, Discriminator: 0.048152; Generator: 0.026496,\n",
      "D(x): 0.375, D(G(z)): 0.428\n",
      "2019-04-09 23:58:36,918 root         INFO     Train Epoch: 37 [3072/8000 (38%)]\tTotal Loss: 3.909799\n",
      "Reconstruction: 3.338349, Regularization: 0.496983, Discriminator: 0.047761; Generator: 0.026706,\n",
      "D(x): 0.378, D(G(z)): 0.425\n",
      "2019-04-09 23:58:37,029 root         INFO     Train Epoch: 37 [3584/8000 (45%)]\tTotal Loss: 4.615089\n",
      "Reconstruction: 4.016424, Regularization: 0.524162, Discriminator: 0.047797; Generator: 0.026705,\n",
      "D(x): 0.377, D(G(z)): 0.425\n",
      "2019-04-09 23:58:37,140 root         INFO     Train Epoch: 37 [4096/8000 (51%)]\tTotal Loss: 3.533875\n",
      "Reconstruction: 2.937451, Regularization: 0.521807, Discriminator: 0.047890; Generator: 0.026727,\n",
      "D(x): 0.376, D(G(z)): 0.425\n",
      "2019-04-09 23:58:37,251 root         INFO     Train Epoch: 37 [4608/8000 (58%)]\tTotal Loss: 3.696317\n",
      "Reconstruction: 3.116805, Regularization: 0.505580, Discriminator: 0.047240; Generator: 0.026692,\n",
      "D(x): 0.384, D(G(z)): 0.426\n",
      "2019-04-09 23:58:37,362 root         INFO     Train Epoch: 37 [5120/8000 (64%)]\tTotal Loss: 4.906171\n",
      "Reconstruction: 4.264468, Regularization: 0.567852, Discriminator: 0.047136; Generator: 0.026716,\n",
      "D(x): 0.385, D(G(z)): 0.425\n",
      "2019-04-09 23:58:37,473 root         INFO     Train Epoch: 37 [5632/8000 (70%)]\tTotal Loss: 3.693519\n",
      "Reconstruction: 3.051342, Regularization: 0.568231, Discriminator: 0.047132; Generator: 0.026813,\n",
      "D(x): 0.384, D(G(z)): 0.424\n",
      "2019-04-09 23:58:37,585 root         INFO     Train Epoch: 37 [6144/8000 (77%)]\tTotal Loss: 2.749504\n",
      "Reconstruction: 2.155532, Regularization: 0.519902, Discriminator: 0.047343; Generator: 0.026727,\n",
      "D(x): 0.383, D(G(z)): 0.425\n",
      "2019-04-09 23:58:37,696 root         INFO     Train Epoch: 37 [6656/8000 (83%)]\tTotal Loss: 3.018211\n",
      "Reconstruction: 2.409188, Regularization: 0.535010, Discriminator: 0.047178; Generator: 0.026834,\n",
      "D(x): 0.384, D(G(z)): 0.424\n",
      "2019-04-09 23:58:37,808 root         INFO     Train Epoch: 37 [7168/8000 (90%)]\tTotal Loss: 2.434803\n",
      "Reconstruction: 1.830894, Regularization: 0.529895, Discriminator: 0.047233; Generator: 0.026780,\n",
      "D(x): 0.383, D(G(z)): 0.424\n",
      "2019-04-09 23:58:37,919 root         INFO     Train Epoch: 37 [7680/8000 (96%)]\tTotal Loss: 3.691937\n",
      "Reconstruction: 3.015928, Regularization: 0.602117, Discriminator: 0.046956; Generator: 0.026936,\n",
      "D(x): 0.385, D(G(z)): 0.422\n",
      "2019-04-09 23:58:38,000 root         INFO     ====> Epoch: 37 Average loss: 3.6622\n",
      "2019-04-09 23:58:38,027 root         INFO     Train Epoch: 38 [0/8000 (0%)]\tTotal Loss: 6.588969\n",
      "Reconstruction: 5.988790, Regularization: 0.525967, Discriminator: 0.047261; Generator: 0.026952,\n",
      "D(x): 0.382, D(G(z)): 0.422\n",
      "2019-04-09 23:58:38,139 root         INFO     Train Epoch: 38 [512/8000 (6%)]\tTotal Loss: 3.777357\n",
      "Reconstruction: 3.113845, Regularization: 0.589495, Discriminator: 0.047110; Generator: 0.026907,\n",
      "D(x): 0.384, D(G(z)): 0.423\n",
      "2019-04-09 23:58:38,249 root         INFO     Train Epoch: 38 [1024/8000 (13%)]\tTotal Loss: 3.234275\n",
      "Reconstruction: 2.628895, Regularization: 0.531531, Discriminator: 0.046930; Generator: 0.026920,\n",
      "D(x): 0.386, D(G(z)): 0.423\n",
      "2019-04-09 23:58:38,360 root         INFO     Train Epoch: 38 [1536/8000 (19%)]\tTotal Loss: 2.908704\n",
      "Reconstruction: 2.276228, Regularization: 0.558545, Discriminator: 0.046857; Generator: 0.027074,\n",
      "D(x): 0.385, D(G(z)): 0.420\n",
      "2019-04-09 23:58:38,471 root         INFO     Train Epoch: 38 [2048/8000 (26%)]\tTotal Loss: 3.140798\n",
      "Reconstruction: 2.481797, Regularization: 0.585335, Discriminator: 0.046621; Generator: 0.027044,\n",
      "D(x): 0.389, D(G(z)): 0.421\n",
      "2019-04-09 23:58:38,581 root         INFO     Train Epoch: 38 [2560/8000 (32%)]\tTotal Loss: 2.782392\n",
      "Reconstruction: 2.192766, Regularization: 0.515861, Discriminator: 0.046721; Generator: 0.027044,\n",
      "D(x): 0.387, D(G(z)): 0.421\n",
      "2019-04-09 23:58:38,692 root         INFO     Train Epoch: 38 [3072/8000 (38%)]\tTotal Loss: 2.323736\n",
      "Reconstruction: 1.729485, Regularization: 0.520704, Discriminator: 0.046511; Generator: 0.027037,\n",
      "D(x): 0.390, D(G(z)): 0.421\n",
      "2019-04-09 23:58:38,801 root         INFO     Train Epoch: 38 [3584/8000 (45%)]\tTotal Loss: 3.620642\n",
      "Reconstruction: 3.070034, Regularization: 0.476879, Discriminator: 0.046623; Generator: 0.027106,\n",
      "D(x): 0.388, D(G(z)): 0.420\n",
      "2019-04-09 23:58:38,909 root         INFO     Train Epoch: 38 [4096/8000 (51%)]\tTotal Loss: 3.805195\n",
      "Reconstruction: 3.215127, Regularization: 0.516699, Discriminator: 0.046229; Generator: 0.027139,\n",
      "D(x): 0.393, D(G(z)): 0.420\n",
      "2019-04-09 23:58:39,017 root         INFO     Train Epoch: 38 [4608/8000 (58%)]\tTotal Loss: 2.859049\n",
      "Reconstruction: 2.282353, Regularization: 0.503199, Discriminator: 0.046322; Generator: 0.027174,\n",
      "D(x): 0.391, D(G(z)): 0.419\n",
      "2019-04-09 23:58:39,125 root         INFO     Train Epoch: 38 [5120/8000 (64%)]\tTotal Loss: 2.850414\n",
      "Reconstruction: 2.235357, Regularization: 0.541550, Discriminator: 0.046329; Generator: 0.027178,\n",
      "D(x): 0.391, D(G(z)): 0.419\n",
      "2019-04-09 23:58:39,233 root         INFO     Train Epoch: 38 [5632/8000 (70%)]\tTotal Loss: 3.848607\n",
      "Reconstruction: 3.244520, Regularization: 0.530567, Discriminator: 0.046366; Generator: 0.027153,\n",
      "D(x): 0.391, D(G(z)): 0.419\n",
      "2019-04-09 23:58:39,341 root         INFO     Train Epoch: 38 [6144/8000 (77%)]\tTotal Loss: 5.004467\n",
      "Reconstruction: 4.411134, Regularization: 0.519916, Discriminator: 0.046134; Generator: 0.027283,\n",
      "D(x): 0.392, D(G(z)): 0.418\n",
      "2019-04-09 23:58:39,450 root         INFO     Train Epoch: 38 [6656/8000 (83%)]\tTotal Loss: 2.630264\n",
      "Reconstruction: 2.053163, Regularization: 0.503663, Discriminator: 0.046173; Generator: 0.027265,\n",
      "D(x): 0.392, D(G(z)): 0.418\n",
      "2019-04-09 23:58:39,558 root         INFO     Train Epoch: 38 [7168/8000 (90%)]\tTotal Loss: 3.734471\n",
      "Reconstruction: 3.131163, Regularization: 0.530010, Discriminator: 0.046016; Generator: 0.027282,\n",
      "D(x): 0.394, D(G(z)): 0.418\n",
      "2019-04-09 23:58:39,666 root         INFO     Train Epoch: 38 [7680/8000 (96%)]\tTotal Loss: 6.845481\n",
      "Reconstruction: 6.234255, Regularization: 0.537838, Discriminator: 0.046113; Generator: 0.027275,\n",
      "D(x): 0.393, D(G(z)): 0.418\n",
      "2019-04-09 23:58:39,745 root         INFO     ====> Epoch: 38 Average loss: 3.6835\n",
      "2019-04-09 23:58:39,772 root         INFO     Train Epoch: 39 [0/8000 (0%)]\tTotal Loss: 2.796482\n",
      "Reconstruction: 2.171326, Regularization: 0.551800, Discriminator: 0.046087; Generator: 0.027269,\n",
      "D(x): 0.393, D(G(z)): 0.418\n",
      "2019-04-09 23:58:39,880 root         INFO     Train Epoch: 39 [512/8000 (6%)]\tTotal Loss: 2.959164\n",
      "Reconstruction: 2.284743, Regularization: 0.601186, Discriminator: 0.045913; Generator: 0.027322,\n",
      "D(x): 0.395, D(G(z)): 0.417\n",
      "2019-04-09 23:58:39,986 root         INFO     Train Epoch: 39 [1024/8000 (13%)]\tTotal Loss: 2.918019\n",
      "Reconstruction: 2.256135, Regularization: 0.588719, Discriminator: 0.045821; Generator: 0.027345,\n",
      "D(x): 0.396, D(G(z)): 0.417\n",
      "2019-04-09 23:58:40,093 root         INFO     Train Epoch: 39 [1536/8000 (19%)]\tTotal Loss: 2.927035\n",
      "Reconstruction: 2.307938, Regularization: 0.545818, Discriminator: 0.045922; Generator: 0.027358,\n",
      "D(x): 0.394, D(G(z)): 0.417\n",
      "2019-04-09 23:58:40,200 root         INFO     Train Epoch: 39 [2048/8000 (26%)]\tTotal Loss: 2.734385\n",
      "Reconstruction: 2.094637, Regularization: 0.566558, Discriminator: 0.045761; Generator: 0.027430,\n",
      "D(x): 0.396, D(G(z)): 0.416\n",
      "2019-04-09 23:58:40,307 root         INFO     Train Epoch: 39 [2560/8000 (32%)]\tTotal Loss: 4.482974\n",
      "Reconstruction: 3.841064, Regularization: 0.568772, Discriminator: 0.045681; Generator: 0.027456,\n",
      "D(x): 0.397, D(G(z)): 0.415\n",
      "2019-04-09 23:58:40,413 root         INFO     Train Epoch: 39 [3072/8000 (38%)]\tTotal Loss: 3.318198\n",
      "Reconstruction: 2.691846, Regularization: 0.553268, Discriminator: 0.045666; Generator: 0.027418,\n",
      "D(x): 0.397, D(G(z)): 0.416\n",
      "2019-04-09 23:58:40,520 root         INFO     Train Epoch: 39 [3584/8000 (45%)]\tTotal Loss: 3.005729\n",
      "Reconstruction: 2.332182, Regularization: 0.600575, Discriminator: 0.045523; Generator: 0.027449,\n",
      "D(x): 0.399, D(G(z)): 0.415\n",
      "2019-04-09 23:58:40,626 root         INFO     Train Epoch: 39 [4096/8000 (51%)]\tTotal Loss: 3.603987\n",
      "Reconstruction: 2.956639, Regularization: 0.574407, Discriminator: 0.045495; Generator: 0.027447,\n",
      "D(x): 0.399, D(G(z)): 0.415\n",
      "2019-04-09 23:58:40,733 root         INFO     Train Epoch: 39 [4608/8000 (58%)]\tTotal Loss: 4.482351\n",
      "Reconstruction: 3.825520, Regularization: 0.583946, Discriminator: 0.045445; Generator: 0.027440,\n",
      "D(x): 0.400, D(G(z)): 0.416\n",
      "2019-04-09 23:58:40,841 root         INFO     Train Epoch: 39 [5120/8000 (64%)]\tTotal Loss: 2.779491\n",
      "Reconstruction: 2.231977, Regularization: 0.474669, Discriminator: 0.045350; Generator: 0.027495,\n",
      "D(x): 0.400, D(G(z)): 0.415\n",
      "2019-04-09 23:58:40,947 root         INFO     Train Epoch: 39 [5632/8000 (70%)]\tTotal Loss: 2.865118\n",
      "Reconstruction: 2.214743, Regularization: 0.577450, Discriminator: 0.045405; Generator: 0.027520,\n",
      "D(x): 0.399, D(G(z)): 0.415\n",
      "2019-04-09 23:58:41,054 root         INFO     Train Epoch: 39 [6144/8000 (77%)]\tTotal Loss: 8.685330\n",
      "Reconstruction: 8.123123, Regularization: 0.489389, Discriminator: 0.045331; Generator: 0.027487,\n",
      "D(x): 0.401, D(G(z)): 0.415\n",
      "2019-04-09 23:58:41,161 root         INFO     Train Epoch: 39 [6656/8000 (83%)]\tTotal Loss: 3.058633\n",
      "Reconstruction: 2.449275, Regularization: 0.536505, Discriminator: 0.045302; Generator: 0.027551,\n",
      "D(x): 0.401, D(G(z)): 0.414\n",
      "2019-04-09 23:58:41,267 root         INFO     Train Epoch: 39 [7168/8000 (90%)]\tTotal Loss: 4.220960\n",
      "Reconstruction: 3.600054, Regularization: 0.548153, Discriminator: 0.045189; Generator: 0.027564,\n",
      "D(x): 0.402, D(G(z)): 0.414\n",
      "2019-04-09 23:58:41,377 root         INFO     Train Epoch: 39 [7680/8000 (96%)]\tTotal Loss: 3.531078\n",
      "Reconstruction: 2.841388, Regularization: 0.616937, Discriminator: 0.045149; Generator: 0.027604,\n",
      "D(x): 0.402, D(G(z)): 0.413\n",
      "2019-04-09 23:58:41,456 root         INFO     ====> Epoch: 39 Average loss: 3.7552\n",
      "2019-04-09 23:58:41,483 root         INFO     Train Epoch: 40 [0/8000 (0%)]\tTotal Loss: 2.727612\n",
      "Reconstruction: 2.147094, Regularization: 0.507796, Discriminator: 0.045119; Generator: 0.027602,\n",
      "D(x): 0.402, D(G(z)): 0.413\n",
      "2019-04-09 23:58:41,593 root         INFO     Train Epoch: 40 [512/8000 (6%)]\tTotal Loss: 3.431687\n",
      "Reconstruction: 2.814356, Regularization: 0.544632, Discriminator: 0.045071; Generator: 0.027627,\n",
      "D(x): 0.403, D(G(z)): 0.413\n",
      "2019-04-09 23:58:41,699 root         INFO     Train Epoch: 40 [1024/8000 (13%)]\tTotal Loss: 5.533744\n",
      "Reconstruction: 4.851346, Regularization: 0.609827, Discriminator: 0.044928; Generator: 0.027643,\n",
      "D(x): 0.404, D(G(z)): 0.413\n",
      "2019-04-09 23:58:41,806 root         INFO     Train Epoch: 40 [1536/8000 (19%)]\tTotal Loss: 4.688685\n",
      "Reconstruction: 4.016350, Regularization: 0.599777, Discriminator: 0.044882; Generator: 0.027676,\n",
      "D(x): 0.405, D(G(z)): 0.412\n",
      "2019-04-09 23:58:41,913 root         INFO     Train Epoch: 40 [2048/8000 (26%)]\tTotal Loss: 3.222562\n",
      "Reconstruction: 2.587208, Regularization: 0.562817, Discriminator: 0.044850; Generator: 0.027687,\n",
      "D(x): 0.405, D(G(z)): 0.412\n",
      "2019-04-09 23:58:42,019 root         INFO     Train Epoch: 40 [2560/8000 (32%)]\tTotal Loss: 3.384082\n",
      "Reconstruction: 2.740796, Regularization: 0.570780, Discriminator: 0.044810; Generator: 0.027696,\n",
      "D(x): 0.406, D(G(z)): 0.412\n",
      "2019-04-09 23:58:42,125 root         INFO     Train Epoch: 40 [3072/8000 (38%)]\tTotal Loss: 3.077882\n",
      "Reconstruction: 2.402520, Regularization: 0.602899, Discriminator: 0.044739; Generator: 0.027724,\n",
      "D(x): 0.406, D(G(z)): 0.412\n",
      "2019-04-09 23:58:42,231 root         INFO     Train Epoch: 40 [3584/8000 (45%)]\tTotal Loss: 4.118085\n",
      "Reconstruction: 3.372024, Regularization: 0.673595, Discriminator: 0.044734; Generator: 0.027731,\n",
      "D(x): 0.406, D(G(z)): 0.412\n",
      "2019-04-09 23:58:42,337 root         INFO     Train Epoch: 40 [4096/8000 (51%)]\tTotal Loss: 3.713347\n",
      "Reconstruction: 3.083899, Regularization: 0.557032, Discriminator: 0.044656; Generator: 0.027760,\n",
      "D(x): 0.407, D(G(z)): 0.411\n",
      "2019-04-09 23:58:42,444 root         INFO     Train Epoch: 40 [4608/8000 (58%)]\tTotal Loss: 3.326660\n",
      "Reconstruction: 2.675623, Regularization: 0.578639, Discriminator: 0.044628; Generator: 0.027770,\n",
      "D(x): 0.407, D(G(z)): 0.411\n",
      "2019-04-09 23:58:42,549 root         INFO     Train Epoch: 40 [5120/8000 (64%)]\tTotal Loss: 2.811211\n",
      "Reconstruction: 2.105828, Regularization: 0.633022, Discriminator: 0.044579; Generator: 0.027782,\n",
      "D(x): 0.408, D(G(z)): 0.411\n",
      "2019-04-09 23:58:42,656 root         INFO     Train Epoch: 40 [5632/8000 (70%)]\tTotal Loss: 4.362187\n",
      "Reconstruction: 3.658044, Regularization: 0.631836, Discriminator: 0.044514; Generator: 0.027792,\n",
      "D(x): 0.409, D(G(z)): 0.411\n",
      "2019-04-09 23:58:42,761 root         INFO     Train Epoch: 40 [6144/8000 (77%)]\tTotal Loss: 8.007003\n",
      "Reconstruction: 7.317877, Regularization: 0.616851, Discriminator: 0.044469; Generator: 0.027805,\n",
      "D(x): 0.409, D(G(z)): 0.411\n",
      "2019-04-09 23:58:42,867 root         INFO     Train Epoch: 40 [6656/8000 (83%)]\tTotal Loss: 3.242901\n",
      "Reconstruction: 2.616075, Regularization: 0.554591, Discriminator: 0.044413; Generator: 0.027822,\n",
      "D(x): 0.410, D(G(z)): 0.411\n",
      "2019-04-09 23:58:42,972 root         INFO     Train Epoch: 40 [7168/8000 (90%)]\tTotal Loss: 8.657471\n",
      "Reconstruction: 7.954246, Regularization: 0.631023, Discriminator: 0.044365; Generator: 0.027836,\n",
      "D(x): 0.410, D(G(z)): 0.410\n",
      "2019-04-09 23:58:43,080 root         INFO     Train Epoch: 40 [7680/8000 (96%)]\tTotal Loss: 3.358871\n",
      "Reconstruction: 2.632638, Regularization: 0.654060, Discriminator: 0.044320; Generator: 0.027852,\n",
      "D(x): 0.410, D(G(z)): 0.410\n",
      "2019-04-09 23:58:43,161 root         INFO     ====> Epoch: 40 Average loss: 3.6323\n",
      "2019-04-09 23:58:43,188 root         INFO     Train Epoch: 41 [0/8000 (0%)]\tTotal Loss: 3.185468\n",
      "Reconstruction: 2.510110, Regularization: 0.603210, Discriminator: 0.044287; Generator: 0.027860,\n",
      "D(x): 0.411, D(G(z)): 0.410\n",
      "2019-04-09 23:58:43,300 root         INFO     Train Epoch: 41 [512/8000 (6%)]\tTotal Loss: 3.652161\n",
      "Reconstruction: 3.034997, Regularization: 0.545052, Discriminator: 0.044235; Generator: 0.027878,\n",
      "D(x): 0.411, D(G(z)): 0.410\n",
      "2019-04-09 23:58:43,411 root         INFO     Train Epoch: 41 [1024/8000 (13%)]\tTotal Loss: 3.787803\n",
      "Reconstruction: 3.079356, Regularization: 0.636367, Discriminator: 0.044180; Generator: 0.027899,\n",
      "D(x): 0.412, D(G(z)): 0.410\n",
      "2019-04-09 23:58:43,523 root         INFO     Train Epoch: 41 [1536/8000 (19%)]\tTotal Loss: 2.604002\n",
      "Reconstruction: 1.928301, Regularization: 0.603668, Discriminator: 0.044127; Generator: 0.027907,\n",
      "D(x): 0.413, D(G(z)): 0.409\n",
      "2019-04-09 23:58:43,634 root         INFO     Train Epoch: 41 [2048/8000 (26%)]\tTotal Loss: 2.832508\n",
      "Reconstruction: 2.129879, Regularization: 0.630602, Discriminator: 0.044104; Generator: 0.027923,\n",
      "D(x): 0.413, D(G(z)): 0.409\n",
      "2019-04-09 23:58:43,746 root         INFO     Train Epoch: 41 [2560/8000 (32%)]\tTotal Loss: 3.663442\n",
      "Reconstruction: 2.976341, Regularization: 0.615107, Discriminator: 0.044059; Generator: 0.027934,\n",
      "D(x): 0.413, D(G(z)): 0.409\n",
      "2019-04-09 23:58:43,858 root         INFO     Train Epoch: 41 [3072/8000 (38%)]\tTotal Loss: 4.436288\n",
      "Reconstruction: 3.726217, Regularization: 0.638099, Discriminator: 0.044018; Generator: 0.027954,\n",
      "D(x): 0.414, D(G(z)): 0.409\n",
      "2019-04-09 23:58:43,969 root         INFO     Train Epoch: 41 [3584/8000 (45%)]\tTotal Loss: 3.296878\n",
      "Reconstruction: 2.604617, Regularization: 0.620319, Discriminator: 0.043981; Generator: 0.027960,\n",
      "D(x): 0.414, D(G(z)): 0.409\n",
      "2019-04-09 23:58:44,081 root         INFO     Train Epoch: 41 [4096/8000 (51%)]\tTotal Loss: 3.435909\n",
      "Reconstruction: 2.704355, Regularization: 0.659615, Discriminator: 0.043962; Generator: 0.027976,\n",
      "D(x): 0.414, D(G(z)): 0.409\n",
      "2019-04-09 23:58:44,191 root         INFO     Train Epoch: 41 [4608/8000 (58%)]\tTotal Loss: 2.904491\n",
      "Reconstruction: 2.226244, Regularization: 0.606404, Discriminator: 0.043852; Generator: 0.027991,\n",
      "D(x): 0.415, D(G(z)): 0.408\n",
      "2019-04-09 23:58:44,301 root         INFO     Train Epoch: 41 [5120/8000 (64%)]\tTotal Loss: 3.100959\n",
      "Reconstruction: 2.438650, Regularization: 0.590503, Discriminator: 0.043798; Generator: 0.028009,\n",
      "D(x): 0.416, D(G(z)): 0.408\n",
      "2019-04-09 23:58:44,410 root         INFO     Train Epoch: 41 [5632/8000 (70%)]\tTotal Loss: 2.986460\n",
      "Reconstruction: 2.298682, Regularization: 0.615980, Discriminator: 0.043800; Generator: 0.027998,\n",
      "D(x): 0.416, D(G(z)): 0.408\n",
      "2019-04-09 23:58:44,520 root         INFO     Train Epoch: 41 [6144/8000 (77%)]\tTotal Loss: 2.635109\n",
      "Reconstruction: 2.015991, Regularization: 0.547327, Discriminator: 0.043768; Generator: 0.028024,\n",
      "D(x): 0.416, D(G(z)): 0.408\n",
      "2019-04-09 23:58:44,630 root         INFO     Train Epoch: 41 [6656/8000 (83%)]\tTotal Loss: 3.642662\n",
      "Reconstruction: 2.964386, Regularization: 0.606546, Discriminator: 0.043659; Generator: 0.028072,\n",
      "D(x): 0.417, D(G(z)): 0.407\n",
      "2019-04-09 23:58:44,740 root         INFO     Train Epoch: 41 [7168/8000 (90%)]\tTotal Loss: 3.225806\n",
      "Reconstruction: 2.485256, Regularization: 0.668807, Discriminator: 0.043644; Generator: 0.028099,\n",
      "D(x): 0.417, D(G(z)): 0.407\n",
      "2019-04-09 23:58:44,850 root         INFO     Train Epoch: 41 [7680/8000 (96%)]\tTotal Loss: 2.997784\n",
      "Reconstruction: 2.308398, Regularization: 0.617651, Discriminator: 0.043661; Generator: 0.028073,\n",
      "D(x): 0.417, D(G(z)): 0.407\n",
      "2019-04-09 23:58:44,930 root         INFO     ====> Epoch: 41 Average loss: 3.6068\n",
      "2019-04-09 23:58:44,957 root         INFO     Train Epoch: 42 [0/8000 (0%)]\tTotal Loss: 2.691122\n",
      "Reconstruction: 2.017260, Regularization: 0.602239, Discriminator: 0.043529; Generator: 0.028095,\n",
      "D(x): 0.419, D(G(z)): 0.407\n",
      "2019-04-09 23:58:45,068 root         INFO     Train Epoch: 42 [512/8000 (6%)]\tTotal Loss: 2.435128\n",
      "Reconstruction: 1.763389, Regularization: 0.600110, Discriminator: 0.043502; Generator: 0.028127,\n",
      "D(x): 0.419, D(G(z)): 0.407\n",
      "2019-04-09 23:58:45,178 root         INFO     Train Epoch: 42 [1024/8000 (13%)]\tTotal Loss: 2.952047\n",
      "Reconstruction: 2.277265, Regularization: 0.603152, Discriminator: 0.043487; Generator: 0.028143,\n",
      "D(x): 0.419, D(G(z)): 0.406\n",
      "2019-04-09 23:58:45,287 root         INFO     Train Epoch: 42 [1536/8000 (19%)]\tTotal Loss: 3.090806\n",
      "Reconstruction: 2.410030, Regularization: 0.609126, Discriminator: 0.043500; Generator: 0.028151,\n",
      "D(x): 0.419, D(G(z)): 0.406\n",
      "2019-04-09 23:58:45,397 root         INFO     Train Epoch: 42 [2048/8000 (26%)]\tTotal Loss: 3.780275\n",
      "Reconstruction: 3.060320, Regularization: 0.648364, Discriminator: 0.043401; Generator: 0.028190,\n",
      "D(x): 0.420, D(G(z)): 0.406\n",
      "2019-04-09 23:58:45,506 root         INFO     Train Epoch: 42 [2560/8000 (32%)]\tTotal Loss: 3.352120\n",
      "Reconstruction: 2.566852, Regularization: 0.713691, Discriminator: 0.043409; Generator: 0.028169,\n",
      "D(x): 0.420, D(G(z)): 0.406\n",
      "2019-04-09 23:58:45,616 root         INFO     Train Epoch: 42 [3072/8000 (38%)]\tTotal Loss: 3.111126\n",
      "Reconstruction: 2.403434, Regularization: 0.636153, Discriminator: 0.043325; Generator: 0.028214,\n",
      "D(x): 0.420, D(G(z)): 0.405\n",
      "2019-04-09 23:58:45,725 root         INFO     Train Epoch: 42 [3584/8000 (45%)]\tTotal Loss: 3.803473\n",
      "Reconstruction: 3.097253, Regularization: 0.634760, Discriminator: 0.043245; Generator: 0.028216,\n",
      "D(x): 0.421, D(G(z)): 0.405\n",
      "2019-04-09 23:58:45,835 root         INFO     Train Epoch: 42 [4096/8000 (51%)]\tTotal Loss: 4.325178\n",
      "Reconstruction: 3.554180, Regularization: 0.699571, Discriminator: 0.043215; Generator: 0.028212,\n",
      "D(x): 0.422, D(G(z)): 0.405\n",
      "2019-04-09 23:58:45,944 root         INFO     Train Epoch: 42 [4608/8000 (58%)]\tTotal Loss: 3.362467\n",
      "Reconstruction: 2.641493, Regularization: 0.649516, Discriminator: 0.043219; Generator: 0.028239,\n",
      "D(x): 0.422, D(G(z)): 0.405\n",
      "2019-04-09 23:58:46,054 root         INFO     Train Epoch: 42 [5120/8000 (64%)]\tTotal Loss: 3.581350\n",
      "Reconstruction: 2.749253, Regularization: 0.760618, Discriminator: 0.043194; Generator: 0.028284,\n",
      "D(x): 0.422, D(G(z)): 0.405\n",
      "2019-04-09 23:58:46,163 root         INFO     Train Epoch: 42 [5632/8000 (70%)]\tTotal Loss: 3.446366\n",
      "Reconstruction: 2.761375, Regularization: 0.613573, Discriminator: 0.043149; Generator: 0.028268,\n",
      "D(x): 0.422, D(G(z)): 0.405\n",
      "2019-04-09 23:58:46,272 root         INFO     Train Epoch: 42 [6144/8000 (77%)]\tTotal Loss: 2.657813\n",
      "Reconstruction: 1.959227, Regularization: 0.627197, Discriminator: 0.043087; Generator: 0.028302,\n",
      "D(x): 0.423, D(G(z)): 0.404\n",
      "2019-04-09 23:58:46,382 root         INFO     Train Epoch: 42 [6656/8000 (83%)]\tTotal Loss: 3.401339\n",
      "Reconstruction: 2.677577, Regularization: 0.652326, Discriminator: 0.043138; Generator: 0.028298,\n",
      "D(x): 0.422, D(G(z)): 0.404\n",
      "2019-04-09 23:58:46,491 root         INFO     Train Epoch: 42 [7168/8000 (90%)]\tTotal Loss: 2.944173\n",
      "Reconstruction: 2.249490, Regularization: 0.623371, Discriminator: 0.043007; Generator: 0.028304,\n",
      "D(x): 0.424, D(G(z)): 0.404\n",
      "2019-04-09 23:58:46,602 root         INFO     Train Epoch: 42 [7680/8000 (96%)]\tTotal Loss: 3.450215\n",
      "Reconstruction: 2.738319, Regularization: 0.640574, Discriminator: 0.042945; Generator: 0.028377,\n",
      "D(x): 0.424, D(G(z)): 0.403\n",
      "2019-04-09 23:58:46,682 root         INFO     ====> Epoch: 42 Average loss: 3.5459\n",
      "2019-04-09 23:58:46,709 root         INFO     Train Epoch: 43 [0/8000 (0%)]\tTotal Loss: 2.840221\n",
      "Reconstruction: 2.147091, Regularization: 0.621839, Discriminator: 0.042911; Generator: 0.028379,\n",
      "D(x): 0.425, D(G(z)): 0.403\n",
      "2019-04-09 23:58:46,820 root         INFO     Train Epoch: 43 [512/8000 (6%)]\tTotal Loss: 3.827540\n",
      "Reconstruction: 3.069116, Regularization: 0.687118, Discriminator: 0.042920; Generator: 0.028386,\n",
      "D(x): 0.424, D(G(z)): 0.403\n",
      "2019-04-09 23:58:46,931 root         INFO     Train Epoch: 43 [1024/8000 (13%)]\tTotal Loss: 3.052904\n",
      "Reconstruction: 2.292521, Regularization: 0.689012, Discriminator: 0.042977; Generator: 0.028393,\n",
      "D(x): 0.424, D(G(z)): 0.403\n",
      "2019-04-09 23:58:47,041 root         INFO     Train Epoch: 43 [1536/8000 (19%)]\tTotal Loss: 3.310015\n",
      "Reconstruction: 2.592007, Regularization: 0.646806, Discriminator: 0.042791; Generator: 0.028411,\n",
      "D(x): 0.426, D(G(z)): 0.403\n",
      "2019-04-09 23:58:47,151 root         INFO     Train Epoch: 43 [2048/8000 (26%)]\tTotal Loss: 3.686103\n",
      "Reconstruction: 2.919065, Regularization: 0.695786, Discriminator: 0.042869; Generator: 0.028383,\n",
      "D(x): 0.425, D(G(z)): 0.403\n",
      "2019-04-09 23:58:47,262 root         INFO     Train Epoch: 43 [2560/8000 (32%)]\tTotal Loss: 3.788365\n",
      "Reconstruction: 3.032940, Regularization: 0.684050, Discriminator: 0.042925; Generator: 0.028449,\n",
      "D(x): 0.424, D(G(z)): 0.402\n",
      "2019-04-09 23:58:47,372 root         INFO     Train Epoch: 43 [3072/8000 (38%)]\tTotal Loss: 2.937039\n",
      "Reconstruction: 2.196412, Regularization: 0.669331, Discriminator: 0.042834; Generator: 0.028463,\n",
      "D(x): 0.425, D(G(z)): 0.402\n",
      "2019-04-09 23:58:47,483 root         INFO     Train Epoch: 43 [3584/8000 (45%)]\tTotal Loss: 2.987241\n",
      "Reconstruction: 2.266521, Regularization: 0.649593, Discriminator: 0.042643; Generator: 0.028483,\n",
      "D(x): 0.427, D(G(z)): 0.402\n",
      "2019-04-09 23:58:47,594 root         INFO     Train Epoch: 43 [4096/8000 (51%)]\tTotal Loss: 2.689288\n",
      "Reconstruction: 1.943172, Regularization: 0.675094, Discriminator: 0.042522; Generator: 0.028500,\n",
      "D(x): 0.429, D(G(z)): 0.402\n",
      "2019-04-09 23:58:47,705 root         INFO     Train Epoch: 43 [4608/8000 (58%)]\tTotal Loss: 3.538255\n",
      "Reconstruction: 2.773695, Regularization: 0.693378, Discriminator: 0.042713; Generator: 0.028469,\n",
      "D(x): 0.426, D(G(z)): 0.402\n",
      "2019-04-09 23:58:47,815 root         INFO     Train Epoch: 43 [5120/8000 (64%)]\tTotal Loss: 2.984743\n",
      "Reconstruction: 2.294484, Regularization: 0.619180, Discriminator: 0.042475; Generator: 0.028605,\n",
      "D(x): 0.428, D(G(z)): 0.400\n",
      "2019-04-09 23:58:47,926 root         INFO     Train Epoch: 43 [5632/8000 (70%)]\tTotal Loss: 2.526047\n",
      "Reconstruction: 1.834129, Regularization: 0.620865, Discriminator: 0.042445; Generator: 0.028608,\n",
      "D(x): 0.429, D(G(z)): 0.400\n",
      "2019-04-09 23:58:48,035 root         INFO     Train Epoch: 43 [6144/8000 (77%)]\tTotal Loss: 3.101569\n",
      "Reconstruction: 2.388035, Regularization: 0.642291, Discriminator: 0.042650; Generator: 0.028592,\n",
      "D(x): 0.426, D(G(z)): 0.401\n",
      "2019-04-09 23:58:48,143 root         INFO     Train Epoch: 43 [6656/8000 (83%)]\tTotal Loss: 3.027872\n",
      "Reconstruction: 2.254795, Regularization: 0.702095, Discriminator: 0.042353; Generator: 0.028629,\n",
      "D(x): 0.430, D(G(z)): 0.400\n",
      "2019-04-09 23:58:48,251 root         INFO     Train Epoch: 43 [7168/8000 (90%)]\tTotal Loss: 3.314239\n",
      "Reconstruction: 2.591220, Regularization: 0.651908, Discriminator: 0.042481; Generator: 0.028630,\n",
      "D(x): 0.428, D(G(z)): 0.400\n",
      "2019-04-09 23:58:48,360 root         INFO     Train Epoch: 43 [7680/8000 (96%)]\tTotal Loss: 2.856465\n",
      "Reconstruction: 2.142883, Regularization: 0.642648, Discriminator: 0.042366; Generator: 0.028569,\n",
      "D(x): 0.430, D(G(z)): 0.401\n",
      "2019-04-09 23:58:48,439 root         INFO     ====> Epoch: 43 Average loss: 3.4894\n",
      "2019-04-09 23:58:48,466 root         INFO     Train Epoch: 44 [0/8000 (0%)]\tTotal Loss: 5.024871\n",
      "Reconstruction: 4.383986, Regularization: 0.569785, Discriminator: 0.042467; Generator: 0.028634,\n",
      "D(x): 0.428, D(G(z)): 0.400\n",
      "2019-04-09 23:58:48,573 root         INFO     Train Epoch: 44 [512/8000 (6%)]\tTotal Loss: 2.498912\n",
      "Reconstruction: 1.788075, Regularization: 0.639873, Discriminator: 0.042328; Generator: 0.028635,\n",
      "D(x): 0.430, D(G(z)): 0.400\n",
      "2019-04-09 23:58:48,681 root         INFO     Train Epoch: 44 [1024/8000 (13%)]\tTotal Loss: 4.455824\n",
      "Reconstruction: 3.653926, Regularization: 0.730759, Discriminator: 0.042455; Generator: 0.028684,\n",
      "D(x): 0.428, D(G(z)): 0.399\n",
      "2019-04-09 23:58:48,787 root         INFO     Train Epoch: 44 [1536/8000 (19%)]\tTotal Loss: 2.815816\n",
      "Reconstruction: 2.021038, Regularization: 0.723803, Discriminator: 0.042202; Generator: 0.028773,\n",
      "D(x): 0.431, D(G(z)): 0.398\n",
      "2019-04-09 23:58:48,894 root         INFO     Train Epoch: 44 [2048/8000 (26%)]\tTotal Loss: 3.051716\n",
      "Reconstruction: 2.338960, Regularization: 0.641653, Discriminator: 0.042345; Generator: 0.028759,\n",
      "D(x): 0.429, D(G(z)): 0.398\n",
      "2019-04-09 23:58:49,001 root         INFO     Train Epoch: 44 [2560/8000 (32%)]\tTotal Loss: 3.172044\n",
      "Reconstruction: 2.403335, Regularization: 0.697541, Discriminator: 0.042403; Generator: 0.028765,\n",
      "D(x): 0.428, D(G(z)): 0.398\n",
      "2019-04-09 23:58:49,108 root         INFO     Train Epoch: 44 [3072/8000 (38%)]\tTotal Loss: 2.699955\n",
      "Reconstruction: 1.975069, Regularization: 0.653827, Discriminator: 0.042274; Generator: 0.028784,\n",
      "D(x): 0.430, D(G(z)): 0.398\n",
      "2019-04-09 23:58:49,215 root         INFO     Train Epoch: 44 [3584/8000 (45%)]\tTotal Loss: 2.917066\n",
      "Reconstruction: 2.212854, Regularization: 0.633270, Discriminator: 0.042130; Generator: 0.028812,\n",
      "D(x): 0.431, D(G(z)): 0.398\n",
      "2019-04-09 23:58:49,323 root         INFO     Train Epoch: 44 [4096/8000 (51%)]\tTotal Loss: 5.895127\n",
      "Reconstruction: 5.163604, Regularization: 0.660396, Discriminator: 0.042287; Generator: 0.028839,\n",
      "D(x): 0.429, D(G(z)): 0.397\n",
      "2019-04-09 23:58:49,431 root         INFO     Train Epoch: 44 [4608/8000 (58%)]\tTotal Loss: 2.586957\n",
      "Reconstruction: 1.906134, Regularization: 0.609828, Discriminator: 0.042146; Generator: 0.028848,\n",
      "D(x): 0.431, D(G(z)): 0.397\n",
      "2019-04-09 23:58:49,540 root         INFO     Train Epoch: 44 [5120/8000 (64%)]\tTotal Loss: 2.461287\n",
      "Reconstruction: 1.718853, Regularization: 0.671562, Discriminator: 0.041952; Generator: 0.028921,\n",
      "D(x): 0.433, D(G(z)): 0.396\n",
      "2019-04-09 23:58:49,648 root         INFO     Train Epoch: 44 [5632/8000 (70%)]\tTotal Loss: 4.770795\n",
      "Reconstruction: 4.104784, Regularization: 0.595078, Discriminator: 0.042082; Generator: 0.028851,\n",
      "D(x): 0.432, D(G(z)): 0.397\n",
      "2019-04-09 23:58:49,756 root         INFO     Train Epoch: 44 [6144/8000 (77%)]\tTotal Loss: 3.125119\n",
      "Reconstruction: 2.302619, Regularization: 0.751481, Discriminator: 0.042120; Generator: 0.028900,\n",
      "D(x): 0.431, D(G(z)): 0.397\n",
      "2019-04-09 23:58:49,864 root         INFO     Train Epoch: 44 [6656/8000 (83%)]\tTotal Loss: 2.993086\n",
      "Reconstruction: 2.237190, Regularization: 0.685055, Discriminator: 0.041864; Generator: 0.028977,\n",
      "D(x): 0.434, D(G(z)): 0.396\n",
      "2019-04-09 23:58:49,972 root         INFO     Train Epoch: 44 [7168/8000 (90%)]\tTotal Loss: 3.450757\n",
      "Reconstruction: 2.623348, Regularization: 0.756292, Discriminator: 0.042220; Generator: 0.028897,\n",
      "D(x): 0.429, D(G(z)): 0.397\n",
      "2019-04-09 23:58:50,078 root         INFO     Train Epoch: 44 [7680/8000 (96%)]\tTotal Loss: 3.717324\n",
      "Reconstruction: 2.938358, Regularization: 0.707986, Discriminator: 0.042092; Generator: 0.028887,\n",
      "D(x): 0.431, D(G(z)): 0.397\n",
      "2019-04-09 23:58:50,157 root         INFO     ====> Epoch: 44 Average loss: 3.3698\n",
      "2019-04-09 23:58:50,184 root         INFO     Train Epoch: 45 [0/8000 (0%)]\tTotal Loss: 2.919450\n",
      "Reconstruction: 2.159997, Regularization: 0.688528, Discriminator: 0.041946; Generator: 0.028979,\n",
      "D(x): 0.432, D(G(z)): 0.396\n",
      "2019-04-09 23:58:50,293 root         INFO     Train Epoch: 45 [512/8000 (6%)]\tTotal Loss: 2.862256\n",
      "Reconstruction: 2.083347, Regularization: 0.708129, Discriminator: 0.041816; Generator: 0.028964,\n",
      "D(x): 0.434, D(G(z)): 0.396\n",
      "2019-04-09 23:58:50,401 root         INFO     Train Epoch: 45 [1024/8000 (13%)]\tTotal Loss: 2.919051\n",
      "Reconstruction: 2.177449, Regularization: 0.671122, Discriminator: 0.041436; Generator: 0.029043,\n",
      "D(x): 0.439, D(G(z)): 0.395\n",
      "2019-04-09 23:58:50,510 root         INFO     Train Epoch: 45 [1536/8000 (19%)]\tTotal Loss: 2.944283\n",
      "Reconstruction: 2.193599, Regularization: 0.679914, Discriminator: 0.041725; Generator: 0.029045,\n",
      "D(x): 0.435, D(G(z)): 0.395\n",
      "2019-04-09 23:58:50,618 root         INFO     Train Epoch: 45 [2048/8000 (26%)]\tTotal Loss: 2.785801\n",
      "Reconstruction: 2.051661, Regularization: 0.663314, Discriminator: 0.041778; Generator: 0.029048,\n",
      "D(x): 0.434, D(G(z)): 0.395\n",
      "2019-04-09 23:58:50,726 root         INFO     Train Epoch: 45 [2560/8000 (32%)]\tTotal Loss: 3.609504\n",
      "Reconstruction: 2.847331, Regularization: 0.691077, Discriminator: 0.041988; Generator: 0.029108,\n",
      "D(x): 0.431, D(G(z)): 0.394\n",
      "2019-04-09 23:58:50,834 root         INFO     Train Epoch: 45 [3072/8000 (38%)]\tTotal Loss: 2.856303\n",
      "Reconstruction: 2.123116, Regularization: 0.662260, Discriminator: 0.041865; Generator: 0.029063,\n",
      "D(x): 0.433, D(G(z)): 0.395\n",
      "2019-04-09 23:58:50,942 root         INFO     Train Epoch: 45 [3584/8000 (45%)]\tTotal Loss: 3.678849\n",
      "Reconstruction: 2.969137, Regularization: 0.638791, Discriminator: 0.041817; Generator: 0.029103,\n",
      "D(x): 0.433, D(G(z)): 0.394\n",
      "2019-04-09 23:58:51,050 root         INFO     Train Epoch: 45 [4096/8000 (51%)]\tTotal Loss: 6.553437\n",
      "Reconstruction: 5.894958, Regularization: 0.587747, Discriminator: 0.041617; Generator: 0.029116,\n",
      "D(x): 0.436, D(G(z)): 0.394\n",
      "2019-04-09 23:58:51,158 root         INFO     Train Epoch: 45 [4608/8000 (58%)]\tTotal Loss: 6.127968\n",
      "Reconstruction: 5.470942, Regularization: 0.586164, Discriminator: 0.041653; Generator: 0.029209,\n",
      "D(x): 0.434, D(G(z)): 0.393\n",
      "2019-04-09 23:58:51,266 root         INFO     Train Epoch: 45 [5120/8000 (64%)]\tTotal Loss: 3.033152\n",
      "Reconstruction: 2.244111, Regularization: 0.718748, Discriminator: 0.041259; Generator: 0.029034,\n",
      "D(x): 0.442, D(G(z)): 0.395\n",
      "2019-04-09 23:58:51,374 root         INFO     Train Epoch: 45 [5632/8000 (70%)]\tTotal Loss: 3.638117\n",
      "Reconstruction: 2.848401, Regularization: 0.719129, Discriminator: 0.041432; Generator: 0.029156,\n",
      "D(x): 0.438, D(G(z)): 0.393\n",
      "2019-04-09 23:58:51,482 root         INFO     Train Epoch: 45 [6144/8000 (77%)]\tTotal Loss: 2.978667\n",
      "Reconstruction: 2.236573, Regularization: 0.671379, Discriminator: 0.041461; Generator: 0.029254,\n",
      "D(x): 0.437, D(G(z)): 0.392\n",
      "2019-04-09 23:58:51,590 root         INFO     Train Epoch: 45 [6656/8000 (83%)]\tTotal Loss: 2.891232\n",
      "Reconstruction: 2.182266, Regularization: 0.638481, Discriminator: 0.041379; Generator: 0.029107,\n",
      "D(x): 0.439, D(G(z)): 0.394\n",
      "2019-04-09 23:58:51,698 root         INFO     Train Epoch: 45 [7168/8000 (90%)]\tTotal Loss: 3.927146\n",
      "Reconstruction: 3.157550, Regularization: 0.699206, Discriminator: 0.041231; Generator: 0.029160,\n",
      "D(x): 0.441, D(G(z)): 0.393\n",
      "2019-04-09 23:58:51,805 root         INFO     Train Epoch: 45 [7680/8000 (96%)]\tTotal Loss: 3.177406\n",
      "Reconstruction: 2.372566, Regularization: 0.733942, Discriminator: 0.041650; Generator: 0.029248,\n",
      "D(x): 0.434, D(G(z)): 0.392\n",
      "2019-04-09 23:58:51,885 root         INFO     ====> Epoch: 45 Average loss: 3.2744\n",
      "2019-04-09 23:58:51,912 root         INFO     Train Epoch: 46 [0/8000 (0%)]\tTotal Loss: 3.370050\n",
      "Reconstruction: 2.664613, Regularization: 0.634984, Discriminator: 0.041247; Generator: 0.029207,\n",
      "D(x): 0.440, D(G(z)): 0.393\n",
      "2019-04-09 23:58:52,020 root         INFO     Train Epoch: 46 [512/8000 (6%)]\tTotal Loss: 2.827101\n",
      "Reconstruction: 2.143177, Regularization: 0.613358, Discriminator: 0.041468; Generator: 0.029098,\n",
      "D(x): 0.438, D(G(z)): 0.394\n",
      "2019-04-09 23:58:52,128 root         INFO     Train Epoch: 46 [1024/8000 (13%)]\tTotal Loss: 3.117308\n",
      "Reconstruction: 2.368914, Regularization: 0.677705, Discriminator: 0.041369; Generator: 0.029320,\n",
      "D(x): 0.437, D(G(z)): 0.391\n",
      "2019-04-09 23:58:52,238 root         INFO     Train Epoch: 46 [1536/8000 (19%)]\tTotal Loss: 2.869841\n",
      "Reconstruction: 2.097473, Regularization: 0.701752, Discriminator: 0.041452; Generator: 0.029164,\n",
      "D(x): 0.438, D(G(z)): 0.393\n",
      "2019-04-09 23:58:52,346 root         INFO     Train Epoch: 46 [2048/8000 (26%)]\tTotal Loss: 3.765891\n",
      "Reconstruction: 2.963545, Regularization: 0.731676, Discriminator: 0.041371; Generator: 0.029299,\n",
      "D(x): 0.438, D(G(z)): 0.392\n",
      "2019-04-09 23:58:52,454 root         INFO     Train Epoch: 46 [2560/8000 (32%)]\tTotal Loss: 3.563122\n",
      "Reconstruction: 2.799330, Regularization: 0.693330, Discriminator: 0.041250; Generator: 0.029212,\n",
      "D(x): 0.440, D(G(z)): 0.393\n",
      "2019-04-09 23:58:52,561 root         INFO     Train Epoch: 46 [3072/8000 (38%)]\tTotal Loss: 2.968156\n",
      "Reconstruction: 2.196493, Regularization: 0.700905, Discriminator: 0.041521; Generator: 0.029237,\n",
      "D(x): 0.436, D(G(z)): 0.392\n",
      "2019-04-09 23:58:52,668 root         INFO     Train Epoch: 46 [3584/8000 (45%)]\tTotal Loss: 3.784975\n",
      "Reconstruction: 3.139527, Regularization: 0.575120, Discriminator: 0.041053; Generator: 0.029275,\n",
      "D(x): 0.442, D(G(z)): 0.392\n",
      "2019-04-09 23:58:52,775 root         INFO     Train Epoch: 46 [4096/8000 (51%)]\tTotal Loss: 2.724979\n",
      "Reconstruction: 1.943024, Regularization: 0.711272, Discriminator: 0.041447; Generator: 0.029237,\n",
      "D(x): 0.437, D(G(z)): 0.392\n",
      "2019-04-09 23:58:52,882 root         INFO     Train Epoch: 46 [4608/8000 (58%)]\tTotal Loss: 3.276680\n",
      "Reconstruction: 2.476320, Regularization: 0.729742, Discriminator: 0.041351; Generator: 0.029267,\n",
      "D(x): 0.438, D(G(z)): 0.392\n",
      "2019-04-09 23:58:52,989 root         INFO     Train Epoch: 46 [5120/8000 (64%)]\tTotal Loss: 2.447418\n",
      "Reconstruction: 1.752296, Regularization: 0.624402, Discriminator: 0.041410; Generator: 0.029310,\n",
      "D(x): 0.437, D(G(z)): 0.391\n",
      "2019-04-09 23:58:53,096 root         INFO     Train Epoch: 46 [5632/8000 (70%)]\tTotal Loss: 2.535447\n",
      "Reconstruction: 1.841574, Regularization: 0.623240, Discriminator: 0.041235; Generator: 0.029398,\n",
      "D(x): 0.439, D(G(z)): 0.390\n",
      "2019-04-09 23:58:53,203 root         INFO     Train Epoch: 46 [6144/8000 (77%)]\tTotal Loss: 2.993556\n",
      "Reconstruction: 2.191448, Regularization: 0.731138, Discriminator: 0.041564; Generator: 0.029406,\n",
      "D(x): 0.434, D(G(z)): 0.390\n",
      "2019-04-09 23:58:53,310 root         INFO     Train Epoch: 46 [6656/8000 (83%)]\tTotal Loss: 2.727698\n",
      "Reconstruction: 2.007999, Regularization: 0.649101, Discriminator: 0.041141; Generator: 0.029458,\n",
      "D(x): 0.439, D(G(z)): 0.390\n",
      "2019-04-09 23:58:53,417 root         INFO     Train Epoch: 46 [7168/8000 (90%)]\tTotal Loss: 2.999292\n",
      "Reconstruction: 2.293848, Regularization: 0.635125, Discriminator: 0.041003; Generator: 0.029316,\n",
      "D(x): 0.443, D(G(z)): 0.391\n",
      "2019-04-09 23:58:53,524 root         INFO     Train Epoch: 46 [7680/8000 (96%)]\tTotal Loss: 3.105221\n",
      "Reconstruction: 2.278143, Regularization: 0.756707, Discriminator: 0.041046; Generator: 0.029325,\n",
      "D(x): 0.442, D(G(z)): 0.391\n",
      "2019-04-09 23:58:53,603 root         INFO     ====> Epoch: 46 Average loss: 3.1498\n",
      "2019-04-09 23:58:53,630 root         INFO     Train Epoch: 47 [0/8000 (0%)]\tTotal Loss: 2.675951\n",
      "Reconstruction: 1.963064, Regularization: 0.642285, Discriminator: 0.041288; Generator: 0.029314,\n",
      "D(x): 0.439, D(G(z)): 0.391\n",
      "2019-04-09 23:58:53,739 root         INFO     Train Epoch: 47 [512/8000 (6%)]\tTotal Loss: 3.136220\n",
      "Reconstruction: 2.357094, Regularization: 0.709053, Discriminator: 0.040831; Generator: 0.029243,\n",
      "D(x): 0.446, D(G(z)): 0.392\n",
      "2019-04-09 23:58:53,847 root         INFO     Train Epoch: 47 [1024/8000 (13%)]\tTotal Loss: 3.187538\n",
      "Reconstruction: 2.379632, Regularization: 0.737163, Discriminator: 0.041441; Generator: 0.029302,\n",
      "D(x): 0.437, D(G(z)): 0.392\n",
      "2019-04-09 23:58:53,956 root         INFO     Train Epoch: 47 [1536/8000 (19%)]\tTotal Loss: 2.882472\n",
      "Reconstruction: 2.123234, Regularization: 0.688883, Discriminator: 0.040985; Generator: 0.029369,\n",
      "D(x): 0.443, D(G(z)): 0.391\n",
      "2019-04-09 23:58:54,064 root         INFO     Train Epoch: 47 [2048/8000 (26%)]\tTotal Loss: 2.904816\n",
      "Reconstruction: 2.189416, Regularization: 0.644771, Discriminator: 0.041278; Generator: 0.029350,\n",
      "D(x): 0.439, D(G(z)): 0.391\n",
      "2019-04-09 23:58:54,172 root         INFO     Train Epoch: 47 [2560/8000 (32%)]\tTotal Loss: 2.409231\n",
      "Reconstruction: 1.715867, Regularization: 0.622967, Discriminator: 0.041086; Generator: 0.029311,\n",
      "D(x): 0.442, D(G(z)): 0.391\n",
      "2019-04-09 23:58:54,282 root         INFO     Train Epoch: 47 [3072/8000 (38%)]\tTotal Loss: 3.036886\n",
      "Reconstruction: 2.238710, Regularization: 0.727667, Discriminator: 0.041241; Generator: 0.029268,\n",
      "D(x): 0.440, D(G(z)): 0.392\n",
      "2019-04-09 23:58:54,390 root         INFO     Train Epoch: 47 [3584/8000 (45%)]\tTotal Loss: 3.782816\n",
      "Reconstruction: 3.016288, Regularization: 0.696109, Discriminator: 0.041159; Generator: 0.029260,\n",
      "D(x): 0.441, D(G(z)): 0.392\n",
      "2019-04-09 23:58:54,499 root         INFO     Train Epoch: 47 [4096/8000 (51%)]\tTotal Loss: 2.985572\n",
      "Reconstruction: 2.229967, Regularization: 0.685217, Discriminator: 0.041239; Generator: 0.029148,\n",
      "D(x): 0.441, D(G(z)): 0.393\n",
      "2019-04-09 23:58:54,608 root         INFO     Train Epoch: 47 [4608/8000 (58%)]\tTotal Loss: 2.880485\n",
      "Reconstruction: 2.135488, Regularization: 0.674711, Discriminator: 0.041021; Generator: 0.029264,\n",
      "D(x): 0.443, D(G(z)): 0.392\n",
      "2019-04-09 23:58:54,716 root         INFO     Train Epoch: 47 [5120/8000 (64%)]\tTotal Loss: 3.280099\n",
      "Reconstruction: 2.487028, Regularization: 0.722674, Discriminator: 0.041216; Generator: 0.029181,\n",
      "D(x): 0.441, D(G(z)): 0.393\n",
      "2019-04-09 23:58:54,825 root         INFO     Train Epoch: 47 [5632/8000 (70%)]\tTotal Loss: 2.658992\n",
      "Reconstruction: 2.014905, Regularization: 0.573981, Discriminator: 0.041017; Generator: 0.029090,\n",
      "D(x): 0.445, D(G(z)): 0.394\n",
      "2019-04-09 23:58:54,935 root         INFO     Train Epoch: 47 [6144/8000 (77%)]\tTotal Loss: 3.056735\n",
      "Reconstruction: 2.279910, Regularization: 0.706428, Discriminator: 0.041192; Generator: 0.029205,\n",
      "D(x): 0.441, D(G(z)): 0.393\n",
      "2019-04-09 23:58:55,045 root         INFO     Train Epoch: 47 [6656/8000 (83%)]\tTotal Loss: 2.795425\n",
      "Reconstruction: 2.036490, Regularization: 0.688679, Discriminator: 0.041232; Generator: 0.029024,\n",
      "D(x): 0.443, D(G(z)): 0.395\n",
      "2019-04-09 23:58:55,156 root         INFO     Train Epoch: 47 [7168/8000 (90%)]\tTotal Loss: 3.295327\n",
      "Reconstruction: 2.402438, Regularization: 0.822703, Discriminator: 0.041124; Generator: 0.029062,\n",
      "D(x): 0.443, D(G(z)): 0.395\n",
      "2019-04-09 23:58:55,266 root         INFO     Train Epoch: 47 [7680/8000 (96%)]\tTotal Loss: 3.227795\n",
      "Reconstruction: 2.400277, Regularization: 0.757193, Discriminator: 0.041365; Generator: 0.028961,\n",
      "D(x): 0.441, D(G(z)): 0.396\n",
      "2019-04-09 23:58:55,346 root         INFO     ====> Epoch: 47 Average loss: 3.0883\n",
      "2019-04-09 23:58:55,373 root         INFO     Train Epoch: 48 [0/8000 (0%)]\tTotal Loss: 3.063542\n",
      "Reconstruction: 2.242144, Regularization: 0.751340, Discriminator: 0.041047; Generator: 0.029010,\n",
      "D(x): 0.445, D(G(z)): 0.395\n",
      "2019-04-09 23:58:55,483 root         INFO     Train Epoch: 48 [512/8000 (6%)]\tTotal Loss: 2.816094\n",
      "Reconstruction: 2.072140, Regularization: 0.674274, Discriminator: 0.040663; Generator: 0.029017,\n",
      "D(x): 0.451, D(G(z)): 0.395\n",
      "2019-04-09 23:58:55,592 root         INFO     Train Epoch: 48 [1024/8000 (13%)]\tTotal Loss: 3.846895\n",
      "Reconstruction: 3.101729, Regularization: 0.675697, Discriminator: 0.040570; Generator: 0.028899,\n",
      "D(x): 0.453, D(G(z)): 0.397\n",
      "2019-04-09 23:58:55,701 root         INFO     Train Epoch: 48 [1536/8000 (19%)]\tTotal Loss: 2.847507\n",
      "Reconstruction: 2.059342, Regularization: 0.718402, Discriminator: 0.040848; Generator: 0.028914,\n",
      "D(x): 0.449, D(G(z)): 0.396\n",
      "2019-04-09 23:58:55,810 root         INFO     Train Epoch: 48 [2048/8000 (26%)]\tTotal Loss: 3.897739\n",
      "Reconstruction: 3.105319, Regularization: 0.722331, Discriminator: 0.041171; Generator: 0.028919,\n",
      "D(x): 0.444, D(G(z)): 0.396\n",
      "2019-04-09 23:58:55,919 root         INFO     Train Epoch: 48 [2560/8000 (32%)]\tTotal Loss: 2.560168\n",
      "Reconstruction: 1.874361, Regularization: 0.616083, Discriminator: 0.040866; Generator: 0.028858,\n",
      "D(x): 0.449, D(G(z)): 0.397\n",
      "2019-04-09 23:58:56,027 root         INFO     Train Epoch: 48 [3072/8000 (38%)]\tTotal Loss: 2.700648\n",
      "Reconstruction: 1.967967, Regularization: 0.662558, Discriminator: 0.041320; Generator: 0.028802,\n",
      "D(x): 0.443, D(G(z)): 0.398\n",
      "2019-04-09 23:58:56,136 root         INFO     Train Epoch: 48 [3584/8000 (45%)]\tTotal Loss: 2.275423\n",
      "Reconstruction: 1.592252, Regularization: 0.613457, Discriminator: 0.041032; Generator: 0.028681,\n",
      "D(x): 0.448, D(G(z)): 0.399\n",
      "2019-04-09 23:58:56,245 root         INFO     Train Epoch: 48 [4096/8000 (51%)]\tTotal Loss: 2.855842\n",
      "Reconstruction: 2.061500, Regularization: 0.725215, Discriminator: 0.040467; Generator: 0.028661,\n",
      "D(x): 0.457, D(G(z)): 0.400\n",
      "2019-04-09 23:58:56,357 root         INFO     Train Epoch: 48 [4608/8000 (58%)]\tTotal Loss: 3.038242\n",
      "Reconstruction: 2.215107, Regularization: 0.753463, Discriminator: 0.040993; Generator: 0.028680,\n",
      "D(x): 0.449, D(G(z)): 0.399\n",
      "2019-04-09 23:58:56,467 root         INFO     Train Epoch: 48 [5120/8000 (64%)]\tTotal Loss: 2.655041\n",
      "Reconstruction: 1.944571, Regularization: 0.640871, Discriminator: 0.041091; Generator: 0.028507,\n",
      "D(x): 0.449, D(G(z)): 0.402\n",
      "2019-04-09 23:58:56,576 root         INFO     Train Epoch: 48 [5632/8000 (70%)]\tTotal Loss: 3.064758\n",
      "Reconstruction: 2.273318, Regularization: 0.721612, Discriminator: 0.041365; Generator: 0.028463,\n",
      "D(x): 0.446, D(G(z)): 0.402\n",
      "2019-04-09 23:58:56,684 root         INFO     Train Epoch: 48 [6144/8000 (77%)]\tTotal Loss: 2.733940\n",
      "Reconstruction: 2.003708, Regularization: 0.660603, Discriminator: 0.041259; Generator: 0.028371,\n",
      "D(x): 0.448, D(G(z)): 0.403\n",
      "2019-04-09 23:58:56,792 root         INFO     Train Epoch: 48 [6656/8000 (83%)]\tTotal Loss: 2.816152\n",
      "Reconstruction: 2.054965, Regularization: 0.692007, Discriminator: 0.040845; Generator: 0.028334,\n",
      "D(x): 0.454, D(G(z)): 0.404\n",
      "2019-04-09 23:58:56,899 root         INFO     Train Epoch: 48 [7168/8000 (90%)]\tTotal Loss: 2.763992\n",
      "Reconstruction: 2.017159, Regularization: 0.677273, Discriminator: 0.041299; Generator: 0.028261,\n",
      "D(x): 0.449, D(G(z)): 0.405\n",
      "2019-04-09 23:58:57,007 root         INFO     Train Epoch: 48 [7680/8000 (96%)]\tTotal Loss: 2.747676\n",
      "Reconstruction: 2.063182, Regularization: 0.614370, Discriminator: 0.041877; Generator: 0.028247,\n",
      "D(x): 0.441, D(G(z)): 0.405\n",
      "2019-04-09 23:58:57,086 root         INFO     ====> Epoch: 48 Average loss: 3.0083\n",
      "2019-04-09 23:58:57,113 root         INFO     Train Epoch: 49 [0/8000 (0%)]\tTotal Loss: 2.499129\n",
      "Reconstruction: 1.841572, Regularization: 0.588212, Discriminator: 0.041172; Generator: 0.028173,\n",
      "D(x): 0.451, D(G(z)): 0.406\n",
      "2019-04-09 23:58:57,222 root         INFO     Train Epoch: 49 [512/8000 (6%)]\tTotal Loss: 2.695758\n",
      "Reconstruction: 1.960109, Regularization: 0.665888, Discriminator: 0.041665; Generator: 0.028096,\n",
      "D(x): 0.445, D(G(z)): 0.407\n",
      "2019-04-09 23:58:57,330 root         INFO     Train Epoch: 49 [1024/8000 (13%)]\tTotal Loss: 3.237799\n",
      "Reconstruction: 2.381379, Regularization: 0.787041, Discriminator: 0.041409; Generator: 0.027971,\n",
      "D(x): 0.451, D(G(z)): 0.409\n",
      "2019-04-09 23:58:57,438 root         INFO     Train Epoch: 49 [1536/8000 (19%)]\tTotal Loss: 2.649214\n",
      "Reconstruction: 1.924581, Regularization: 0.655751, Discriminator: 0.041000; Generator: 0.027882,\n",
      "D(x): 0.457, D(G(z)): 0.410\n",
      "2019-04-09 23:58:57,545 root         INFO     Train Epoch: 49 [2048/8000 (26%)]\tTotal Loss: 3.066808\n",
      "Reconstruction: 2.273777, Regularization: 0.724056, Discriminator: 0.041165; Generator: 0.027809,\n",
      "D(x): 0.455, D(G(z)): 0.411\n",
      "2019-04-09 23:58:57,653 root         INFO     Train Epoch: 49 [2560/8000 (32%)]\tTotal Loss: 3.183155\n",
      "Reconstruction: 2.348142, Regularization: 0.765999, Discriminator: 0.041304; Generator: 0.027710,\n",
      "D(x): 0.454, D(G(z)): 0.412\n",
      "2019-04-09 23:58:57,761 root         INFO     Train Epoch: 49 [3072/8000 (38%)]\tTotal Loss: 2.961607\n",
      "Reconstruction: 2.145469, Regularization: 0.747550, Discriminator: 0.040929; Generator: 0.027658,\n",
      "D(x): 0.460, D(G(z)): 0.413\n",
      "2019-04-09 23:58:57,869 root         INFO     Train Epoch: 49 [3584/8000 (45%)]\tTotal Loss: 3.031770\n",
      "Reconstruction: 2.236518, Regularization: 0.726674, Discriminator: 0.040939; Generator: 0.027639,\n",
      "D(x): 0.460, D(G(z)): 0.413\n",
      "2019-04-09 23:58:57,977 root         INFO     Train Epoch: 49 [4096/8000 (51%)]\tTotal Loss: 2.735117\n",
      "Reconstruction: 1.976656, Regularization: 0.690075, Discriminator: 0.040849; Generator: 0.027536,\n",
      "D(x): 0.463, D(G(z)): 0.414\n",
      "2019-04-09 23:58:58,085 root         INFO     Train Epoch: 49 [4608/8000 (58%)]\tTotal Loss: 2.696710\n",
      "Reconstruction: 1.929239, Regularization: 0.698200, Discriminator: 0.041854; Generator: 0.027417,\n",
      "D(x): 0.450, D(G(z)): 0.416\n",
      "2019-04-09 23:58:58,193 root         INFO     Train Epoch: 49 [5120/8000 (64%)]\tTotal Loss: 2.839903\n",
      "Reconstruction: 2.087750, Regularization: 0.683562, Discriminator: 0.041266; Generator: 0.027325,\n",
      "D(x): 0.459, D(G(z)): 0.417\n",
      "2019-04-09 23:58:58,301 root         INFO     Train Epoch: 49 [5632/8000 (70%)]\tTotal Loss: 2.773358\n",
      "Reconstruction: 2.021325, Regularization: 0.683226, Discriminator: 0.041574; Generator: 0.027233,\n",
      "D(x): 0.456, D(G(z)): 0.418\n",
      "2019-04-09 23:58:58,408 root         INFO     Train Epoch: 49 [6144/8000 (77%)]\tTotal Loss: 2.587406\n",
      "Reconstruction: 1.854565, Regularization: 0.663993, Discriminator: 0.041716; Generator: 0.027132,\n",
      "D(x): 0.455, D(G(z)): 0.420\n",
      "2019-04-09 23:58:58,516 root         INFO     Train Epoch: 49 [6656/8000 (83%)]\tTotal Loss: 2.605083\n",
      "Reconstruction: 1.877661, Regularization: 0.658575, Discriminator: 0.041816; Generator: 0.027031,\n",
      "D(x): 0.454, D(G(z)): 0.421\n",
      "2019-04-09 23:58:58,624 root         INFO     Train Epoch: 49 [7168/8000 (90%)]\tTotal Loss: 3.197395\n",
      "Reconstruction: 2.314360, Regularization: 0.814417, Discriminator: 0.041655; Generator: 0.026963,\n",
      "D(x): 0.457, D(G(z)): 0.422\n",
      "2019-04-09 23:58:58,732 root         INFO     Train Epoch: 49 [7680/8000 (96%)]\tTotal Loss: 2.769208\n",
      "Reconstruction: 2.016525, Regularization: 0.683988, Discriminator: 0.041838; Generator: 0.026857,\n",
      "D(x): 0.455, D(G(z)): 0.423\n",
      "2019-04-09 23:58:58,812 root         INFO     ====> Epoch: 49 Average loss: 2.9600\n",
      "2019-04-09 23:58:58,839 root         INFO     Train Epoch: 50 [0/8000 (0%)]\tTotal Loss: 3.230758\n",
      "Reconstruction: 2.506090, Regularization: 0.655947, Discriminator: 0.041925; Generator: 0.026797,\n",
      "D(x): 0.455, D(G(z)): 0.424\n",
      "2019-04-09 23:58:58,948 root         INFO     Train Epoch: 50 [512/8000 (6%)]\tTotal Loss: 2.725769\n",
      "Reconstruction: 2.036399, Regularization: 0.620477, Discriminator: 0.042229; Generator: 0.026664,\n",
      "D(x): 0.452, D(G(z)): 0.426\n",
      "2019-04-09 23:58:59,056 root         INFO     Train Epoch: 50 [1024/8000 (13%)]\tTotal Loss: 3.045153\n",
      "Reconstruction: 2.258887, Regularization: 0.717693, Discriminator: 0.041982; Generator: 0.026591,\n",
      "D(x): 0.457, D(G(z)): 0.427\n",
      "2019-04-09 23:58:59,165 root         INFO     Train Epoch: 50 [1536/8000 (19%)]\tTotal Loss: 2.919998\n",
      "Reconstruction: 2.125845, Regularization: 0.726057, Discriminator: 0.041607; Generator: 0.026489,\n",
      "D(x): 0.463, D(G(z)): 0.428\n",
      "2019-04-09 23:58:59,273 root         INFO     Train Epoch: 50 [2048/8000 (26%)]\tTotal Loss: 2.649612\n",
      "Reconstruction: 1.935601, Regularization: 0.645352, Discriminator: 0.042249; Generator: 0.026410,\n",
      "D(x): 0.454, D(G(z)): 0.430\n",
      "2019-04-09 23:58:59,382 root         INFO     Train Epoch: 50 [2560/8000 (32%)]\tTotal Loss: 2.822838\n",
      "Reconstruction: 2.039922, Regularization: 0.714631, Discriminator: 0.042010; Generator: 0.026274,\n",
      "D(x): 0.459, D(G(z)): 0.431\n",
      "2019-04-09 23:58:59,490 root         INFO     Train Epoch: 50 [3072/8000 (38%)]\tTotal Loss: 2.813038\n",
      "Reconstruction: 2.106520, Regularization: 0.638498, Discriminator: 0.041831; Generator: 0.026189,\n",
      "D(x): 0.464, D(G(z)): 0.433\n",
      "2019-04-09 23:58:59,599 root         INFO     Train Epoch: 50 [3584/8000 (45%)]\tTotal Loss: 2.793715\n",
      "Reconstruction: 2.005836, Regularization: 0.720450, Discriminator: 0.041306; Generator: 0.026123,\n",
      "D(x): 0.472, D(G(z)): 0.433\n",
      "2019-04-09 23:58:59,708 root         INFO     Train Epoch: 50 [4096/8000 (51%)]\tTotal Loss: 2.605319\n",
      "Reconstruction: 1.939268, Regularization: 0.597532, Discriminator: 0.042495; Generator: 0.026023,\n",
      "D(x): 0.455, D(G(z)): 0.435\n",
      "2019-04-09 23:58:59,816 root         INFO     Train Epoch: 50 [4608/8000 (58%)]\tTotal Loss: 2.740704\n",
      "Reconstruction: 2.025077, Regularization: 0.646980, Discriminator: 0.042727; Generator: 0.025920,\n",
      "D(x): 0.453, D(G(z)): 0.436\n",
      "2019-04-09 23:58:59,924 root         INFO     Train Epoch: 50 [5120/8000 (64%)]\tTotal Loss: 2.967937\n",
      "Reconstruction: 2.180755, Regularization: 0.718785, Discriminator: 0.042555; Generator: 0.025841,\n",
      "D(x): 0.457, D(G(z)): 0.437\n",
      "2019-04-09 23:59:00,032 root         INFO     Train Epoch: 50 [5632/8000 (70%)]\tTotal Loss: 2.594458\n",
      "Reconstruction: 1.883683, Regularization: 0.642408, Discriminator: 0.042646; Generator: 0.025721,\n",
      "D(x): 0.456, D(G(z)): 0.439\n",
      "2019-04-09 23:59:00,141 root         INFO     Train Epoch: 50 [6144/8000 (77%)]\tTotal Loss: 2.981517\n",
      "Reconstruction: 2.224004, Regularization: 0.688939, Discriminator: 0.042976; Generator: 0.025597,\n",
      "D(x): 0.453, D(G(z)): 0.441\n",
      "2019-04-09 23:59:00,249 root         INFO     Train Epoch: 50 [6656/8000 (83%)]\tTotal Loss: 2.958908\n",
      "Reconstruction: 2.152380, Regularization: 0.738384, Discriminator: 0.042622; Generator: 0.025520,\n",
      "D(x): 0.459, D(G(z)): 0.442\n",
      "2019-04-09 23:59:00,357 root         INFO     Train Epoch: 50 [7168/8000 (90%)]\tTotal Loss: 3.932301\n",
      "Reconstruction: 3.124783, Regularization: 0.739544, Discriminator: 0.042573; Generator: 0.025400,\n",
      "D(x): 0.461, D(G(z)): 0.444\n",
      "2019-04-09 23:59:00,466 root         INFO     Train Epoch: 50 [7680/8000 (96%)]\tTotal Loss: 2.826377\n",
      "Reconstruction: 2.088837, Regularization: 0.669400, Discriminator: 0.042853; Generator: 0.025287,\n",
      "D(x): 0.459, D(G(z)): 0.445\n",
      "2019-04-09 23:59:00,545 root         INFO     ====> Epoch: 50 Average loss: 2.9458\n",
      "2019-04-09 23:59:00,572 root         INFO     Train Epoch: 51 [0/8000 (0%)]\tTotal Loss: 2.912891\n",
      "Reconstruction: 2.124777, Regularization: 0.720044, Discriminator: 0.042848; Generator: 0.025221,\n",
      "D(x): 0.459, D(G(z)): 0.446\n",
      "2019-04-09 23:59:00,679 root         INFO     Train Epoch: 51 [512/8000 (6%)]\tTotal Loss: 3.042225\n",
      "Reconstruction: 2.268920, Regularization: 0.705496, Discriminator: 0.042689; Generator: 0.025119,\n",
      "D(x): 0.463, D(G(z)): 0.448\n",
      "2019-04-09 23:59:00,786 root         INFO     Train Epoch: 51 [1024/8000 (13%)]\tTotal Loss: 2.654630\n",
      "Reconstruction: 1.937668, Regularization: 0.649149, Discriminator: 0.042742; Generator: 0.025070,\n",
      "D(x): 0.462, D(G(z)): 0.448\n",
      "2019-04-09 23:59:00,892 root         INFO     Train Epoch: 51 [1536/8000 (19%)]\tTotal Loss: 2.892145\n",
      "Reconstruction: 2.147980, Regularization: 0.675913, Discriminator: 0.043309; Generator: 0.024943,\n",
      "D(x): 0.456, D(G(z)): 0.450\n",
      "2019-04-09 23:59:00,997 root         INFO     Train Epoch: 51 [2048/8000 (26%)]\tTotal Loss: 2.635046\n",
      "Reconstruction: 1.929903, Regularization: 0.637095, Discriminator: 0.043215; Generator: 0.024833,\n",
      "D(x): 0.458, D(G(z)): 0.452\n",
      "2019-04-09 23:59:01,103 root         INFO     Train Epoch: 51 [2560/8000 (32%)]\tTotal Loss: 2.847502\n",
      "Reconstruction: 2.086646, Regularization: 0.693092, Discriminator: 0.043005; Generator: 0.024758,\n",
      "D(x): 0.462, D(G(z)): 0.453\n",
      "2019-04-09 23:59:01,209 root         INFO     Train Epoch: 51 [3072/8000 (38%)]\tTotal Loss: 2.432472\n",
      "Reconstruction: 1.776053, Regularization: 0.589239, Discriminator: 0.042498; Generator: 0.024682,\n",
      "D(x): 0.471, D(G(z)): 0.454\n",
      "2019-04-09 23:59:01,315 root         INFO     Train Epoch: 51 [3584/8000 (45%)]\tTotal Loss: 3.115589\n",
      "Reconstruction: 2.341557, Regularization: 0.706562, Discriminator: 0.042899; Generator: 0.024572,\n",
      "D(x): 0.466, D(G(z)): 0.456\n",
      "2019-04-09 23:59:01,421 root         INFO     Train Epoch: 51 [4096/8000 (51%)]\tTotal Loss: 2.704162\n",
      "Reconstruction: 1.974762, Regularization: 0.661985, Discriminator: 0.042964; Generator: 0.024451,\n",
      "D(x): 0.467, D(G(z)): 0.457\n",
      "2019-04-09 23:59:01,527 root         INFO     Train Epoch: 51 [4608/8000 (58%)]\tTotal Loss: 3.014400\n",
      "Reconstruction: 2.229719, Regularization: 0.716967, Discriminator: 0.043344; Generator: 0.024370,\n",
      "D(x): 0.462, D(G(z)): 0.458\n",
      "2019-04-09 23:59:01,633 root         INFO     Train Epoch: 51 [5120/8000 (64%)]\tTotal Loss: 2.923167\n",
      "Reconstruction: 2.175055, Regularization: 0.680751, Discriminator: 0.043085; Generator: 0.024277,\n",
      "D(x): 0.467, D(G(z)): 0.460\n",
      "2019-04-09 23:59:01,739 root         INFO     Train Epoch: 51 [5632/8000 (70%)]\tTotal Loss: 2.870595\n",
      "Reconstruction: 2.113283, Regularization: 0.690018, Discriminator: 0.043086; Generator: 0.024209,\n",
      "D(x): 0.468, D(G(z)): 0.461\n",
      "2019-04-09 23:59:01,845 root         INFO     Train Epoch: 51 [6144/8000 (77%)]\tTotal Loss: 2.941200\n",
      "Reconstruction: 2.165370, Regularization: 0.707947, Discriminator: 0.043783; Generator: 0.024101,\n",
      "D(x): 0.459, D(G(z)): 0.462\n",
      "2019-04-09 23:59:01,951 root         INFO     Train Epoch: 51 [6656/8000 (83%)]\tTotal Loss: 3.203132\n",
      "Reconstruction: 2.379088, Regularization: 0.755894, Discriminator: 0.044116; Generator: 0.024033,\n",
      "D(x): 0.455, D(G(z)): 0.463\n",
      "2019-04-09 23:59:02,057 root         INFO     Train Epoch: 51 [7168/8000 (90%)]\tTotal Loss: 3.382810\n",
      "Reconstruction: 2.574796, Regularization: 0.741162, Discriminator: 0.042918; Generator: 0.023934,\n",
      "D(x): 0.474, D(G(z)): 0.465\n",
      "2019-04-09 23:59:02,163 root         INFO     Train Epoch: 51 [7680/8000 (96%)]\tTotal Loss: 2.524056\n",
      "Reconstruction: 1.801343, Regularization: 0.655888, Discriminator: 0.042947; Generator: 0.023878,\n",
      "D(x): 0.475, D(G(z)): 0.466\n",
      "2019-04-09 23:59:02,241 root         INFO     ====> Epoch: 51 Average loss: 2.9193\n",
      "2019-04-09 23:59:02,268 root         INFO     Train Epoch: 52 [0/8000 (0%)]\tTotal Loss: 2.608035\n",
      "Reconstruction: 1.903527, Regularization: 0.637139, Discriminator: 0.043538; Generator: 0.023831,\n",
      "D(x): 0.466, D(G(z)): 0.466\n",
      "2019-04-09 23:59:02,376 root         INFO     Train Epoch: 52 [512/8000 (6%)]\tTotal Loss: 2.617802\n",
      "Reconstruction: 1.862907, Regularization: 0.687922, Discriminator: 0.043237; Generator: 0.023737,\n",
      "D(x): 0.472, D(G(z)): 0.468\n",
      "2019-04-09 23:59:02,484 root         INFO     Train Epoch: 52 [1024/8000 (13%)]\tTotal Loss: 2.964735\n",
      "Reconstruction: 2.206349, Regularization: 0.690917, Discriminator: 0.043829; Generator: 0.023640,\n",
      "D(x): 0.464, D(G(z)): 0.469\n",
      "2019-04-09 23:59:02,592 root         INFO     Train Epoch: 52 [1536/8000 (19%)]\tTotal Loss: 3.304651\n",
      "Reconstruction: 2.496510, Regularization: 0.740840, Discriminator: 0.043726; Generator: 0.023575,\n",
      "D(x): 0.467, D(G(z)): 0.470\n",
      "2019-04-09 23:59:02,699 root         INFO     Train Epoch: 52 [2048/8000 (26%)]\tTotal Loss: 3.279624\n",
      "Reconstruction: 2.422657, Regularization: 0.789760, Discriminator: 0.043702; Generator: 0.023504,\n",
      "D(x): 0.468, D(G(z)): 0.471\n",
      "2019-04-09 23:59:02,807 root         INFO     Train Epoch: 52 [2560/8000 (32%)]\tTotal Loss: 2.892164\n",
      "Reconstruction: 2.133053, Regularization: 0.692334, Discriminator: 0.043357; Generator: 0.023420,\n",
      "D(x): 0.474, D(G(z)): 0.473\n",
      "2019-04-09 23:59:02,914 root         INFO     Train Epoch: 52 [3072/8000 (38%)]\tTotal Loss: 3.025555\n",
      "Reconstruction: 2.190112, Regularization: 0.768386, Discriminator: 0.043727; Generator: 0.023330,\n",
      "D(x): 0.470, D(G(z)): 0.474\n",
      "2019-04-09 23:59:03,022 root         INFO     Train Epoch: 52 [3584/8000 (45%)]\tTotal Loss: 2.534893\n",
      "Reconstruction: 1.859195, Regularization: 0.609395, Discriminator: 0.043009; Generator: 0.023294,\n",
      "D(x): 0.481, D(G(z)): 0.475\n",
      "2019-04-09 23:59:03,129 root         INFO     Train Epoch: 52 [4096/8000 (51%)]\tTotal Loss: 2.960440\n",
      "Reconstruction: 2.182304, Regularization: 0.711071, Discriminator: 0.043866; Generator: 0.023198,\n",
      "D(x): 0.470, D(G(z)): 0.476\n",
      "2019-04-09 23:59:03,236 root         INFO     Train Epoch: 52 [4608/8000 (58%)]\tTotal Loss: 2.646117\n",
      "Reconstruction: 1.958330, Regularization: 0.621448, Discriminator: 0.043268; Generator: 0.023072,\n",
      "D(x): 0.480, D(G(z)): 0.478\n",
      "2019-04-09 23:59:03,344 root         INFO     Train Epoch: 52 [5120/8000 (64%)]\tTotal Loss: 2.555716\n",
      "Reconstruction: 1.901231, Regularization: 0.588005, Discriminator: 0.043427; Generator: 0.023054,\n",
      "D(x): 0.478, D(G(z)): 0.478\n",
      "2019-04-09 23:59:03,452 root         INFO     Train Epoch: 52 [5632/8000 (70%)]\tTotal Loss: 2.674996\n",
      "Reconstruction: 1.919627, Regularization: 0.689017, Discriminator: 0.043396; Generator: 0.022956,\n",
      "D(x): 0.480, D(G(z)): 0.480\n",
      "2019-04-09 23:59:03,560 root         INFO     Train Epoch: 52 [6144/8000 (77%)]\tTotal Loss: 3.902815\n",
      "Reconstruction: 3.116107, Regularization: 0.720198, Discriminator: 0.043570; Generator: 0.022940,\n",
      "D(x): 0.477, D(G(z)): 0.480\n",
      "2019-04-09 23:59:03,667 root         INFO     Train Epoch: 52 [6656/8000 (83%)]\tTotal Loss: 2.515770\n",
      "Reconstruction: 1.823758, Regularization: 0.625825, Discriminator: 0.043303; Generator: 0.022884,\n",
      "D(x): 0.482, D(G(z)): 0.481\n",
      "2019-04-09 23:59:03,775 root         INFO     Train Epoch: 52 [7168/8000 (90%)]\tTotal Loss: 2.756207\n",
      "Reconstruction: 2.016069, Regularization: 0.673618, Discriminator: 0.043710; Generator: 0.022810,\n",
      "D(x): 0.477, D(G(z)): 0.482\n",
      "2019-04-09 23:59:03,881 root         INFO     Train Epoch: 52 [7680/8000 (96%)]\tTotal Loss: 2.948796\n",
      "Reconstruction: 2.223306, Regularization: 0.659272, Discriminator: 0.043528; Generator: 0.022690,\n",
      "D(x): 0.481, D(G(z)): 0.484\n",
      "2019-04-09 23:59:03,960 root         INFO     ====> Epoch: 52 Average loss: 2.9094\n",
      "2019-04-09 23:59:03,987 root         INFO     Train Epoch: 53 [0/8000 (0%)]\tTotal Loss: 2.619673\n",
      "Reconstruction: 1.926484, Regularization: 0.626777, Discriminator: 0.043681; Generator: 0.022730,\n",
      "D(x): 0.479, D(G(z)): 0.483\n",
      "2019-04-09 23:59:04,095 root         INFO     Train Epoch: 53 [512/8000 (6%)]\tTotal Loss: 2.996008\n",
      "Reconstruction: 2.240396, Regularization: 0.689052, Discriminator: 0.043915; Generator: 0.022644,\n",
      "D(x): 0.476, D(G(z)): 0.485\n",
      "2019-04-09 23:59:04,203 root         INFO     Train Epoch: 53 [1024/8000 (13%)]\tTotal Loss: 3.052127\n",
      "Reconstruction: 2.276883, Regularization: 0.708814, Discriminator: 0.043864; Generator: 0.022566,\n",
      "D(x): 0.478, D(G(z)): 0.486\n",
      "2019-04-09 23:59:04,311 root         INFO     Train Epoch: 53 [1536/8000 (19%)]\tTotal Loss: 3.118116\n",
      "Reconstruction: 2.374178, Regularization: 0.678170, Discriminator: 0.043235; Generator: 0.022532,\n",
      "D(x): 0.488, D(G(z)): 0.486\n",
      "2019-04-09 23:59:04,419 root         INFO     Train Epoch: 53 [2048/8000 (26%)]\tTotal Loss: 2.793353\n",
      "Reconstruction: 2.064976, Regularization: 0.661893, Discriminator: 0.044015; Generator: 0.022469,\n",
      "D(x): 0.477, D(G(z)): 0.487\n",
      "2019-04-09 23:59:04,527 root         INFO     Train Epoch: 53 [2560/8000 (32%)]\tTotal Loss: 3.072174\n",
      "Reconstruction: 2.291933, Regularization: 0.713756, Discriminator: 0.044100; Generator: 0.022385,\n",
      "D(x): 0.477, D(G(z)): 0.489\n",
      "2019-04-09 23:59:04,636 root         INFO     Train Epoch: 53 [3072/8000 (38%)]\tTotal Loss: 2.783513\n",
      "Reconstruction: 2.024398, Regularization: 0.693083, Discriminator: 0.043683; Generator: 0.022349,\n",
      "D(x): 0.484, D(G(z)): 0.489\n",
      "2019-04-09 23:59:04,745 root         INFO     Train Epoch: 53 [3584/8000 (45%)]\tTotal Loss: 3.113998\n",
      "Reconstruction: 2.333637, Regularization: 0.714370, Discriminator: 0.043700; Generator: 0.022291,\n",
      "D(x): 0.485, D(G(z)): 0.490\n",
      "2019-04-09 23:59:04,853 root         INFO     Train Epoch: 53 [4096/8000 (51%)]\tTotal Loss: 3.028384\n",
      "Reconstruction: 2.244703, Regularization: 0.717423, Discriminator: 0.044067; Generator: 0.022191,\n",
      "D(x): 0.480, D(G(z)): 0.492\n",
      "2019-04-09 23:59:04,960 root         INFO     Train Epoch: 53 [4608/8000 (58%)]\tTotal Loss: 2.588999\n",
      "Reconstruction: 1.871381, Regularization: 0.651989, Discriminator: 0.043474; Generator: 0.022155,\n",
      "D(x): 0.490, D(G(z)): 0.492\n",
      "2019-04-09 23:59:05,068 root         INFO     Train Epoch: 53 [5120/8000 (64%)]\tTotal Loss: 2.960775\n",
      "Reconstruction: 2.226062, Regularization: 0.668672, Discriminator: 0.043962; Generator: 0.022079,\n",
      "D(x): 0.484, D(G(z)): 0.493\n",
      "2019-04-09 23:59:05,175 root         INFO     Train Epoch: 53 [5632/8000 (70%)]\tTotal Loss: 2.438344\n",
      "Reconstruction: 1.757089, Regularization: 0.615829, Discriminator: 0.043357; Generator: 0.022070,\n",
      "D(x): 0.493, D(G(z)): 0.494\n",
      "2019-04-09 23:59:05,283 root         INFO     Train Epoch: 53 [6144/8000 (77%)]\tTotal Loss: 2.473850\n",
      "Reconstruction: 1.784278, Regularization: 0.624150, Discriminator: 0.043432; Generator: 0.021990,\n",
      "D(x): 0.493, D(G(z)): 0.495\n",
      "2019-04-09 23:59:05,391 root         INFO     Train Epoch: 53 [6656/8000 (83%)]\tTotal Loss: 2.543947\n",
      "Reconstruction: 1.856711, Regularization: 0.621844, Discriminator: 0.043479; Generator: 0.021913,\n",
      "D(x): 0.494, D(G(z)): 0.496\n",
      "2019-04-09 23:59:05,498 root         INFO     Train Epoch: 53 [7168/8000 (90%)]\tTotal Loss: 3.218791\n",
      "Reconstruction: 2.366188, Regularization: 0.786702, Discriminator: 0.044020; Generator: 0.021881,\n",
      "D(x): 0.486, D(G(z)): 0.496\n",
      "2019-04-09 23:59:05,606 root         INFO     Train Epoch: 53 [7680/8000 (96%)]\tTotal Loss: 3.007542\n",
      "Reconstruction: 2.237494, Regularization: 0.704140, Discriminator: 0.044084; Generator: 0.021824,\n",
      "D(x): 0.486, D(G(z)): 0.497\n",
      "2019-04-09 23:59:05,686 root         INFO     ====> Epoch: 53 Average loss: 2.8572\n",
      "2019-04-09 23:59:05,712 root         INFO     Train Epoch: 54 [0/8000 (0%)]\tTotal Loss: 2.339368\n",
      "Reconstruction: 1.682523, Regularization: 0.591692, Discriminator: 0.043318; Generator: 0.021835,\n",
      "D(x): 0.498, D(G(z)): 0.497\n",
      "2019-04-09 23:59:05,820 root         INFO     Train Epoch: 54 [512/8000 (6%)]\tTotal Loss: 2.631944\n",
      "Reconstruction: 1.961952, Regularization: 0.604286, Discriminator: 0.043969; Generator: 0.021737,\n",
      "D(x): 0.489, D(G(z)): 0.499\n",
      "2019-04-09 23:59:05,928 root         INFO     Train Epoch: 54 [1024/8000 (13%)]\tTotal Loss: 2.704541\n",
      "Reconstruction: 1.990404, Regularization: 0.648642, Discriminator: 0.043847; Generator: 0.021648,\n",
      "D(x): 0.492, D(G(z)): 0.500\n",
      "2019-04-09 23:59:06,035 root         INFO     Train Epoch: 54 [1536/8000 (19%)]\tTotal Loss: 2.607740\n",
      "Reconstruction: 1.901765, Regularization: 0.640706, Discriminator: 0.043586; Generator: 0.021684,\n",
      "D(x): 0.496, D(G(z)): 0.500\n",
      "2019-04-09 23:59:06,142 root         INFO     Train Epoch: 54 [2048/8000 (26%)]\tTotal Loss: 2.675841\n",
      "Reconstruction: 1.945997, Regularization: 0.664493, Discriminator: 0.043708; Generator: 0.021643,\n",
      "D(x): 0.494, D(G(z)): 0.500\n",
      "2019-04-09 23:59:06,249 root         INFO     Train Epoch: 54 [2560/8000 (32%)]\tTotal Loss: 2.695412\n",
      "Reconstruction: 1.887424, Regularization: 0.742972, Discriminator: 0.043492; Generator: 0.021524,\n",
      "D(x): 0.500, D(G(z)): 0.502\n",
      "2019-04-09 23:59:06,356 root         INFO     Train Epoch: 54 [3072/8000 (38%)]\tTotal Loss: 2.760751\n",
      "Reconstruction: 2.000520, Regularization: 0.695136, Discriminator: 0.043569; Generator: 0.021526,\n",
      "D(x): 0.498, D(G(z)): 0.502\n",
      "2019-04-09 23:59:06,463 root         INFO     Train Epoch: 54 [3584/8000 (45%)]\tTotal Loss: 2.548187\n",
      "Reconstruction: 1.841153, Regularization: 0.641995, Discriminator: 0.043514; Generator: 0.021525,\n",
      "D(x): 0.499, D(G(z)): 0.502\n",
      "2019-04-09 23:59:06,571 root         INFO     Train Epoch: 54 [4096/8000 (51%)]\tTotal Loss: 2.922682\n",
      "Reconstruction: 2.228868, Regularization: 0.628585, Discriminator: 0.043749; Generator: 0.021480,\n",
      "D(x): 0.496, D(G(z)): 0.503\n",
      "2019-04-09 23:59:06,679 root         INFO     Train Epoch: 54 [4608/8000 (58%)]\tTotal Loss: 2.816458\n",
      "Reconstruction: 2.065794, Regularization: 0.685407, Discriminator: 0.043943; Generator: 0.021314,\n",
      "D(x): 0.496, D(G(z)): 0.506\n",
      "2019-04-09 23:59:06,787 root         INFO     Train Epoch: 54 [5120/8000 (64%)]\tTotal Loss: 2.665535\n",
      "Reconstruction: 1.935049, Regularization: 0.665282, Discriminator: 0.043897; Generator: 0.021308,\n",
      "D(x): 0.497, D(G(z)): 0.506\n",
      "2019-04-09 23:59:06,895 root         INFO     Train Epoch: 54 [5632/8000 (70%)]\tTotal Loss: 2.840505\n",
      "Reconstruction: 2.145728, Regularization: 0.629958, Discriminator: 0.043461; Generator: 0.021358,\n",
      "D(x): 0.503, D(G(z)): 0.505\n",
      "2019-04-09 23:59:07,002 root         INFO     Train Epoch: 54 [6144/8000 (77%)]\tTotal Loss: 2.918074\n",
      "Reconstruction: 2.205024, Regularization: 0.648257, Discriminator: 0.043530; Generator: 0.021264,\n",
      "D(x): 0.503, D(G(z)): 0.506\n",
      "2019-04-09 23:59:07,110 root         INFO     Train Epoch: 54 [6656/8000 (83%)]\tTotal Loss: 2.941428\n",
      "Reconstruction: 2.147507, Regularization: 0.728792, Discriminator: 0.043887; Generator: 0.021243,\n",
      "D(x): 0.498, D(G(z)): 0.507\n",
      "2019-04-09 23:59:07,217 root         INFO     Train Epoch: 54 [7168/8000 (90%)]\tTotal Loss: 2.945507\n",
      "Reconstruction: 2.142581, Regularization: 0.737880, Discriminator: 0.043783; Generator: 0.021264,\n",
      "D(x): 0.499, D(G(z)): 0.506\n",
      "2019-04-09 23:59:07,324 root         INFO     Train Epoch: 54 [7680/8000 (96%)]\tTotal Loss: 2.922609\n",
      "Reconstruction: 2.121278, Regularization: 0.736156, Discriminator: 0.043919; Generator: 0.021257,\n",
      "D(x): 0.497, D(G(z)): 0.507\n",
      "2019-04-09 23:59:07,404 root         INFO     ====> Epoch: 54 Average loss: 2.8458\n",
      "2019-04-09 23:59:07,430 root         INFO     Train Epoch: 55 [0/8000 (0%)]\tTotal Loss: 3.063806\n",
      "Reconstruction: 2.243655, Regularization: 0.755107, Discriminator: 0.043894; Generator: 0.021150,\n",
      "D(x): 0.499, D(G(z)): 0.508\n",
      "2019-04-09 23:59:07,538 root         INFO     Train Epoch: 55 [512/8000 (6%)]\tTotal Loss: 2.673605\n",
      "Reconstruction: 1.964806, Regularization: 0.644107, Discriminator: 0.043578; Generator: 0.021114,\n",
      "D(x): 0.505, D(G(z)): 0.509\n",
      "2019-04-09 23:59:07,646 root         INFO     Train Epoch: 55 [1024/8000 (13%)]\tTotal Loss: 3.034028\n",
      "Reconstruction: 2.239686, Regularization: 0.729343, Discriminator: 0.043917; Generator: 0.021081,\n",
      "D(x): 0.500, D(G(z)): 0.509\n",
      "2019-04-09 23:59:07,754 root         INFO     Train Epoch: 55 [1536/8000 (19%)]\tTotal Loss: 2.936754\n",
      "Reconstruction: 2.145185, Regularization: 0.726886, Discriminator: 0.043639; Generator: 0.021044,\n",
      "D(x): 0.505, D(G(z)): 0.510\n",
      "2019-04-09 23:59:07,862 root         INFO     Train Epoch: 55 [2048/8000 (26%)]\tTotal Loss: 2.567953\n",
      "Reconstruction: 1.871770, Regularization: 0.631446, Discriminator: 0.043710; Generator: 0.021028,\n",
      "D(x): 0.504, D(G(z)): 0.510\n",
      "2019-04-09 23:59:07,970 root         INFO     Train Epoch: 55 [2560/8000 (32%)]\tTotal Loss: 2.373742\n",
      "Reconstruction: 1.729349, Regularization: 0.579765, Discriminator: 0.043607; Generator: 0.021021,\n",
      "D(x): 0.506, D(G(z)): 0.510\n",
      "2019-04-09 23:59:08,078 root         INFO     Train Epoch: 55 [3072/8000 (38%)]\tTotal Loss: 2.826059\n",
      "Reconstruction: 2.067764, Regularization: 0.693677, Discriminator: 0.043671; Generator: 0.020948,\n",
      "D(x): 0.506, D(G(z)): 0.512\n",
      "2019-04-09 23:59:08,186 root         INFO     Train Epoch: 55 [3584/8000 (45%)]\tTotal Loss: 2.765659\n",
      "Reconstruction: 1.981886, Regularization: 0.718909, Discriminator: 0.043956; Generator: 0.020908,\n",
      "D(x): 0.502, D(G(z)): 0.512\n",
      "2019-04-09 23:59:08,295 root         INFO     Train Epoch: 55 [4096/8000 (51%)]\tTotal Loss: 2.724188\n",
      "Reconstruction: 1.911897, Regularization: 0.747764, Discriminator: 0.043641; Generator: 0.020885,\n",
      "D(x): 0.508, D(G(z)): 0.513\n",
      "2019-04-09 23:59:08,404 root         INFO     Train Epoch: 55 [4608/8000 (58%)]\tTotal Loss: 3.099988\n",
      "Reconstruction: 2.312420, Regularization: 0.722817, Discriminator: 0.043914; Generator: 0.020836,\n",
      "D(x): 0.504, D(G(z)): 0.513\n",
      "2019-04-09 23:59:08,513 root         INFO     Train Epoch: 55 [5120/8000 (64%)]\tTotal Loss: 3.040702\n",
      "Reconstruction: 2.185902, Regularization: 0.790026, Discriminator: 0.043993; Generator: 0.020782,\n",
      "D(x): 0.504, D(G(z)): 0.514\n",
      "2019-04-09 23:59:08,624 root         INFO     Train Epoch: 55 [5632/8000 (70%)]\tTotal Loss: 2.766342\n",
      "Reconstruction: 2.006972, Regularization: 0.694849, Discriminator: 0.043683; Generator: 0.020838,\n",
      "D(x): 0.508, D(G(z)): 0.513\n",
      "2019-04-09 23:59:08,732 root         INFO     Train Epoch: 55 [6144/8000 (77%)]\tTotal Loss: 3.100881\n",
      "Reconstruction: 2.301536, Regularization: 0.734589, Discriminator: 0.043940; Generator: 0.020816,\n",
      "D(x): 0.504, D(G(z)): 0.514\n",
      "2019-04-09 23:59:08,841 root         INFO     Train Epoch: 55 [6656/8000 (83%)]\tTotal Loss: 2.527306\n",
      "Reconstruction: 1.830627, Regularization: 0.632283, Discriminator: 0.043672; Generator: 0.020724,\n",
      "D(x): 0.510, D(G(z)): 0.515\n",
      "2019-04-09 23:59:08,950 root         INFO     Train Epoch: 55 [7168/8000 (90%)]\tTotal Loss: 2.759434\n",
      "Reconstruction: 2.034012, Regularization: 0.660864, Discriminator: 0.043890; Generator: 0.020668,\n",
      "D(x): 0.507, D(G(z)): 0.516\n",
      "2019-04-09 23:59:09,058 root         INFO     Train Epoch: 55 [7680/8000 (96%)]\tTotal Loss: 2.769390\n",
      "Reconstruction: 2.020119, Regularization: 0.684803, Discriminator: 0.043860; Generator: 0.020607,\n",
      "D(x): 0.509, D(G(z)): 0.517\n",
      "2019-04-09 23:59:09,139 root         INFO     ====> Epoch: 55 Average loss: 2.8270\n",
      "2019-04-09 23:59:09,166 root         INFO     Train Epoch: 56 [0/8000 (0%)]\tTotal Loss: 2.795934\n",
      "Reconstruction: 2.008811, Regularization: 0.722692, Discriminator: 0.043822; Generator: 0.020609,\n",
      "D(x): 0.510, D(G(z)): 0.517\n",
      "2019-04-09 23:59:09,275 root         INFO     Train Epoch: 56 [512/8000 (6%)]\tTotal Loss: 3.226972\n",
      "Reconstruction: 2.349344, Regularization: 0.813059, Discriminator: 0.043938; Generator: 0.020632,\n",
      "D(x): 0.507, D(G(z)): 0.517\n",
      "2019-04-09 23:59:09,383 root         INFO     Train Epoch: 56 [1024/8000 (13%)]\tTotal Loss: 2.765605\n",
      "Reconstruction: 1.968318, Regularization: 0.733036, Discriminator: 0.043768; Generator: 0.020482,\n",
      "D(x): 0.513, D(G(z)): 0.519\n",
      "2019-04-09 23:59:09,492 root         INFO     Train Epoch: 56 [1536/8000 (19%)]\tTotal Loss: 2.501081\n",
      "Reconstruction: 1.762382, Regularization: 0.674514, Discriminator: 0.043608; Generator: 0.020576,\n",
      "D(x): 0.514, D(G(z)): 0.518\n",
      "2019-04-09 23:59:09,601 root         INFO     Train Epoch: 56 [2048/8000 (26%)]\tTotal Loss: 3.175572\n",
      "Reconstruction: 2.279082, Regularization: 0.831921, Discriminator: 0.044059; Generator: 0.020511,\n",
      "D(x): 0.508, D(G(z)): 0.519\n",
      "2019-04-09 23:59:09,710 root         INFO     Train Epoch: 56 [2560/8000 (32%)]\tTotal Loss: 2.693654\n",
      "Reconstruction: 1.954701, Regularization: 0.674693, Discriminator: 0.043734; Generator: 0.020526,\n",
      "D(x): 0.512, D(G(z)): 0.519\n",
      "2019-04-09 23:59:09,818 root         INFO     Train Epoch: 56 [3072/8000 (38%)]\tTotal Loss: 2.515964\n",
      "Reconstruction: 1.792543, Regularization: 0.659369, Discriminator: 0.043590; Generator: 0.020462,\n",
      "D(x): 0.516, D(G(z)): 0.520\n",
      "2019-04-09 23:59:09,927 root         INFO     Train Epoch: 56 [3584/8000 (45%)]\tTotal Loss: 2.926429\n",
      "Reconstruction: 2.169633, Regularization: 0.692703, Discriminator: 0.043665; Generator: 0.020428,\n",
      "D(x): 0.515, D(G(z)): 0.520\n",
      "2019-04-09 23:59:10,035 root         INFO     Train Epoch: 56 [4096/8000 (51%)]\tTotal Loss: 2.615345\n",
      "Reconstruction: 1.898706, Regularization: 0.652560, Discriminator: 0.043566; Generator: 0.020513,\n",
      "D(x): 0.516, D(G(z)): 0.519\n",
      "2019-04-09 23:59:10,144 root         INFO     Train Epoch: 56 [4608/8000 (58%)]\tTotal Loss: 2.599196\n",
      "Reconstruction: 1.920873, Regularization: 0.614337, Discriminator: 0.043588; Generator: 0.020399,\n",
      "D(x): 0.517, D(G(z)): 0.521\n",
      "2019-04-09 23:59:10,252 root         INFO     Train Epoch: 56 [5120/8000 (64%)]\tTotal Loss: 2.278403\n",
      "Reconstruction: 1.624383, Regularization: 0.590202, Discriminator: 0.043366; Generator: 0.020452,\n",
      "D(x): 0.520, D(G(z)): 0.520\n",
      "2019-04-09 23:59:10,361 root         INFO     Train Epoch: 56 [5632/8000 (70%)]\tTotal Loss: 2.869970\n",
      "Reconstruction: 2.099671, Regularization: 0.706131, Discriminator: 0.043807; Generator: 0.020360,\n",
      "D(x): 0.514, D(G(z)): 0.521\n",
      "2019-04-09 23:59:10,469 root         INFO     Train Epoch: 56 [6144/8000 (77%)]\tTotal Loss: 2.823966\n",
      "Reconstruction: 2.038945, Regularization: 0.720935, Discriminator: 0.043733; Generator: 0.020352,\n",
      "D(x): 0.516, D(G(z)): 0.521\n",
      "2019-04-09 23:59:10,578 root         INFO     Train Epoch: 56 [6656/8000 (83%)]\tTotal Loss: 2.785650\n",
      "Reconstruction: 2.012502, Regularization: 0.709111, Discriminator: 0.043690; Generator: 0.020347,\n",
      "D(x): 0.516, D(G(z)): 0.521\n",
      "2019-04-09 23:59:10,686 root         INFO     Train Epoch: 56 [7168/8000 (90%)]\tTotal Loss: 2.626979\n",
      "Reconstruction: 1.900644, Regularization: 0.662381, Discriminator: 0.043649; Generator: 0.020306,\n",
      "D(x): 0.518, D(G(z)): 0.522\n",
      "2019-04-09 23:59:10,795 root         INFO     Train Epoch: 56 [7680/8000 (96%)]\tTotal Loss: 2.762691\n",
      "Reconstruction: 1.947634, Regularization: 0.751010, Discriminator: 0.043715; Generator: 0.020332,\n",
      "D(x): 0.516, D(G(z)): 0.522\n",
      "2019-04-09 23:59:10,874 root         INFO     ====> Epoch: 56 Average loss: 2.8071\n",
      "2019-04-09 23:59:10,901 root         INFO     Train Epoch: 57 [0/8000 (0%)]\tTotal Loss: 2.699376\n",
      "Reconstruction: 1.920573, Regularization: 0.714696, Discriminator: 0.043815; Generator: 0.020291,\n",
      "D(x): 0.515, D(G(z)): 0.522\n",
      "2019-04-09 23:59:11,010 root         INFO     Train Epoch: 57 [512/8000 (6%)]\tTotal Loss: 2.431190\n",
      "Reconstruction: 1.712011, Regularization: 0.655211, Discriminator: 0.043731; Generator: 0.020238,\n",
      "D(x): 0.518, D(G(z)): 0.523\n",
      "2019-04-09 23:59:11,119 root         INFO     Train Epoch: 57 [1024/8000 (13%)]\tTotal Loss: 2.997612\n",
      "Reconstruction: 2.146786, Regularization: 0.786685, Discriminator: 0.043917; Generator: 0.020224,\n",
      "D(x): 0.515, D(G(z)): 0.524\n",
      "2019-04-09 23:59:11,227 root         INFO     Train Epoch: 57 [1536/8000 (19%)]\tTotal Loss: 2.591890\n",
      "Reconstruction: 1.887667, Regularization: 0.640293, Discriminator: 0.043715; Generator: 0.020214,\n",
      "D(x): 0.518, D(G(z)): 0.524\n",
      "2019-04-09 23:59:11,337 root         INFO     Train Epoch: 57 [2048/8000 (26%)]\tTotal Loss: 3.059228\n",
      "Reconstruction: 2.226510, Regularization: 0.768605, Discriminator: 0.043908; Generator: 0.020206,\n",
      "D(x): 0.515, D(G(z)): 0.524\n",
      "2019-04-09 23:59:11,445 root         INFO     Train Epoch: 57 [2560/8000 (32%)]\tTotal Loss: 2.633957\n",
      "Reconstruction: 1.900391, Regularization: 0.669619, Discriminator: 0.043793; Generator: 0.020155,\n",
      "D(x): 0.518, D(G(z)): 0.525\n",
      "2019-04-09 23:59:11,552 root         INFO     Train Epoch: 57 [3072/8000 (38%)]\tTotal Loss: 3.131204\n",
      "Reconstruction: 2.261627, Regularization: 0.805514, Discriminator: 0.043874; Generator: 0.020190,\n",
      "D(x): 0.516, D(G(z)): 0.524\n",
      "2019-04-09 23:59:11,660 root         INFO     Train Epoch: 57 [3584/8000 (45%)]\tTotal Loss: 3.305964\n",
      "Reconstruction: 2.473628, Regularization: 0.768316, Discriminator: 0.043868; Generator: 0.020152,\n",
      "D(x): 0.517, D(G(z)): 0.525\n",
      "2019-04-09 23:59:11,765 root         INFO     Train Epoch: 57 [4096/8000 (51%)]\tTotal Loss: 2.634142\n",
      "Reconstruction: 1.882492, Regularization: 0.687783, Discriminator: 0.043744; Generator: 0.020123,\n",
      "D(x): 0.520, D(G(z)): 0.525\n",
      "2019-04-09 23:59:11,870 root         INFO     Train Epoch: 57 [4608/8000 (58%)]\tTotal Loss: 2.960532\n",
      "Reconstruction: 2.120008, Regularization: 0.776602, Discriminator: 0.043808; Generator: 0.020115,\n",
      "D(x): 0.519, D(G(z)): 0.525\n",
      "2019-04-09 23:59:11,976 root         INFO     Train Epoch: 57 [5120/8000 (64%)]\tTotal Loss: 3.158364\n",
      "Reconstruction: 2.287106, Regularization: 0.807177, Discriminator: 0.044004; Generator: 0.020077,\n",
      "D(x): 0.516, D(G(z)): 0.526\n",
      "2019-04-09 23:59:12,082 root         INFO     Train Epoch: 57 [5632/8000 (70%)]\tTotal Loss: 2.761581\n",
      "Reconstruction: 1.954559, Regularization: 0.743136, Discriminator: 0.043864; Generator: 0.020023,\n",
      "D(x): 0.519, D(G(z)): 0.527\n",
      "2019-04-09 23:59:12,187 root         INFO     Train Epoch: 57 [6144/8000 (77%)]\tTotal Loss: 2.787501\n",
      "Reconstruction: 1.899203, Regularization: 0.824445, Discriminator: 0.043810; Generator: 0.020043,\n",
      "D(x): 0.520, D(G(z)): 0.527\n",
      "2019-04-09 23:59:12,292 root         INFO     Train Epoch: 57 [6656/8000 (83%)]\tTotal Loss: 2.366281\n",
      "Reconstruction: 1.667408, Regularization: 0.635222, Discriminator: 0.043590; Generator: 0.020061,\n",
      "D(x): 0.523, D(G(z)): 0.526\n",
      "2019-04-09 23:59:12,396 root         INFO     Train Epoch: 57 [7168/8000 (90%)]\tTotal Loss: 2.966159\n",
      "Reconstruction: 2.108427, Regularization: 0.793831, Discriminator: 0.043845; Generator: 0.020056,\n",
      "D(x): 0.519, D(G(z)): 0.526\n",
      "2019-04-09 23:59:12,501 root         INFO     Train Epoch: 57 [7680/8000 (96%)]\tTotal Loss: 2.604512\n",
      "Reconstruction: 1.832752, Regularization: 0.708017, Discriminator: 0.043722; Generator: 0.020020,\n",
      "D(x): 0.522, D(G(z)): 0.527\n",
      "2019-04-09 23:59:12,579 root         INFO     ====> Epoch: 57 Average loss: 2.8076\n",
      "2019-04-09 23:59:12,606 root         INFO     Train Epoch: 58 [0/8000 (0%)]\tTotal Loss: 2.798075\n",
      "Reconstruction: 1.988170, Regularization: 0.746144, Discriminator: 0.043742; Generator: 0.020019,\n",
      "D(x): 0.522, D(G(z)): 0.527\n",
      "2019-04-09 23:59:12,714 root         INFO     Train Epoch: 58 [512/8000 (6%)]\tTotal Loss: 2.581148\n",
      "Reconstruction: 1.833008, Regularization: 0.684335, Discriminator: 0.043817; Generator: 0.019989,\n",
      "D(x): 0.521, D(G(z)): 0.527\n",
      "2019-04-09 23:59:12,821 root         INFO     Train Epoch: 58 [1024/8000 (13%)]\tTotal Loss: 2.564077\n",
      "Reconstruction: 1.792565, Regularization: 0.707875, Discriminator: 0.043582; Generator: 0.020056,\n",
      "D(x): 0.524, D(G(z)): 0.526\n",
      "2019-04-09 23:59:12,929 root         INFO     Train Epoch: 58 [1536/8000 (19%)]\tTotal Loss: 2.771239\n",
      "Reconstruction: 1.979386, Regularization: 0.728053, Discriminator: 0.043791; Generator: 0.020009,\n",
      "D(x): 0.521, D(G(z)): 0.527\n",
      "2019-04-09 23:59:13,036 root         INFO     Train Epoch: 58 [2048/8000 (26%)]\tTotal Loss: 2.522061\n",
      "Reconstruction: 1.785710, Regularization: 0.672687, Discriminator: 0.043700; Generator: 0.019963,\n",
      "D(x): 0.523, D(G(z)): 0.528\n",
      "2019-04-09 23:59:13,144 root         INFO     Train Epoch: 58 [2560/8000 (32%)]\tTotal Loss: 2.891250\n",
      "Reconstruction: 2.067115, Regularization: 0.760409, Discriminator: 0.043775; Generator: 0.019952,\n",
      "D(x): 0.522, D(G(z)): 0.528\n",
      "2019-04-09 23:59:13,252 root         INFO     Train Epoch: 58 [3072/8000 (38%)]\tTotal Loss: 3.075208\n",
      "Reconstruction: 2.130301, Regularization: 0.881056, Discriminator: 0.043937; Generator: 0.019914,\n",
      "D(x): 0.520, D(G(z)): 0.529\n",
      "2019-04-09 23:59:13,359 root         INFO     Train Epoch: 58 [3584/8000 (45%)]\tTotal Loss: 2.660025\n",
      "Reconstruction: 1.899938, Regularization: 0.696375, Discriminator: 0.043780; Generator: 0.019931,\n",
      "D(x): 0.523, D(G(z)): 0.528\n",
      "2019-04-09 23:59:13,465 root         INFO     Train Epoch: 58 [4096/8000 (51%)]\tTotal Loss: 2.483348\n",
      "Reconstruction: 1.768876, Regularization: 0.650882, Discriminator: 0.043687; Generator: 0.019904,\n",
      "D(x): 0.525, D(G(z)): 0.529\n",
      "2019-04-09 23:59:13,572 root         INFO     Train Epoch: 58 [4608/8000 (58%)]\tTotal Loss: 2.953188\n",
      "Reconstruction: 2.101971, Regularization: 0.787502, Discriminator: 0.043810; Generator: 0.019905,\n",
      "D(x): 0.522, D(G(z)): 0.529\n",
      "2019-04-09 23:59:13,679 root         INFO     Train Epoch: 58 [5120/8000 (64%)]\tTotal Loss: 2.437392\n",
      "Reconstruction: 1.732825, Regularization: 0.641015, Discriminator: 0.043633; Generator: 0.019918,\n",
      "D(x): 0.525, D(G(z)): 0.529\n",
      "2019-04-09 23:59:13,786 root         INFO     Train Epoch: 58 [5632/8000 (70%)]\tTotal Loss: 2.920338\n",
      "Reconstruction: 2.091671, Regularization: 0.764966, Discriminator: 0.043824; Generator: 0.019877,\n",
      "D(x): 0.523, D(G(z)): 0.529\n",
      "2019-04-09 23:59:13,893 root         INFO     Train Epoch: 58 [6144/8000 (77%)]\tTotal Loss: 2.867258\n",
      "Reconstruction: 2.034221, Regularization: 0.769371, Discriminator: 0.043787; Generator: 0.019879,\n",
      "D(x): 0.523, D(G(z)): 0.529\n",
      "2019-04-09 23:59:14,000 root         INFO     Train Epoch: 58 [6656/8000 (83%)]\tTotal Loss: 2.713270\n",
      "Reconstruction: 1.924132, Regularization: 0.725527, Discriminator: 0.043740; Generator: 0.019871,\n",
      "D(x): 0.524, D(G(z)): 0.529\n",
      "2019-04-09 23:59:14,106 root         INFO     Train Epoch: 58 [7168/8000 (90%)]\tTotal Loss: 2.686587\n",
      "Reconstruction: 1.865024, Regularization: 0.758042, Discriminator: 0.043650; Generator: 0.019871,\n",
      "D(x): 0.526, D(G(z)): 0.529\n",
      "2019-04-09 23:59:14,213 root         INFO     Train Epoch: 58 [7680/8000 (96%)]\tTotal Loss: 2.880834\n",
      "Reconstruction: 2.018570, Regularization: 0.798633, Discriminator: 0.043767; Generator: 0.019865,\n",
      "D(x): 0.524, D(G(z)): 0.530\n",
      "2019-04-09 23:59:14,292 root         INFO     ====> Epoch: 58 Average loss: 2.7796\n",
      "2019-04-09 23:59:14,318 root         INFO     Train Epoch: 59 [0/8000 (0%)]\tTotal Loss: 2.778871\n",
      "Reconstruction: 1.915615, Regularization: 0.799655, Discriminator: 0.043751; Generator: 0.019850,\n",
      "D(x): 0.525, D(G(z)): 0.530\n",
      "2019-04-09 23:59:14,428 root         INFO     Train Epoch: 59 [512/8000 (6%)]\tTotal Loss: 2.558116\n",
      "Reconstruction: 1.781092, Regularization: 0.713566, Discriminator: 0.043650; Generator: 0.019808,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:14,536 root         INFO     Train Epoch: 59 [1024/8000 (13%)]\tTotal Loss: 2.899295\n",
      "Reconstruction: 2.054009, Regularization: 0.781660, Discriminator: 0.043792; Generator: 0.019834,\n",
      "D(x): 0.524, D(G(z)): 0.530\n",
      "2019-04-09 23:59:14,644 root         INFO     Train Epoch: 59 [1536/8000 (19%)]\tTotal Loss: 2.364408\n",
      "Reconstruction: 1.635842, Regularization: 0.665148, Discriminator: 0.043607; Generator: 0.019812,\n",
      "D(x): 0.528, D(G(z)): 0.530\n",
      "2019-04-09 23:59:14,752 root         INFO     Train Epoch: 59 [2048/8000 (26%)]\tTotal Loss: 2.852992\n",
      "Reconstruction: 1.955048, Regularization: 0.834337, Discriminator: 0.043768; Generator: 0.019838,\n",
      "D(x): 0.524, D(G(z)): 0.530\n",
      "2019-04-09 23:59:14,860 root         INFO     Train Epoch: 59 [2560/8000 (32%)]\tTotal Loss: 2.923204\n",
      "Reconstruction: 2.028606, Regularization: 0.831033, Discriminator: 0.043761; Generator: 0.019803,\n",
      "D(x): 0.525, D(G(z)): 0.531\n",
      "2019-04-09 23:59:14,968 root         INFO     Train Epoch: 59 [3072/8000 (38%)]\tTotal Loss: 3.014770\n",
      "Reconstruction: 2.135669, Regularization: 0.815493, Discriminator: 0.043813; Generator: 0.019794,\n",
      "D(x): 0.525, D(G(z)): 0.531\n",
      "2019-04-09 23:59:15,076 root         INFO     Train Epoch: 59 [3584/8000 (45%)]\tTotal Loss: 2.804801\n",
      "Reconstruction: 1.976999, Regularization: 0.764297, Discriminator: 0.043695; Generator: 0.019810,\n",
      "D(x): 0.526, D(G(z)): 0.531\n",
      "2019-04-09 23:59:15,184 root         INFO     Train Epoch: 59 [4096/8000 (51%)]\tTotal Loss: 2.725661\n",
      "Reconstruction: 1.899554, Regularization: 0.762599, Discriminator: 0.043702; Generator: 0.019806,\n",
      "D(x): 0.526, D(G(z)): 0.531\n",
      "2019-04-09 23:59:15,293 root         INFO     Train Epoch: 59 [4608/8000 (58%)]\tTotal Loss: 2.819087\n",
      "Reconstruction: 1.961792, Regularization: 0.793783, Discriminator: 0.043682; Generator: 0.019830,\n",
      "D(x): 0.526, D(G(z)): 0.530\n",
      "2019-04-09 23:59:15,403 root         INFO     Train Epoch: 59 [5120/8000 (64%)]\tTotal Loss: 2.531970\n",
      "Reconstruction: 1.761055, Regularization: 0.707432, Discriminator: 0.043690; Generator: 0.019793,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:15,512 root         INFO     Train Epoch: 59 [5632/8000 (70%)]\tTotal Loss: 2.536140\n",
      "Reconstruction: 1.802675, Regularization: 0.670029, Discriminator: 0.043649; Generator: 0.019787,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:15,621 root         INFO     Train Epoch: 59 [6144/8000 (77%)]\tTotal Loss: 2.608509\n",
      "Reconstruction: 1.878320, Regularization: 0.666752, Discriminator: 0.043604; Generator: 0.019832,\n",
      "D(x): 0.527, D(G(z)): 0.530\n",
      "2019-04-09 23:59:15,730 root         INFO     Train Epoch: 59 [6656/8000 (83%)]\tTotal Loss: 2.869583\n",
      "Reconstruction: 2.032924, Regularization: 0.773157, Discriminator: 0.043702; Generator: 0.019801,\n",
      "D(x): 0.526, D(G(z)): 0.531\n",
      "2019-04-09 23:59:15,839 root         INFO     Train Epoch: 59 [7168/8000 (90%)]\tTotal Loss: 2.762256\n",
      "Reconstruction: 1.966246, Regularization: 0.732543, Discriminator: 0.043700; Generator: 0.019767,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:15,948 root         INFO     Train Epoch: 59 [7680/8000 (96%)]\tTotal Loss: 3.058601\n",
      "Reconstruction: 2.085885, Regularization: 0.909207, Discriminator: 0.043698; Generator: 0.019812,\n",
      "D(x): 0.526, D(G(z)): 0.530\n",
      "2019-04-09 23:59:16,029 root         INFO     ====> Epoch: 59 Average loss: 2.7428\n",
      "2019-04-09 23:59:16,056 root         INFO     Train Epoch: 60 [0/8000 (0%)]\tTotal Loss: 2.643731\n",
      "Reconstruction: 1.863482, Regularization: 0.716805, Discriminator: 0.043648; Generator: 0.019796,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:16,168 root         INFO     Train Epoch: 60 [512/8000 (6%)]\tTotal Loss: 2.698276\n",
      "Reconstruction: 1.868160, Regularization: 0.766716, Discriminator: 0.043625; Generator: 0.019774,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:16,280 root         INFO     Train Epoch: 60 [1024/8000 (13%)]\tTotal Loss: 3.063475\n",
      "Reconstruction: 2.156999, Regularization: 0.842919, Discriminator: 0.043791; Generator: 0.019766,\n",
      "D(x): 0.525, D(G(z)): 0.531\n",
      "2019-04-09 23:59:16,392 root         INFO     Train Epoch: 60 [1536/8000 (19%)]\tTotal Loss: 2.883660\n",
      "Reconstruction: 1.957414, Regularization: 0.862776, Discriminator: 0.043685; Generator: 0.019785,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:16,502 root         INFO     Train Epoch: 60 [2048/8000 (26%)]\tTotal Loss: 2.793044\n",
      "Reconstruction: 1.908365, Regularization: 0.821233, Discriminator: 0.043674; Generator: 0.019772,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:16,612 root         INFO     Train Epoch: 60 [2560/8000 (32%)]\tTotal Loss: 2.651323\n",
      "Reconstruction: 1.816049, Regularization: 0.771883, Discriminator: 0.043618; Generator: 0.019773,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:16,722 root         INFO     Train Epoch: 60 [3072/8000 (38%)]\tTotal Loss: 2.658404\n",
      "Reconstruction: 1.828882, Regularization: 0.766128, Discriminator: 0.043645; Generator: 0.019748,\n",
      "D(x): 0.528, D(G(z)): 0.532\n",
      "2019-04-09 23:59:16,833 root         INFO     Train Epoch: 60 [3584/8000 (45%)]\tTotal Loss: 2.900337\n",
      "Reconstruction: 1.966182, Regularization: 0.870690, Discriminator: 0.043692; Generator: 0.019772,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:16,943 root         INFO     Train Epoch: 60 [4096/8000 (51%)]\tTotal Loss: 2.416740\n",
      "Reconstruction: 1.646955, Regularization: 0.706428, Discriminator: 0.043605; Generator: 0.019753,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:17,053 root         INFO     Train Epoch: 60 [4608/8000 (58%)]\tTotal Loss: 2.602440\n",
      "Reconstruction: 1.785166, Regularization: 0.753913, Discriminator: 0.043584; Generator: 0.019778,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:17,164 root         INFO     Train Epoch: 60 [5120/8000 (64%)]\tTotal Loss: 2.608240\n",
      "Reconstruction: 1.800319, Regularization: 0.744536, Discriminator: 0.043634; Generator: 0.019751,\n",
      "D(x): 0.528, D(G(z)): 0.532\n",
      "2019-04-09 23:59:17,274 root         INFO     Train Epoch: 60 [5632/8000 (70%)]\tTotal Loss: 2.258250\n",
      "Reconstruction: 1.540981, Regularization: 0.653980, Discriminator: 0.043522; Generator: 0.019767,\n",
      "D(x): 0.530, D(G(z)): 0.531\n",
      "2019-04-09 23:59:17,384 root         INFO     Train Epoch: 60 [6144/8000 (77%)]\tTotal Loss: 2.904422\n",
      "Reconstruction: 1.996071, Regularization: 0.844925, Discriminator: 0.043664; Generator: 0.019762,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:17,494 root         INFO     Train Epoch: 60 [6656/8000 (83%)]\tTotal Loss: 2.941960\n",
      "Reconstruction: 2.000387, Regularization: 0.878115, Discriminator: 0.043699; Generator: 0.019758,\n",
      "D(x): 0.527, D(G(z)): 0.531\n",
      "2019-04-09 23:59:17,604 root         INFO     Train Epoch: 60 [7168/8000 (90%)]\tTotal Loss: 2.472247\n",
      "Reconstruction: 1.692974, Regularization: 0.715932, Discriminator: 0.043590; Generator: 0.019751,\n",
      "D(x): 0.529, D(G(z)): 0.532\n",
      "2019-04-09 23:59:17,715 root         INFO     Train Epoch: 60 [7680/8000 (96%)]\tTotal Loss: 2.767606\n",
      "Reconstruction: 1.887204, Regularization: 0.817021, Discriminator: 0.043624; Generator: 0.019757,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:17,796 root         INFO     ====> Epoch: 60 Average loss: 2.7271\n",
      "2019-04-09 23:59:17,823 root         INFO     Train Epoch: 61 [0/8000 (0%)]\tTotal Loss: 2.429397\n",
      "Reconstruction: 1.706029, Regularization: 0.660057, Discriminator: 0.043569; Generator: 0.019742,\n",
      "D(x): 0.530, D(G(z)): 0.532\n",
      "2019-04-09 23:59:17,934 root         INFO     Train Epoch: 61 [512/8000 (6%)]\tTotal Loss: 2.730734\n",
      "Reconstruction: 1.866277, Regularization: 0.801096, Discriminator: 0.043601; Generator: 0.019760,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,044 root         INFO     Train Epoch: 61 [1024/8000 (13%)]\tTotal Loss: 2.841708\n",
      "Reconstruction: 1.922984, Regularization: 0.855341, Discriminator: 0.043614; Generator: 0.019768,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,154 root         INFO     Train Epoch: 61 [1536/8000 (19%)]\tTotal Loss: 2.715528\n",
      "Reconstruction: 1.856732, Regularization: 0.795439, Discriminator: 0.043575; Generator: 0.019783,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,263 root         INFO     Train Epoch: 61 [2048/8000 (26%)]\tTotal Loss: 2.729019\n",
      "Reconstruction: 1.889111, Regularization: 0.776553, Discriminator: 0.043583; Generator: 0.019771,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,373 root         INFO     Train Epoch: 61 [2560/8000 (32%)]\tTotal Loss: 2.370521\n",
      "Reconstruction: 1.610867, Regularization: 0.696336, Discriminator: 0.043534; Generator: 0.019785,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,483 root         INFO     Train Epoch: 61 [3072/8000 (38%)]\tTotal Loss: 2.774379\n",
      "Reconstruction: 1.905147, Regularization: 0.805854, Discriminator: 0.043597; Generator: 0.019781,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,592 root         INFO     Train Epoch: 61 [3584/8000 (45%)]\tTotal Loss: 3.096014\n",
      "Reconstruction: 2.087824, Regularization: 0.944774, Discriminator: 0.043633; Generator: 0.019783,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,702 root         INFO     Train Epoch: 61 [4096/8000 (51%)]\tTotal Loss: 2.751934\n",
      "Reconstruction: 1.908524, Regularization: 0.780046, Discriminator: 0.043583; Generator: 0.019781,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,812 root         INFO     Train Epoch: 61 [4608/8000 (58%)]\tTotal Loss: 2.505661\n",
      "Reconstruction: 1.726912, Regularization: 0.715408, Discriminator: 0.043550; Generator: 0.019790,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:18,922 root         INFO     Train Epoch: 61 [5120/8000 (64%)]\tTotal Loss: 2.521159\n",
      "Reconstruction: 1.722021, Regularization: 0.735796, Discriminator: 0.043553; Generator: 0.019789,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,032 root         INFO     Train Epoch: 61 [5632/8000 (70%)]\tTotal Loss: 2.714417\n",
      "Reconstruction: 1.843380, Regularization: 0.807684, Discriminator: 0.043568; Generator: 0.019786,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,142 root         INFO     Train Epoch: 61 [6144/8000 (77%)]\tTotal Loss: 2.761514\n",
      "Reconstruction: 1.910495, Regularization: 0.787660, Discriminator: 0.043554; Generator: 0.019805,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,252 root         INFO     Train Epoch: 61 [6656/8000 (83%)]\tTotal Loss: 2.620165\n",
      "Reconstruction: 1.753579, Regularization: 0.803241, Discriminator: 0.043553; Generator: 0.019791,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,361 root         INFO     Train Epoch: 61 [7168/8000 (90%)]\tTotal Loss: 2.729963\n",
      "Reconstruction: 1.816229, Regularization: 0.850357, Discriminator: 0.043569; Generator: 0.019807,\n",
      "D(x): 0.528, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,471 root         INFO     Train Epoch: 61 [7680/8000 (96%)]\tTotal Loss: 2.718450\n",
      "Reconstruction: 1.792295, Regularization: 0.862797, Discriminator: 0.043549; Generator: 0.019810,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,551 root         INFO     ====> Epoch: 61 Average loss: 2.7120\n",
      "2019-04-09 23:59:19,579 root         INFO     Train Epoch: 62 [0/8000 (0%)]\tTotal Loss: 2.416320\n",
      "Reconstruction: 1.647834, Regularization: 0.705157, Discriminator: 0.043525; Generator: 0.019805,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,690 root         INFO     Train Epoch: 62 [512/8000 (6%)]\tTotal Loss: 2.846928\n",
      "Reconstruction: 1.841084, Regularization: 0.942473, Discriminator: 0.043553; Generator: 0.019818,\n",
      "D(x): 0.528, D(G(z)): 0.530\n",
      "2019-04-09 23:59:19,801 root         INFO     Train Epoch: 62 [1024/8000 (13%)]\tTotal Loss: 2.671590\n",
      "Reconstruction: 1.788776, Regularization: 0.819458, Discriminator: 0.043550; Generator: 0.019806,\n",
      "D(x): 0.529, D(G(z)): 0.531\n",
      "2019-04-09 23:59:19,911 root         INFO     Train Epoch: 62 [1536/8000 (19%)]\tTotal Loss: 2.845010\n",
      "Reconstruction: 1.876519, Regularization: 0.905128, Discriminator: 0.043534; Generator: 0.019829,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,022 root         INFO     Train Epoch: 62 [2048/8000 (26%)]\tTotal Loss: 2.577489\n",
      "Reconstruction: 1.757718, Regularization: 0.756441, Discriminator: 0.043506; Generator: 0.019825,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,133 root         INFO     Train Epoch: 62 [2560/8000 (32%)]\tTotal Loss: 2.570988\n",
      "Reconstruction: 1.748886, Regularization: 0.758757, Discriminator: 0.043517; Generator: 0.019828,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,244 root         INFO     Train Epoch: 62 [3072/8000 (38%)]\tTotal Loss: 2.913333\n",
      "Reconstruction: 1.953884, Regularization: 0.896081, Discriminator: 0.043531; Generator: 0.019838,\n",
      "D(x): 0.528, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,354 root         INFO     Train Epoch: 62 [3584/8000 (45%)]\tTotal Loss: 2.829060\n",
      "Reconstruction: 1.923956, Regularization: 0.841751, Discriminator: 0.043512; Generator: 0.019841,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,464 root         INFO     Train Epoch: 62 [4096/8000 (51%)]\tTotal Loss: 2.744569\n",
      "Reconstruction: 1.871071, Regularization: 0.810141, Discriminator: 0.043521; Generator: 0.019835,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,572 root         INFO     Train Epoch: 62 [4608/8000 (58%)]\tTotal Loss: 2.700122\n",
      "Reconstruction: 1.771540, Regularization: 0.865227, Discriminator: 0.043519; Generator: 0.019837,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,680 root         INFO     Train Epoch: 62 [5120/8000 (64%)]\tTotal Loss: 2.638493\n",
      "Reconstruction: 1.831438, Regularization: 0.743708, Discriminator: 0.043488; Generator: 0.019860,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,788 root         INFO     Train Epoch: 62 [5632/8000 (70%)]\tTotal Loss: 2.629389\n",
      "Reconstruction: 1.800266, Regularization: 0.765774, Discriminator: 0.043499; Generator: 0.019850,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:20,896 root         INFO     Train Epoch: 62 [6144/8000 (77%)]\tTotal Loss: 2.809678\n",
      "Reconstruction: 1.912992, Regularization: 0.833328, Discriminator: 0.043501; Generator: 0.019858,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:21,004 root         INFO     Train Epoch: 62 [6656/8000 (83%)]\tTotal Loss: 2.672697\n",
      "Reconstruction: 1.800421, Regularization: 0.808926, Discriminator: 0.043483; Generator: 0.019867,\n",
      "D(x): 0.529, D(G(z)): 0.530\n",
      "2019-04-09 23:59:21,112 root         INFO     Train Epoch: 62 [7168/8000 (90%)]\tTotal Loss: 2.652826\n",
      "Reconstruction: 1.761652, Regularization: 0.827821, Discriminator: 0.043477; Generator: 0.019876,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,220 root         INFO     Train Epoch: 62 [7680/8000 (96%)]\tTotal Loss: 2.544996\n",
      "Reconstruction: 1.694019, Regularization: 0.787630, Discriminator: 0.043470; Generator: 0.019877,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,299 root         INFO     ====> Epoch: 62 Average loss: 2.7011\n",
      "2019-04-09 23:59:21,327 root         INFO     Train Epoch: 63 [0/8000 (0%)]\tTotal Loss: 2.785105\n",
      "Reconstruction: 1.909932, Regularization: 0.811811, Discriminator: 0.043473; Generator: 0.019889,\n",
      "D(x): 0.528, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,436 root         INFO     Train Epoch: 63 [512/8000 (6%)]\tTotal Loss: 2.546465\n",
      "Reconstruction: 1.738405, Regularization: 0.744707, Discriminator: 0.043464; Generator: 0.019889,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,546 root         INFO     Train Epoch: 63 [1024/8000 (13%)]\tTotal Loss: 2.514540\n",
      "Reconstruction: 1.642006, Regularization: 0.809185, Discriminator: 0.043459; Generator: 0.019891,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,655 root         INFO     Train Epoch: 63 [1536/8000 (19%)]\tTotal Loss: 2.901388\n",
      "Reconstruction: 1.928430, Regularization: 0.909588, Discriminator: 0.043463; Generator: 0.019907,\n",
      "D(x): 0.528, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,764 root         INFO     Train Epoch: 63 [2048/8000 (26%)]\tTotal Loss: 2.888118\n",
      "Reconstruction: 1.904889, Regularization: 0.919864, Discriminator: 0.043460; Generator: 0.019905,\n",
      "D(x): 0.528, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,873 root         INFO     Train Epoch: 63 [2560/8000 (32%)]\tTotal Loss: 2.962248\n",
      "Reconstruction: 2.008473, Regularization: 0.890415, Discriminator: 0.043456; Generator: 0.019905,\n",
      "D(x): 0.528, D(G(z)): 0.529\n",
      "2019-04-09 23:59:21,982 root         INFO     Train Epoch: 63 [3072/8000 (38%)]\tTotal Loss: 2.689770\n",
      "Reconstruction: 1.783621, Regularization: 0.842799, Discriminator: 0.043446; Generator: 0.019905,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:22,091 root         INFO     Train Epoch: 63 [3584/8000 (45%)]\tTotal Loss: 2.734654\n",
      "Reconstruction: 1.835033, Regularization: 0.836270, Discriminator: 0.043444; Generator: 0.019908,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:22,200 root         INFO     Train Epoch: 63 [4096/8000 (51%)]\tTotal Loss: 2.842585\n",
      "Reconstruction: 1.855958, Regularization: 0.923274, Discriminator: 0.043437; Generator: 0.019915,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:22,308 root         INFO     Train Epoch: 63 [4608/8000 (58%)]\tTotal Loss: 2.625874\n",
      "Reconstruction: 1.801987, Regularization: 0.760541, Discriminator: 0.043428; Generator: 0.019919,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:22,417 root         INFO     Train Epoch: 63 [5120/8000 (64%)]\tTotal Loss: 2.414972\n",
      "Reconstruction: 1.617093, Regularization: 0.734531, Discriminator: 0.043422; Generator: 0.019926,\n",
      "D(x): 0.529, D(G(z)): 0.529\n",
      "2019-04-09 23:59:22,526 root         INFO     Train Epoch: 63 [5632/8000 (70%)]\tTotal Loss: 2.762436\n",
      "Reconstruction: 1.818934, Regularization: 0.880153, Discriminator: 0.043418; Generator: 0.019932,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:22,636 root         INFO     Train Epoch: 63 [6144/8000 (77%)]\tTotal Loss: 2.411626\n",
      "Reconstruction: 1.563346, Regularization: 0.784926, Discriminator: 0.043413; Generator: 0.019941,\n",
      "D(x): 0.528, D(G(z)): 0.528\n",
      "2019-04-09 23:59:22,745 root         INFO     Train Epoch: 63 [6656/8000 (83%)]\tTotal Loss: 2.763657\n",
      "Reconstruction: 1.802386, Regularization: 0.897915, Discriminator: 0.043406; Generator: 0.019949,\n",
      "D(x): 0.528, D(G(z)): 0.528\n",
      "2019-04-09 23:59:22,854 root         INFO     Train Epoch: 63 [7168/8000 (90%)]\tTotal Loss: 2.620811\n",
      "Reconstruction: 1.684461, Regularization: 0.872996, Discriminator: 0.043403; Generator: 0.019951,\n",
      "D(x): 0.528, D(G(z)): 0.528\n",
      "2019-04-09 23:59:22,963 root         INFO     Train Epoch: 63 [7680/8000 (96%)]\tTotal Loss: 2.370608\n",
      "Reconstruction: 1.576643, Regularization: 0.730608, Discriminator: 0.043401; Generator: 0.019955,\n",
      "D(x): 0.528, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,043 root         INFO     ====> Epoch: 63 Average loss: 2.6911\n",
      "2019-04-09 23:59:23,071 root         INFO     Train Epoch: 64 [0/8000 (0%)]\tTotal Loss: 2.539283\n",
      "Reconstruction: 1.686208, Regularization: 0.789728, Discriminator: 0.043393; Generator: 0.019954,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,181 root         INFO     Train Epoch: 64 [512/8000 (6%)]\tTotal Loss: 2.563736\n",
      "Reconstruction: 1.706905, Regularization: 0.793486, Discriminator: 0.043387; Generator: 0.019958,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,290 root         INFO     Train Epoch: 64 [1024/8000 (13%)]\tTotal Loss: 2.414126\n",
      "Reconstruction: 1.622351, Regularization: 0.728427, Discriminator: 0.043387; Generator: 0.019962,\n",
      "D(x): 0.528, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,399 root         INFO     Train Epoch: 64 [1536/8000 (19%)]\tTotal Loss: 2.646176\n",
      "Reconstruction: 1.730848, Regularization: 0.851992, Discriminator: 0.043368; Generator: 0.019967,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,507 root         INFO     Train Epoch: 64 [2048/8000 (26%)]\tTotal Loss: 2.519472\n",
      "Reconstruction: 1.675576, Regularization: 0.780553, Discriminator: 0.043372; Generator: 0.019971,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,617 root         INFO     Train Epoch: 64 [2560/8000 (32%)]\tTotal Loss: 2.529380\n",
      "Reconstruction: 1.659330, Regularization: 0.806708, Discriminator: 0.043370; Generator: 0.019973,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,727 root         INFO     Train Epoch: 64 [3072/8000 (38%)]\tTotal Loss: 2.724303\n",
      "Reconstruction: 1.811773, Regularization: 0.849212, Discriminator: 0.043341; Generator: 0.019977,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,837 root         INFO     Train Epoch: 64 [3584/8000 (45%)]\tTotal Loss: 2.441520\n",
      "Reconstruction: 1.628873, Regularization: 0.749312, Discriminator: 0.043361; Generator: 0.019974,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:23,946 root         INFO     Train Epoch: 64 [4096/8000 (51%)]\tTotal Loss: 2.650201\n",
      "Reconstruction: 1.736563, Regularization: 0.850314, Discriminator: 0.043343; Generator: 0.019982,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:24,056 root         INFO     Train Epoch: 64 [4608/8000 (58%)]\tTotal Loss: 2.816406\n",
      "Reconstruction: 1.804903, Regularization: 0.948204, Discriminator: 0.043320; Generator: 0.019979,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:24,166 root         INFO     Train Epoch: 64 [5120/8000 (64%)]\tTotal Loss: 2.473131\n",
      "Reconstruction: 1.609980, Regularization: 0.799828, Discriminator: 0.043338; Generator: 0.019984,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:24,275 root         INFO     Train Epoch: 64 [5632/8000 (70%)]\tTotal Loss: 2.915680\n",
      "Reconstruction: 1.956572, Regularization: 0.895845, Discriminator: 0.043268; Generator: 0.019996,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:24,385 root         INFO     Train Epoch: 64 [6144/8000 (77%)]\tTotal Loss: 2.834882\n",
      "Reconstruction: 1.903229, Regularization: 0.868394, Discriminator: 0.043271; Generator: 0.019988,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:24,495 root         INFO     Train Epoch: 64 [6656/8000 (83%)]\tTotal Loss: 2.694638\n",
      "Reconstruction: 1.822298, Regularization: 0.809053, Discriminator: 0.043301; Generator: 0.019987,\n",
      "D(x): 0.529, D(G(z)): 0.528\n",
      "2019-04-09 23:59:24,605 root         INFO     Train Epoch: 64 [7168/8000 (90%)]\tTotal Loss: 2.491961\n",
      "Reconstruction: 1.597359, Regularization: 0.831303, Discriminator: 0.043303; Generator: 0.019996,\n",
      "D(x): 0.529, D(G(z)): 0.527\n",
      "2019-04-09 23:59:24,714 root         INFO     Train Epoch: 64 [7680/8000 (96%)]\tTotal Loss: 2.811556\n",
      "Reconstruction: 1.817906, Regularization: 0.930386, Discriminator: 0.043273; Generator: 0.019991,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:24,794 root         INFO     ====> Epoch: 64 Average loss: 2.6860\n",
      "2019-04-09 23:59:24,822 root         INFO     Train Epoch: 65 [0/8000 (0%)]\tTotal Loss: 2.369749\n",
      "Reconstruction: 1.544817, Regularization: 0.761628, Discriminator: 0.043306; Generator: 0.019998,\n",
      "D(x): 0.529, D(G(z)): 0.527\n",
      "2019-04-09 23:59:24,933 root         INFO     Train Epoch: 65 [512/8000 (6%)]\tTotal Loss: 2.430725\n",
      "Reconstruction: 1.611055, Regularization: 0.756379, Discriminator: 0.043290; Generator: 0.020001,\n",
      "D(x): 0.529, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,043 root         INFO     Train Epoch: 65 [1024/8000 (13%)]\tTotal Loss: 2.719098\n",
      "Reconstruction: 1.808974, Regularization: 0.846870, Discriminator: 0.043249; Generator: 0.020005,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,153 root         INFO     Train Epoch: 65 [1536/8000 (19%)]\tTotal Loss: 2.890605\n",
      "Reconstruction: 1.924988, Regularization: 0.902394, Discriminator: 0.043226; Generator: 0.019997,\n",
      "D(x): 0.531, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,262 root         INFO     Train Epoch: 65 [2048/8000 (26%)]\tTotal Loss: 2.639914\n",
      "Reconstruction: 1.694403, Regularization: 0.882285, Discriminator: 0.043217; Generator: 0.020009,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,372 root         INFO     Train Epoch: 65 [2560/8000 (32%)]\tTotal Loss: 2.620381\n",
      "Reconstruction: 1.739168, Regularization: 0.817976, Discriminator: 0.043216; Generator: 0.020021,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,482 root         INFO     Train Epoch: 65 [3072/8000 (38%)]\tTotal Loss: 2.352557\n",
      "Reconstruction: 1.541346, Regularization: 0.747917, Discriminator: 0.043291; Generator: 0.020003,\n",
      "D(x): 0.529, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,592 root         INFO     Train Epoch: 65 [3584/8000 (45%)]\tTotal Loss: 2.632567\n",
      "Reconstruction: 1.717616, Regularization: 0.851734, Discriminator: 0.043183; Generator: 0.020034,\n",
      "D(x): 0.531, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,702 root         INFO     Train Epoch: 65 [4096/8000 (51%)]\tTotal Loss: 2.468108\n",
      "Reconstruction: 1.607422, Regularization: 0.797460, Discriminator: 0.043199; Generator: 0.020028,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,812 root         INFO     Train Epoch: 65 [4608/8000 (58%)]\tTotal Loss: 2.666444\n",
      "Reconstruction: 1.806052, Regularization: 0.797158, Discriminator: 0.043221; Generator: 0.020013,\n",
      "D(x): 0.530, D(G(z)): 0.527\n",
      "2019-04-09 23:59:25,921 root         INFO     Train Epoch: 65 [5120/8000 (64%)]\tTotal Loss: 2.571295\n",
      "Reconstruction: 1.705306, Regularization: 0.802779, Discriminator: 0.043198; Generator: 0.020013,\n",
      "D(x): 0.531, D(G(z)): 0.527\n",
      "2019-04-09 23:59:26,030 root         INFO     Train Epoch: 65 [5632/8000 (70%)]\tTotal Loss: 2.863219\n",
      "Reconstruction: 1.891927, Regularization: 0.908138, Discriminator: 0.043134; Generator: 0.020020,\n",
      "D(x): 0.532, D(G(z)): 0.527\n",
      "2019-04-09 23:59:26,140 root         INFO     Train Epoch: 65 [6144/8000 (77%)]\tTotal Loss: 3.018058\n",
      "Reconstruction: 1.951032, Regularization: 1.003961, Discriminator: 0.043028; Generator: 0.020036,\n",
      "D(x): 0.533, D(G(z)): 0.527\n",
      "2019-04-09 23:59:26,250 root         INFO     Train Epoch: 65 [6656/8000 (83%)]\tTotal Loss: 2.702702\n",
      "Reconstruction: 1.771364, Regularization: 0.868181, Discriminator: 0.043117; Generator: 0.020040,\n",
      "D(x): 0.532, D(G(z)): 0.527\n",
      "2019-04-09 23:59:26,361 root         INFO     Train Epoch: 65 [7168/8000 (90%)]\tTotal Loss: 2.916086\n",
      "Reconstruction: 1.919082, Regularization: 0.933860, Discriminator: 0.043112; Generator: 0.020031,\n",
      "D(x): 0.532, D(G(z)): 0.527\n",
      "2019-04-09 23:59:26,471 root         INFO     Train Epoch: 65 [7680/8000 (96%)]\tTotal Loss: 2.579512\n",
      "Reconstruction: 1.705742, Regularization: 0.810538, Discriminator: 0.043176; Generator: 0.020057,\n",
      "D(x): 0.530, D(G(z)): 0.526\n",
      "2019-04-09 23:59:26,551 root         INFO     ====> Epoch: 65 Average loss: 2.7053\n",
      "2019-04-09 23:59:26,578 root         INFO     Train Epoch: 66 [0/8000 (0%)]\tTotal Loss: 2.503340\n",
      "Reconstruction: 1.685881, Regularization: 0.754235, Discriminator: 0.043160; Generator: 0.020063,\n",
      "D(x): 0.530, D(G(z)): 0.526\n",
      "2019-04-09 23:59:26,686 root         INFO     Train Epoch: 66 [512/8000 (6%)]\tTotal Loss: 3.072630\n",
      "Reconstruction: 2.029397, Regularization: 0.980162, Discriminator: 0.043009; Generator: 0.020062,\n",
      "D(x): 0.533, D(G(z)): 0.526\n",
      "2019-04-09 23:59:26,794 root         INFO     Train Epoch: 66 [1024/8000 (13%)]\tTotal Loss: 2.904727\n",
      "Reconstruction: 1.923352, Regularization: 0.918250, Discriminator: 0.043057; Generator: 0.020068,\n",
      "D(x): 0.532, D(G(z)): 0.526\n",
      "2019-04-09 23:59:26,901 root         INFO     Train Epoch: 66 [1536/8000 (19%)]\tTotal Loss: 2.666101\n",
      "Reconstruction: 1.719871, Regularization: 0.883016, Discriminator: 0.043096; Generator: 0.020117,\n",
      "D(x): 0.531, D(G(z)): 0.525\n",
      "2019-04-09 23:59:27,009 root         INFO     Train Epoch: 66 [2048/8000 (26%)]\tTotal Loss: 2.788411\n",
      "Reconstruction: 1.858593, Regularization: 0.866643, Discriminator: 0.043064; Generator: 0.020111,\n",
      "D(x): 0.531, D(G(z)): 0.525\n",
      "2019-04-09 23:59:27,116 root         INFO     Train Epoch: 66 [2560/8000 (32%)]\tTotal Loss: 2.538080\n",
      "Reconstruction: 1.692331, Regularization: 0.782562, Discriminator: 0.043059; Generator: 0.020128,\n",
      "D(x): 0.531, D(G(z)): 0.525\n",
      "2019-04-09 23:59:27,223 root         INFO     Train Epoch: 66 [3072/8000 (38%)]\tTotal Loss: 2.672987\n",
      "Reconstruction: 1.741877, Regularization: 0.867897, Discriminator: 0.043083; Generator: 0.020129,\n",
      "D(x): 0.531, D(G(z)): 0.525\n",
      "2019-04-09 23:59:27,331 root         INFO     Train Epoch: 66 [3584/8000 (45%)]\tTotal Loss: 2.665573\n",
      "Reconstruction: 1.743315, Regularization: 0.859029, Discriminator: 0.043091; Generator: 0.020138,\n",
      "D(x): 0.530, D(G(z)): 0.525\n",
      "2019-04-09 23:59:27,439 root         INFO     Train Epoch: 66 [4096/8000 (51%)]\tTotal Loss: 2.895605\n",
      "Reconstruction: 1.878018, Regularization: 0.954475, Discriminator: 0.042928; Generator: 0.020185,\n",
      "D(x): 0.532, D(G(z)): 0.524\n",
      "2019-04-09 23:59:27,546 root         INFO     Train Epoch: 66 [4608/8000 (58%)]\tTotal Loss: 3.007771\n",
      "Reconstruction: 1.962002, Regularization: 0.982724, Discriminator: 0.042839; Generator: 0.020206,\n",
      "D(x): 0.533, D(G(z)): 0.524\n",
      "2019-04-09 23:59:27,654 root         INFO     Train Epoch: 66 [5120/8000 (64%)]\tTotal Loss: 2.730529\n",
      "Reconstruction: 1.808175, Regularization: 0.859125, Discriminator: 0.043039; Generator: 0.020189,\n",
      "D(x): 0.530, D(G(z)): 0.524\n",
      "2019-04-09 23:59:27,761 root         INFO     Train Epoch: 66 [5632/8000 (70%)]\tTotal Loss: 2.655609\n",
      "Reconstruction: 1.702042, Regularization: 0.890327, Discriminator: 0.043054; Generator: 0.020186,\n",
      "D(x): 0.530, D(G(z)): 0.524\n",
      "2019-04-09 23:59:27,868 root         INFO     Train Epoch: 66 [6144/8000 (77%)]\tTotal Loss: 2.948978\n",
      "Reconstruction: 1.972312, Regularization: 0.913534, Discriminator: 0.042918; Generator: 0.020214,\n",
      "D(x): 0.532, D(G(z)): 0.524\n",
      "2019-04-09 23:59:27,975 root         INFO     Train Epoch: 66 [6656/8000 (83%)]\tTotal Loss: 2.567552\n",
      "Reconstruction: 1.697975, Regularization: 0.806355, Discriminator: 0.042944; Generator: 0.020278,\n",
      "D(x): 0.530, D(G(z)): 0.523\n",
      "2019-04-09 23:59:28,082 root         INFO     Train Epoch: 66 [7168/8000 (90%)]\tTotal Loss: 2.877961\n",
      "Reconstruction: 1.902326, Regularization: 0.912430, Discriminator: 0.042878; Generator: 0.020327,\n",
      "D(x): 0.530, D(G(z)): 0.522\n",
      "2019-04-09 23:59:28,189 root         INFO     Train Epoch: 66 [7680/8000 (96%)]\tTotal Loss: 2.470857\n",
      "Reconstruction: 1.658074, Regularization: 0.749344, Discriminator: 0.043108; Generator: 0.020331,\n",
      "D(x): 0.526, D(G(z)): 0.522\n",
      "2019-04-09 23:59:28,267 root         INFO     ====> Epoch: 66 Average loss: 2.7641\n",
      "2019-04-09 23:59:28,294 root         INFO     Train Epoch: 67 [0/8000 (0%)]\tTotal Loss: 2.824649\n",
      "Reconstruction: 1.846476, Regularization: 0.914839, Discriminator: 0.042949; Generator: 0.020386,\n",
      "D(x): 0.528, D(G(z)): 0.521\n",
      "2019-04-09 23:59:28,401 root         INFO     Train Epoch: 67 [512/8000 (6%)]\tTotal Loss: 2.933911\n",
      "Reconstruction: 2.026695, Regularization: 0.843930, Discriminator: 0.042893; Generator: 0.020394,\n",
      "D(x): 0.529, D(G(z)): 0.521\n",
      "2019-04-09 23:59:28,508 root         INFO     Train Epoch: 67 [1024/8000 (13%)]\tTotal Loss: 2.647784\n",
      "Reconstruction: 1.729335, Regularization: 0.855121, Discriminator: 0.042922; Generator: 0.020407,\n",
      "D(x): 0.528, D(G(z)): 0.520\n",
      "2019-04-09 23:59:28,616 root         INFO     Train Epoch: 67 [1536/8000 (19%)]\tTotal Loss: 2.768823\n",
      "Reconstruction: 1.835668, Regularization: 0.869670, Discriminator: 0.042996; Generator: 0.020489,\n",
      "D(x): 0.525, D(G(z)): 0.519\n",
      "2019-04-09 23:59:28,722 root         INFO     Train Epoch: 67 [2048/8000 (26%)]\tTotal Loss: 2.835690\n",
      "Reconstruction: 1.930282, Regularization: 0.841908, Discriminator: 0.042939; Generator: 0.020561,\n",
      "D(x): 0.525, D(G(z)): 0.518\n",
      "2019-04-09 23:59:28,831 root         INFO     Train Epoch: 67 [2560/8000 (32%)]\tTotal Loss: 2.884352\n",
      "Reconstruction: 1.900409, Regularization: 0.920520, Discriminator: 0.042812; Generator: 0.020612,\n",
      "D(x): 0.526, D(G(z)): 0.517\n",
      "2019-04-09 23:59:28,942 root         INFO     Train Epoch: 67 [3072/8000 (38%)]\tTotal Loss: 2.639863\n",
      "Reconstruction: 1.742958, Regularization: 0.833253, Discriminator: 0.043000; Generator: 0.020652,\n",
      "D(x): 0.522, D(G(z)): 0.516\n",
      "2019-04-09 23:59:29,052 root         INFO     Train Epoch: 67 [3584/8000 (45%)]\tTotal Loss: 2.954108\n",
      "Reconstruction: 1.935627, Regularization: 0.954806, Discriminator: 0.042977; Generator: 0.020698,\n",
      "D(x): 0.522, D(G(z)): 0.516\n",
      "2019-04-09 23:59:29,163 root         INFO     Train Epoch: 67 [4096/8000 (51%)]\tTotal Loss: 2.957870\n",
      "Reconstruction: 1.999638, Regularization: 0.894718, Discriminator: 0.042740; Generator: 0.020774,\n",
      "D(x): 0.525, D(G(z)): 0.514\n",
      "2019-04-09 23:59:29,273 root         INFO     Train Epoch: 67 [4608/8000 (58%)]\tTotal Loss: 2.967351\n",
      "Reconstruction: 1.996056, Regularization: 0.907603, Discriminator: 0.042872; Generator: 0.020820,\n",
      "D(x): 0.522, D(G(z)): 0.514\n",
      "2019-04-09 23:59:29,383 root         INFO     Train Epoch: 67 [5120/8000 (64%)]\tTotal Loss: 2.943392\n",
      "Reconstruction: 1.933833, Regularization: 0.945800, Discriminator: 0.042883; Generator: 0.020876,\n",
      "D(x): 0.520, D(G(z)): 0.513\n",
      "2019-04-09 23:59:29,492 root         INFO     Train Epoch: 67 [5632/8000 (70%)]\tTotal Loss: 2.821330\n",
      "Reconstruction: 1.854678, Regularization: 0.902758, Discriminator: 0.042951; Generator: 0.020943,\n",
      "D(x): 0.518, D(G(z)): 0.512\n",
      "2019-04-09 23:59:29,602 root         INFO     Train Epoch: 67 [6144/8000 (77%)]\tTotal Loss: 2.758574\n",
      "Reconstruction: 1.830760, Regularization: 0.863918, Discriminator: 0.042890; Generator: 0.021006,\n",
      "D(x): 0.518, D(G(z)): 0.511\n",
      "2019-04-09 23:59:29,713 root         INFO     Train Epoch: 67 [6656/8000 (83%)]\tTotal Loss: 2.774806\n",
      "Reconstruction: 1.885087, Regularization: 0.825728, Discriminator: 0.042906; Generator: 0.021084,\n",
      "D(x): 0.516, D(G(z)): 0.509\n",
      "2019-04-09 23:59:29,823 root         INFO     Train Epoch: 67 [7168/8000 (90%)]\tTotal Loss: 2.956174\n",
      "Reconstruction: 2.008708, Regularization: 0.883474, Discriminator: 0.042856; Generator: 0.021136,\n",
      "D(x): 0.516, D(G(z)): 0.508\n",
      "2019-04-09 23:59:29,933 root         INFO     Train Epoch: 67 [7680/8000 (96%)]\tTotal Loss: 2.693843\n",
      "Reconstruction: 1.757088, Regularization: 0.872567, Discriminator: 0.042984; Generator: 0.021204,\n",
      "D(x): 0.513, D(G(z)): 0.507\n",
      "2019-04-09 23:59:30,014 root         INFO     ====> Epoch: 67 Average loss: 2.8072\n",
      "2019-04-09 23:59:30,041 root         INFO     Train Epoch: 68 [0/8000 (0%)]\tTotal Loss: 2.799977\n",
      "Reconstruction: 1.812122, Regularization: 0.923598, Discriminator: 0.042974; Generator: 0.021283,\n",
      "D(x): 0.512, D(G(z)): 0.506\n",
      "2019-04-09 23:59:30,153 root         INFO     Train Epoch: 68 [512/8000 (6%)]\tTotal Loss: 2.992995\n",
      "Reconstruction: 1.982410, Regularization: 0.946252, Discriminator: 0.043024; Generator: 0.021309,\n",
      "D(x): 0.511, D(G(z)): 0.506\n",
      "2019-04-09 23:59:30,265 root         INFO     Train Epoch: 68 [1024/8000 (13%)]\tTotal Loss: 2.729949\n",
      "Reconstruction: 1.836731, Regularization: 0.828770, Discriminator: 0.043062; Generator: 0.021386,\n",
      "D(x): 0.509, D(G(z)): 0.504\n",
      "2019-04-09 23:59:30,377 root         INFO     Train Epoch: 68 [1536/8000 (19%)]\tTotal Loss: 2.897033\n",
      "Reconstruction: 1.961567, Regularization: 0.871253, Discriminator: 0.042760; Generator: 0.021453,\n",
      "D(x): 0.513, D(G(z)): 0.503\n",
      "2019-04-09 23:59:30,489 root         INFO     Train Epoch: 68 [2048/8000 (26%)]\tTotal Loss: 2.553503\n",
      "Reconstruction: 1.757024, Regularization: 0.731954, Discriminator: 0.042997; Generator: 0.021529,\n",
      "D(x): 0.507, D(G(z)): 0.502\n",
      "2019-04-09 23:59:30,601 root         INFO     Train Epoch: 68 [2560/8000 (32%)]\tTotal Loss: 3.039875\n",
      "Reconstruction: 2.050826, Regularization: 0.924586, Discriminator: 0.042841; Generator: 0.021622,\n",
      "D(x): 0.509, D(G(z)): 0.501\n",
      "2019-04-09 23:59:30,712 root         INFO     Train Epoch: 68 [3072/8000 (38%)]\tTotal Loss: 3.000345\n",
      "Reconstruction: 2.035014, Regularization: 0.900474, Discriminator: 0.043180; Generator: 0.021677,\n",
      "D(x): 0.502, D(G(z)): 0.500\n",
      "2019-04-09 23:59:30,825 root         INFO     Train Epoch: 68 [3584/8000 (45%)]\tTotal Loss: 3.106364\n",
      "Reconstruction: 2.035880, Regularization: 1.005769, Discriminator: 0.042985; Generator: 0.021730,\n",
      "D(x): 0.504, D(G(z)): 0.499\n",
      "2019-04-09 23:59:30,936 root         INFO     Train Epoch: 68 [4096/8000 (51%)]\tTotal Loss: 2.899302\n",
      "Reconstruction: 1.899365, Regularization: 0.934989, Discriminator: 0.043165; Generator: 0.021782,\n",
      "D(x): 0.501, D(G(z)): 0.498\n",
      "2019-04-09 23:59:31,049 root         INFO     Train Epoch: 68 [4608/8000 (58%)]\tTotal Loss: 2.478041\n",
      "Reconstruction: 1.576606, Regularization: 0.836246, Discriminator: 0.043331; Generator: 0.021859,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:31,160 root         INFO     Train Epoch: 68 [5120/8000 (64%)]\tTotal Loss: 2.876656\n",
      "Reconstruction: 1.919951, Regularization: 0.891564, Discriminator: 0.043198; Generator: 0.021943,\n",
      "D(x): 0.498, D(G(z)): 0.496\n",
      "2019-04-09 23:59:31,269 root         INFO     Train Epoch: 68 [5632/8000 (70%)]\tTotal Loss: 2.778929\n",
      "Reconstruction: 1.866130, Regularization: 0.847695, Discriminator: 0.043109; Generator: 0.021995,\n",
      "D(x): 0.498, D(G(z)): 0.495\n",
      "2019-04-09 23:59:31,378 root         INFO     Train Epoch: 68 [6144/8000 (77%)]\tTotal Loss: 3.019753\n",
      "Reconstruction: 1.935442, Regularization: 1.018709, Discriminator: 0.043564; Generator: 0.022037,\n",
      "D(x): 0.490, D(G(z)): 0.494\n",
      "2019-04-09 23:59:31,487 root         INFO     Train Epoch: 68 [6656/8000 (83%)]\tTotal Loss: 2.536156\n",
      "Reconstruction: 1.690355, Regularization: 0.780551, Discriminator: 0.043175; Generator: 0.022074,\n",
      "D(x): 0.496, D(G(z)): 0.493\n",
      "2019-04-09 23:59:31,596 root         INFO     Train Epoch: 68 [7168/8000 (90%)]\tTotal Loss: 2.803746\n",
      "Reconstruction: 1.899710, Regularization: 0.838569, Discriminator: 0.043348; Generator: 0.022118,\n",
      "D(x): 0.493, D(G(z)): 0.493\n",
      "2019-04-09 23:59:31,705 root         INFO     Train Epoch: 68 [7680/8000 (96%)]\tTotal Loss: 2.791275\n",
      "Reconstruction: 1.880777, Regularization: 0.845027, Discriminator: 0.043281; Generator: 0.022189,\n",
      "D(x): 0.492, D(G(z)): 0.492\n",
      "2019-04-09 23:59:31,785 root         INFO     ====> Epoch: 68 Average loss: 2.8480\n",
      "2019-04-09 23:59:31,812 root         INFO     Train Epoch: 69 [0/8000 (0%)]\tTotal Loss: 2.824496\n",
      "Reconstruction: 1.855420, Regularization: 0.903352, Discriminator: 0.043510; Generator: 0.022214,\n",
      "D(x): 0.489, D(G(z)): 0.491\n",
      "2019-04-09 23:59:31,922 root         INFO     Train Epoch: 69 [512/8000 (6%)]\tTotal Loss: 2.982567\n",
      "Reconstruction: 2.064614, Regularization: 0.852310, Discriminator: 0.043384; Generator: 0.022258,\n",
      "D(x): 0.490, D(G(z)): 0.491\n",
      "2019-04-09 23:59:32,032 root         INFO     Train Epoch: 69 [1024/8000 (13%)]\tTotal Loss: 3.242663\n",
      "Reconstruction: 2.193148, Regularization: 0.983792, Discriminator: 0.043419; Generator: 0.022304,\n",
      "D(x): 0.489, D(G(z)): 0.490\n",
      "2019-04-09 23:59:32,142 root         INFO     Train Epoch: 69 [1536/8000 (19%)]\tTotal Loss: 2.745077\n",
      "Reconstruction: 1.853822, Regularization: 0.825546, Discriminator: 0.043365; Generator: 0.022344,\n",
      "D(x): 0.489, D(G(z)): 0.489\n",
      "2019-04-09 23:59:32,252 root         INFO     Train Epoch: 69 [2048/8000 (26%)]\tTotal Loss: 2.844458\n",
      "Reconstruction: 1.952302, Regularization: 0.826396, Discriminator: 0.043340; Generator: 0.022420,\n",
      "D(x): 0.488, D(G(z)): 0.488\n",
      "2019-04-09 23:59:32,363 root         INFO     Train Epoch: 69 [2560/8000 (32%)]\tTotal Loss: 2.264857\n",
      "Reconstruction: 1.446543, Regularization: 0.752333, Discriminator: 0.043530; Generator: 0.022452,\n",
      "D(x): 0.485, D(G(z)): 0.488\n",
      "2019-04-09 23:59:32,473 root         INFO     Train Epoch: 69 [3072/8000 (38%)]\tTotal Loss: 2.695926\n",
      "Reconstruction: 1.774039, Regularization: 0.855836, Discriminator: 0.043551; Generator: 0.022500,\n",
      "D(x): 0.484, D(G(z)): 0.487\n",
      "2019-04-09 23:59:32,581 root         INFO     Train Epoch: 69 [3584/8000 (45%)]\tTotal Loss: 2.679008\n",
      "Reconstruction: 1.771255, Regularization: 0.841560, Discriminator: 0.043649; Generator: 0.022544,\n",
      "D(x): 0.482, D(G(z)): 0.486\n",
      "2019-04-09 23:59:32,692 root         INFO     Train Epoch: 69 [4096/8000 (51%)]\tTotal Loss: 2.649367\n",
      "Reconstruction: 1.744865, Regularization: 0.838303, Discriminator: 0.043603; Generator: 0.022596,\n",
      "D(x): 0.481, D(G(z)): 0.485\n",
      "2019-04-09 23:59:32,801 root         INFO     Train Epoch: 69 [4608/8000 (58%)]\tTotal Loss: 2.658784\n",
      "Reconstruction: 1.760085, Regularization: 0.832608, Discriminator: 0.043508; Generator: 0.022583,\n",
      "D(x): 0.483, D(G(z)): 0.485\n",
      "2019-04-09 23:59:32,910 root         INFO     Train Epoch: 69 [5120/8000 (64%)]\tTotal Loss: 3.120051\n",
      "Reconstruction: 2.067974, Regularization: 0.985871, Discriminator: 0.043583; Generator: 0.022623,\n",
      "D(x): 0.481, D(G(z)): 0.485\n",
      "2019-04-09 23:59:33,019 root         INFO     Train Epoch: 69 [5632/8000 (70%)]\tTotal Loss: 2.876842\n",
      "Reconstruction: 1.930195, Regularization: 0.880530, Discriminator: 0.043485; Generator: 0.022632,\n",
      "D(x): 0.483, D(G(z)): 0.485\n",
      "2019-04-09 23:59:33,129 root         INFO     Train Epoch: 69 [6144/8000 (77%)]\tTotal Loss: 3.314776\n",
      "Reconstruction: 2.366127, Regularization: 0.882405, Discriminator: 0.043592; Generator: 0.022651,\n",
      "D(x): 0.481, D(G(z)): 0.484\n",
      "2019-04-09 23:59:33,239 root         INFO     Train Epoch: 69 [6656/8000 (83%)]\tTotal Loss: 2.660783\n",
      "Reconstruction: 1.768963, Regularization: 0.825665, Discriminator: 0.043479; Generator: 0.022677,\n",
      "D(x): 0.482, D(G(z)): 0.484\n",
      "2019-04-09 23:59:33,349 root         INFO     Train Epoch: 69 [7168/8000 (90%)]\tTotal Loss: 2.933398\n",
      "Reconstruction: 1.949986, Regularization: 0.917052, Discriminator: 0.043669; Generator: 0.022690,\n",
      "D(x): 0.479, D(G(z)): 0.484\n",
      "2019-04-09 23:59:33,459 root         INFO     Train Epoch: 69 [7680/8000 (96%)]\tTotal Loss: 2.912012\n",
      "Reconstruction: 1.950131, Regularization: 0.895538, Discriminator: 0.043653; Generator: 0.022691,\n",
      "D(x): 0.479, D(G(z)): 0.484\n",
      "2019-04-09 23:59:33,540 root         INFO     ====> Epoch: 69 Average loss: 2.8556\n",
      "2019-04-09 23:59:33,567 root         INFO     Train Epoch: 70 [0/8000 (0%)]\tTotal Loss: 2.941483\n",
      "Reconstruction: 1.993644, Regularization: 0.881586, Discriminator: 0.043539; Generator: 0.022713,\n",
      "D(x): 0.481, D(G(z)): 0.483\n",
      "2019-04-09 23:59:33,681 root         INFO     Train Epoch: 70 [512/8000 (6%)]\tTotal Loss: 2.633801\n",
      "Reconstruction: 1.772310, Regularization: 0.795338, Discriminator: 0.043392; Generator: 0.022760,\n",
      "D(x): 0.482, D(G(z)): 0.483\n",
      "2019-04-09 23:59:33,793 root         INFO     Train Epoch: 70 [1024/8000 (13%)]\tTotal Loss: 2.537414\n",
      "Reconstruction: 1.690493, Regularization: 0.780662, Discriminator: 0.043564; Generator: 0.022696,\n",
      "D(x): 0.481, D(G(z)): 0.484\n",
      "2019-04-09 23:59:33,904 root         INFO     Train Epoch: 70 [1536/8000 (19%)]\tTotal Loss: 2.666314\n",
      "Reconstruction: 1.749028, Regularization: 0.850986, Discriminator: 0.043573; Generator: 0.022728,\n",
      "D(x): 0.480, D(G(z)): 0.483\n",
      "2019-04-09 23:59:34,016 root         INFO     Train Epoch: 70 [2048/8000 (26%)]\tTotal Loss: 2.768345\n",
      "Reconstruction: 1.827206, Regularization: 0.874798, Discriminator: 0.043597; Generator: 0.022744,\n",
      "D(x): 0.479, D(G(z)): 0.483\n",
      "2019-04-09 23:59:34,128 root         INFO     Train Epoch: 70 [2560/8000 (32%)]\tTotal Loss: 2.731866\n",
      "Reconstruction: 1.843548, Regularization: 0.822007, Discriminator: 0.043511; Generator: 0.022800,\n",
      "D(x): 0.480, D(G(z)): 0.482\n",
      "2019-04-09 23:59:34,239 root         INFO     Train Epoch: 70 [3072/8000 (38%)]\tTotal Loss: 3.078259\n",
      "Reconstruction: 2.076624, Regularization: 0.935229, Discriminator: 0.043624; Generator: 0.022782,\n",
      "D(x): 0.478, D(G(z)): 0.482\n",
      "2019-04-09 23:59:34,351 root         INFO     Train Epoch: 70 [3584/8000 (45%)]\tTotal Loss: 3.338210\n",
      "Reconstruction: 2.446484, Regularization: 0.825470, Discriminator: 0.043386; Generator: 0.022870,\n",
      "D(x): 0.481, D(G(z)): 0.481\n",
      "2019-04-09 23:59:34,461 root         INFO     Train Epoch: 70 [4096/8000 (51%)]\tTotal Loss: 3.368754\n",
      "Reconstruction: 2.482748, Regularization: 0.819733, Discriminator: 0.043374; Generator: 0.022899,\n",
      "D(x): 0.481, D(G(z)): 0.481\n",
      "2019-04-09 23:59:34,571 root         INFO     Train Epoch: 70 [4608/8000 (58%)]\tTotal Loss: 3.170741\n",
      "Reconstruction: 2.106496, Regularization: 0.997822, Discriminator: 0.043503; Generator: 0.022921,\n",
      "D(x): 0.478, D(G(z)): 0.480\n",
      "2019-04-09 23:59:34,682 root         INFO     Train Epoch: 70 [5120/8000 (64%)]\tTotal Loss: 2.779963\n",
      "Reconstruction: 1.881647, Regularization: 0.832054, Discriminator: 0.043415; Generator: 0.022847,\n",
      "D(x): 0.481, D(G(z)): 0.481\n",
      "2019-04-09 23:59:34,793 root         INFO     Train Epoch: 70 [5632/8000 (70%)]\tTotal Loss: 3.087915\n",
      "Reconstruction: 2.065221, Regularization: 0.956309, Discriminator: 0.043478; Generator: 0.022907,\n",
      "D(x): 0.479, D(G(z)): 0.480\n",
      "2019-04-09 23:59:34,905 root         INFO     Train Epoch: 70 [6144/8000 (77%)]\tTotal Loss: 3.000545\n",
      "Reconstruction: 1.986667, Regularization: 0.947457, Discriminator: 0.043502; Generator: 0.022919,\n",
      "D(x): 0.478, D(G(z)): 0.480\n",
      "2019-04-09 23:59:35,016 root         INFO     Train Epoch: 70 [6656/8000 (83%)]\tTotal Loss: 2.914799\n",
      "Reconstruction: 2.018798, Regularization: 0.829655, Discriminator: 0.043383; Generator: 0.022962,\n",
      "D(x): 0.479, D(G(z)): 0.480\n",
      "2019-04-09 23:59:35,127 root         INFO     Train Epoch: 70 [7168/8000 (90%)]\tTotal Loss: 2.702302\n",
      "Reconstruction: 1.852331, Regularization: 0.783664, Discriminator: 0.043326; Generator: 0.022980,\n",
      "D(x): 0.480, D(G(z)): 0.479\n",
      "2019-04-09 23:59:35,238 root         INFO     Train Epoch: 70 [7680/8000 (96%)]\tTotal Loss: 3.035874\n",
      "Reconstruction: 2.093927, Regularization: 0.875591, Discriminator: 0.043424; Generator: 0.022932,\n",
      "D(x): 0.479, D(G(z)): 0.480\n",
      "2019-04-09 23:59:35,318 root         INFO     ====> Epoch: 70 Average loss: 2.8212\n",
      "2019-04-09 23:59:35,345 root         INFO     Train Epoch: 71 [0/8000 (0%)]\tTotal Loss: 2.939721\n",
      "Reconstruction: 1.944627, Regularization: 0.928721, Discriminator: 0.043382; Generator: 0.022991,\n",
      "D(x): 0.479, D(G(z)): 0.479\n",
      "2019-04-09 23:59:35,458 root         INFO     Train Epoch: 71 [512/8000 (6%)]\tTotal Loss: 2.748649\n",
      "Reconstruction: 1.851262, Regularization: 0.831114, Discriminator: 0.043388; Generator: 0.022885,\n",
      "D(x): 0.481, D(G(z)): 0.481\n",
      "2019-04-09 23:59:35,570 root         INFO     Train Epoch: 71 [1024/8000 (13%)]\tTotal Loss: 2.784323\n",
      "Reconstruction: 1.870745, Regularization: 0.847332, Discriminator: 0.043359; Generator: 0.022887,\n",
      "D(x): 0.481, D(G(z)): 0.481\n",
      "2019-04-09 23:59:35,682 root         INFO     Train Epoch: 71 [1536/8000 (19%)]\tTotal Loss: 2.732727\n",
      "Reconstruction: 1.826509, Regularization: 0.839977, Discriminator: 0.043332; Generator: 0.022908,\n",
      "D(x): 0.481, D(G(z)): 0.480\n",
      "2019-04-09 23:59:35,795 root         INFO     Train Epoch: 71 [2048/8000 (26%)]\tTotal Loss: 2.975385\n",
      "Reconstruction: 2.071564, Regularization: 0.837596, Discriminator: 0.043337; Generator: 0.022888,\n",
      "D(x): 0.481, D(G(z)): 0.481\n",
      "2019-04-09 23:59:35,906 root         INFO     Train Epoch: 71 [2560/8000 (32%)]\tTotal Loss: 2.791796\n",
      "Reconstruction: 1.879712, Regularization: 0.845884, Discriminator: 0.043296; Generator: 0.022904,\n",
      "D(x): 0.482, D(G(z)): 0.481\n",
      "2019-04-09 23:59:36,016 root         INFO     Train Epoch: 71 [3072/8000 (38%)]\tTotal Loss: 2.698184\n",
      "Reconstruction: 1.804030, Regularization: 0.827979, Discriminator: 0.043326; Generator: 0.022849,\n",
      "D(x): 0.482, D(G(z)): 0.481\n",
      "2019-04-09 23:59:36,126 root         INFO     Train Epoch: 71 [3584/8000 (45%)]\tTotal Loss: 2.644359\n",
      "Reconstruction: 1.710402, Regularization: 0.867790, Discriminator: 0.043256; Generator: 0.022911,\n",
      "D(x): 0.482, D(G(z)): 0.480\n",
      "2019-04-09 23:59:36,235 root         INFO     Train Epoch: 71 [4096/8000 (51%)]\tTotal Loss: 2.703759\n",
      "Reconstruction: 1.814570, Regularization: 0.823024, Discriminator: 0.043314; Generator: 0.022851,\n",
      "D(x): 0.482, D(G(z)): 0.481\n",
      "2019-04-09 23:59:36,344 root         INFO     Train Epoch: 71 [4608/8000 (58%)]\tTotal Loss: 3.137640\n",
      "Reconstruction: 2.204390, Regularization: 0.867067, Discriminator: 0.043307; Generator: 0.022876,\n",
      "D(x): 0.482, D(G(z)): 0.481\n",
      "2019-04-09 23:59:36,453 root         INFO     Train Epoch: 71 [5120/8000 (64%)]\tTotal Loss: 2.890907\n",
      "Reconstruction: 1.930870, Regularization: 0.893865, Discriminator: 0.043295; Generator: 0.022877,\n",
      "D(x): 0.482, D(G(z)): 0.481\n",
      "2019-04-09 23:59:36,563 root         INFO     Train Epoch: 71 [5632/8000 (70%)]\tTotal Loss: 2.657436\n",
      "Reconstruction: 1.745339, Regularization: 0.846002, Discriminator: 0.043269; Generator: 0.022826,\n",
      "D(x): 0.483, D(G(z)): 0.482\n",
      "2019-04-09 23:59:36,673 root         INFO     Train Epoch: 71 [6144/8000 (77%)]\tTotal Loss: 2.431229\n",
      "Reconstruction: 1.566966, Regularization: 0.798224, Discriminator: 0.043234; Generator: 0.022806,\n",
      "D(x): 0.484, D(G(z)): 0.482\n",
      "2019-04-09 23:59:36,783 root         INFO     Train Epoch: 71 [6656/8000 (83%)]\tTotal Loss: 2.704232\n",
      "Reconstruction: 1.776206, Regularization: 0.861992, Discriminator: 0.043265; Generator: 0.022769,\n",
      "D(x): 0.484, D(G(z)): 0.483\n",
      "2019-04-09 23:59:36,892 root         INFO     Train Epoch: 71 [7168/8000 (90%)]\tTotal Loss: 2.530245\n",
      "Reconstruction: 1.640289, Regularization: 0.823996, Discriminator: 0.043154; Generator: 0.022805,\n",
      "D(x): 0.485, D(G(z)): 0.482\n",
      "2019-04-09 23:59:37,001 root         INFO     Train Epoch: 71 [7680/8000 (96%)]\tTotal Loss: 2.740947\n",
      "Reconstruction: 1.856622, Regularization: 0.818323, Discriminator: 0.043258; Generator: 0.022745,\n",
      "D(x): 0.485, D(G(z)): 0.483\n",
      "2019-04-09 23:59:37,082 root         INFO     ====> Epoch: 71 Average loss: 2.7948\n",
      "2019-04-09 23:59:37,110 root         INFO     Train Epoch: 72 [0/8000 (0%)]\tTotal Loss: 2.700448\n",
      "Reconstruction: 1.798171, Regularization: 0.836278, Discriminator: 0.043203; Generator: 0.022796,\n",
      "D(x): 0.485, D(G(z)): 0.482\n",
      "2019-04-09 23:59:37,223 root         INFO     Train Epoch: 72 [512/8000 (6%)]\tTotal Loss: 2.899581\n",
      "Reconstruction: 1.922368, Regularization: 0.911184, Discriminator: 0.043318; Generator: 0.022710,\n",
      "D(x): 0.484, D(G(z)): 0.483\n",
      "2019-04-09 23:59:37,336 root         INFO     Train Epoch: 72 [1024/8000 (13%)]\tTotal Loss: 2.686272\n",
      "Reconstruction: 1.843238, Regularization: 0.777122, Discriminator: 0.043229; Generator: 0.022684,\n",
      "D(x): 0.486, D(G(z)): 0.484\n",
      "2019-04-09 23:59:37,448 root         INFO     Train Epoch: 72 [1536/8000 (19%)]\tTotal Loss: 2.771864\n",
      "Reconstruction: 1.770456, Regularization: 0.935467, Discriminator: 0.043266; Generator: 0.022674,\n",
      "D(x): 0.485, D(G(z)): 0.484\n",
      "2019-04-09 23:59:37,560 root         INFO     Train Epoch: 72 [2048/8000 (26%)]\tTotal Loss: 2.772731\n",
      "Reconstruction: 1.799144, Regularization: 0.907765, Discriminator: 0.043220; Generator: 0.022602,\n",
      "D(x): 0.487, D(G(z)): 0.485\n",
      "2019-04-09 23:59:37,673 root         INFO     Train Epoch: 72 [2560/8000 (32%)]\tTotal Loss: 2.695120\n",
      "Reconstruction: 1.769992, Regularization: 0.859344, Discriminator: 0.043164; Generator: 0.022620,\n",
      "D(x): 0.488, D(G(z)): 0.485\n",
      "2019-04-09 23:59:37,785 root         INFO     Train Epoch: 72 [3072/8000 (38%)]\tTotal Loss: 2.604965\n",
      "Reconstruction: 1.723566, Regularization: 0.815619, Discriminator: 0.043196; Generator: 0.022585,\n",
      "D(x): 0.488, D(G(z)): 0.485\n",
      "2019-04-09 23:59:37,897 root         INFO     Train Epoch: 72 [3584/8000 (45%)]\tTotal Loss: 2.668780\n",
      "Reconstruction: 1.722709, Regularization: 0.880319, Discriminator: 0.043194; Generator: 0.022558,\n",
      "D(x): 0.488, D(G(z)): 0.486\n",
      "2019-04-09 23:59:38,009 root         INFO     Train Epoch: 72 [4096/8000 (51%)]\tTotal Loss: 2.609143\n",
      "Reconstruction: 1.752402, Regularization: 0.790988, Discriminator: 0.043229; Generator: 0.022524,\n",
      "D(x): 0.488, D(G(z)): 0.486\n",
      "2019-04-09 23:59:38,121 root         INFO     Train Epoch: 72 [4608/8000 (58%)]\tTotal Loss: 2.902828\n",
      "Reconstruction: 1.905310, Regularization: 0.931678, Discriminator: 0.043374; Generator: 0.022466,\n",
      "D(x): 0.487, D(G(z)): 0.487\n",
      "2019-04-09 23:59:38,233 root         INFO     Train Epoch: 72 [5120/8000 (64%)]\tTotal Loss: 2.516981\n",
      "Reconstruction: 1.694543, Regularization: 0.756745, Discriminator: 0.043261; Generator: 0.022432,\n",
      "D(x): 0.489, D(G(z)): 0.488\n",
      "2019-04-09 23:59:38,345 root         INFO     Train Epoch: 72 [5632/8000 (70%)]\tTotal Loss: 2.899443\n",
      "Reconstruction: 1.876456, Regularization: 0.957308, Discriminator: 0.043262; Generator: 0.022417,\n",
      "D(x): 0.489, D(G(z)): 0.488\n",
      "2019-04-09 23:59:38,456 root         INFO     Train Epoch: 72 [6144/8000 (77%)]\tTotal Loss: 2.461780\n",
      "Reconstruction: 1.597470, Regularization: 0.798799, Discriminator: 0.043129; Generator: 0.022382,\n",
      "D(x): 0.492, D(G(z)): 0.489\n",
      "2019-04-09 23:59:38,568 root         INFO     Train Epoch: 72 [6656/8000 (83%)]\tTotal Loss: 2.521624\n",
      "Reconstruction: 1.660532, Regularization: 0.795676, Discriminator: 0.043069; Generator: 0.022346,\n",
      "D(x): 0.493, D(G(z)): 0.489\n",
      "2019-04-09 23:59:38,679 root         INFO     Train Epoch: 72 [7168/8000 (90%)]\tTotal Loss: 2.687688\n",
      "Reconstruction: 1.789642, Regularization: 0.832494, Discriminator: 0.043224; Generator: 0.022327,\n",
      "D(x): 0.491, D(G(z)): 0.489\n",
      "2019-04-09 23:59:38,791 root         INFO     Train Epoch: 72 [7680/8000 (96%)]\tTotal Loss: 2.569031\n",
      "Reconstruction: 1.586593, Regularization: 0.916940, Discriminator: 0.043227; Generator: 0.022272,\n",
      "D(x): 0.492, D(G(z)): 0.490\n",
      "2019-04-09 23:59:38,872 root         INFO     ====> Epoch: 72 Average loss: 2.7198\n",
      "2019-04-09 23:59:38,900 root         INFO     Train Epoch: 73 [0/8000 (0%)]\tTotal Loss: 2.485157\n",
      "Reconstruction: 1.607164, Regularization: 0.812429, Discriminator: 0.043310; Generator: 0.022255,\n",
      "D(x): 0.491, D(G(z)): 0.491\n",
      "2019-04-09 23:59:39,011 root         INFO     Train Epoch: 73 [512/8000 (6%)]\tTotal Loss: 2.561647\n",
      "Reconstruction: 1.652739, Regularization: 0.843469, Discriminator: 0.043223; Generator: 0.022217,\n",
      "D(x): 0.493, D(G(z)): 0.491\n",
      "2019-04-09 23:59:39,123 root         INFO     Train Epoch: 73 [1024/8000 (13%)]\tTotal Loss: 2.602128\n",
      "Reconstruction: 1.668191, Regularization: 0.868498, Discriminator: 0.043268; Generator: 0.022171,\n",
      "D(x): 0.493, D(G(z)): 0.492\n",
      "2019-04-09 23:59:39,235 root         INFO     Train Epoch: 73 [1536/8000 (19%)]\tTotal Loss: 2.761034\n",
      "Reconstruction: 1.848360, Regularization: 0.847154, Discriminator: 0.043382; Generator: 0.022137,\n",
      "D(x): 0.492, D(G(z)): 0.492\n",
      "2019-04-09 23:59:39,347 root         INFO     Train Epoch: 73 [2048/8000 (26%)]\tTotal Loss: 2.919841\n",
      "Reconstruction: 1.916085, Regularization: 0.938286, Discriminator: 0.043369; Generator: 0.022101,\n",
      "D(x): 0.492, D(G(z)): 0.493\n",
      "2019-04-09 23:59:39,458 root         INFO     Train Epoch: 73 [2560/8000 (32%)]\tTotal Loss: 2.689841\n",
      "Reconstruction: 1.753827, Regularization: 0.870720, Discriminator: 0.043251; Generator: 0.022043,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-09 23:59:39,569 root         INFO     Train Epoch: 73 [3072/8000 (38%)]\tTotal Loss: 2.732129\n",
      "Reconstruction: 1.764112, Regularization: 0.902703, Discriminator: 0.043320; Generator: 0.021993,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-09 23:59:39,681 root         INFO     Train Epoch: 73 [3584/8000 (45%)]\tTotal Loss: 2.750681\n",
      "Reconstruction: 1.778645, Regularization: 0.906824, Discriminator: 0.043260; Generator: 0.021953,\n",
      "D(x): 0.496, D(G(z)): 0.495\n",
      "2019-04-09 23:59:39,793 root         INFO     Train Epoch: 73 [4096/8000 (51%)]\tTotal Loss: 2.724220\n",
      "Reconstruction: 1.761570, Regularization: 0.897351, Discriminator: 0.043384; Generator: 0.021914,\n",
      "D(x): 0.495, D(G(z)): 0.496\n",
      "2019-04-09 23:59:39,905 root         INFO     Train Epoch: 73 [4608/8000 (58%)]\tTotal Loss: 2.736187\n",
      "Reconstruction: 1.820048, Regularization: 0.850978, Discriminator: 0.043281; Generator: 0.021880,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-09 23:59:40,017 root         INFO     Train Epoch: 73 [5120/8000 (64%)]\tTotal Loss: 2.513614\n",
      "Reconstruction: 1.674861, Regularization: 0.773599, Discriminator: 0.043328; Generator: 0.021826,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:40,129 root         INFO     Train Epoch: 73 [5632/8000 (70%)]\tTotal Loss: 2.779020\n",
      "Reconstruction: 1.781946, Regularization: 0.932021, Discriminator: 0.043277; Generator: 0.021776,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-09 23:59:40,240 root         INFO     Train Epoch: 73 [6144/8000 (77%)]\tTotal Loss: 2.615358\n",
      "Reconstruction: 1.701403, Regularization: 0.848897, Discriminator: 0.043321; Generator: 0.021737,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-09 23:59:40,352 root         INFO     Train Epoch: 73 [6656/8000 (83%)]\tTotal Loss: 2.724805\n",
      "Reconstruction: 1.759974, Regularization: 0.899840, Discriminator: 0.043295; Generator: 0.021697,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-09 23:59:40,463 root         INFO     Train Epoch: 73 [7168/8000 (90%)]\tTotal Loss: 2.638114\n",
      "Reconstruction: 1.730793, Regularization: 0.842387, Discriminator: 0.043287; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-09 23:59:40,574 root         INFO     Train Epoch: 73 [7680/8000 (96%)]\tTotal Loss: 2.772102\n",
      "Reconstruction: 1.769700, Regularization: 0.937474, Discriminator: 0.043316; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-09 23:59:40,654 root         INFO     ====> Epoch: 73 Average loss: 2.6711\n",
      "2019-04-09 23:59:40,681 root         INFO     Train Epoch: 74 [0/8000 (0%)]\tTotal Loss: 2.619751\n",
      "Reconstruction: 1.668513, Regularization: 0.886179, Discriminator: 0.043470; Generator: 0.021589,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-09 23:59:40,791 root         INFO     Train Epoch: 74 [512/8000 (6%)]\tTotal Loss: 2.503785\n",
      "Reconstruction: 1.550758, Regularization: 0.888163, Discriminator: 0.043342; Generator: 0.021522,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:40,902 root         INFO     Train Epoch: 74 [1024/8000 (13%)]\tTotal Loss: 2.710612\n",
      "Reconstruction: 1.671205, Regularization: 0.974585, Discriminator: 0.043334; Generator: 0.021488,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-09 23:59:41,012 root         INFO     Train Epoch: 74 [1536/8000 (19%)]\tTotal Loss: 2.798894\n",
      "Reconstruction: 1.815898, Regularization: 0.918105, Discriminator: 0.043434; Generator: 0.021457,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-09 23:59:41,122 root         INFO     Train Epoch: 74 [2048/8000 (26%)]\tTotal Loss: 2.706680\n",
      "Reconstruction: 1.750028, Regularization: 0.891802, Discriminator: 0.043422; Generator: 0.021427,\n",
      "D(x): 0.502, D(G(z)): 0.504\n",
      "2019-04-09 23:59:41,231 root         INFO     Train Epoch: 74 [2560/8000 (32%)]\tTotal Loss: 2.468385\n",
      "Reconstruction: 1.629271, Regularization: 0.774283, Discriminator: 0.043453; Generator: 0.021378,\n",
      "D(x): 0.502, D(G(z)): 0.505\n",
      "2019-04-09 23:59:41,342 root         INFO     Train Epoch: 74 [3072/8000 (38%)]\tTotal Loss: 2.563921\n",
      "Reconstruction: 1.685011, Regularization: 0.814084, Discriminator: 0.043495; Generator: 0.021331,\n",
      "D(x): 0.503, D(G(z)): 0.505\n",
      "2019-04-09 23:59:41,452 root         INFO     Train Epoch: 74 [3584/8000 (45%)]\tTotal Loss: 2.779655\n",
      "Reconstruction: 1.847730, Regularization: 0.867225, Discriminator: 0.043398; Generator: 0.021302,\n",
      "D(x): 0.505, D(G(z)): 0.506\n",
      "2019-04-09 23:59:41,560 root         INFO     Train Epoch: 74 [4096/8000 (51%)]\tTotal Loss: 2.859489\n",
      "Reconstruction: 1.811908, Regularization: 0.982885, Discriminator: 0.043449; Generator: 0.021247,\n",
      "D(x): 0.505, D(G(z)): 0.507\n",
      "2019-04-09 23:59:41,667 root         INFO     Train Epoch: 74 [4608/8000 (58%)]\tTotal Loss: 2.296684\n",
      "Reconstruction: 1.470709, Regularization: 0.761379, Discriminator: 0.043357; Generator: 0.021239,\n",
      "D(x): 0.506, D(G(z)): 0.507\n",
      "2019-04-09 23:59:41,775 root         INFO     Train Epoch: 74 [5120/8000 (64%)]\tTotal Loss: 2.714788\n",
      "Reconstruction: 1.722198, Regularization: 0.927980, Discriminator: 0.043424; Generator: 0.021185,\n",
      "D(x): 0.506, D(G(z)): 0.508\n",
      "2019-04-09 23:59:41,882 root         INFO     Train Epoch: 74 [5632/8000 (70%)]\tTotal Loss: 2.787127\n",
      "Reconstruction: 1.815234, Regularization: 0.907240, Discriminator: 0.043475; Generator: 0.021178,\n",
      "D(x): 0.505, D(G(z)): 0.508\n",
      "2019-04-09 23:59:41,990 root         INFO     Train Epoch: 74 [6144/8000 (77%)]\tTotal Loss: 2.585803\n",
      "Reconstruction: 1.674494, Regularization: 0.846762, Discriminator: 0.043422; Generator: 0.021124,\n",
      "D(x): 0.507, D(G(z)): 0.509\n",
      "2019-04-09 23:59:42,100 root         INFO     Train Epoch: 74 [6656/8000 (83%)]\tTotal Loss: 2.695770\n",
      "Reconstruction: 1.695578, Regularization: 0.935672, Discriminator: 0.043411; Generator: 0.021108,\n",
      "D(x): 0.508, D(G(z)): 0.509\n",
      "2019-04-09 23:59:42,210 root         INFO     Train Epoch: 74 [7168/8000 (90%)]\tTotal Loss: 2.682091\n",
      "Reconstruction: 1.706414, Regularization: 0.911111, Discriminator: 0.043478; Generator: 0.021088,\n",
      "D(x): 0.507, D(G(z)): 0.509\n",
      "2019-04-09 23:59:42,320 root         INFO     Train Epoch: 74 [7680/8000 (96%)]\tTotal Loss: 2.823119\n",
      "Reconstruction: 1.773513, Regularization: 0.985117, Discriminator: 0.043425; Generator: 0.021063,\n",
      "D(x): 0.508, D(G(z)): 0.510\n",
      "2019-04-09 23:59:42,400 root         INFO     ====> Epoch: 74 Average loss: 2.6463\n",
      "2019-04-09 23:59:42,427 root         INFO     Train Epoch: 75 [0/8000 (0%)]\tTotal Loss: 2.659782\n",
      "Reconstruction: 1.704134, Regularization: 0.891150, Discriminator: 0.043458; Generator: 0.021039,\n",
      "D(x): 0.508, D(G(z)): 0.510\n",
      "2019-04-09 23:59:42,536 root         INFO     Train Epoch: 75 [512/8000 (6%)]\tTotal Loss: 2.743999\n",
      "Reconstruction: 1.757495, Regularization: 0.922000, Discriminator: 0.043455; Generator: 0.021049,\n",
      "D(x): 0.508, D(G(z)): 0.510\n",
      "2019-04-09 23:59:42,643 root         INFO     Train Epoch: 75 [1024/8000 (13%)]\tTotal Loss: 2.839301\n",
      "Reconstruction: 1.787793, Regularization: 0.987045, Discriminator: 0.043443; Generator: 0.021020,\n",
      "D(x): 0.509, D(G(z)): 0.510\n",
      "2019-04-09 23:59:42,751 root         INFO     Train Epoch: 75 [1536/8000 (19%)]\tTotal Loss: 2.607297\n",
      "Reconstruction: 1.590684, Regularization: 0.952208, Discriminator: 0.043421; Generator: 0.020984,\n",
      "D(x): 0.510, D(G(z)): 0.511\n",
      "2019-04-09 23:59:42,859 root         INFO     Train Epoch: 75 [2048/8000 (26%)]\tTotal Loss: 2.547921\n",
      "Reconstruction: 1.589530, Regularization: 0.893996, Discriminator: 0.043413; Generator: 0.020981,\n",
      "D(x): 0.510, D(G(z)): 0.511\n",
      "2019-04-09 23:59:42,967 root         INFO     Train Epoch: 75 [2560/8000 (32%)]\tTotal Loss: 2.516202\n",
      "Reconstruction: 1.608614, Regularization: 0.843239, Discriminator: 0.043379; Generator: 0.020970,\n",
      "D(x): 0.511, D(G(z)): 0.511\n",
      "2019-04-09 23:59:43,075 root         INFO     Train Epoch: 75 [3072/8000 (38%)]\tTotal Loss: 2.697075\n",
      "Reconstruction: 1.755208, Regularization: 0.877488, Discriminator: 0.043417; Generator: 0.020961,\n",
      "D(x): 0.510, D(G(z)): 0.511\n",
      "2019-04-09 23:59:43,183 root         INFO     Train Epoch: 75 [3584/8000 (45%)]\tTotal Loss: 2.625663\n",
      "Reconstruction: 1.671078, Regularization: 0.890237, Discriminator: 0.043396; Generator: 0.020951,\n",
      "D(x): 0.511, D(G(z)): 0.511\n",
      "2019-04-09 23:59:43,291 root         INFO     Train Epoch: 75 [4096/8000 (51%)]\tTotal Loss: 2.817193\n",
      "Reconstruction: 1.790307, Regularization: 0.962533, Discriminator: 0.043419; Generator: 0.020934,\n",
      "D(x): 0.510, D(G(z)): 0.512\n",
      "2019-04-09 23:59:43,399 root         INFO     Train Epoch: 75 [4608/8000 (58%)]\tTotal Loss: 2.570626\n",
      "Reconstruction: 1.643408, Regularization: 0.862923, Discriminator: 0.043372; Generator: 0.020923,\n",
      "D(x): 0.511, D(G(z)): 0.512\n",
      "2019-04-09 23:59:43,507 root         INFO     Train Epoch: 75 [5120/8000 (64%)]\tTotal Loss: 2.865480\n",
      "Reconstruction: 1.812348, Regularization: 0.988834, Discriminator: 0.043367; Generator: 0.020931,\n",
      "D(x): 0.511, D(G(z)): 0.512\n",
      "2019-04-09 23:59:43,613 root         INFO     Train Epoch: 75 [5632/8000 (70%)]\tTotal Loss: 2.528700\n",
      "Reconstruction: 1.592653, Regularization: 0.871779, Discriminator: 0.043347; Generator: 0.020921,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:43,724 root         INFO     Train Epoch: 75 [6144/8000 (77%)]\tTotal Loss: 2.625717\n",
      "Reconstruction: 1.660004, Regularization: 0.901428, Discriminator: 0.043362; Generator: 0.020923,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:43,835 root         INFO     Train Epoch: 75 [6656/8000 (83%)]\tTotal Loss: 2.897318\n",
      "Reconstruction: 1.845766, Regularization: 0.987291, Discriminator: 0.043343; Generator: 0.020919,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:43,941 root         INFO     Train Epoch: 75 [7168/8000 (90%)]\tTotal Loss: 2.927613\n",
      "Reconstruction: 1.896340, Regularization: 0.967013, Discriminator: 0.043337; Generator: 0.020922,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,048 root         INFO     Train Epoch: 75 [7680/8000 (96%)]\tTotal Loss: 2.480851\n",
      "Reconstruction: 1.614829, Regularization: 0.801767, Discriminator: 0.043337; Generator: 0.020919,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,127 root         INFO     ====> Epoch: 75 Average loss: 2.6446\n",
      "2019-04-09 23:59:44,154 root         INFO     Train Epoch: 76 [0/8000 (0%)]\tTotal Loss: 2.672869\n",
      "Reconstruction: 1.685344, Regularization: 0.923282, Discriminator: 0.043323; Generator: 0.020922,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,267 root         INFO     Train Epoch: 76 [512/8000 (6%)]\tTotal Loss: 2.620338\n",
      "Reconstruction: 1.664044, Regularization: 0.892048, Discriminator: 0.043323; Generator: 0.020923,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,378 root         INFO     Train Epoch: 76 [1024/8000 (13%)]\tTotal Loss: 2.727761\n",
      "Reconstruction: 1.781025, Regularization: 0.882509, Discriminator: 0.043303; Generator: 0.020924,\n",
      "D(x): 0.513, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,490 root         INFO     Train Epoch: 76 [1536/8000 (19%)]\tTotal Loss: 2.579002\n",
      "Reconstruction: 1.608065, Regularization: 0.906691, Discriminator: 0.043314; Generator: 0.020931,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,601 root         INFO     Train Epoch: 76 [2048/8000 (26%)]\tTotal Loss: 2.746426\n",
      "Reconstruction: 1.791749, Regularization: 0.890442, Discriminator: 0.043293; Generator: 0.020942,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,712 root         INFO     Train Epoch: 76 [2560/8000 (32%)]\tTotal Loss: 2.726526\n",
      "Reconstruction: 1.761833, Regularization: 0.900464, Discriminator: 0.043287; Generator: 0.020942,\n",
      "D(x): 0.512, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,823 root         INFO     Train Epoch: 76 [3072/8000 (38%)]\tTotal Loss: 2.566698\n",
      "Reconstruction: 1.647918, Regularization: 0.854563, Discriminator: 0.043271; Generator: 0.020945,\n",
      "D(x): 0.513, D(G(z)): 0.512\n",
      "2019-04-09 23:59:44,933 root         INFO     Train Epoch: 76 [3584/8000 (45%)]\tTotal Loss: 2.554630\n",
      "Reconstruction: 1.592874, Regularization: 0.897553, Discriminator: 0.043250; Generator: 0.020953,\n",
      "D(x): 0.513, D(G(z)): 0.511\n",
      "2019-04-09 23:59:45,044 root         INFO     Train Epoch: 76 [4096/8000 (51%)]\tTotal Loss: 2.705150\n",
      "Reconstruction: 1.742358, Regularization: 0.898563, Discriminator: 0.043267; Generator: 0.020961,\n",
      "D(x): 0.512, D(G(z)): 0.511\n",
      "2019-04-09 23:59:45,155 root         INFO     Train Epoch: 76 [4608/8000 (58%)]\tTotal Loss: 2.754912\n",
      "Reconstruction: 1.747819, Regularization: 0.942853, Discriminator: 0.043270; Generator: 0.020970,\n",
      "D(x): 0.512, D(G(z)): 0.511\n",
      "2019-04-09 23:59:45,266 root         INFO     Train Epoch: 76 [5120/8000 (64%)]\tTotal Loss: 2.564959\n",
      "Reconstruction: 1.707655, Regularization: 0.793095, Discriminator: 0.043232; Generator: 0.020977,\n",
      "D(x): 0.513, D(G(z)): 0.511\n",
      "2019-04-09 23:59:45,376 root         INFO     Train Epoch: 76 [5632/8000 (70%)]\tTotal Loss: 2.475439\n",
      "Reconstruction: 1.589016, Regularization: 0.822156, Discriminator: 0.043280; Generator: 0.020987,\n",
      "D(x): 0.512, D(G(z)): 0.511\n",
      "2019-04-09 23:59:45,487 root         INFO     Train Epoch: 76 [6144/8000 (77%)]\tTotal Loss: 2.644516\n",
      "Reconstruction: 1.717401, Regularization: 0.862852, Discriminator: 0.043268; Generator: 0.020995,\n",
      "D(x): 0.512, D(G(z)): 0.511\n",
      "2019-04-09 23:59:45,598 root         INFO     Train Epoch: 76 [6656/8000 (83%)]\tTotal Loss: 2.589086\n",
      "Reconstruction: 1.691407, Regularization: 0.833449, Discriminator: 0.043219; Generator: 0.021011,\n",
      "D(x): 0.512, D(G(z)): 0.511\n",
      "2019-04-09 23:59:45,709 root         INFO     Train Epoch: 76 [7168/8000 (90%)]\tTotal Loss: 2.908931\n",
      "Reconstruction: 1.868714, Regularization: 0.975947, Discriminator: 0.043253; Generator: 0.021018,\n",
      "D(x): 0.512, D(G(z)): 0.510\n",
      "2019-04-09 23:59:45,820 root         INFO     Train Epoch: 76 [7680/8000 (96%)]\tTotal Loss: 2.585549\n",
      "Reconstruction: 1.670200, Regularization: 0.851060, Discriminator: 0.043260; Generator: 0.021029,\n",
      "D(x): 0.511, D(G(z)): 0.510\n",
      "2019-04-09 23:59:45,901 root         INFO     ====> Epoch: 76 Average loss: 2.6472\n",
      "2019-04-09 23:59:45,928 root         INFO     Train Epoch: 77 [0/8000 (0%)]\tTotal Loss: 2.438853\n",
      "Reconstruction: 1.566750, Regularization: 0.807856, Discriminator: 0.043208; Generator: 0.021040,\n",
      "D(x): 0.512, D(G(z)): 0.510\n",
      "2019-04-09 23:59:46,039 root         INFO     Train Epoch: 77 [512/8000 (6%)]\tTotal Loss: 2.741799\n",
      "Reconstruction: 1.753670, Regularization: 0.923800, Discriminator: 0.043281; Generator: 0.021049,\n",
      "D(x): 0.511, D(G(z)): 0.510\n",
      "2019-04-09 23:59:46,149 root         INFO     Train Epoch: 77 [1024/8000 (13%)]\tTotal Loss: 2.690717\n",
      "Reconstruction: 1.749397, Regularization: 0.877027, Discriminator: 0.043232; Generator: 0.021062,\n",
      "D(x): 0.511, D(G(z)): 0.510\n",
      "2019-04-09 23:59:46,259 root         INFO     Train Epoch: 77 [1536/8000 (19%)]\tTotal Loss: 2.733965\n",
      "Reconstruction: 1.733053, Regularization: 0.936617, Discriminator: 0.043223; Generator: 0.021073,\n",
      "D(x): 0.511, D(G(z)): 0.509\n",
      "2019-04-09 23:59:46,369 root         INFO     Train Epoch: 77 [2048/8000 (26%)]\tTotal Loss: 2.340832\n",
      "Reconstruction: 1.517281, Regularization: 0.759202, Discriminator: 0.043264; Generator: 0.021085,\n",
      "D(x): 0.510, D(G(z)): 0.509\n",
      "2019-04-09 23:59:46,479 root         INFO     Train Epoch: 77 [2560/8000 (32%)]\tTotal Loss: 2.673561\n",
      "Reconstruction: 1.678155, Regularization: 0.931133, Discriminator: 0.043180; Generator: 0.021093,\n",
      "D(x): 0.512, D(G(z)): 0.509\n",
      "2019-04-09 23:59:46,589 root         INFO     Train Epoch: 77 [3072/8000 (38%)]\tTotal Loss: 2.561500\n",
      "Reconstruction: 1.668851, Regularization: 0.828346, Discriminator: 0.043199; Generator: 0.021103,\n",
      "D(x): 0.511, D(G(z)): 0.509\n",
      "2019-04-09 23:59:46,699 root         INFO     Train Epoch: 77 [3584/8000 (45%)]\tTotal Loss: 2.882198\n",
      "Reconstruction: 1.763562, Regularization: 1.054231, Discriminator: 0.043290; Generator: 0.021115,\n",
      "D(x): 0.510, D(G(z)): 0.509\n",
      "2019-04-09 23:59:46,810 root         INFO     Train Epoch: 77 [4096/8000 (51%)]\tTotal Loss: 2.697442\n",
      "Reconstruction: 1.641085, Regularization: 0.991939, Discriminator: 0.043287; Generator: 0.021131,\n",
      "D(x): 0.509, D(G(z)): 0.509\n",
      "2019-04-09 23:59:46,920 root         INFO     Train Epoch: 77 [4608/8000 (58%)]\tTotal Loss: 2.755554\n",
      "Reconstruction: 1.716872, Regularization: 0.974288, Discriminator: 0.043249; Generator: 0.021145,\n",
      "D(x): 0.510, D(G(z)): 0.508\n",
      "2019-04-09 23:59:47,030 root         INFO     Train Epoch: 77 [5120/8000 (64%)]\tTotal Loss: 2.502464\n",
      "Reconstruction: 1.597632, Regularization: 0.840372, Discriminator: 0.043301; Generator: 0.021159,\n",
      "D(x): 0.509, D(G(z)): 0.508\n",
      "2019-04-09 23:59:47,141 root         INFO     Train Epoch: 77 [5632/8000 (70%)]\tTotal Loss: 2.797343\n",
      "Reconstruction: 1.781388, Regularization: 0.951542, Discriminator: 0.043238; Generator: 0.021174,\n",
      "D(x): 0.509, D(G(z)): 0.508\n",
      "2019-04-09 23:59:47,251 root         INFO     Train Epoch: 77 [6144/8000 (77%)]\tTotal Loss: 2.598881\n",
      "Reconstruction: 1.710761, Regularization: 0.823644, Discriminator: 0.043286; Generator: 0.021190,\n",
      "D(x): 0.508, D(G(z)): 0.508\n",
      "2019-04-09 23:59:47,361 root         INFO     Train Epoch: 77 [6656/8000 (83%)]\tTotal Loss: 2.479462\n",
      "Reconstruction: 1.544834, Regularization: 0.870132, Discriminator: 0.043287; Generator: 0.021208,\n",
      "D(x): 0.508, D(G(z)): 0.507\n",
      "2019-04-09 23:59:47,471 root         INFO     Train Epoch: 77 [7168/8000 (90%)]\tTotal Loss: 2.419833\n",
      "Reconstruction: 1.578104, Regularization: 0.777182, Discriminator: 0.043319; Generator: 0.021228,\n",
      "D(x): 0.507, D(G(z)): 0.507\n",
      "2019-04-09 23:59:47,581 root         INFO     Train Epoch: 77 [7680/8000 (96%)]\tTotal Loss: 2.701595\n",
      "Reconstruction: 1.746053, Regularization: 0.890998, Discriminator: 0.043298; Generator: 0.021246,\n",
      "D(x): 0.507, D(G(z)): 0.507\n",
      "2019-04-09 23:59:47,661 root         INFO     ====> Epoch: 77 Average loss: 2.6607\n",
      "2019-04-09 23:59:47,688 root         INFO     Train Epoch: 78 [0/8000 (0%)]\tTotal Loss: 2.530457\n",
      "Reconstruction: 1.639747, Regularization: 0.826122, Discriminator: 0.043329; Generator: 0.021259,\n",
      "D(x): 0.506, D(G(z)): 0.506\n",
      "2019-04-09 23:59:47,799 root         INFO     Train Epoch: 78 [512/8000 (6%)]\tTotal Loss: 2.893597\n",
      "Reconstruction: 1.835481, Regularization: 0.993566, Discriminator: 0.043266; Generator: 0.021283,\n",
      "D(x): 0.507, D(G(z)): 0.506\n",
      "2019-04-09 23:59:47,910 root         INFO     Train Epoch: 78 [1024/8000 (13%)]\tTotal Loss: 2.512761\n",
      "Reconstruction: 1.615507, Regularization: 0.832677, Discriminator: 0.043274; Generator: 0.021302,\n",
      "D(x): 0.507, D(G(z)): 0.506\n",
      "2019-04-09 23:59:48,022 root         INFO     Train Epoch: 78 [1536/8000 (19%)]\tTotal Loss: 2.941436\n",
      "Reconstruction: 1.900079, Regularization: 0.976706, Discriminator: 0.043326; Generator: 0.021325,\n",
      "D(x): 0.505, D(G(z)): 0.505\n",
      "2019-04-09 23:59:48,133 root         INFO     Train Epoch: 78 [2048/8000 (26%)]\tTotal Loss: 2.597315\n",
      "Reconstruction: 1.633634, Regularization: 0.898963, Discriminator: 0.043362; Generator: 0.021356,\n",
      "D(x): 0.504, D(G(z)): 0.505\n",
      "2019-04-09 23:59:48,244 root         INFO     Train Epoch: 78 [2560/8000 (32%)]\tTotal Loss: 2.558996\n",
      "Reconstruction: 1.674004, Regularization: 0.820283, Discriminator: 0.043331; Generator: 0.021378,\n",
      "D(x): 0.504, D(G(z)): 0.505\n",
      "2019-04-09 23:59:48,355 root         INFO     Train Epoch: 78 [3072/8000 (38%)]\tTotal Loss: 2.661913\n",
      "Reconstruction: 1.730836, Regularization: 0.866340, Discriminator: 0.043329; Generator: 0.021408,\n",
      "D(x): 0.504, D(G(z)): 0.504\n",
      "2019-04-09 23:59:48,466 root         INFO     Train Epoch: 78 [3584/8000 (45%)]\tTotal Loss: 2.688233\n",
      "Reconstruction: 1.735877, Regularization: 0.887551, Discriminator: 0.043367; Generator: 0.021437,\n",
      "D(x): 0.503, D(G(z)): 0.504\n",
      "2019-04-09 23:59:48,577 root         INFO     Train Epoch: 78 [4096/8000 (51%)]\tTotal Loss: 2.699083\n",
      "Reconstruction: 1.730869, Regularization: 0.903365, Discriminator: 0.043375; Generator: 0.021474,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-09 23:59:48,688 root         INFO     Train Epoch: 78 [4608/8000 (58%)]\tTotal Loss: 2.772641\n",
      "Reconstruction: 1.702152, Regularization: 1.005651, Discriminator: 0.043313; Generator: 0.021523,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:48,799 root         INFO     Train Epoch: 78 [5120/8000 (64%)]\tTotal Loss: 2.724498\n",
      "Reconstruction: 1.725920, Regularization: 0.933649, Discriminator: 0.043380; Generator: 0.021550,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-09 23:59:48,910 root         INFO     Train Epoch: 78 [5632/8000 (70%)]\tTotal Loss: 2.792029\n",
      "Reconstruction: 1.845134, Regularization: 0.881983, Discriminator: 0.043322; Generator: 0.021590,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-09 23:59:49,021 root         INFO     Train Epoch: 78 [6144/8000 (77%)]\tTotal Loss: 2.527663\n",
      "Reconstruction: 1.649246, Regularization: 0.813449, Discriminator: 0.043330; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-09 23:59:49,132 root         INFO     Train Epoch: 78 [6656/8000 (83%)]\tTotal Loss: 2.902167\n",
      "Reconstruction: 1.877851, Regularization: 0.959317, Discriminator: 0.043336; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-09 23:59:49,243 root         INFO     Train Epoch: 78 [7168/8000 (90%)]\tTotal Loss: 2.864500\n",
      "Reconstruction: 1.906621, Regularization: 0.892880, Discriminator: 0.043261; Generator: 0.021738,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-09 23:59:49,354 root         INFO     Train Epoch: 78 [7680/8000 (96%)]\tTotal Loss: 2.572013\n",
      "Reconstruction: 1.714613, Regularization: 0.792335, Discriminator: 0.043301; Generator: 0.021765,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-09 23:59:49,435 root         INFO     ====> Epoch: 78 Average loss: 2.6823\n",
      "2019-04-09 23:59:49,462 root         INFO     Train Epoch: 79 [0/8000 (0%)]\tTotal Loss: 2.490788\n",
      "Reconstruction: 1.664206, Regularization: 0.761470, Discriminator: 0.043364; Generator: 0.021748,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-09 23:59:49,572 root         INFO     Train Epoch: 79 [512/8000 (6%)]\tTotal Loss: 2.724746\n",
      "Reconstruction: 1.752363, Regularization: 0.907259, Discriminator: 0.043324; Generator: 0.021799,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-09 23:59:49,683 root         INFO     Train Epoch: 79 [1024/8000 (13%)]\tTotal Loss: 2.676259\n",
      "Reconstruction: 1.702646, Regularization: 0.908482, Discriminator: 0.043292; Generator: 0.021840,\n",
      "D(x): 0.498, D(G(z)): 0.497\n",
      "2019-04-09 23:59:49,793 root         INFO     Train Epoch: 79 [1536/8000 (19%)]\tTotal Loss: 2.756222\n",
      "Reconstruction: 1.695742, Regularization: 0.995330, Discriminator: 0.043309; Generator: 0.021841,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:49,903 root         INFO     Train Epoch: 79 [2048/8000 (26%)]\tTotal Loss: 2.957202\n",
      "Reconstruction: 1.930084, Regularization: 0.961961, Discriminator: 0.043261; Generator: 0.021896,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-09 23:59:50,013 root         INFO     Train Epoch: 79 [2560/8000 (32%)]\tTotal Loss: 2.568173\n",
      "Reconstruction: 1.621604, Regularization: 0.881352, Discriminator: 0.043313; Generator: 0.021904,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:50,122 root         INFO     Train Epoch: 79 [3072/8000 (38%)]\tTotal Loss: 2.727222\n",
      "Reconstruction: 1.787196, Regularization: 0.874788, Discriminator: 0.043296; Generator: 0.021941,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:50,232 root         INFO     Train Epoch: 79 [3584/8000 (45%)]\tTotal Loss: 2.898588\n",
      "Reconstruction: 1.851728, Regularization: 0.981652, Discriminator: 0.043221; Generator: 0.021988,\n",
      "D(x): 0.496, D(G(z)): 0.495\n",
      "2019-04-09 23:59:50,341 root         INFO     Train Epoch: 79 [4096/8000 (51%)]\tTotal Loss: 2.328067\n",
      "Reconstruction: 1.542811, Regularization: 0.719921, Discriminator: 0.043327; Generator: 0.022008,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:50,450 root         INFO     Train Epoch: 79 [4608/8000 (58%)]\tTotal Loss: 2.579969\n",
      "Reconstruction: 1.687411, Regularization: 0.827241, Discriminator: 0.043334; Generator: 0.021983,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-09 23:59:50,560 root         INFO     Train Epoch: 79 [5120/8000 (64%)]\tTotal Loss: 2.692932\n",
      "Reconstruction: 1.710510, Regularization: 0.917154, Discriminator: 0.043260; Generator: 0.022009,\n",
      "D(x): 0.496, D(G(z)): 0.494\n",
      "2019-04-09 23:59:50,670 root         INFO     Train Epoch: 79 [5632/8000 (70%)]\tTotal Loss: 2.671209\n",
      "Reconstruction: 1.724641, Regularization: 0.881263, Discriminator: 0.043265; Generator: 0.022040,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-09 23:59:50,780 root         INFO     Train Epoch: 79 [6144/8000 (77%)]\tTotal Loss: 2.751205\n",
      "Reconstruction: 1.797609, Regularization: 0.888283, Discriminator: 0.043266; Generator: 0.022047,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-09 23:59:50,891 root         INFO     Train Epoch: 79 [6656/8000 (83%)]\tTotal Loss: 2.591905\n",
      "Reconstruction: 1.737834, Regularization: 0.788717, Discriminator: 0.043306; Generator: 0.022048,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:51,002 root         INFO     Train Epoch: 79 [7168/8000 (90%)]\tTotal Loss: 2.667989\n",
      "Reconstruction: 1.699311, Regularization: 0.903382, Discriminator: 0.043294; Generator: 0.022003,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-09 23:59:51,113 root         INFO     Train Epoch: 79 [7680/8000 (96%)]\tTotal Loss: 2.610316\n",
      "Reconstruction: 1.692151, Regularization: 0.852824, Discriminator: 0.043300; Generator: 0.022040,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:51,193 root         INFO     ====> Epoch: 79 Average loss: 2.6962\n",
      "2019-04-09 23:59:51,221 root         INFO     Train Epoch: 80 [0/8000 (0%)]\tTotal Loss: 2.995032\n",
      "Reconstruction: 2.109037, Regularization: 0.820635, Discriminator: 0.043377; Generator: 0.021982,\n",
      "D(x): 0.494, D(G(z)): 0.495\n",
      "2019-04-09 23:59:51,331 root         INFO     Train Epoch: 80 [512/8000 (6%)]\tTotal Loss: 2.662438\n",
      "Reconstruction: 1.724382, Regularization: 0.872831, Discriminator: 0.043256; Generator: 0.021969,\n",
      "D(x): 0.496, D(G(z)): 0.495\n",
      "2019-04-09 23:59:51,441 root         INFO     Train Epoch: 80 [1024/8000 (13%)]\tTotal Loss: 2.524575\n",
      "Reconstruction: 1.638322, Regularization: 0.820976, Discriminator: 0.043341; Generator: 0.021936,\n",
      "D(x): 0.495, D(G(z)): 0.496\n",
      "2019-04-09 23:59:51,551 root         INFO     Train Epoch: 80 [1536/8000 (19%)]\tTotal Loss: 2.859370\n",
      "Reconstruction: 1.734923, Regularization: 1.059284, Discriminator: 0.043246; Generator: 0.021917,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-09 23:59:51,660 root         INFO     Train Epoch: 80 [2048/8000 (26%)]\tTotal Loss: 2.370012\n",
      "Reconstruction: 1.532828, Regularization: 0.771906, Discriminator: 0.043348; Generator: 0.021929,\n",
      "D(x): 0.495, D(G(z)): 0.496\n",
      "2019-04-09 23:59:51,770 root         INFO     Train Epoch: 80 [2560/8000 (32%)]\tTotal Loss: 2.699873\n",
      "Reconstruction: 1.692266, Regularization: 0.942410, Discriminator: 0.043296; Generator: 0.021901,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-09 23:59:51,880 root         INFO     Train Epoch: 80 [3072/8000 (38%)]\tTotal Loss: 3.007002\n",
      "Reconstruction: 1.946129, Regularization: 0.995645, Discriminator: 0.043315; Generator: 0.021913,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:51,990 root         INFO     Train Epoch: 80 [3584/8000 (45%)]\tTotal Loss: 2.640360\n",
      "Reconstruction: 1.764863, Regularization: 0.810232, Discriminator: 0.043401; Generator: 0.021864,\n",
      "D(x): 0.496, D(G(z)): 0.497\n",
      "2019-04-09 23:59:52,100 root         INFO     Train Epoch: 80 [4096/8000 (51%)]\tTotal Loss: 2.984102\n",
      "Reconstruction: 1.875553, Regularization: 1.043430, Discriminator: 0.043317; Generator: 0.021802,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-09 23:59:52,210 root         INFO     Train Epoch: 80 [4608/8000 (58%)]\tTotal Loss: 2.550989\n",
      "Reconstruction: 1.653676, Regularization: 0.832111, Discriminator: 0.043458; Generator: 0.021745,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-09 23:59:52,319 root         INFO     Train Epoch: 80 [5120/8000 (64%)]\tTotal Loss: 2.756920\n",
      "Reconstruction: 1.794161, Regularization: 0.897617, Discriminator: 0.043383; Generator: 0.021758,\n",
      "D(x): 0.497, D(G(z)): 0.498\n",
      "2019-04-09 23:59:52,429 root         INFO     Train Epoch: 80 [5632/8000 (70%)]\tTotal Loss: 2.556690\n",
      "Reconstruction: 1.678754, Regularization: 0.812776, Discriminator: 0.043416; Generator: 0.021744,\n",
      "D(x): 0.497, D(G(z)): 0.499\n",
      "2019-04-09 23:59:52,538 root         INFO     Train Epoch: 80 [6144/8000 (77%)]\tTotal Loss: 2.869718\n",
      "Reconstruction: 1.823579, Regularization: 0.981080, Discriminator: 0.043371; Generator: 0.021688,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-09 23:59:52,647 root         INFO     Train Epoch: 80 [6656/8000 (83%)]\tTotal Loss: 2.839079\n",
      "Reconstruction: 1.814060, Regularization: 0.960003, Discriminator: 0.043355; Generator: 0.021661,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-09 23:59:52,757 root         INFO     Train Epoch: 80 [7168/8000 (90%)]\tTotal Loss: 2.788795\n",
      "Reconstruction: 1.827097, Regularization: 0.896671, Discriminator: 0.043371; Generator: 0.021656,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-09 23:59:52,867 root         INFO     Train Epoch: 80 [7680/8000 (96%)]\tTotal Loss: 2.321777\n",
      "Reconstruction: 1.515870, Regularization: 0.740877, Discriminator: 0.043402; Generator: 0.021628,\n",
      "D(x): 0.499, D(G(z)): 0.501\n",
      "2019-04-09 23:59:52,947 root         INFO     ====> Epoch: 80 Average loss: 2.7302\n",
      "2019-04-09 23:59:52,974 root         INFO     Train Epoch: 81 [0/8000 (0%)]\tTotal Loss: 2.685440\n",
      "Reconstruction: 1.760192, Regularization: 0.860261, Discriminator: 0.043384; Generator: 0.021603,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-09 23:59:53,088 root         INFO     Train Epoch: 81 [512/8000 (6%)]\tTotal Loss: 2.876950\n",
      "Reconstruction: 1.900453, Regularization: 0.911488, Discriminator: 0.043414; Generator: 0.021594,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-09 23:59:53,199 root         INFO     Train Epoch: 81 [1024/8000 (13%)]\tTotal Loss: 2.848179\n",
      "Reconstruction: 1.855166, Regularization: 0.928066, Discriminator: 0.043396; Generator: 0.021551,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-09 23:59:53,310 root         INFO     Train Epoch: 81 [1536/8000 (19%)]\tTotal Loss: 2.632261\n",
      "Reconstruction: 1.750746, Regularization: 0.816552, Discriminator: 0.043406; Generator: 0.021557,\n",
      "D(x): 0.500, D(G(z)): 0.502\n",
      "2019-04-09 23:59:53,422 root         INFO     Train Epoch: 81 [2048/8000 (26%)]\tTotal Loss: 2.640045\n",
      "Reconstruction: 1.686224, Regularization: 0.888909, Discriminator: 0.043372; Generator: 0.021540,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-09 23:59:53,533 root         INFO     Train Epoch: 81 [2560/8000 (32%)]\tTotal Loss: 2.683921\n",
      "Reconstruction: 1.746205, Regularization: 0.872825, Discriminator: 0.043352; Generator: 0.021539,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-09 23:59:53,645 root         INFO     Train Epoch: 81 [3072/8000 (38%)]\tTotal Loss: 2.608963\n",
      "Reconstruction: 1.758205, Regularization: 0.785870, Discriminator: 0.043380; Generator: 0.021508,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:53,757 root         INFO     Train Epoch: 81 [3584/8000 (45%)]\tTotal Loss: 2.529755\n",
      "Reconstruction: 1.638933, Regularization: 0.825948, Discriminator: 0.043363; Generator: 0.021511,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:53,868 root         INFO     Train Epoch: 81 [4096/8000 (51%)]\tTotal Loss: 2.643482\n",
      "Reconstruction: 1.718636, Regularization: 0.859988, Discriminator: 0.043352; Generator: 0.021506,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:53,978 root         INFO     Train Epoch: 81 [4608/8000 (58%)]\tTotal Loss: 2.516209\n",
      "Reconstruction: 1.659276, Regularization: 0.792066, Discriminator: 0.043370; Generator: 0.021497,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,089 root         INFO     Train Epoch: 81 [5120/8000 (64%)]\tTotal Loss: 2.669424\n",
      "Reconstruction: 1.750697, Regularization: 0.853880, Discriminator: 0.043350; Generator: 0.021497,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,199 root         INFO     Train Epoch: 81 [5632/8000 (70%)]\tTotal Loss: 2.724468\n",
      "Reconstruction: 1.826295, Regularization: 0.833352, Discriminator: 0.043338; Generator: 0.021481,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,309 root         INFO     Train Epoch: 81 [6144/8000 (77%)]\tTotal Loss: 2.962874\n",
      "Reconstruction: 1.957350, Regularization: 0.940731, Discriminator: 0.043325; Generator: 0.021468,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,421 root         INFO     Train Epoch: 81 [6656/8000 (83%)]\tTotal Loss: 3.106658\n",
      "Reconstruction: 2.010140, Regularization: 1.031738, Discriminator: 0.043302; Generator: 0.021478,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,531 root         INFO     Train Epoch: 81 [7168/8000 (90%)]\tTotal Loss: 2.688202\n",
      "Reconstruction: 1.724143, Regularization: 0.899240, Discriminator: 0.043345; Generator: 0.021474,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,642 root         INFO     Train Epoch: 81 [7680/8000 (96%)]\tTotal Loss: 2.863656\n",
      "Reconstruction: 1.830400, Regularization: 0.968437, Discriminator: 0.043337; Generator: 0.021482,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,723 root         INFO     ====> Epoch: 81 Average loss: 2.7571\n",
      "2019-04-09 23:59:54,749 root         INFO     Train Epoch: 82 [0/8000 (0%)]\tTotal Loss: 2.866690\n",
      "Reconstruction: 1.915450, Regularization: 0.886432, Discriminator: 0.043316; Generator: 0.021492,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,860 root         INFO     Train Epoch: 82 [512/8000 (6%)]\tTotal Loss: 2.625183\n",
      "Reconstruction: 1.742659, Regularization: 0.817633, Discriminator: 0.043396; Generator: 0.021496,\n",
      "D(x): 0.501, D(G(z)): 0.503\n",
      "2019-04-09 23:59:54,968 root         INFO     Train Epoch: 82 [1024/8000 (13%)]\tTotal Loss: 2.776753\n",
      "Reconstruction: 1.819908, Regularization: 0.891974, Discriminator: 0.043357; Generator: 0.021514,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:55,076 root         INFO     Train Epoch: 82 [1536/8000 (19%)]\tTotal Loss: 3.056770\n",
      "Reconstruction: 1.973542, Regularization: 1.018386, Discriminator: 0.043319; Generator: 0.021523,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:55,184 root         INFO     Train Epoch: 82 [2048/8000 (26%)]\tTotal Loss: 2.732701\n",
      "Reconstruction: 1.792330, Regularization: 0.875498, Discriminator: 0.043334; Generator: 0.021539,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-09 23:59:55,296 root         INFO     Train Epoch: 82 [2560/8000 (32%)]\tTotal Loss: 2.814010\n",
      "Reconstruction: 1.866175, Regularization: 0.882941, Discriminator: 0.043337; Generator: 0.021556,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-09 23:59:55,407 root         INFO     Train Epoch: 82 [3072/8000 (38%)]\tTotal Loss: 2.729247\n",
      "Reconstruction: 1.794366, Regularization: 0.870010, Discriminator: 0.043303; Generator: 0.021567,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-09 23:59:55,517 root         INFO     Train Epoch: 82 [3584/8000 (45%)]\tTotal Loss: 3.005218\n",
      "Reconstruction: 1.990736, Regularization: 0.949570, Discriminator: 0.043328; Generator: 0.021585,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-09 23:59:55,628 root         INFO     Train Epoch: 82 [4096/8000 (51%)]\tTotal Loss: 2.834603\n",
      "Reconstruction: 1.865907, Regularization: 0.903782, Discriminator: 0.043311; Generator: 0.021603,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-09 23:59:55,739 root         INFO     Train Epoch: 82 [4608/8000 (58%)]\tTotal Loss: 2.950129\n",
      "Reconstruction: 1.879031, Regularization: 1.006148, Discriminator: 0.043331; Generator: 0.021620,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-09 23:59:55,850 root         INFO     Train Epoch: 82 [5120/8000 (64%)]\tTotal Loss: 3.012858\n",
      "Reconstruction: 1.948124, Regularization: 0.999714, Discriminator: 0.043381; Generator: 0.021640,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-09 23:59:55,961 root         INFO     Train Epoch: 82 [5632/8000 (70%)]\tTotal Loss: 2.858503\n",
      "Reconstruction: 1.844674, Regularization: 0.948834, Discriminator: 0.043332; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-09 23:59:56,072 root         INFO     Train Epoch: 82 [6144/8000 (77%)]\tTotal Loss: 2.716962\n",
      "Reconstruction: 1.754082, Regularization: 0.897826, Discriminator: 0.043363; Generator: 0.021691,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-09 23:59:56,183 root         INFO     Train Epoch: 82 [6656/8000 (83%)]\tTotal Loss: 2.553525\n",
      "Reconstruction: 1.700133, Regularization: 0.788320, Discriminator: 0.043355; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-09 23:59:56,294 root         INFO     Train Epoch: 82 [7168/8000 (90%)]\tTotal Loss: 2.602129\n",
      "Reconstruction: 1.707747, Regularization: 0.829272, Discriminator: 0.043363; Generator: 0.021747,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-09 23:59:56,404 root         INFO     Train Epoch: 82 [7680/8000 (96%)]\tTotal Loss: 3.072570\n",
      "Reconstruction: 1.988888, Regularization: 1.018577, Discriminator: 0.043335; Generator: 0.021769,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-09 23:59:56,485 root         INFO     ====> Epoch: 82 Average loss: 2.7622\n",
      "2019-04-09 23:59:56,512 root         INFO     Train Epoch: 83 [0/8000 (0%)]\tTotal Loss: 2.823497\n",
      "Reconstruction: 1.901635, Regularization: 0.856732, Discriminator: 0.043343; Generator: 0.021786,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-09 23:59:56,623 root         INFO     Train Epoch: 83 [512/8000 (6%)]\tTotal Loss: 2.933686\n",
      "Reconstruction: 1.951275, Regularization: 0.917256, Discriminator: 0.043326; Generator: 0.021828,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:56,734 root         INFO     Train Epoch: 83 [1024/8000 (13%)]\tTotal Loss: 2.795388\n",
      "Reconstruction: 1.894717, Regularization: 0.835496, Discriminator: 0.043324; Generator: 0.021852,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:56,845 root         INFO     Train Epoch: 83 [1536/8000 (19%)]\tTotal Loss: 2.954141\n",
      "Reconstruction: 1.868457, Regularization: 1.020484, Discriminator: 0.043337; Generator: 0.021863,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:56,956 root         INFO     Train Epoch: 83 [2048/8000 (26%)]\tTotal Loss: 2.626898\n",
      "Reconstruction: 1.740439, Regularization: 0.821250, Discriminator: 0.043318; Generator: 0.021892,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:57,066 root         INFO     Train Epoch: 83 [2560/8000 (32%)]\tTotal Loss: 2.892101\n",
      "Reconstruction: 1.956010, Regularization: 0.870864, Discriminator: 0.043315; Generator: 0.021912,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:57,177 root         INFO     Train Epoch: 83 [3072/8000 (38%)]\tTotal Loss: 2.900732\n",
      "Reconstruction: 1.936879, Regularization: 0.898604, Discriminator: 0.043320; Generator: 0.021929,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:57,288 root         INFO     Train Epoch: 83 [3584/8000 (45%)]\tTotal Loss: 2.679111\n",
      "Reconstruction: 1.786177, Regularization: 0.827681, Discriminator: 0.043314; Generator: 0.021939,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:57,398 root         INFO     Train Epoch: 83 [4096/8000 (51%)]\tTotal Loss: 2.726364\n",
      "Reconstruction: 1.771052, Regularization: 0.890041, Discriminator: 0.043307; Generator: 0.021964,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-09 23:59:57,509 root         INFO     Train Epoch: 83 [4608/8000 (58%)]\tTotal Loss: 2.705012\n",
      "Reconstruction: 1.754322, Regularization: 0.885393, Discriminator: 0.043300; Generator: 0.021997,\n",
      "D(x): 0.495, D(G(z)): 0.495\n",
      "2019-04-09 23:59:57,620 root         INFO     Train Epoch: 83 [5120/8000 (64%)]\tTotal Loss: 2.781422\n",
      "Reconstruction: 1.858584, Regularization: 0.857503, Discriminator: 0.043312; Generator: 0.022023,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:57,731 root         INFO     Train Epoch: 83 [5632/8000 (70%)]\tTotal Loss: 2.371208\n",
      "Reconstruction: 1.592944, Regularization: 0.712940, Discriminator: 0.043279; Generator: 0.022045,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-09 23:59:57,842 root         INFO     Train Epoch: 83 [6144/8000 (77%)]\tTotal Loss: 2.991929\n",
      "Reconstruction: 2.122521, Regularization: 0.804058, Discriminator: 0.043294; Generator: 0.022056,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:57,950 root         INFO     Train Epoch: 83 [6656/8000 (83%)]\tTotal Loss: 2.998935\n",
      "Reconstruction: 2.008831, Regularization: 0.924711, Discriminator: 0.043324; Generator: 0.022069,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:58,060 root         INFO     Train Epoch: 83 [7168/8000 (90%)]\tTotal Loss: 2.493540\n",
      "Reconstruction: 1.642715, Regularization: 0.785463, Discriminator: 0.043282; Generator: 0.022080,\n",
      "D(x): 0.494, D(G(z)): 0.493\n",
      "2019-04-09 23:59:58,168 root         INFO     Train Epoch: 83 [7680/8000 (96%)]\tTotal Loss: 2.628748\n",
      "Reconstruction: 1.786467, Regularization: 0.776902, Discriminator: 0.043289; Generator: 0.022090,\n",
      "D(x): 0.494, D(G(z)): 0.493\n",
      "2019-04-09 23:59:58,249 root         INFO     ====> Epoch: 83 Average loss: 2.7674\n",
      "2019-04-09 23:59:58,276 root         INFO     Train Epoch: 84 [0/8000 (0%)]\tTotal Loss: 3.037863\n",
      "Reconstruction: 1.956423, Regularization: 1.016067, Discriminator: 0.043307; Generator: 0.022067,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:58,388 root         INFO     Train Epoch: 84 [512/8000 (6%)]\tTotal Loss: 2.656056\n",
      "Reconstruction: 1.717433, Regularization: 0.873278, Discriminator: 0.043293; Generator: 0.022052,\n",
      "D(x): 0.494, D(G(z)): 0.494\n",
      "2019-04-09 23:59:58,499 root         INFO     Train Epoch: 84 [1024/8000 (13%)]\tTotal Loss: 2.783916\n",
      "Reconstruction: 1.840777, Regularization: 0.877818, Discriminator: 0.043277; Generator: 0.022044,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-09 23:59:58,610 root         INFO     Train Epoch: 84 [1536/8000 (19%)]\tTotal Loss: 2.734754\n",
      "Reconstruction: 1.787131, Regularization: 0.882331, Discriminator: 0.043246; Generator: 0.022047,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-09 23:59:58,720 root         INFO     Train Epoch: 84 [2048/8000 (26%)]\tTotal Loss: 2.625197\n",
      "Reconstruction: 1.680451, Regularization: 0.879458, Discriminator: 0.043258; Generator: 0.022030,\n",
      "D(x): 0.495, D(G(z)): 0.494\n",
      "2019-04-09 23:59:58,832 root         INFO     Train Epoch: 84 [2560/8000 (32%)]\tTotal Loss: 2.092780\n",
      "Reconstruction: 1.345151, Regularization: 0.682375, Discriminator: 0.043235; Generator: 0.022019,\n",
      "D(x): 0.496, D(G(z)): 0.494\n",
      "2019-04-09 23:59:58,943 root         INFO     Train Epoch: 84 [3072/8000 (38%)]\tTotal Loss: 2.537797\n",
      "Reconstruction: 1.684580, Regularization: 0.787967, Discriminator: 0.043265; Generator: 0.021985,\n",
      "D(x): 0.496, D(G(z)): 0.495\n",
      "2019-04-09 23:59:59,054 root         INFO     Train Epoch: 84 [3584/8000 (45%)]\tTotal Loss: 2.767547\n",
      "Reconstruction: 1.864253, Regularization: 0.838066, Discriminator: 0.043252; Generator: 0.021976,\n",
      "D(x): 0.496, D(G(z)): 0.495\n",
      "2019-04-09 23:59:59,165 root         INFO     Train Epoch: 84 [4096/8000 (51%)]\tTotal Loss: 2.789426\n",
      "Reconstruction: 1.850322, Regularization: 0.873880, Discriminator: 0.043304; Generator: 0.021920,\n",
      "D(x): 0.496, D(G(z)): 0.496\n",
      "2019-04-09 23:59:59,276 root         INFO     Train Epoch: 84 [4608/8000 (58%)]\tTotal Loss: 2.614888\n",
      "Reconstruction: 1.757405, Regularization: 0.792320, Discriminator: 0.043265; Generator: 0.021899,\n",
      "D(x): 0.497, D(G(z)): 0.496\n",
      "2019-04-09 23:59:59,387 root         INFO     Train Epoch: 84 [5120/8000 (64%)]\tTotal Loss: 2.964840\n",
      "Reconstruction: 1.906291, Regularization: 0.993347, Discriminator: 0.043355; Generator: 0.021846,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:59,498 root         INFO     Train Epoch: 84 [5632/8000 (70%)]\tTotal Loss: 3.013397\n",
      "Reconstruction: 1.959232, Regularization: 0.989013, Discriminator: 0.043330; Generator: 0.021823,\n",
      "D(x): 0.497, D(G(z)): 0.497\n",
      "2019-04-09 23:59:59,610 root         INFO     Train Epoch: 84 [6144/8000 (77%)]\tTotal Loss: 2.642725\n",
      "Reconstruction: 1.737900, Regularization: 0.839731, Discriminator: 0.043290; Generator: 0.021803,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-09 23:59:59,721 root         INFO     Train Epoch: 84 [6656/8000 (83%)]\tTotal Loss: 2.678924\n",
      "Reconstruction: 1.783047, Regularization: 0.830809, Discriminator: 0.043320; Generator: 0.021747,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-09 23:59:59,831 root         INFO     Train Epoch: 84 [7168/8000 (90%)]\tTotal Loss: 2.742396\n",
      "Reconstruction: 1.788832, Regularization: 0.888519, Discriminator: 0.043325; Generator: 0.021721,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-09 23:59:59,942 root         INFO     Train Epoch: 84 [7680/8000 (96%)]\tTotal Loss: 2.787296\n",
      "Reconstruction: 1.811289, Regularization: 0.911001, Discriminator: 0.043300; Generator: 0.021706,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:00,023 root         INFO     ====> Epoch: 84 Average loss: 2.7407\n",
      "2019-04-10 00:00:00,051 root         INFO     Train Epoch: 85 [0/8000 (0%)]\tTotal Loss: 2.694207\n",
      "Reconstruction: 1.802550, Regularization: 0.826686, Discriminator: 0.043290; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:00,163 root         INFO     Train Epoch: 85 [512/8000 (6%)]\tTotal Loss: 2.865320\n",
      "Reconstruction: 1.870918, Regularization: 0.929410, Discriminator: 0.043324; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:00,274 root         INFO     Train Epoch: 85 [1024/8000 (13%)]\tTotal Loss: 2.629450\n",
      "Reconstruction: 1.679773, Regularization: 0.884736, Discriminator: 0.043303; Generator: 0.021639,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:00,386 root         INFO     Train Epoch: 85 [1536/8000 (19%)]\tTotal Loss: 2.847757\n",
      "Reconstruction: 1.863641, Regularization: 0.919182, Discriminator: 0.043286; Generator: 0.021648,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:00,498 root         INFO     Train Epoch: 85 [2048/8000 (26%)]\tTotal Loss: 2.691929\n",
      "Reconstruction: 1.779601, Regularization: 0.847421, Discriminator: 0.043298; Generator: 0.021609,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:00,609 root         INFO     Train Epoch: 85 [2560/8000 (32%)]\tTotal Loss: 2.692887\n",
      "Reconstruction: 1.815849, Regularization: 0.812150, Discriminator: 0.043274; Generator: 0.021613,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:00,721 root         INFO     Train Epoch: 85 [3072/8000 (38%)]\tTotal Loss: 2.516549\n",
      "Reconstruction: 1.660463, Regularization: 0.791203, Discriminator: 0.043292; Generator: 0.021592,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:00,833 root         INFO     Train Epoch: 85 [3584/8000 (45%)]\tTotal Loss: 2.450597\n",
      "Reconstruction: 1.626831, Regularization: 0.758897, Discriminator: 0.043262; Generator: 0.021607,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:00,944 root         INFO     Train Epoch: 85 [4096/8000 (51%)]\tTotal Loss: 2.757977\n",
      "Reconstruction: 1.787023, Regularization: 0.906043, Discriminator: 0.043283; Generator: 0.021628,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:01,056 root         INFO     Train Epoch: 85 [4608/8000 (58%)]\tTotal Loss: 2.480385\n",
      "Reconstruction: 1.587648, Regularization: 0.827853, Discriminator: 0.043292; Generator: 0.021593,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:01,168 root         INFO     Train Epoch: 85 [5120/8000 (64%)]\tTotal Loss: 2.817049\n",
      "Reconstruction: 1.873996, Regularization: 0.878172, Discriminator: 0.043309; Generator: 0.021572,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:01,279 root         INFO     Train Epoch: 85 [5632/8000 (70%)]\tTotal Loss: 2.449698\n",
      "Reconstruction: 1.555905, Regularization: 0.828933, Discriminator: 0.043309; Generator: 0.021551,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:01,390 root         INFO     Train Epoch: 85 [6144/8000 (77%)]\tTotal Loss: 2.645402\n",
      "Reconstruction: 1.698852, Regularization: 0.881653, Discriminator: 0.043331; Generator: 0.021567,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:00:01,501 root         INFO     Train Epoch: 85 [6656/8000 (83%)]\tTotal Loss: 2.897525\n",
      "Reconstruction: 1.949317, Regularization: 0.883386, Discriminator: 0.043277; Generator: 0.021545,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:00:01,619 root         INFO     Train Epoch: 85 [7168/8000 (90%)]\tTotal Loss: 2.775189\n",
      "Reconstruction: 1.798373, Regularization: 0.911932, Discriminator: 0.043361; Generator: 0.021523,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:01,731 root         INFO     Train Epoch: 85 [7680/8000 (96%)]\tTotal Loss: 2.484327\n",
      "Reconstruction: 1.620645, Regularization: 0.798844, Discriminator: 0.043288; Generator: 0.021551,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:01,815 root         INFO     ====> Epoch: 85 Average loss: 2.6952\n",
      "2019-04-10 00:00:01,842 root         INFO     Train Epoch: 86 [0/8000 (0%)]\tTotal Loss: 3.047983\n",
      "Reconstruction: 2.004426, Regularization: 0.978705, Discriminator: 0.043291; Generator: 0.021562,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:01,953 root         INFO     Train Epoch: 86 [512/8000 (6%)]\tTotal Loss: 2.696601\n",
      "Reconstruction: 1.885906, Regularization: 0.745874, Discriminator: 0.043279; Generator: 0.021542,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:00:02,062 root         INFO     Train Epoch: 86 [1024/8000 (13%)]\tTotal Loss: 2.728669\n",
      "Reconstruction: 1.829425, Regularization: 0.834394, Discriminator: 0.043287; Generator: 0.021562,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:02,173 root         INFO     Train Epoch: 86 [1536/8000 (19%)]\tTotal Loss: 2.700023\n",
      "Reconstruction: 1.700555, Regularization: 0.934568, Discriminator: 0.043358; Generator: 0.021543,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:00:02,282 root         INFO     Train Epoch: 86 [2048/8000 (26%)]\tTotal Loss: 2.735552\n",
      "Reconstruction: 1.801146, Regularization: 0.869501, Discriminator: 0.043361; Generator: 0.021544,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:00:02,390 root         INFO     Train Epoch: 86 [2560/8000 (32%)]\tTotal Loss: 2.510287\n",
      "Reconstruction: 1.643652, Regularization: 0.801690, Discriminator: 0.043370; Generator: 0.021574,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:02,498 root         INFO     Train Epoch: 86 [3072/8000 (38%)]\tTotal Loss: 2.761809\n",
      "Reconstruction: 1.792154, Regularization: 0.904668, Discriminator: 0.043392; Generator: 0.021595,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:02,605 root         INFO     Train Epoch: 86 [3584/8000 (45%)]\tTotal Loss: 2.713846\n",
      "Reconstruction: 1.802528, Regularization: 0.846345, Discriminator: 0.043348; Generator: 0.021626,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:02,712 root         INFO     Train Epoch: 86 [4096/8000 (51%)]\tTotal Loss: 2.701906\n",
      "Reconstruction: 1.722994, Regularization: 0.913906, Discriminator: 0.043376; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:02,820 root         INFO     Train Epoch: 86 [4608/8000 (58%)]\tTotal Loss: 2.771252\n",
      "Reconstruction: 1.813457, Regularization: 0.892769, Discriminator: 0.043374; Generator: 0.021652,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:02,929 root         INFO     Train Epoch: 86 [5120/8000 (64%)]\tTotal Loss: 2.802507\n",
      "Reconstruction: 1.790055, Regularization: 0.947397, Discriminator: 0.043346; Generator: 0.021709,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:03,040 root         INFO     Train Epoch: 86 [5632/8000 (70%)]\tTotal Loss: 2.778055\n",
      "Reconstruction: 1.798191, Regularization: 0.914824, Discriminator: 0.043365; Generator: 0.021676,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:03,153 root         INFO     Train Epoch: 86 [6144/8000 (77%)]\tTotal Loss: 2.737945\n",
      "Reconstruction: 1.838948, Regularization: 0.833948, Discriminator: 0.043332; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:03,265 root         INFO     Train Epoch: 86 [6656/8000 (83%)]\tTotal Loss: 2.510892\n",
      "Reconstruction: 1.629635, Regularization: 0.816211, Discriminator: 0.043360; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:03,377 root         INFO     Train Epoch: 86 [7168/8000 (90%)]\tTotal Loss: 2.872895\n",
      "Reconstruction: 1.816919, Regularization: 0.990870, Discriminator: 0.043380; Generator: 0.021725,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:00:03,491 root         INFO     Train Epoch: 86 [7680/8000 (96%)]\tTotal Loss: 2.982104\n",
      "Reconstruction: 1.945800, Regularization: 0.971219, Discriminator: 0.043363; Generator: 0.021721,\n",
      "D(x): 0.498, D(G(z)): 0.499\n",
      "2019-04-10 00:00:03,572 root         INFO     ====> Epoch: 86 Average loss: 2.6919\n",
      "2019-04-10 00:00:03,599 root         INFO     Train Epoch: 87 [0/8000 (0%)]\tTotal Loss: 2.500124\n",
      "Reconstruction: 1.606992, Regularization: 0.828085, Discriminator: 0.043346; Generator: 0.021701,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:03,709 root         INFO     Train Epoch: 87 [512/8000 (6%)]\tTotal Loss: 2.623858\n",
      "Reconstruction: 1.692207, Regularization: 0.866609, Discriminator: 0.043330; Generator: 0.021712,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:03,820 root         INFO     Train Epoch: 87 [1024/8000 (13%)]\tTotal Loss: 2.532222\n",
      "Reconstruction: 1.668475, Regularization: 0.798694, Discriminator: 0.043344; Generator: 0.021708,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:03,936 root         INFO     Train Epoch: 87 [1536/8000 (19%)]\tTotal Loss: 2.798311\n",
      "Reconstruction: 1.823602, Regularization: 0.909636, Discriminator: 0.043353; Generator: 0.021720,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:04,051 root         INFO     Train Epoch: 87 [2048/8000 (26%)]\tTotal Loss: 2.514419\n",
      "Reconstruction: 1.641943, Regularization: 0.807439, Discriminator: 0.043320; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:04,165 root         INFO     Train Epoch: 87 [2560/8000 (32%)]\tTotal Loss: 2.672636\n",
      "Reconstruction: 1.764782, Regularization: 0.842807, Discriminator: 0.043335; Generator: 0.021711,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:04,277 root         INFO     Train Epoch: 87 [3072/8000 (38%)]\tTotal Loss: 2.446126\n",
      "Reconstruction: 1.551154, Regularization: 0.829959, Discriminator: 0.043325; Generator: 0.021689,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:04,389 root         INFO     Train Epoch: 87 [3584/8000 (45%)]\tTotal Loss: 2.915900\n",
      "Reconstruction: 1.924281, Regularization: 0.926552, Discriminator: 0.043366; Generator: 0.021701,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:04,502 root         INFO     Train Epoch: 87 [4096/8000 (51%)]\tTotal Loss: 2.746312\n",
      "Reconstruction: 1.851611, Regularization: 0.829684, Discriminator: 0.043342; Generator: 0.021674,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:04,613 root         INFO     Train Epoch: 87 [4608/8000 (58%)]\tTotal Loss: 2.772475\n",
      "Reconstruction: 1.797006, Regularization: 0.910489, Discriminator: 0.043334; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:04,724 root         INFO     Train Epoch: 87 [5120/8000 (64%)]\tTotal Loss: 2.733994\n",
      "Reconstruction: 1.761534, Regularization: 0.907492, Discriminator: 0.043342; Generator: 0.021626,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:04,838 root         INFO     Train Epoch: 87 [5632/8000 (70%)]\tTotal Loss: 2.567032\n",
      "Reconstruction: 1.656088, Regularization: 0.846010, Discriminator: 0.043329; Generator: 0.021605,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:04,953 root         INFO     Train Epoch: 87 [6144/8000 (77%)]\tTotal Loss: 2.578015\n",
      "Reconstruction: 1.644937, Regularization: 0.868148, Discriminator: 0.043330; Generator: 0.021601,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:05,065 root         INFO     Train Epoch: 87 [6656/8000 (83%)]\tTotal Loss: 2.746515\n",
      "Reconstruction: 1.756637, Regularization: 0.924950, Discriminator: 0.043345; Generator: 0.021583,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:05,182 root         INFO     Train Epoch: 87 [7168/8000 (90%)]\tTotal Loss: 2.673584\n",
      "Reconstruction: 1.736884, Regularization: 0.871759, Discriminator: 0.043359; Generator: 0.021582,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:05,306 root         INFO     Train Epoch: 87 [7680/8000 (96%)]\tTotal Loss: 2.508295\n",
      "Reconstruction: 1.661863, Regularization: 0.781552, Discriminator: 0.043320; Generator: 0.021561,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:05,406 root         INFO     ====> Epoch: 87 Average loss: 2.6877\n",
      "2019-04-10 00:00:05,435 root         INFO     Train Epoch: 88 [0/8000 (0%)]\tTotal Loss: 2.484250\n",
      "Reconstruction: 1.618597, Regularization: 0.800785, Discriminator: 0.043313; Generator: 0.021555,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:05,564 root         INFO     Train Epoch: 88 [512/8000 (6%)]\tTotal Loss: 2.657614\n",
      "Reconstruction: 1.767896, Regularization: 0.824807, Discriminator: 0.043353; Generator: 0.021558,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:00:05,682 root         INFO     Train Epoch: 88 [1024/8000 (13%)]\tTotal Loss: 2.472423\n",
      "Reconstruction: 1.521967, Regularization: 0.885596, Discriminator: 0.043326; Generator: 0.021534,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:05,799 root         INFO     Train Epoch: 88 [1536/8000 (19%)]\tTotal Loss: 2.592901\n",
      "Reconstruction: 1.712938, Regularization: 0.815105, Discriminator: 0.043346; Generator: 0.021511,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:05,916 root         INFO     Train Epoch: 88 [2048/8000 (26%)]\tTotal Loss: 2.666041\n",
      "Reconstruction: 1.726294, Regularization: 0.874923, Discriminator: 0.043318; Generator: 0.021506,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:00:06,034 root         INFO     Train Epoch: 88 [2560/8000 (32%)]\tTotal Loss: 2.492107\n",
      "Reconstruction: 1.608834, Regularization: 0.818443, Discriminator: 0.043324; Generator: 0.021507,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:06,148 root         INFO     Train Epoch: 88 [3072/8000 (38%)]\tTotal Loss: 2.582723\n",
      "Reconstruction: 1.694666, Regularization: 0.823233, Discriminator: 0.043323; Generator: 0.021500,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:00:06,262 root         INFO     Train Epoch: 88 [3584/8000 (45%)]\tTotal Loss: 2.701292\n",
      "Reconstruction: 1.734529, Regularization: 0.901934, Discriminator: 0.043329; Generator: 0.021501,\n",
      "D(x): 0.502, D(G(z)): 0.503\n",
      "2019-04-10 00:00:06,380 root         INFO     Train Epoch: 88 [4096/8000 (51%)]\tTotal Loss: 2.586504\n",
      "Reconstruction: 1.702929, Regularization: 0.818751, Discriminator: 0.043324; Generator: 0.021500,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:00:06,496 root         INFO     Train Epoch: 88 [4608/8000 (58%)]\tTotal Loss: 2.789954\n",
      "Reconstruction: 1.877509, Regularization: 0.847618, Discriminator: 0.043324; Generator: 0.021503,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:00:06,613 root         INFO     Train Epoch: 88 [5120/8000 (64%)]\tTotal Loss: 2.786772\n",
      "Reconstruction: 1.747994, Regularization: 0.973961, Discriminator: 0.043317; Generator: 0.021500,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:00:06,730 root         INFO     Train Epoch: 88 [5632/8000 (70%)]\tTotal Loss: 2.721107\n",
      "Reconstruction: 1.743763, Regularization: 0.912519, Discriminator: 0.043317; Generator: 0.021508,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:00:06,848 root         INFO     Train Epoch: 88 [6144/8000 (77%)]\tTotal Loss: 2.936110\n",
      "Reconstruction: 1.923336, Regularization: 0.947957, Discriminator: 0.043311; Generator: 0.021506,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:00:06,965 root         INFO     Train Epoch: 88 [6656/8000 (83%)]\tTotal Loss: 2.672100\n",
      "Reconstruction: 1.710307, Regularization: 0.896978, Discriminator: 0.043315; Generator: 0.021500,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:00:07,082 root         INFO     Train Epoch: 88 [7168/8000 (90%)]\tTotal Loss: 2.798914\n",
      "Reconstruction: 1.835923, Regularization: 0.898177, Discriminator: 0.043307; Generator: 0.021508,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:00:07,199 root         INFO     Train Epoch: 88 [7680/8000 (96%)]\tTotal Loss: 2.570547\n",
      "Reconstruction: 1.609508, Regularization: 0.896226, Discriminator: 0.043308; Generator: 0.021505,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:00:07,282 root         INFO     ====> Epoch: 88 Average loss: 2.6566\n",
      "2019-04-10 00:00:07,310 root         INFO     Train Epoch: 89 [0/8000 (0%)]\tTotal Loss: 2.564626\n",
      "Reconstruction: 1.665967, Regularization: 0.833852, Discriminator: 0.043303; Generator: 0.021505,\n",
      "D(x): 0.503, D(G(z)): 0.503\n",
      "2019-04-10 00:00:07,423 root         INFO     Train Epoch: 89 [512/8000 (6%)]\tTotal Loss: 2.824892\n",
      "Reconstruction: 1.831757, Regularization: 0.928316, Discriminator: 0.043295; Generator: 0.021523,\n",
      "D(x): 0.503, D(G(z)): 0.502\n",
      "2019-04-10 00:00:07,535 root         INFO     Train Epoch: 89 [1024/8000 (13%)]\tTotal Loss: 2.729449\n",
      "Reconstruction: 1.763899, Regularization: 0.900718, Discriminator: 0.043301; Generator: 0.021532,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:07,648 root         INFO     Train Epoch: 89 [1536/8000 (19%)]\tTotal Loss: 2.534683\n",
      "Reconstruction: 1.644820, Regularization: 0.825012, Discriminator: 0.043305; Generator: 0.021545,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:07,761 root         INFO     Train Epoch: 89 [2048/8000 (26%)]\tTotal Loss: 2.765979\n",
      "Reconstruction: 1.854254, Regularization: 0.846879, Discriminator: 0.043291; Generator: 0.021554,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:07,875 root         INFO     Train Epoch: 89 [2560/8000 (32%)]\tTotal Loss: 2.618040\n",
      "Reconstruction: 1.697233, Regularization: 0.855949, Discriminator: 0.043301; Generator: 0.021556,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:07,990 root         INFO     Train Epoch: 89 [3072/8000 (38%)]\tTotal Loss: 2.651924\n",
      "Reconstruction: 1.779578, Regularization: 0.807471, Discriminator: 0.043297; Generator: 0.021578,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:08,100 root         INFO     Train Epoch: 89 [3584/8000 (45%)]\tTotal Loss: 2.878630\n",
      "Reconstruction: 1.741402, Regularization: 1.072360, Discriminator: 0.043292; Generator: 0.021576,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:08,206 root         INFO     Train Epoch: 89 [4096/8000 (51%)]\tTotal Loss: 3.021562\n",
      "Reconstruction: 2.016911, Regularization: 0.939789, Discriminator: 0.043279; Generator: 0.021583,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:08,313 root         INFO     Train Epoch: 89 [4608/8000 (58%)]\tTotal Loss: 2.444252\n",
      "Reconstruction: 1.571529, Regularization: 0.807833, Discriminator: 0.043297; Generator: 0.021592,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:08,421 root         INFO     Train Epoch: 89 [5120/8000 (64%)]\tTotal Loss: 2.540749\n",
      "Reconstruction: 1.622508, Regularization: 0.853330, Discriminator: 0.043316; Generator: 0.021596,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:08,530 root         INFO     Train Epoch: 89 [5632/8000 (70%)]\tTotal Loss: 2.513425\n",
      "Reconstruction: 1.636531, Regularization: 0.811975, Discriminator: 0.043307; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:08,638 root         INFO     Train Epoch: 89 [6144/8000 (77%)]\tTotal Loss: 2.597516\n",
      "Reconstruction: 1.691468, Regularization: 0.841115, Discriminator: 0.043305; Generator: 0.021628,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:08,747 root         INFO     Train Epoch: 89 [6656/8000 (83%)]\tTotal Loss: 2.593163\n",
      "Reconstruction: 1.700840, Regularization: 0.827374, Discriminator: 0.043295; Generator: 0.021653,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:08,857 root         INFO     Train Epoch: 89 [7168/8000 (90%)]\tTotal Loss: 2.665433\n",
      "Reconstruction: 1.660324, Regularization: 0.940163, Discriminator: 0.043292; Generator: 0.021654,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:08,966 root         INFO     Train Epoch: 89 [7680/8000 (96%)]\tTotal Loss: 2.828963\n",
      "Reconstruction: 1.826848, Regularization: 0.937152, Discriminator: 0.043277; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:09,046 root         INFO     ====> Epoch: 89 Average loss: 2.6634\n",
      "2019-04-10 00:00:09,074 root         INFO     Train Epoch: 90 [0/8000 (0%)]\tTotal Loss: 2.684868\n",
      "Reconstruction: 1.760848, Regularization: 0.859033, Discriminator: 0.043314; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:09,185 root         INFO     Train Epoch: 90 [512/8000 (6%)]\tTotal Loss: 2.819216\n",
      "Reconstruction: 1.874885, Regularization: 0.879338, Discriminator: 0.043293; Generator: 0.021701,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:09,295 root         INFO     Train Epoch: 90 [1024/8000 (13%)]\tTotal Loss: 2.875382\n",
      "Reconstruction: 1.822967, Regularization: 0.987437, Discriminator: 0.043252; Generator: 0.021726,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:09,405 root         INFO     Train Epoch: 90 [1536/8000 (19%)]\tTotal Loss: 2.542109\n",
      "Reconstruction: 1.665539, Regularization: 0.811531, Discriminator: 0.043317; Generator: 0.021723,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:09,515 root         INFO     Train Epoch: 90 [2048/8000 (26%)]\tTotal Loss: 2.613853\n",
      "Reconstruction: 1.720369, Regularization: 0.828434, Discriminator: 0.043320; Generator: 0.021729,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:09,625 root         INFO     Train Epoch: 90 [2560/8000 (32%)]\tTotal Loss: 2.601429\n",
      "Reconstruction: 1.650931, Regularization: 0.885498, Discriminator: 0.043271; Generator: 0.021730,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:09,735 root         INFO     Train Epoch: 90 [3072/8000 (38%)]\tTotal Loss: 2.855719\n",
      "Reconstruction: 1.872792, Regularization: 0.917929, Discriminator: 0.043272; Generator: 0.021726,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:09,844 root         INFO     Train Epoch: 90 [3584/8000 (45%)]\tTotal Loss: 2.808054\n",
      "Reconstruction: 1.803524, Regularization: 0.939552, Discriminator: 0.043293; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:09,952 root         INFO     Train Epoch: 90 [4096/8000 (51%)]\tTotal Loss: 2.883141\n",
      "Reconstruction: 1.841935, Regularization: 0.976241, Discriminator: 0.043281; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:10,062 root         INFO     Train Epoch: 90 [4608/8000 (58%)]\tTotal Loss: 2.730159\n",
      "Reconstruction: 1.784538, Regularization: 0.880631, Discriminator: 0.043329; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:10,171 root         INFO     Train Epoch: 90 [5120/8000 (64%)]\tTotal Loss: 2.517595\n",
      "Reconstruction: 1.568364, Regularization: 0.884266, Discriminator: 0.043309; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:10,280 root         INFO     Train Epoch: 90 [5632/8000 (70%)]\tTotal Loss: 2.680949\n",
      "Reconstruction: 1.714316, Regularization: 0.901682, Discriminator: 0.043308; Generator: 0.021642,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:10,389 root         INFO     Train Epoch: 90 [6144/8000 (77%)]\tTotal Loss: 2.607661\n",
      "Reconstruction: 1.659816, Regularization: 0.882886, Discriminator: 0.043334; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:10,498 root         INFO     Train Epoch: 90 [6656/8000 (83%)]\tTotal Loss: 3.006437\n",
      "Reconstruction: 2.030253, Regularization: 0.911226, Discriminator: 0.043302; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:10,607 root         INFO     Train Epoch: 90 [7168/8000 (90%)]\tTotal Loss: 3.044771\n",
      "Reconstruction: 1.950846, Regularization: 1.029017, Discriminator: 0.043288; Generator: 0.021620,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:10,717 root         INFO     Train Epoch: 90 [7680/8000 (96%)]\tTotal Loss: 2.586080\n",
      "Reconstruction: 1.678772, Regularization: 0.842350, Discriminator: 0.043348; Generator: 0.021609,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:10,797 root         INFO     ====> Epoch: 90 Average loss: 2.7147\n",
      "2019-04-10 00:00:10,825 root         INFO     Train Epoch: 91 [0/8000 (0%)]\tTotal Loss: 2.718053\n",
      "Reconstruction: 1.765120, Regularization: 0.887993, Discriminator: 0.043336; Generator: 0.021604,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:10,935 root         INFO     Train Epoch: 91 [512/8000 (6%)]\tTotal Loss: 3.037004\n",
      "Reconstruction: 2.101763, Regularization: 0.870283, Discriminator: 0.043334; Generator: 0.021625,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,045 root         INFO     Train Epoch: 91 [1024/8000 (13%)]\tTotal Loss: 2.720628\n",
      "Reconstruction: 1.789706, Regularization: 0.865970, Discriminator: 0.043345; Generator: 0.021607,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,156 root         INFO     Train Epoch: 91 [1536/8000 (19%)]\tTotal Loss: 2.533503\n",
      "Reconstruction: 1.628079, Regularization: 0.840442, Discriminator: 0.043367; Generator: 0.021614,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,266 root         INFO     Train Epoch: 91 [2048/8000 (26%)]\tTotal Loss: 2.543631\n",
      "Reconstruction: 1.662485, Regularization: 0.816165, Discriminator: 0.043361; Generator: 0.021619,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,376 root         INFO     Train Epoch: 91 [2560/8000 (32%)]\tTotal Loss: 2.839475\n",
      "Reconstruction: 1.814342, Regularization: 0.960175, Discriminator: 0.043339; Generator: 0.021619,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,484 root         INFO     Train Epoch: 91 [3072/8000 (38%)]\tTotal Loss: 3.083788\n",
      "Reconstruction: 2.002162, Regularization: 1.016690, Discriminator: 0.043326; Generator: 0.021610,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,593 root         INFO     Train Epoch: 91 [3584/8000 (45%)]\tTotal Loss: 2.660333\n",
      "Reconstruction: 1.699709, Regularization: 0.895671, Discriminator: 0.043324; Generator: 0.021629,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,703 root         INFO     Train Epoch: 91 [4096/8000 (51%)]\tTotal Loss: 3.061121\n",
      "Reconstruction: 2.001948, Regularization: 0.994236, Discriminator: 0.043314; Generator: 0.021622,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:11,812 root         INFO     Train Epoch: 91 [4608/8000 (58%)]\tTotal Loss: 2.839662\n",
      "Reconstruction: 2.023278, Regularization: 0.751374, Discriminator: 0.043358; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:11,921 root         INFO     Train Epoch: 91 [5120/8000 (64%)]\tTotal Loss: 2.805792\n",
      "Reconstruction: 1.849329, Regularization: 0.891482, Discriminator: 0.043359; Generator: 0.021621,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:12,030 root         INFO     Train Epoch: 91 [5632/8000 (70%)]\tTotal Loss: 2.902198\n",
      "Reconstruction: 1.899446, Regularization: 0.937768, Discriminator: 0.043346; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:12,140 root         INFO     Train Epoch: 91 [6144/8000 (77%)]\tTotal Loss: 2.847676\n",
      "Reconstruction: 1.856616, Regularization: 0.926072, Discriminator: 0.043327; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:12,249 root         INFO     Train Epoch: 91 [6656/8000 (83%)]\tTotal Loss: 2.587717\n",
      "Reconstruction: 1.693787, Regularization: 0.828899, Discriminator: 0.043357; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:12,359 root         INFO     Train Epoch: 91 [7168/8000 (90%)]\tTotal Loss: 2.933578\n",
      "Reconstruction: 1.897108, Regularization: 0.971435, Discriminator: 0.043326; Generator: 0.021708,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:12,468 root         INFO     Train Epoch: 91 [7680/8000 (96%)]\tTotal Loss: 2.580582\n",
      "Reconstruction: 1.656356, Regularization: 0.859159, Discriminator: 0.043358; Generator: 0.021710,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:12,548 root         INFO     ====> Epoch: 91 Average loss: 2.7276\n",
      "2019-04-10 00:00:12,575 root         INFO     Train Epoch: 92 [0/8000 (0%)]\tTotal Loss: 2.833378\n",
      "Reconstruction: 1.809408, Regularization: 0.958910, Discriminator: 0.043347; Generator: 0.021713,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:12,686 root         INFO     Train Epoch: 92 [512/8000 (6%)]\tTotal Loss: 2.867097\n",
      "Reconstruction: 1.843502, Regularization: 0.958527, Discriminator: 0.043345; Generator: 0.021723,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:12,797 root         INFO     Train Epoch: 92 [1024/8000 (13%)]\tTotal Loss: 2.649394\n",
      "Reconstruction: 1.786001, Regularization: 0.798292, Discriminator: 0.043344; Generator: 0.021757,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:12,907 root         INFO     Train Epoch: 92 [1536/8000 (19%)]\tTotal Loss: 2.790445\n",
      "Reconstruction: 1.822645, Regularization: 0.902697, Discriminator: 0.043341; Generator: 0.021761,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,018 root         INFO     Train Epoch: 92 [2048/8000 (26%)]\tTotal Loss: 2.674842\n",
      "Reconstruction: 1.730680, Regularization: 0.879057, Discriminator: 0.043342; Generator: 0.021763,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,129 root         INFO     Train Epoch: 92 [2560/8000 (32%)]\tTotal Loss: 2.696832\n",
      "Reconstruction: 1.776638, Regularization: 0.855092, Discriminator: 0.043334; Generator: 0.021767,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,239 root         INFO     Train Epoch: 92 [3072/8000 (38%)]\tTotal Loss: 2.760098\n",
      "Reconstruction: 1.830774, Regularization: 0.864208, Discriminator: 0.043341; Generator: 0.021775,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,350 root         INFO     Train Epoch: 92 [3584/8000 (45%)]\tTotal Loss: 2.485449\n",
      "Reconstruction: 1.637454, Regularization: 0.782900, Discriminator: 0.043315; Generator: 0.021781,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,460 root         INFO     Train Epoch: 92 [4096/8000 (51%)]\tTotal Loss: 2.553132\n",
      "Reconstruction: 1.677443, Regularization: 0.810587, Discriminator: 0.043335; Generator: 0.021767,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,571 root         INFO     Train Epoch: 92 [4608/8000 (58%)]\tTotal Loss: 2.710577\n",
      "Reconstruction: 1.816866, Regularization: 0.828625, Discriminator: 0.043323; Generator: 0.021763,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,681 root         INFO     Train Epoch: 92 [5120/8000 (64%)]\tTotal Loss: 2.614113\n",
      "Reconstruction: 1.756854, Regularization: 0.792152, Discriminator: 0.043321; Generator: 0.021786,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,792 root         INFO     Train Epoch: 92 [5632/8000 (70%)]\tTotal Loss: 2.723023\n",
      "Reconstruction: 1.745350, Regularization: 0.912591, Discriminator: 0.043308; Generator: 0.021774,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:13,902 root         INFO     Train Epoch: 92 [6144/8000 (77%)]\tTotal Loss: 2.721643\n",
      "Reconstruction: 1.829295, Regularization: 0.827279, Discriminator: 0.043316; Generator: 0.021754,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:14,013 root         INFO     Train Epoch: 92 [6656/8000 (83%)]\tTotal Loss: 2.467432\n",
      "Reconstruction: 1.596463, Regularization: 0.805910, Discriminator: 0.043312; Generator: 0.021747,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:14,123 root         INFO     Train Epoch: 92 [7168/8000 (90%)]\tTotal Loss: 2.958093\n",
      "Reconstruction: 1.998362, Regularization: 0.894670, Discriminator: 0.043319; Generator: 0.021742,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:14,234 root         INFO     Train Epoch: 92 [7680/8000 (96%)]\tTotal Loss: 2.738724\n",
      "Reconstruction: 1.807874, Regularization: 0.865801, Discriminator: 0.043318; Generator: 0.021731,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:14,314 root         INFO     ====> Epoch: 92 Average loss: 2.7474\n",
      "2019-04-10 00:00:14,341 root         INFO     Train Epoch: 93 [0/8000 (0%)]\tTotal Loss: 2.487122\n",
      "Reconstruction: 1.670874, Regularization: 0.751216, Discriminator: 0.043312; Generator: 0.021720,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:14,452 root         INFO     Train Epoch: 93 [512/8000 (6%)]\tTotal Loss: 2.617015\n",
      "Reconstruction: 1.736906, Regularization: 0.815093, Discriminator: 0.043312; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:14,561 root         INFO     Train Epoch: 93 [1024/8000 (13%)]\tTotal Loss: 2.654123\n",
      "Reconstruction: 1.748302, Regularization: 0.840800, Discriminator: 0.043314; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:14,670 root         INFO     Train Epoch: 93 [1536/8000 (19%)]\tTotal Loss: 2.630443\n",
      "Reconstruction: 1.725453, Regularization: 0.839998, Discriminator: 0.043305; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:14,779 root         INFO     Train Epoch: 93 [2048/8000 (26%)]\tTotal Loss: 2.718607\n",
      "Reconstruction: 1.731745, Regularization: 0.921878, Discriminator: 0.043310; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:14,885 root         INFO     Train Epoch: 93 [2560/8000 (32%)]\tTotal Loss: 2.978369\n",
      "Reconstruction: 1.959137, Regularization: 0.954261, Discriminator: 0.043308; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:14,993 root         INFO     Train Epoch: 93 [3072/8000 (38%)]\tTotal Loss: 2.768639\n",
      "Reconstruction: 1.815174, Regularization: 0.888509, Discriminator: 0.043305; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:15,101 root         INFO     Train Epoch: 93 [3584/8000 (45%)]\tTotal Loss: 2.851566\n",
      "Reconstruction: 1.881012, Regularization: 0.905615, Discriminator: 0.043296; Generator: 0.021642,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:15,210 root         INFO     Train Epoch: 93 [4096/8000 (51%)]\tTotal Loss: 2.620146\n",
      "Reconstruction: 1.785083, Regularization: 0.770144, Discriminator: 0.043279; Generator: 0.021639,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:15,318 root         INFO     Train Epoch: 93 [4608/8000 (58%)]\tTotal Loss: 2.747853\n",
      "Reconstruction: 1.788587, Regularization: 0.894323, Discriminator: 0.043307; Generator: 0.021636,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:15,425 root         INFO     Train Epoch: 93 [5120/8000 (64%)]\tTotal Loss: 2.998604\n",
      "Reconstruction: 2.053349, Regularization: 0.880304, Discriminator: 0.043287; Generator: 0.021664,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:15,533 root         INFO     Train Epoch: 93 [5632/8000 (70%)]\tTotal Loss: 2.869764\n",
      "Reconstruction: 1.841192, Regularization: 0.963584, Discriminator: 0.043290; Generator: 0.021697,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:15,641 root         INFO     Train Epoch: 93 [6144/8000 (77%)]\tTotal Loss: 2.485579\n",
      "Reconstruction: 1.631863, Regularization: 0.788719, Discriminator: 0.043295; Generator: 0.021701,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:15,750 root         INFO     Train Epoch: 93 [6656/8000 (83%)]\tTotal Loss: 2.765992\n",
      "Reconstruction: 1.769429, Regularization: 0.931523, Discriminator: 0.043332; Generator: 0.021707,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:15,859 root         INFO     Train Epoch: 93 [7168/8000 (90%)]\tTotal Loss: 2.758285\n",
      "Reconstruction: 1.729477, Regularization: 0.963761, Discriminator: 0.043313; Generator: 0.021733,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:15,967 root         INFO     Train Epoch: 93 [7680/8000 (96%)]\tTotal Loss: 2.557646\n",
      "Reconstruction: 1.626122, Regularization: 0.866469, Discriminator: 0.043303; Generator: 0.021752,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:16,047 root         INFO     ====> Epoch: 93 Average loss: 2.7239\n",
      "2019-04-10 00:00:16,074 root         INFO     Train Epoch: 94 [0/8000 (0%)]\tTotal Loss: 2.759296\n",
      "Reconstruction: 1.789678, Regularization: 0.904553, Discriminator: 0.043311; Generator: 0.021756,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:00:16,186 root         INFO     Train Epoch: 94 [512/8000 (6%)]\tTotal Loss: 2.376109\n",
      "Reconstruction: 1.465906, Regularization: 0.845148, Discriminator: 0.043293; Generator: 0.021761,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:00:16,296 root         INFO     Train Epoch: 94 [1024/8000 (13%)]\tTotal Loss: 2.826568\n",
      "Reconstruction: 1.870134, Regularization: 0.891350, Discriminator: 0.043296; Generator: 0.021788,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:16,406 root         INFO     Train Epoch: 94 [1536/8000 (19%)]\tTotal Loss: 2.857175\n",
      "Reconstruction: 1.887799, Regularization: 0.904297, Discriminator: 0.043302; Generator: 0.021778,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:16,516 root         INFO     Train Epoch: 94 [2048/8000 (26%)]\tTotal Loss: 2.721514\n",
      "Reconstruction: 1.738226, Regularization: 0.918205, Discriminator: 0.043320; Generator: 0.021762,\n",
      "D(x): 0.498, D(G(z)): 0.498\n",
      "2019-04-10 00:00:16,626 root         INFO     Train Epoch: 94 [2560/8000 (32%)]\tTotal Loss: 2.770746\n",
      "Reconstruction: 1.824669, Regularization: 0.881018, Discriminator: 0.043307; Generator: 0.021752,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:16,734 root         INFO     Train Epoch: 94 [3072/8000 (38%)]\tTotal Loss: 2.711814\n",
      "Reconstruction: 1.717857, Regularization: 0.928882, Discriminator: 0.043304; Generator: 0.021772,\n",
      "D(x): 0.499, D(G(z)): 0.498\n",
      "2019-04-10 00:00:16,842 root         INFO     Train Epoch: 94 [3584/8000 (45%)]\tTotal Loss: 2.463511\n",
      "Reconstruction: 1.653791, Regularization: 0.744689, Discriminator: 0.043280; Generator: 0.021752,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:16,950 root         INFO     Train Epoch: 94 [4096/8000 (51%)]\tTotal Loss: 2.733877\n",
      "Reconstruction: 1.793264, Regularization: 0.875576, Discriminator: 0.043332; Generator: 0.021705,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:17,059 root         INFO     Train Epoch: 94 [4608/8000 (58%)]\tTotal Loss: 2.718418\n",
      "Reconstruction: 1.700552, Regularization: 0.952831, Discriminator: 0.043336; Generator: 0.021699,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:17,168 root         INFO     Train Epoch: 94 [5120/8000 (64%)]\tTotal Loss: 2.617814\n",
      "Reconstruction: 1.695371, Regularization: 0.857437, Discriminator: 0.043332; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:17,277 root         INFO     Train Epoch: 94 [5632/8000 (70%)]\tTotal Loss: 2.760323\n",
      "Reconstruction: 1.798526, Regularization: 0.896804, Discriminator: 0.043316; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:17,386 root         INFO     Train Epoch: 94 [6144/8000 (77%)]\tTotal Loss: 2.659258\n",
      "Reconstruction: 1.727485, Regularization: 0.866783, Discriminator: 0.043312; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:17,495 root         INFO     Train Epoch: 94 [6656/8000 (83%)]\tTotal Loss: 2.923023\n",
      "Reconstruction: 1.944085, Regularization: 0.913913, Discriminator: 0.043357; Generator: 0.021667,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:17,604 root         INFO     Train Epoch: 94 [7168/8000 (90%)]\tTotal Loss: 2.433975\n",
      "Reconstruction: 1.562672, Regularization: 0.806333, Discriminator: 0.043313; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:17,713 root         INFO     Train Epoch: 94 [7680/8000 (96%)]\tTotal Loss: 2.818146\n",
      "Reconstruction: 1.856055, Regularization: 0.897093, Discriminator: 0.043345; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:17,793 root         INFO     ====> Epoch: 94 Average loss: 2.7027\n",
      "2019-04-10 00:00:17,821 root         INFO     Train Epoch: 95 [0/8000 (0%)]\tTotal Loss: 2.824939\n",
      "Reconstruction: 1.879502, Regularization: 0.880452, Discriminator: 0.043335; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:17,931 root         INFO     Train Epoch: 95 [512/8000 (6%)]\tTotal Loss: 2.471358\n",
      "Reconstruction: 1.584702, Regularization: 0.821716, Discriminator: 0.043326; Generator: 0.021613,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:18,042 root         INFO     Train Epoch: 95 [1024/8000 (13%)]\tTotal Loss: 2.732818\n",
      "Reconstruction: 1.791638, Regularization: 0.876227, Discriminator: 0.043347; Generator: 0.021605,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:18,153 root         INFO     Train Epoch: 95 [1536/8000 (19%)]\tTotal Loss: 2.785005\n",
      "Reconstruction: 1.760946, Regularization: 0.959121, Discriminator: 0.043373; Generator: 0.021566,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:00:18,263 root         INFO     Train Epoch: 95 [2048/8000 (26%)]\tTotal Loss: 2.491844\n",
      "Reconstruction: 1.609252, Regularization: 0.817705, Discriminator: 0.043330; Generator: 0.021557,\n",
      "D(x): 0.502, D(G(z)): 0.502\n",
      "2019-04-10 00:00:18,373 root         INFO     Train Epoch: 95 [2560/8000 (32%)]\tTotal Loss: 2.981656\n",
      "Reconstruction: 1.891577, Regularization: 1.025179, Discriminator: 0.043357; Generator: 0.021543,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:00:18,484 root         INFO     Train Epoch: 95 [3072/8000 (38%)]\tTotal Loss: 2.536490\n",
      "Reconstruction: 1.659776, Regularization: 0.811823, Discriminator: 0.043323; Generator: 0.021567,\n",
      "D(x): 0.501, D(G(z)): 0.502\n",
      "2019-04-10 00:00:18,594 root         INFO     Train Epoch: 95 [3584/8000 (45%)]\tTotal Loss: 2.667742\n",
      "Reconstruction: 1.740080, Regularization: 0.862753, Discriminator: 0.043336; Generator: 0.021573,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:18,705 root         INFO     Train Epoch: 95 [4096/8000 (51%)]\tTotal Loss: 2.621677\n",
      "Reconstruction: 1.735394, Regularization: 0.821379, Discriminator: 0.043320; Generator: 0.021583,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:18,816 root         INFO     Train Epoch: 95 [4608/8000 (58%)]\tTotal Loss: 2.346744\n",
      "Reconstruction: 1.552998, Regularization: 0.728838, Discriminator: 0.043332; Generator: 0.021576,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:18,926 root         INFO     Train Epoch: 95 [5120/8000 (64%)]\tTotal Loss: 2.866919\n",
      "Reconstruction: 1.853847, Regularization: 0.948142, Discriminator: 0.043343; Generator: 0.021587,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,036 root         INFO     Train Epoch: 95 [5632/8000 (70%)]\tTotal Loss: 2.677742\n",
      "Reconstruction: 1.680282, Regularization: 0.932531, Discriminator: 0.043337; Generator: 0.021592,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,147 root         INFO     Train Epoch: 95 [6144/8000 (77%)]\tTotal Loss: 2.860377\n",
      "Reconstruction: 1.811979, Regularization: 0.983449, Discriminator: 0.043356; Generator: 0.021593,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,257 root         INFO     Train Epoch: 95 [6656/8000 (83%)]\tTotal Loss: 2.588372\n",
      "Reconstruction: 1.653226, Regularization: 0.870210, Discriminator: 0.043343; Generator: 0.021592,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,367 root         INFO     Train Epoch: 95 [7168/8000 (90%)]\tTotal Loss: 2.622127\n",
      "Reconstruction: 1.665393, Regularization: 0.891795, Discriminator: 0.043333; Generator: 0.021606,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,478 root         INFO     Train Epoch: 95 [7680/8000 (96%)]\tTotal Loss: 2.662838\n",
      "Reconstruction: 1.649960, Regularization: 0.947936, Discriminator: 0.043345; Generator: 0.021597,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,559 root         INFO     ====> Epoch: 95 Average loss: 2.6595\n",
      "2019-04-10 00:00:19,586 root         INFO     Train Epoch: 96 [0/8000 (0%)]\tTotal Loss: 2.674671\n",
      "Reconstruction: 1.735031, Regularization: 0.874697, Discriminator: 0.043339; Generator: 0.021604,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,698 root         INFO     Train Epoch: 96 [512/8000 (6%)]\tTotal Loss: 2.708118\n",
      "Reconstruction: 1.814772, Regularization: 0.828403, Discriminator: 0.043330; Generator: 0.021613,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,809 root         INFO     Train Epoch: 96 [1024/8000 (13%)]\tTotal Loss: 2.561869\n",
      "Reconstruction: 1.635853, Regularization: 0.861055, Discriminator: 0.043337; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:19,920 root         INFO     Train Epoch: 96 [1536/8000 (19%)]\tTotal Loss: 2.530808\n",
      "Reconstruction: 1.626784, Regularization: 0.839068, Discriminator: 0.043328; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:20,032 root         INFO     Train Epoch: 96 [2048/8000 (26%)]\tTotal Loss: 2.702475\n",
      "Reconstruction: 1.770670, Regularization: 0.866846, Discriminator: 0.043326; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:20,142 root         INFO     Train Epoch: 96 [2560/8000 (32%)]\tTotal Loss: 2.646358\n",
      "Reconstruction: 1.603950, Regularization: 0.977431, Discriminator: 0.043333; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:20,254 root         INFO     Train Epoch: 96 [3072/8000 (38%)]\tTotal Loss: 2.891832\n",
      "Reconstruction: 1.847226, Regularization: 0.979615, Discriminator: 0.043325; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:20,365 root         INFO     Train Epoch: 96 [3584/8000 (45%)]\tTotal Loss: 2.666751\n",
      "Reconstruction: 1.725575, Regularization: 0.876188, Discriminator: 0.043322; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:20,476 root         INFO     Train Epoch: 96 [4096/8000 (51%)]\tTotal Loss: 2.961156\n",
      "Reconstruction: 1.901508, Regularization: 0.994652, Discriminator: 0.043322; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:20,588 root         INFO     Train Epoch: 96 [4608/8000 (58%)]\tTotal Loss: 2.296363\n",
      "Reconstruction: 1.464856, Regularization: 0.766517, Discriminator: 0.043309; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:20,698 root         INFO     Train Epoch: 96 [5120/8000 (64%)]\tTotal Loss: 2.744474\n",
      "Reconstruction: 1.744974, Regularization: 0.934487, Discriminator: 0.043319; Generator: 0.021693,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:20,808 root         INFO     Train Epoch: 96 [5632/8000 (70%)]\tTotal Loss: 2.663079\n",
      "Reconstruction: 1.711081, Regularization: 0.887000, Discriminator: 0.043316; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:20,918 root         INFO     Train Epoch: 96 [6144/8000 (77%)]\tTotal Loss: 2.741913\n",
      "Reconstruction: 1.745587, Regularization: 0.931359, Discriminator: 0.043302; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:21,027 root         INFO     Train Epoch: 96 [6656/8000 (83%)]\tTotal Loss: 2.368003\n",
      "Reconstruction: 1.481060, Regularization: 0.821985, Discriminator: 0.043316; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:21,136 root         INFO     Train Epoch: 96 [7168/8000 (90%)]\tTotal Loss: 2.558186\n",
      "Reconstruction: 1.663858, Regularization: 0.829382, Discriminator: 0.043324; Generator: 0.021622,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:21,244 root         INFO     Train Epoch: 96 [7680/8000 (96%)]\tTotal Loss: 2.295458\n",
      "Reconstruction: 1.467438, Regularization: 0.763106, Discriminator: 0.043314; Generator: 0.021601,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:21,323 root         INFO     ====> Epoch: 96 Average loss: 2.6452\n",
      "2019-04-10 00:00:21,350 root         INFO     Train Epoch: 97 [0/8000 (0%)]\tTotal Loss: 2.528652\n",
      "Reconstruction: 1.567196, Regularization: 0.896549, Discriminator: 0.043314; Generator: 0.021594,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:21,462 root         INFO     Train Epoch: 97 [512/8000 (6%)]\tTotal Loss: 2.729857\n",
      "Reconstruction: 1.713155, Regularization: 0.951815, Discriminator: 0.043302; Generator: 0.021584,\n",
      "D(x): 0.502, D(G(z)): 0.501\n",
      "2019-04-10 00:00:21,573 root         INFO     Train Epoch: 97 [1024/8000 (13%)]\tTotal Loss: 2.585262\n",
      "Reconstruction: 1.662861, Regularization: 0.857485, Discriminator: 0.043317; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:21,684 root         INFO     Train Epoch: 97 [1536/8000 (19%)]\tTotal Loss: 2.658355\n",
      "Reconstruction: 1.735541, Regularization: 0.857903, Discriminator: 0.043307; Generator: 0.021604,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:21,794 root         INFO     Train Epoch: 97 [2048/8000 (26%)]\tTotal Loss: 2.840327\n",
      "Reconstruction: 1.769003, Regularization: 1.006410, Discriminator: 0.043289; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:21,904 root         INFO     Train Epoch: 97 [2560/8000 (32%)]\tTotal Loss: 2.436339\n",
      "Reconstruction: 1.634780, Regularization: 0.736610, Discriminator: 0.043329; Generator: 0.021619,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:22,014 root         INFO     Train Epoch: 97 [3072/8000 (38%)]\tTotal Loss: 2.750441\n",
      "Reconstruction: 1.780056, Regularization: 0.905447, Discriminator: 0.043291; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:22,124 root         INFO     Train Epoch: 97 [3584/8000 (45%)]\tTotal Loss: 2.707546\n",
      "Reconstruction: 1.773420, Regularization: 0.869187, Discriminator: 0.043299; Generator: 0.021640,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:22,234 root         INFO     Train Epoch: 97 [4096/8000 (51%)]\tTotal Loss: 2.618318\n",
      "Reconstruction: 1.651652, Regularization: 0.901739, Discriminator: 0.043308; Generator: 0.021620,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:22,344 root         INFO     Train Epoch: 97 [4608/8000 (58%)]\tTotal Loss: 2.695254\n",
      "Reconstruction: 1.786666, Regularization: 0.843660, Discriminator: 0.043303; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:22,454 root         INFO     Train Epoch: 97 [5120/8000 (64%)]\tTotal Loss: 2.731357\n",
      "Reconstruction: 1.740378, Regularization: 0.926037, Discriminator: 0.043292; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:22,564 root         INFO     Train Epoch: 97 [5632/8000 (70%)]\tTotal Loss: 2.381271\n",
      "Reconstruction: 1.586596, Regularization: 0.729707, Discriminator: 0.043334; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:22,674 root         INFO     Train Epoch: 97 [6144/8000 (77%)]\tTotal Loss: 2.559233\n",
      "Reconstruction: 1.681307, Regularization: 0.812966, Discriminator: 0.043325; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:22,784 root         INFO     Train Epoch: 97 [6656/8000 (83%)]\tTotal Loss: 2.487823\n",
      "Reconstruction: 1.687379, Regularization: 0.735468, Discriminator: 0.043319; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:22,895 root         INFO     Train Epoch: 97 [7168/8000 (90%)]\tTotal Loss: 2.644890\n",
      "Reconstruction: 1.732892, Regularization: 0.847025, Discriminator: 0.043310; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,005 root         INFO     Train Epoch: 97 [7680/8000 (96%)]\tTotal Loss: 2.423088\n",
      "Reconstruction: 1.581380, Regularization: 0.776711, Discriminator: 0.043311; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,086 root         INFO     ====> Epoch: 97 Average loss: 2.6679\n",
      "2019-04-10 00:00:23,113 root         INFO     Train Epoch: 98 [0/8000 (0%)]\tTotal Loss: 2.585548\n",
      "Reconstruction: 1.661227, Regularization: 0.859353, Discriminator: 0.043302; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,226 root         INFO     Train Epoch: 98 [512/8000 (6%)]\tTotal Loss: 2.651949\n",
      "Reconstruction: 1.720489, Regularization: 0.866484, Discriminator: 0.043316; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,337 root         INFO     Train Epoch: 98 [1024/8000 (13%)]\tTotal Loss: 2.820338\n",
      "Reconstruction: 1.852032, Regularization: 0.903318, Discriminator: 0.043318; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,449 root         INFO     Train Epoch: 98 [1536/8000 (19%)]\tTotal Loss: 2.552060\n",
      "Reconstruction: 1.677284, Regularization: 0.809758, Discriminator: 0.043323; Generator: 0.021695,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:23,560 root         INFO     Train Epoch: 98 [2048/8000 (26%)]\tTotal Loss: 2.705999\n",
      "Reconstruction: 1.810297, Regularization: 0.830701, Discriminator: 0.043339; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,673 root         INFO     Train Epoch: 98 [2560/8000 (32%)]\tTotal Loss: 2.598928\n",
      "Reconstruction: 1.769992, Regularization: 0.763918, Discriminator: 0.043355; Generator: 0.021662,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,784 root         INFO     Train Epoch: 98 [3072/8000 (38%)]\tTotal Loss: 2.706246\n",
      "Reconstruction: 1.776797, Regularization: 0.864447, Discriminator: 0.043329; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:23,896 root         INFO     Train Epoch: 98 [3584/8000 (45%)]\tTotal Loss: 2.752610\n",
      "Reconstruction: 1.748207, Regularization: 0.939421, Discriminator: 0.043317; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,008 root         INFO     Train Epoch: 98 [4096/8000 (51%)]\tTotal Loss: 2.431647\n",
      "Reconstruction: 1.569469, Regularization: 0.797149, Discriminator: 0.043343; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,116 root         INFO     Train Epoch: 98 [4608/8000 (58%)]\tTotal Loss: 2.729398\n",
      "Reconstruction: 1.785651, Regularization: 0.878725, Discriminator: 0.043332; Generator: 0.021690,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,224 root         INFO     Train Epoch: 98 [5120/8000 (64%)]\tTotal Loss: 2.709973\n",
      "Reconstruction: 1.727144, Regularization: 0.917852, Discriminator: 0.043320; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,334 root         INFO     Train Epoch: 98 [5632/8000 (70%)]\tTotal Loss: 2.589789\n",
      "Reconstruction: 1.655034, Regularization: 0.869742, Discriminator: 0.043336; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,445 root         INFO     Train Epoch: 98 [6144/8000 (77%)]\tTotal Loss: 2.672352\n",
      "Reconstruction: 1.817705, Regularization: 0.789621, Discriminator: 0.043371; Generator: 0.021655,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,556 root         INFO     Train Epoch: 98 [6656/8000 (83%)]\tTotal Loss: 2.805545\n",
      "Reconstruction: 1.765471, Regularization: 0.975091, Discriminator: 0.043340; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,667 root         INFO     Train Epoch: 98 [7168/8000 (90%)]\tTotal Loss: 2.670252\n",
      "Reconstruction: 1.734371, Regularization: 0.870881, Discriminator: 0.043355; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,778 root         INFO     Train Epoch: 98 [7680/8000 (96%)]\tTotal Loss: 2.951563\n",
      "Reconstruction: 1.940430, Regularization: 0.946163, Discriminator: 0.043333; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,859 root         INFO     ====> Epoch: 98 Average loss: 2.7464\n",
      "2019-04-10 00:00:24,887 root         INFO     Train Epoch: 99 [0/8000 (0%)]\tTotal Loss: 2.863034\n",
      "Reconstruction: 1.853953, Regularization: 0.944099, Discriminator: 0.043337; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:24,999 root         INFO     Train Epoch: 99 [512/8000 (6%)]\tTotal Loss: 2.643718\n",
      "Reconstruction: 1.734699, Regularization: 0.844016, Discriminator: 0.043345; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,111 root         INFO     Train Epoch: 99 [1024/8000 (13%)]\tTotal Loss: 2.701937\n",
      "Reconstruction: 1.804934, Regularization: 0.832004, Discriminator: 0.043336; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,224 root         INFO     Train Epoch: 99 [1536/8000 (19%)]\tTotal Loss: 3.116576\n",
      "Reconstruction: 2.043599, Regularization: 1.008002, Discriminator: 0.043311; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,335 root         INFO     Train Epoch: 99 [2048/8000 (26%)]\tTotal Loss: 2.791290\n",
      "Reconstruction: 1.786383, Regularization: 0.939917, Discriminator: 0.043337; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,445 root         INFO     Train Epoch: 99 [2560/8000 (32%)]\tTotal Loss: 2.709539\n",
      "Reconstruction: 1.831750, Regularization: 0.812794, Discriminator: 0.043348; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,553 root         INFO     Train Epoch: 99 [3072/8000 (38%)]\tTotal Loss: 2.945778\n",
      "Reconstruction: 1.951460, Regularization: 0.929324, Discriminator: 0.043327; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,662 root         INFO     Train Epoch: 99 [3584/8000 (45%)]\tTotal Loss: 2.847486\n",
      "Reconstruction: 1.869029, Regularization: 0.913444, Discriminator: 0.043328; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,771 root         INFO     Train Epoch: 99 [4096/8000 (51%)]\tTotal Loss: 2.720850\n",
      "Reconstruction: 1.768397, Regularization: 0.887437, Discriminator: 0.043327; Generator: 0.021690,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,880 root         INFO     Train Epoch: 99 [4608/8000 (58%)]\tTotal Loss: 2.545020\n",
      "Reconstruction: 1.686731, Regularization: 0.793260, Discriminator: 0.043337; Generator: 0.021692,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:25,989 root         INFO     Train Epoch: 99 [5120/8000 (64%)]\tTotal Loss: 2.757941\n",
      "Reconstruction: 1.826934, Regularization: 0.865975, Discriminator: 0.043325; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,099 root         INFO     Train Epoch: 99 [5632/8000 (70%)]\tTotal Loss: 2.564089\n",
      "Reconstruction: 1.729891, Regularization: 0.769167, Discriminator: 0.043325; Generator: 0.021705,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,208 root         INFO     Train Epoch: 99 [6144/8000 (77%)]\tTotal Loss: 2.524050\n",
      "Reconstruction: 1.629887, Regularization: 0.829111, Discriminator: 0.043317; Generator: 0.021735,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,320 root         INFO     Train Epoch: 99 [6656/8000 (83%)]\tTotal Loss: 2.994755\n",
      "Reconstruction: 1.939760, Regularization: 0.989933, Discriminator: 0.043321; Generator: 0.021740,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,431 root         INFO     Train Epoch: 99 [7168/8000 (90%)]\tTotal Loss: 2.931090\n",
      "Reconstruction: 1.921107, Regularization: 0.944926, Discriminator: 0.043317; Generator: 0.021740,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,542 root         INFO     Train Epoch: 99 [7680/8000 (96%)]\tTotal Loss: 3.047060\n",
      "Reconstruction: 1.988917, Regularization: 0.993097, Discriminator: 0.043320; Generator: 0.021727,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,622 root         INFO     ====> Epoch: 99 Average loss: 2.7866\n",
      "2019-04-10 00:00:26,649 root         INFO     Train Epoch: 100 [0/8000 (0%)]\tTotal Loss: 2.862852\n",
      "Reconstruction: 1.882339, Regularization: 0.915476, Discriminator: 0.043317; Generator: 0.021720,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,760 root         INFO     Train Epoch: 100 [512/8000 (6%)]\tTotal Loss: 2.844509\n",
      "Reconstruction: 1.923906, Regularization: 0.855570, Discriminator: 0.043312; Generator: 0.021721,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,871 root         INFO     Train Epoch: 100 [1024/8000 (13%)]\tTotal Loss: 2.684565\n",
      "Reconstruction: 1.783954, Regularization: 0.835584, Discriminator: 0.043311; Generator: 0.021717,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:26,982 root         INFO     Train Epoch: 100 [1536/8000 (19%)]\tTotal Loss: 3.282524\n",
      "Reconstruction: 2.380636, Regularization: 0.836874, Discriminator: 0.043315; Generator: 0.021698,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:27,092 root         INFO     Train Epoch: 100 [2048/8000 (26%)]\tTotal Loss: 2.913960\n",
      "Reconstruction: 1.951048, Regularization: 0.897896, Discriminator: 0.043311; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:27,203 root         INFO     Train Epoch: 100 [2560/8000 (32%)]\tTotal Loss: 2.867174\n",
      "Reconstruction: 1.843520, Regularization: 0.958678, Discriminator: 0.043309; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:27,313 root         INFO     Train Epoch: 100 [3072/8000 (38%)]\tTotal Loss: 2.427890\n",
      "Reconstruction: 1.640050, Regularization: 0.722884, Discriminator: 0.043285; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:27,424 root         INFO     Train Epoch: 100 [3584/8000 (45%)]\tTotal Loss: 3.067896\n",
      "Reconstruction: 1.950572, Regularization: 1.052332, Discriminator: 0.043322; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:27,534 root         INFO     Train Epoch: 100 [4096/8000 (51%)]\tTotal Loss: 2.865406\n",
      "Reconstruction: 1.894156, Regularization: 0.906279, Discriminator: 0.043302; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:27,645 root         INFO     Train Epoch: 100 [4608/8000 (58%)]\tTotal Loss: 2.777573\n",
      "Reconstruction: 1.935406, Regularization: 0.777239, Discriminator: 0.043288; Generator: 0.021641,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:27,756 root         INFO     Train Epoch: 100 [5120/8000 (64%)]\tTotal Loss: 2.823477\n",
      "Reconstruction: 1.880396, Regularization: 0.878133, Discriminator: 0.043308; Generator: 0.021641,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:27,866 root         INFO     Train Epoch: 100 [5632/8000 (70%)]\tTotal Loss: 2.446673\n",
      "Reconstruction: 1.673233, Regularization: 0.708515, Discriminator: 0.043282; Generator: 0.021644,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:27,977 root         INFO     Train Epoch: 100 [6144/8000 (77%)]\tTotal Loss: 2.796994\n",
      "Reconstruction: 1.842243, Regularization: 0.889769, Discriminator: 0.043315; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:28,088 root         INFO     Train Epoch: 100 [6656/8000 (83%)]\tTotal Loss: 2.879629\n",
      "Reconstruction: 1.820191, Regularization: 0.994428, Discriminator: 0.043338; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:28,198 root         INFO     Train Epoch: 100 [7168/8000 (90%)]\tTotal Loss: 2.679272\n",
      "Reconstruction: 1.726916, Regularization: 0.887326, Discriminator: 0.043323; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:28,308 root         INFO     Train Epoch: 100 [7680/8000 (96%)]\tTotal Loss: 2.548306\n",
      "Reconstruction: 1.670133, Regularization: 0.813158, Discriminator: 0.043315; Generator: 0.021700,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:28,390 root         INFO     ====> Epoch: 100 Average loss: 2.7574\n",
      "2019-04-10 00:00:28,417 root         INFO     Train Epoch: 101 [0/8000 (0%)]\tTotal Loss: 2.674002\n",
      "Reconstruction: 1.768654, Regularization: 0.840319, Discriminator: 0.043321; Generator: 0.021708,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:28,528 root         INFO     Train Epoch: 101 [512/8000 (6%)]\tTotal Loss: 2.930781\n",
      "Reconstruction: 1.929409, Regularization: 0.936316, Discriminator: 0.043319; Generator: 0.021736,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:28,638 root         INFO     Train Epoch: 101 [1024/8000 (13%)]\tTotal Loss: 2.632552\n",
      "Reconstruction: 1.708007, Regularization: 0.859485, Discriminator: 0.043322; Generator: 0.021738,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:28,749 root         INFO     Train Epoch: 101 [1536/8000 (19%)]\tTotal Loss: 2.886089\n",
      "Reconstruction: 1.909741, Regularization: 0.911295, Discriminator: 0.043331; Generator: 0.021722,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:28,859 root         INFO     Train Epoch: 101 [2048/8000 (26%)]\tTotal Loss: 2.629525\n",
      "Reconstruction: 1.681159, Regularization: 0.883352, Discriminator: 0.043327; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:28,970 root         INFO     Train Epoch: 101 [2560/8000 (32%)]\tTotal Loss: 2.989131\n",
      "Reconstruction: 1.909854, Regularization: 1.014216, Discriminator: 0.043342; Generator: 0.021719,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:29,080 root         INFO     Train Epoch: 101 [3072/8000 (38%)]\tTotal Loss: 2.805735\n",
      "Reconstruction: 1.794247, Regularization: 0.946469, Discriminator: 0.043337; Generator: 0.021682,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:29,191 root         INFO     Train Epoch: 101 [3584/8000 (45%)]\tTotal Loss: 2.802188\n",
      "Reconstruction: 1.799992, Regularization: 0.937177, Discriminator: 0.043349; Generator: 0.021671,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:29,301 root         INFO     Train Epoch: 101 [4096/8000 (51%)]\tTotal Loss: 2.591109\n",
      "Reconstruction: 1.692299, Regularization: 0.833831, Discriminator: 0.043332; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:29,412 root         INFO     Train Epoch: 101 [4608/8000 (58%)]\tTotal Loss: 2.768993\n",
      "Reconstruction: 1.833216, Regularization: 0.870809, Discriminator: 0.043352; Generator: 0.021615,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:29,522 root         INFO     Train Epoch: 101 [5120/8000 (64%)]\tTotal Loss: 2.728380\n",
      "Reconstruction: 1.709802, Regularization: 0.953625, Discriminator: 0.043357; Generator: 0.021595,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:29,633 root         INFO     Train Epoch: 101 [5632/8000 (70%)]\tTotal Loss: 2.874374\n",
      "Reconstruction: 1.848590, Regularization: 0.960849, Discriminator: 0.043361; Generator: 0.021575,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:29,743 root         INFO     Train Epoch: 101 [6144/8000 (77%)]\tTotal Loss: 2.647599\n",
      "Reconstruction: 1.690128, Regularization: 0.892543, Discriminator: 0.043344; Generator: 0.021584,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:29,853 root         INFO     Train Epoch: 101 [6656/8000 (83%)]\tTotal Loss: 2.294554\n",
      "Reconstruction: 1.455129, Regularization: 0.774491, Discriminator: 0.043323; Generator: 0.021612,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:29,964 root         INFO     Train Epoch: 101 [7168/8000 (90%)]\tTotal Loss: 2.597263\n",
      "Reconstruction: 1.715788, Regularization: 0.816553, Discriminator: 0.043323; Generator: 0.021599,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:30,074 root         INFO     Train Epoch: 101 [7680/8000 (96%)]\tTotal Loss: 2.586607\n",
      "Reconstruction: 1.716191, Regularization: 0.805485, Discriminator: 0.043334; Generator: 0.021597,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:30,154 root         INFO     ====> Epoch: 101 Average loss: 2.6823\n",
      "2019-04-10 00:00:30,182 root         INFO     Train Epoch: 102 [0/8000 (0%)]\tTotal Loss: 2.641263\n",
      "Reconstruction: 1.750245, Regularization: 0.826089, Discriminator: 0.043322; Generator: 0.021608,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:30,293 root         INFO     Train Epoch: 102 [512/8000 (6%)]\tTotal Loss: 2.428670\n",
      "Reconstruction: 1.576771, Regularization: 0.786960, Discriminator: 0.043326; Generator: 0.021613,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:30,404 root         INFO     Train Epoch: 102 [1024/8000 (13%)]\tTotal Loss: 2.741331\n",
      "Reconstruction: 1.718971, Regularization: 0.957406, Discriminator: 0.043350; Generator: 0.021605,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:30,515 root         INFO     Train Epoch: 102 [1536/8000 (19%)]\tTotal Loss: 2.729484\n",
      "Reconstruction: 1.687319, Regularization: 0.977200, Discriminator: 0.043340; Generator: 0.021625,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:30,625 root         INFO     Train Epoch: 102 [2048/8000 (26%)]\tTotal Loss: 2.967133\n",
      "Reconstruction: 1.878018, Regularization: 1.024159, Discriminator: 0.043346; Generator: 0.021610,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:30,735 root         INFO     Train Epoch: 102 [2560/8000 (32%)]\tTotal Loss: 2.499924\n",
      "Reconstruction: 1.645290, Regularization: 0.789661, Discriminator: 0.043334; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:30,846 root         INFO     Train Epoch: 102 [3072/8000 (38%)]\tTotal Loss: 2.550442\n",
      "Reconstruction: 1.663789, Regularization: 0.821663, Discriminator: 0.043334; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:30,956 root         INFO     Train Epoch: 102 [3584/8000 (45%)]\tTotal Loss: 2.659333\n",
      "Reconstruction: 1.773286, Regularization: 0.821052, Discriminator: 0.043332; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,065 root         INFO     Train Epoch: 102 [4096/8000 (51%)]\tTotal Loss: 2.761604\n",
      "Reconstruction: 1.787039, Regularization: 0.909564, Discriminator: 0.043333; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,175 root         INFO     Train Epoch: 102 [4608/8000 (58%)]\tTotal Loss: 2.782221\n",
      "Reconstruction: 1.777437, Regularization: 0.939803, Discriminator: 0.043333; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,284 root         INFO     Train Epoch: 102 [5120/8000 (64%)]\tTotal Loss: 2.412613\n",
      "Reconstruction: 1.619236, Regularization: 0.728383, Discriminator: 0.043319; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,394 root         INFO     Train Epoch: 102 [5632/8000 (70%)]\tTotal Loss: 2.624382\n",
      "Reconstruction: 1.651477, Regularization: 0.907912, Discriminator: 0.043324; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,503 root         INFO     Train Epoch: 102 [6144/8000 (77%)]\tTotal Loss: 3.044250\n",
      "Reconstruction: 1.907484, Regularization: 1.071783, Discriminator: 0.043326; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,613 root         INFO     Train Epoch: 102 [6656/8000 (83%)]\tTotal Loss: 2.480315\n",
      "Reconstruction: 1.599037, Regularization: 0.816298, Discriminator: 0.043314; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,723 root         INFO     Train Epoch: 102 [7168/8000 (90%)]\tTotal Loss: 2.779464\n",
      "Reconstruction: 1.794096, Regularization: 0.920390, Discriminator: 0.043315; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,832 root         INFO     Train Epoch: 102 [7680/8000 (96%)]\tTotal Loss: 2.512988\n",
      "Reconstruction: 1.650224, Regularization: 0.797802, Discriminator: 0.043317; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:31,913 root         INFO     ====> Epoch: 102 Average loss: 2.6574\n",
      "2019-04-10 00:00:31,940 root         INFO     Train Epoch: 103 [0/8000 (0%)]\tTotal Loss: 2.674134\n",
      "Reconstruction: 1.679566, Regularization: 0.929598, Discriminator: 0.043318; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:32,051 root         INFO     Train Epoch: 103 [512/8000 (6%)]\tTotal Loss: 2.802534\n",
      "Reconstruction: 1.834826, Regularization: 0.902719, Discriminator: 0.043324; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:32,161 root         INFO     Train Epoch: 103 [1024/8000 (13%)]\tTotal Loss: 2.341300\n",
      "Reconstruction: 1.514241, Regularization: 0.762112, Discriminator: 0.043319; Generator: 0.021628,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:32,270 root         INFO     Train Epoch: 103 [1536/8000 (19%)]\tTotal Loss: 2.560205\n",
      "Reconstruction: 1.665068, Regularization: 0.830196, Discriminator: 0.043316; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:32,380 root         INFO     Train Epoch: 103 [2048/8000 (26%)]\tTotal Loss: 2.706840\n",
      "Reconstruction: 1.732112, Regularization: 0.909785, Discriminator: 0.043317; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:32,492 root         INFO     Train Epoch: 103 [2560/8000 (32%)]\tTotal Loss: 2.857733\n",
      "Reconstruction: 1.882999, Regularization: 0.909785, Discriminator: 0.043315; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:32,604 root         INFO     Train Epoch: 103 [3072/8000 (38%)]\tTotal Loss: 2.359829\n",
      "Reconstruction: 1.556950, Regularization: 0.737946, Discriminator: 0.043316; Generator: 0.021617,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:32,715 root         INFO     Train Epoch: 103 [3584/8000 (45%)]\tTotal Loss: 2.803167\n",
      "Reconstruction: 1.794582, Regularization: 0.943661, Discriminator: 0.043300; Generator: 0.021624,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:32,824 root         INFO     Train Epoch: 103 [4096/8000 (51%)]\tTotal Loss: 2.799701\n",
      "Reconstruction: 1.800877, Regularization: 0.933891, Discriminator: 0.043297; Generator: 0.021636,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:32,933 root         INFO     Train Epoch: 103 [4608/8000 (58%)]\tTotal Loss: 2.928951\n",
      "Reconstruction: 1.840261, Regularization: 1.023766, Discriminator: 0.043285; Generator: 0.021640,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,042 root         INFO     Train Epoch: 103 [5120/8000 (64%)]\tTotal Loss: 2.738054\n",
      "Reconstruction: 1.774485, Regularization: 0.898623, Discriminator: 0.043306; Generator: 0.021639,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,152 root         INFO     Train Epoch: 103 [5632/8000 (70%)]\tTotal Loss: 2.871588\n",
      "Reconstruction: 1.869142, Regularization: 0.937483, Discriminator: 0.043298; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,261 root         INFO     Train Epoch: 103 [6144/8000 (77%)]\tTotal Loss: 2.749107\n",
      "Reconstruction: 1.807439, Regularization: 0.876703, Discriminator: 0.043305; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,371 root         INFO     Train Epoch: 103 [6656/8000 (83%)]\tTotal Loss: 2.723439\n",
      "Reconstruction: 1.794595, Regularization: 0.863865, Discriminator: 0.043312; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,480 root         INFO     Train Epoch: 103 [7168/8000 (90%)]\tTotal Loss: 2.733608\n",
      "Reconstruction: 1.725026, Regularization: 0.943612, Discriminator: 0.043303; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,590 root         INFO     Train Epoch: 103 [7680/8000 (96%)]\tTotal Loss: 2.848228\n",
      "Reconstruction: 1.830447, Regularization: 0.952815, Discriminator: 0.043294; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,669 root         INFO     ====> Epoch: 103 Average loss: 2.6523\n",
      "2019-04-10 00:00:33,697 root         INFO     Train Epoch: 104 [0/8000 (0%)]\tTotal Loss: 2.374200\n",
      "Reconstruction: 1.525419, Regularization: 0.783778, Discriminator: 0.043325; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,808 root         INFO     Train Epoch: 104 [512/8000 (6%)]\tTotal Loss: 2.851100\n",
      "Reconstruction: 1.825864, Regularization: 0.960244, Discriminator: 0.043312; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:33,919 root         INFO     Train Epoch: 104 [1024/8000 (13%)]\tTotal Loss: 2.614767\n",
      "Reconstruction: 1.612675, Regularization: 0.937107, Discriminator: 0.043307; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,030 root         INFO     Train Epoch: 104 [1536/8000 (19%)]\tTotal Loss: 2.836844\n",
      "Reconstruction: 1.815428, Regularization: 0.956432, Discriminator: 0.043293; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,140 root         INFO     Train Epoch: 104 [2048/8000 (26%)]\tTotal Loss: 2.601708\n",
      "Reconstruction: 1.671230, Regularization: 0.865498, Discriminator: 0.043325; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,252 root         INFO     Train Epoch: 104 [2560/8000 (32%)]\tTotal Loss: 2.666501\n",
      "Reconstruction: 1.750650, Regularization: 0.850859, Discriminator: 0.043323; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,363 root         INFO     Train Epoch: 104 [3072/8000 (38%)]\tTotal Loss: 2.683491\n",
      "Reconstruction: 1.649381, Regularization: 0.969145, Discriminator: 0.043297; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,474 root         INFO     Train Epoch: 104 [3584/8000 (45%)]\tTotal Loss: 2.814255\n",
      "Reconstruction: 1.793376, Regularization: 0.955916, Discriminator: 0.043310; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,585 root         INFO     Train Epoch: 104 [4096/8000 (51%)]\tTotal Loss: 2.513345\n",
      "Reconstruction: 1.642925, Regularization: 0.805449, Discriminator: 0.043350; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:34,695 root         INFO     Train Epoch: 104 [4608/8000 (58%)]\tTotal Loss: 2.756349\n",
      "Reconstruction: 1.779531, Regularization: 0.911849, Discriminator: 0.043321; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,806 root         INFO     Train Epoch: 104 [5120/8000 (64%)]\tTotal Loss: 2.987015\n",
      "Reconstruction: 2.011778, Regularization: 0.910263, Discriminator: 0.043303; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:34,916 root         INFO     Train Epoch: 104 [5632/8000 (70%)]\tTotal Loss: 2.697073\n",
      "Reconstruction: 1.675353, Regularization: 0.956760, Discriminator: 0.043313; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,027 root         INFO     Train Epoch: 104 [6144/8000 (77%)]\tTotal Loss: 2.578636\n",
      "Reconstruction: 1.640193, Regularization: 0.873470, Discriminator: 0.043332; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,138 root         INFO     Train Epoch: 104 [6656/8000 (83%)]\tTotal Loss: 2.849384\n",
      "Reconstruction: 1.791509, Regularization: 0.992904, Discriminator: 0.043322; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,248 root         INFO     Train Epoch: 104 [7168/8000 (90%)]\tTotal Loss: 2.660247\n",
      "Reconstruction: 1.692379, Regularization: 0.902878, Discriminator: 0.043334; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,358 root         INFO     Train Epoch: 104 [7680/8000 (96%)]\tTotal Loss: 2.644362\n",
      "Reconstruction: 1.700367, Regularization: 0.878999, Discriminator: 0.043329; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,438 root         INFO     ====> Epoch: 104 Average loss: 2.6878\n",
      "2019-04-10 00:00:35,466 root         INFO     Train Epoch: 105 [0/8000 (0%)]\tTotal Loss: 2.828343\n",
      "Reconstruction: 1.726628, Regularization: 1.036721, Discriminator: 0.043321; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,579 root         INFO     Train Epoch: 105 [512/8000 (6%)]\tTotal Loss: 2.567851\n",
      "Reconstruction: 1.675804, Regularization: 0.827045, Discriminator: 0.043339; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,691 root         INFO     Train Epoch: 105 [1024/8000 (13%)]\tTotal Loss: 2.669611\n",
      "Reconstruction: 1.717731, Regularization: 0.886858, Discriminator: 0.043340; Generator: 0.021681,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,802 root         INFO     Train Epoch: 105 [1536/8000 (19%)]\tTotal Loss: 3.395024\n",
      "Reconstruction: 2.472034, Regularization: 0.857962, Discriminator: 0.043347; Generator: 0.021681,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:35,914 root         INFO     Train Epoch: 105 [2048/8000 (26%)]\tTotal Loss: 2.844301\n",
      "Reconstruction: 1.779865, Regularization: 0.999405, Discriminator: 0.043327; Generator: 0.021703,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:36,026 root         INFO     Train Epoch: 105 [2560/8000 (32%)]\tTotal Loss: 2.787704\n",
      "Reconstruction: 1.840012, Regularization: 0.882661, Discriminator: 0.043332; Generator: 0.021699,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:36,137 root         INFO     Train Epoch: 105 [3072/8000 (38%)]\tTotal Loss: 2.366671\n",
      "Reconstruction: 1.541501, Regularization: 0.760140, Discriminator: 0.043334; Generator: 0.021697,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:36,249 root         INFO     Train Epoch: 105 [3584/8000 (45%)]\tTotal Loss: 2.549523\n",
      "Reconstruction: 1.637699, Regularization: 0.846807, Discriminator: 0.043330; Generator: 0.021688,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:36,361 root         INFO     Train Epoch: 105 [4096/8000 (51%)]\tTotal Loss: 2.700989\n",
      "Reconstruction: 1.805195, Regularization: 0.830781, Discriminator: 0.043336; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:36,472 root         INFO     Train Epoch: 105 [4608/8000 (58%)]\tTotal Loss: 3.032347\n",
      "Reconstruction: 2.140507, Regularization: 0.826832, Discriminator: 0.043325; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:36,584 root         INFO     Train Epoch: 105 [5120/8000 (64%)]\tTotal Loss: 2.663716\n",
      "Reconstruction: 1.697227, Regularization: 0.901505, Discriminator: 0.043323; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:36,696 root         INFO     Train Epoch: 105 [5632/8000 (70%)]\tTotal Loss: 2.346315\n",
      "Reconstruction: 1.505391, Regularization: 0.775947, Discriminator: 0.043319; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:36,807 root         INFO     Train Epoch: 105 [6144/8000 (77%)]\tTotal Loss: 2.675681\n",
      "Reconstruction: 1.795625, Regularization: 0.815080, Discriminator: 0.043319; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:36,918 root         INFO     Train Epoch: 105 [6656/8000 (83%)]\tTotal Loss: 2.635099\n",
      "Reconstruction: 1.756711, Regularization: 0.813408, Discriminator: 0.043314; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,030 root         INFO     Train Epoch: 105 [7168/8000 (90%)]\tTotal Loss: 2.557147\n",
      "Reconstruction: 1.656696, Regularization: 0.835468, Discriminator: 0.043313; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,141 root         INFO     Train Epoch: 105 [7680/8000 (96%)]\tTotal Loss: 2.477148\n",
      "Reconstruction: 1.627735, Regularization: 0.784428, Discriminator: 0.043312; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,222 root         INFO     ====> Epoch: 105 Average loss: 2.7181\n",
      "2019-04-10 00:00:37,249 root         INFO     Train Epoch: 106 [0/8000 (0%)]\tTotal Loss: 2.690499\n",
      "Reconstruction: 1.791846, Regularization: 0.833664, Discriminator: 0.043313; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,362 root         INFO     Train Epoch: 106 [512/8000 (6%)]\tTotal Loss: 2.649229\n",
      "Reconstruction: 1.694014, Regularization: 0.890222, Discriminator: 0.043305; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,473 root         INFO     Train Epoch: 106 [1024/8000 (13%)]\tTotal Loss: 2.500604\n",
      "Reconstruction: 1.639354, Regularization: 0.796275, Discriminator: 0.043305; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,585 root         INFO     Train Epoch: 106 [1536/8000 (19%)]\tTotal Loss: 2.621280\n",
      "Reconstruction: 1.721643, Regularization: 0.834650, Discriminator: 0.043313; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,696 root         INFO     Train Epoch: 106 [2048/8000 (26%)]\tTotal Loss: 3.160947\n",
      "Reconstruction: 2.098528, Regularization: 0.997415, Discriminator: 0.043316; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,807 root         INFO     Train Epoch: 106 [2560/8000 (32%)]\tTotal Loss: 2.551275\n",
      "Reconstruction: 1.663833, Regularization: 0.822450, Discriminator: 0.043309; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:37,916 root         INFO     Train Epoch: 106 [3072/8000 (38%)]\tTotal Loss: 2.864269\n",
      "Reconstruction: 1.886319, Regularization: 0.912947, Discriminator: 0.043318; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:38,027 root         INFO     Train Epoch: 106 [3584/8000 (45%)]\tTotal Loss: 2.653631\n",
      "Reconstruction: 1.709586, Regularization: 0.879041, Discriminator: 0.043306; Generator: 0.021699,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:38,136 root         INFO     Train Epoch: 106 [4096/8000 (51%)]\tTotal Loss: 3.033610\n",
      "Reconstruction: 1.959535, Regularization: 1.009059, Discriminator: 0.043328; Generator: 0.021687,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:38,246 root         INFO     Train Epoch: 106 [4608/8000 (58%)]\tTotal Loss: 2.681101\n",
      "Reconstruction: 1.686565, Regularization: 0.929516, Discriminator: 0.043314; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:38,355 root         INFO     Train Epoch: 106 [5120/8000 (64%)]\tTotal Loss: 2.654294\n",
      "Reconstruction: 1.748647, Regularization: 0.840648, Discriminator: 0.043296; Generator: 0.021703,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:38,465 root         INFO     Train Epoch: 106 [5632/8000 (70%)]\tTotal Loss: 2.704176\n",
      "Reconstruction: 1.780897, Regularization: 0.858278, Discriminator: 0.043304; Generator: 0.021697,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:38,574 root         INFO     Train Epoch: 106 [6144/8000 (77%)]\tTotal Loss: 2.684386\n",
      "Reconstruction: 1.699772, Regularization: 0.919596, Discriminator: 0.043314; Generator: 0.021704,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:38,685 root         INFO     Train Epoch: 106 [6656/8000 (83%)]\tTotal Loss: 2.437294\n",
      "Reconstruction: 1.602789, Regularization: 0.769529, Discriminator: 0.043300; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:38,796 root         INFO     Train Epoch: 106 [7168/8000 (90%)]\tTotal Loss: 2.526001\n",
      "Reconstruction: 1.729756, Regularization: 0.731266, Discriminator: 0.043307; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:38,907 root         INFO     Train Epoch: 106 [7680/8000 (96%)]\tTotal Loss: 2.759740\n",
      "Reconstruction: 1.812624, Regularization: 0.882143, Discriminator: 0.043333; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:38,987 root         INFO     ====> Epoch: 106 Average loss: 2.7032\n",
      "2019-04-10 00:00:39,015 root         INFO     Train Epoch: 107 [0/8000 (0%)]\tTotal Loss: 2.689020\n",
      "Reconstruction: 1.822321, Regularization: 0.801747, Discriminator: 0.043299; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:39,127 root         INFO     Train Epoch: 107 [512/8000 (6%)]\tTotal Loss: 2.779396\n",
      "Reconstruction: 1.810200, Regularization: 0.904233, Discriminator: 0.043341; Generator: 0.021622,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:39,238 root         INFO     Train Epoch: 107 [1024/8000 (13%)]\tTotal Loss: 2.590258\n",
      "Reconstruction: 1.628946, Regularization: 0.896362, Discriminator: 0.043331; Generator: 0.021618,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:39,350 root         INFO     Train Epoch: 107 [1536/8000 (19%)]\tTotal Loss: 2.792858\n",
      "Reconstruction: 1.787314, Regularization: 0.940568, Discriminator: 0.043335; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:39,461 root         INFO     Train Epoch: 107 [2048/8000 (26%)]\tTotal Loss: 2.648311\n",
      "Reconstruction: 1.718322, Regularization: 0.865011, Discriminator: 0.043334; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:39,573 root         INFO     Train Epoch: 107 [2560/8000 (32%)]\tTotal Loss: 2.818964\n",
      "Reconstruction: 1.784527, Regularization: 0.969462, Discriminator: 0.043355; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:39,684 root         INFO     Train Epoch: 107 [3072/8000 (38%)]\tTotal Loss: 2.660661\n",
      "Reconstruction: 1.753996, Regularization: 0.841698, Discriminator: 0.043333; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:39,795 root         INFO     Train Epoch: 107 [3584/8000 (45%)]\tTotal Loss: 2.793418\n",
      "Reconstruction: 1.737434, Regularization: 0.990990, Discriminator: 0.043353; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:39,906 root         INFO     Train Epoch: 107 [4096/8000 (51%)]\tTotal Loss: 2.853993\n",
      "Reconstruction: 1.859460, Regularization: 0.929573, Discriminator: 0.043332; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:00:40,018 root         INFO     Train Epoch: 107 [4608/8000 (58%)]\tTotal Loss: 2.625040\n",
      "Reconstruction: 1.649247, Regularization: 0.910796, Discriminator: 0.043352; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,129 root         INFO     Train Epoch: 107 [5120/8000 (64%)]\tTotal Loss: 2.573296\n",
      "Reconstruction: 1.666428, Regularization: 0.841884, Discriminator: 0.043327; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,240 root         INFO     Train Epoch: 107 [5632/8000 (70%)]\tTotal Loss: 2.567268\n",
      "Reconstruction: 1.623636, Regularization: 0.878623, Discriminator: 0.043338; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,351 root         INFO     Train Epoch: 107 [6144/8000 (77%)]\tTotal Loss: 2.920667\n",
      "Reconstruction: 1.854199, Regularization: 1.001453, Discriminator: 0.043355; Generator: 0.021660,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,462 root         INFO     Train Epoch: 107 [6656/8000 (83%)]\tTotal Loss: 2.751464\n",
      "Reconstruction: 1.719677, Regularization: 0.966780, Discriminator: 0.043343; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,572 root         INFO     Train Epoch: 107 [7168/8000 (90%)]\tTotal Loss: 2.853132\n",
      "Reconstruction: 1.850971, Regularization: 0.937167, Discriminator: 0.043333; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,684 root         INFO     Train Epoch: 107 [7680/8000 (96%)]\tTotal Loss: 2.741222\n",
      "Reconstruction: 1.732209, Regularization: 0.944024, Discriminator: 0.043337; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,764 root         INFO     ====> Epoch: 107 Average loss: 2.6489\n",
      "2019-04-10 00:00:40,791 root         INFO     Train Epoch: 108 [0/8000 (0%)]\tTotal Loss: 2.628459\n",
      "Reconstruction: 1.694183, Regularization: 0.869296, Discriminator: 0.043330; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:40,903 root         INFO     Train Epoch: 108 [512/8000 (6%)]\tTotal Loss: 2.439547\n",
      "Reconstruction: 1.561493, Regularization: 0.813077, Discriminator: 0.043326; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:41,014 root         INFO     Train Epoch: 108 [1024/8000 (13%)]\tTotal Loss: 2.797199\n",
      "Reconstruction: 1.714493, Regularization: 1.017717, Discriminator: 0.043336; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:41,125 root         INFO     Train Epoch: 108 [1536/8000 (19%)]\tTotal Loss: 2.571941\n",
      "Reconstruction: 1.665073, Regularization: 0.841897, Discriminator: 0.043324; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:41,236 root         INFO     Train Epoch: 108 [2048/8000 (26%)]\tTotal Loss: 2.682799\n",
      "Reconstruction: 1.650041, Regularization: 0.967782, Discriminator: 0.043328; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:41,348 root         INFO     Train Epoch: 108 [2560/8000 (32%)]\tTotal Loss: 2.737590\n",
      "Reconstruction: 1.699273, Regularization: 0.973341, Discriminator: 0.043324; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:41,459 root         INFO     Train Epoch: 108 [3072/8000 (38%)]\tTotal Loss: 2.706459\n",
      "Reconstruction: 1.735230, Regularization: 0.906269, Discriminator: 0.043323; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:41,570 root         INFO     Train Epoch: 108 [3584/8000 (45%)]\tTotal Loss: 2.761862\n",
      "Reconstruction: 1.882268, Regularization: 0.814646, Discriminator: 0.043323; Generator: 0.021625,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:41,680 root         INFO     Train Epoch: 108 [4096/8000 (51%)]\tTotal Loss: 2.833663\n",
      "Reconstruction: 1.832154, Regularization: 0.936564, Discriminator: 0.043316; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:00:41,789 root         INFO     Train Epoch: 108 [4608/8000 (58%)]\tTotal Loss: 2.750072\n",
      "Reconstruction: 1.765528, Regularization: 0.919599, Discriminator: 0.043313; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:41,899 root         INFO     Train Epoch: 108 [5120/8000 (64%)]\tTotal Loss: 2.534880\n",
      "Reconstruction: 1.589343, Regularization: 0.880577, Discriminator: 0.043321; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,008 root         INFO     Train Epoch: 108 [5632/8000 (70%)]\tTotal Loss: 2.559849\n",
      "Reconstruction: 1.646591, Regularization: 0.848296, Discriminator: 0.043316; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,118 root         INFO     Train Epoch: 108 [6144/8000 (77%)]\tTotal Loss: 2.814507\n",
      "Reconstruction: 1.721145, Regularization: 1.028398, Discriminator: 0.043310; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,228 root         INFO     Train Epoch: 108 [6656/8000 (83%)]\tTotal Loss: 2.616033\n",
      "Reconstruction: 1.637268, Regularization: 0.913789, Discriminator: 0.043308; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,337 root         INFO     Train Epoch: 108 [7168/8000 (90%)]\tTotal Loss: 2.205713\n",
      "Reconstruction: 1.415900, Regularization: 0.724817, Discriminator: 0.043328; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,446 root         INFO     Train Epoch: 108 [7680/8000 (96%)]\tTotal Loss: 2.816228\n",
      "Reconstruction: 1.806123, Regularization: 0.945124, Discriminator: 0.043304; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,526 root         INFO     ====> Epoch: 108 Average loss: 2.6140\n",
      "2019-04-10 00:00:42,553 root         INFO     Train Epoch: 109 [0/8000 (0%)]\tTotal Loss: 2.770921\n",
      "Reconstruction: 1.798223, Regularization: 0.907712, Discriminator: 0.043313; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,666 root         INFO     Train Epoch: 109 [512/8000 (6%)]\tTotal Loss: 2.523202\n",
      "Reconstruction: 1.570178, Regularization: 0.888029, Discriminator: 0.043313; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,777 root         INFO     Train Epoch: 109 [1024/8000 (13%)]\tTotal Loss: 2.648323\n",
      "Reconstruction: 1.721716, Regularization: 0.861616, Discriminator: 0.043306; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:42,888 root         INFO     Train Epoch: 109 [1536/8000 (19%)]\tTotal Loss: 2.731894\n",
      "Reconstruction: 1.767797, Regularization: 0.899111, Discriminator: 0.043306; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,000 root         INFO     Train Epoch: 109 [2048/8000 (26%)]\tTotal Loss: 2.636736\n",
      "Reconstruction: 1.663615, Regularization: 0.908159, Discriminator: 0.043293; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,111 root         INFO     Train Epoch: 109 [2560/8000 (32%)]\tTotal Loss: 2.703871\n",
      "Reconstruction: 1.734918, Regularization: 0.903992, Discriminator: 0.043315; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,223 root         INFO     Train Epoch: 109 [3072/8000 (38%)]\tTotal Loss: 2.743355\n",
      "Reconstruction: 1.776551, Regularization: 0.901837, Discriminator: 0.043311; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,333 root         INFO     Train Epoch: 109 [3584/8000 (45%)]\tTotal Loss: 2.700253\n",
      "Reconstruction: 1.778285, Regularization: 0.857004, Discriminator: 0.043323; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,445 root         INFO     Train Epoch: 109 [4096/8000 (51%)]\tTotal Loss: 2.495367\n",
      "Reconstruction: 1.572272, Regularization: 0.858142, Discriminator: 0.043322; Generator: 0.021631,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,555 root         INFO     Train Epoch: 109 [4608/8000 (58%)]\tTotal Loss: 2.701120\n",
      "Reconstruction: 1.717047, Regularization: 0.919124, Discriminator: 0.043304; Generator: 0.021644,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,666 root         INFO     Train Epoch: 109 [5120/8000 (64%)]\tTotal Loss: 2.530916\n",
      "Reconstruction: 1.534740, Regularization: 0.931221, Discriminator: 0.043307; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,776 root         INFO     Train Epoch: 109 [5632/8000 (70%)]\tTotal Loss: 2.690757\n",
      "Reconstruction: 1.771787, Regularization: 0.853998, Discriminator: 0.043319; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,887 root         INFO     Train Epoch: 109 [6144/8000 (77%)]\tTotal Loss: 2.491220\n",
      "Reconstruction: 1.494658, Regularization: 0.931574, Discriminator: 0.043328; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:43,997 root         INFO     Train Epoch: 109 [6656/8000 (83%)]\tTotal Loss: 2.530119\n",
      "Reconstruction: 1.606625, Regularization: 0.858494, Discriminator: 0.043346; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,106 root         INFO     Train Epoch: 109 [7168/8000 (90%)]\tTotal Loss: 2.846026\n",
      "Reconstruction: 1.757961, Regularization: 1.023091, Discriminator: 0.043315; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,216 root         INFO     Train Epoch: 109 [7680/8000 (96%)]\tTotal Loss: 2.767112\n",
      "Reconstruction: 1.806804, Regularization: 0.895333, Discriminator: 0.043332; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,297 root         INFO     ====> Epoch: 109 Average loss: 2.6455\n",
      "2019-04-10 00:00:44,324 root         INFO     Train Epoch: 110 [0/8000 (0%)]\tTotal Loss: 2.683098\n",
      "Reconstruction: 1.714383, Regularization: 0.903728, Discriminator: 0.043331; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,437 root         INFO     Train Epoch: 110 [512/8000 (6%)]\tTotal Loss: 3.206672\n",
      "Reconstruction: 2.270921, Regularization: 0.870732, Discriminator: 0.043331; Generator: 0.021688,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,549 root         INFO     Train Epoch: 110 [1024/8000 (13%)]\tTotal Loss: 2.919210\n",
      "Reconstruction: 1.943377, Regularization: 0.910815, Discriminator: 0.043328; Generator: 0.021691,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,659 root         INFO     Train Epoch: 110 [1536/8000 (19%)]\tTotal Loss: 2.785604\n",
      "Reconstruction: 1.716088, Regularization: 1.004515, Discriminator: 0.043320; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,767 root         INFO     Train Epoch: 110 [2048/8000 (26%)]\tTotal Loss: 2.919333\n",
      "Reconstruction: 1.798715, Regularization: 1.055626, Discriminator: 0.043324; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,875 root         INFO     Train Epoch: 110 [2560/8000 (32%)]\tTotal Loss: 2.627051\n",
      "Reconstruction: 1.596456, Regularization: 0.965595, Discriminator: 0.043328; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:44,983 root         INFO     Train Epoch: 110 [3072/8000 (38%)]\tTotal Loss: 2.490134\n",
      "Reconstruction: 1.624529, Regularization: 0.800613, Discriminator: 0.043341; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,091 root         INFO     Train Epoch: 110 [3584/8000 (45%)]\tTotal Loss: 2.295901\n",
      "Reconstruction: 1.475598, Regularization: 0.755304, Discriminator: 0.043344; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,199 root         INFO     Train Epoch: 110 [4096/8000 (51%)]\tTotal Loss: 2.393691\n",
      "Reconstruction: 1.541817, Regularization: 0.786870, Discriminator: 0.043341; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,307 root         INFO     Train Epoch: 110 [4608/8000 (58%)]\tTotal Loss: 2.426691\n",
      "Reconstruction: 1.578320, Regularization: 0.783390, Discriminator: 0.043334; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,415 root         INFO     Train Epoch: 110 [5120/8000 (64%)]\tTotal Loss: 3.066223\n",
      "Reconstruction: 2.117176, Regularization: 0.884060, Discriminator: 0.043329; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,523 root         INFO     Train Epoch: 110 [5632/8000 (70%)]\tTotal Loss: 2.580443\n",
      "Reconstruction: 1.612420, Regularization: 0.903029, Discriminator: 0.043329; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,631 root         INFO     Train Epoch: 110 [6144/8000 (77%)]\tTotal Loss: 2.620492\n",
      "Reconstruction: 1.656915, Regularization: 0.898594, Discriminator: 0.043328; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,739 root         INFO     Train Epoch: 110 [6656/8000 (83%)]\tTotal Loss: 2.528299\n",
      "Reconstruction: 1.539517, Regularization: 0.923787, Discriminator: 0.043323; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,849 root         INFO     Train Epoch: 110 [7168/8000 (90%)]\tTotal Loss: 2.646565\n",
      "Reconstruction: 1.597095, Regularization: 0.984499, Discriminator: 0.043321; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:45,958 root         INFO     Train Epoch: 110 [7680/8000 (96%)]\tTotal Loss: 2.603137\n",
      "Reconstruction: 1.565768, Regularization: 0.972386, Discriminator: 0.043320; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,037 root         INFO     ====> Epoch: 110 Average loss: 2.6469\n",
      "2019-04-10 00:00:46,065 root         INFO     Train Epoch: 111 [0/8000 (0%)]\tTotal Loss: 2.652029\n",
      "Reconstruction: 1.678026, Regularization: 0.909021, Discriminator: 0.043318; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,177 root         INFO     Train Epoch: 111 [512/8000 (6%)]\tTotal Loss: 2.421600\n",
      "Reconstruction: 1.475290, Regularization: 0.881319, Discriminator: 0.043320; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,289 root         INFO     Train Epoch: 111 [1024/8000 (13%)]\tTotal Loss: 2.925501\n",
      "Reconstruction: 1.888602, Regularization: 0.971899, Discriminator: 0.043315; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,400 root         INFO     Train Epoch: 111 [1536/8000 (19%)]\tTotal Loss: 2.781070\n",
      "Reconstruction: 1.785679, Regularization: 0.930404, Discriminator: 0.043311; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,512 root         INFO     Train Epoch: 111 [2048/8000 (26%)]\tTotal Loss: 2.498425\n",
      "Reconstruction: 1.547415, Regularization: 0.886014, Discriminator: 0.043316; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,624 root         INFO     Train Epoch: 111 [2560/8000 (32%)]\tTotal Loss: 2.561938\n",
      "Reconstruction: 1.611734, Regularization: 0.885205, Discriminator: 0.043317; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,736 root         INFO     Train Epoch: 111 [3072/8000 (38%)]\tTotal Loss: 2.693635\n",
      "Reconstruction: 1.737160, Regularization: 0.891486, Discriminator: 0.043307; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,848 root         INFO     Train Epoch: 111 [3584/8000 (45%)]\tTotal Loss: 2.349871\n",
      "Reconstruction: 1.511786, Regularization: 0.773105, Discriminator: 0.043305; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:46,959 root         INFO     Train Epoch: 111 [4096/8000 (51%)]\tTotal Loss: 2.690824\n",
      "Reconstruction: 1.736485, Regularization: 0.889354, Discriminator: 0.043314; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,071 root         INFO     Train Epoch: 111 [4608/8000 (58%)]\tTotal Loss: 2.762550\n",
      "Reconstruction: 1.818755, Regularization: 0.878815, Discriminator: 0.043306; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,182 root         INFO     Train Epoch: 111 [5120/8000 (64%)]\tTotal Loss: 2.744318\n",
      "Reconstruction: 1.704481, Regularization: 0.974849, Discriminator: 0.043317; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,294 root         INFO     Train Epoch: 111 [5632/8000 (70%)]\tTotal Loss: 2.615129\n",
      "Reconstruction: 1.708682, Regularization: 0.841478, Discriminator: 0.043301; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,407 root         INFO     Train Epoch: 111 [6144/8000 (77%)]\tTotal Loss: 2.709051\n",
      "Reconstruction: 1.715690, Regularization: 0.928377, Discriminator: 0.043322; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,519 root         INFO     Train Epoch: 111 [6656/8000 (83%)]\tTotal Loss: 2.633484\n",
      "Reconstruction: 1.601646, Regularization: 0.966839, Discriminator: 0.043314; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,631 root         INFO     Train Epoch: 111 [7168/8000 (90%)]\tTotal Loss: 2.701912\n",
      "Reconstruction: 1.749112, Regularization: 0.887808, Discriminator: 0.043317; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,742 root         INFO     Train Epoch: 111 [7680/8000 (96%)]\tTotal Loss: 2.645402\n",
      "Reconstruction: 1.787447, Regularization: 0.792972, Discriminator: 0.043289; Generator: 0.021694,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:47,823 root         INFO     ====> Epoch: 111 Average loss: 2.6451\n",
      "2019-04-10 00:00:47,850 root         INFO     Train Epoch: 112 [0/8000 (0%)]\tTotal Loss: 2.608266\n",
      "Reconstruction: 1.648098, Regularization: 0.895190, Discriminator: 0.043323; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:47,963 root         INFO     Train Epoch: 112 [512/8000 (6%)]\tTotal Loss: 2.810186\n",
      "Reconstruction: 1.726959, Regularization: 1.018225, Discriminator: 0.043336; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,073 root         INFO     Train Epoch: 112 [1024/8000 (13%)]\tTotal Loss: 2.989764\n",
      "Reconstruction: 1.883593, Regularization: 1.041165, Discriminator: 0.043339; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,182 root         INFO     Train Epoch: 112 [1536/8000 (19%)]\tTotal Loss: 2.801942\n",
      "Reconstruction: 1.760471, Regularization: 0.976473, Discriminator: 0.043325; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,292 root         INFO     Train Epoch: 112 [2048/8000 (26%)]\tTotal Loss: 2.679145\n",
      "Reconstruction: 1.664776, Regularization: 0.949384, Discriminator: 0.043347; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,401 root         INFO     Train Epoch: 112 [2560/8000 (32%)]\tTotal Loss: 2.721264\n",
      "Reconstruction: 1.651991, Regularization: 1.004290, Discriminator: 0.043339; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,511 root         INFO     Train Epoch: 112 [3072/8000 (38%)]\tTotal Loss: 2.478984\n",
      "Reconstruction: 1.559981, Regularization: 0.854038, Discriminator: 0.043327; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,621 root         INFO     Train Epoch: 112 [3584/8000 (45%)]\tTotal Loss: 2.500228\n",
      "Reconstruction: 1.605404, Regularization: 0.829865, Discriminator: 0.043316; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,731 root         INFO     Train Epoch: 112 [4096/8000 (51%)]\tTotal Loss: 2.600358\n",
      "Reconstruction: 1.652752, Regularization: 0.882629, Discriminator: 0.043330; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,840 root         INFO     Train Epoch: 112 [4608/8000 (58%)]\tTotal Loss: 2.343480\n",
      "Reconstruction: 1.464938, Regularization: 0.813576, Discriminator: 0.043318; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:48,950 root         INFO     Train Epoch: 112 [5120/8000 (64%)]\tTotal Loss: 2.712783\n",
      "Reconstruction: 1.685266, Regularization: 0.962525, Discriminator: 0.043341; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,059 root         INFO     Train Epoch: 112 [5632/8000 (70%)]\tTotal Loss: 2.735360\n",
      "Reconstruction: 1.680267, Regularization: 0.990101, Discriminator: 0.043342; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,168 root         INFO     Train Epoch: 112 [6144/8000 (77%)]\tTotal Loss: 2.715477\n",
      "Reconstruction: 1.730596, Regularization: 0.919898, Discriminator: 0.043331; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,278 root         INFO     Train Epoch: 112 [6656/8000 (83%)]\tTotal Loss: 2.638955\n",
      "Reconstruction: 1.726833, Regularization: 0.847144, Discriminator: 0.043326; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,387 root         INFO     Train Epoch: 112 [7168/8000 (90%)]\tTotal Loss: 2.476552\n",
      "Reconstruction: 1.553615, Regularization: 0.857951, Discriminator: 0.043329; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,496 root         INFO     Train Epoch: 112 [7680/8000 (96%)]\tTotal Loss: 2.398297\n",
      "Reconstruction: 1.574784, Regularization: 0.758514, Discriminator: 0.043318; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,576 root         INFO     ====> Epoch: 112 Average loss: 2.6290\n",
      "2019-04-10 00:00:49,604 root         INFO     Train Epoch: 113 [0/8000 (0%)]\tTotal Loss: 2.395407\n",
      "Reconstruction: 1.597095, Regularization: 0.733331, Discriminator: 0.043320; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,716 root         INFO     Train Epoch: 113 [512/8000 (6%)]\tTotal Loss: 2.701567\n",
      "Reconstruction: 1.739236, Regularization: 0.897347, Discriminator: 0.043325; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,827 root         INFO     Train Epoch: 113 [1024/8000 (13%)]\tTotal Loss: 2.735678\n",
      "Reconstruction: 1.742666, Regularization: 0.928026, Discriminator: 0.043332; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:49,938 root         INFO     Train Epoch: 113 [1536/8000 (19%)]\tTotal Loss: 2.534300\n",
      "Reconstruction: 1.627016, Regularization: 0.842291, Discriminator: 0.043330; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,049 root         INFO     Train Epoch: 113 [2048/8000 (26%)]\tTotal Loss: 2.575501\n",
      "Reconstruction: 1.697097, Regularization: 0.813434, Discriminator: 0.043322; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,160 root         INFO     Train Epoch: 113 [2560/8000 (32%)]\tTotal Loss: 2.813837\n",
      "Reconstruction: 1.752429, Regularization: 0.996428, Discriminator: 0.043324; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,270 root         INFO     Train Epoch: 113 [3072/8000 (38%)]\tTotal Loss: 2.629423\n",
      "Reconstruction: 1.641194, Regularization: 0.923256, Discriminator: 0.043327; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,381 root         INFO     Train Epoch: 113 [3584/8000 (45%)]\tTotal Loss: 2.713202\n",
      "Reconstruction: 1.734550, Regularization: 0.913683, Discriminator: 0.043322; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,492 root         INFO     Train Epoch: 113 [4096/8000 (51%)]\tTotal Loss: 2.747422\n",
      "Reconstruction: 1.754621, Regularization: 0.927841, Discriminator: 0.043315; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,603 root         INFO     Train Epoch: 113 [4608/8000 (58%)]\tTotal Loss: 2.761542\n",
      "Reconstruction: 1.732154, Regularization: 0.964419, Discriminator: 0.043315; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,713 root         INFO     Train Epoch: 113 [5120/8000 (64%)]\tTotal Loss: 2.415348\n",
      "Reconstruction: 1.526636, Regularization: 0.823746, Discriminator: 0.043317; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,824 root         INFO     Train Epoch: 113 [5632/8000 (70%)]\tTotal Loss: 2.436447\n",
      "Reconstruction: 1.520426, Regularization: 0.851047, Discriminator: 0.043316; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:50,935 root         INFO     Train Epoch: 113 [6144/8000 (77%)]\tTotal Loss: 2.853053\n",
      "Reconstruction: 1.813643, Regularization: 0.974449, Discriminator: 0.043303; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,046 root         INFO     Train Epoch: 113 [6656/8000 (83%)]\tTotal Loss: 2.548757\n",
      "Reconstruction: 1.650249, Regularization: 0.833520, Discriminator: 0.043322; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,157 root         INFO     Train Epoch: 113 [7168/8000 (90%)]\tTotal Loss: 2.433451\n",
      "Reconstruction: 1.536199, Regularization: 0.832265, Discriminator: 0.043316; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,268 root         INFO     Train Epoch: 113 [7680/8000 (96%)]\tTotal Loss: 2.554560\n",
      "Reconstruction: 1.618855, Regularization: 0.870722, Discriminator: 0.043316; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,348 root         INFO     ====> Epoch: 113 Average loss: 2.6166\n",
      "2019-04-10 00:00:51,375 root         INFO     Train Epoch: 114 [0/8000 (0%)]\tTotal Loss: 2.563118\n",
      "Reconstruction: 1.595884, Regularization: 0.902255, Discriminator: 0.043312; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,486 root         INFO     Train Epoch: 114 [512/8000 (6%)]\tTotal Loss: 2.724916\n",
      "Reconstruction: 1.642736, Regularization: 1.017211, Discriminator: 0.043306; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,595 root         INFO     Train Epoch: 114 [1024/8000 (13%)]\tTotal Loss: 2.332331\n",
      "Reconstruction: 1.454607, Regularization: 0.812724, Discriminator: 0.043311; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,703 root         INFO     Train Epoch: 114 [1536/8000 (19%)]\tTotal Loss: 2.892137\n",
      "Reconstruction: 1.757735, Regularization: 1.069423, Discriminator: 0.043298; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,811 root         INFO     Train Epoch: 114 [2048/8000 (26%)]\tTotal Loss: 2.872979\n",
      "Reconstruction: 1.816697, Regularization: 0.991298, Discriminator: 0.043314; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:51,919 root         INFO     Train Epoch: 114 [2560/8000 (32%)]\tTotal Loss: 2.456887\n",
      "Reconstruction: 1.589293, Regularization: 0.802603, Discriminator: 0.043326; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,028 root         INFO     Train Epoch: 114 [3072/8000 (38%)]\tTotal Loss: 2.441668\n",
      "Reconstruction: 1.583555, Regularization: 0.793110, Discriminator: 0.043344; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,135 root         INFO     Train Epoch: 114 [3584/8000 (45%)]\tTotal Loss: 2.534908\n",
      "Reconstruction: 1.590304, Regularization: 0.879637, Discriminator: 0.043316; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,244 root         INFO     Train Epoch: 114 [4096/8000 (51%)]\tTotal Loss: 2.582503\n",
      "Reconstruction: 1.600352, Regularization: 0.917189, Discriminator: 0.043319; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,353 root         INFO     Train Epoch: 114 [4608/8000 (58%)]\tTotal Loss: 2.512874\n",
      "Reconstruction: 1.531792, Regularization: 0.916122, Discriminator: 0.043322; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,461 root         INFO     Train Epoch: 114 [5120/8000 (64%)]\tTotal Loss: 2.618961\n",
      "Reconstruction: 1.675383, Regularization: 0.878605, Discriminator: 0.043322; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,570 root         INFO     Train Epoch: 114 [5632/8000 (70%)]\tTotal Loss: 2.700654\n",
      "Reconstruction: 1.677292, Regularization: 0.958385, Discriminator: 0.043320; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,679 root         INFO     Train Epoch: 114 [6144/8000 (77%)]\tTotal Loss: 2.630348\n",
      "Reconstruction: 1.669154, Regularization: 0.896211, Discriminator: 0.043324; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,787 root         INFO     Train Epoch: 114 [6656/8000 (83%)]\tTotal Loss: 2.699998\n",
      "Reconstruction: 1.753819, Regularization: 0.881199, Discriminator: 0.043329; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:52,896 root         INFO     Train Epoch: 114 [7168/8000 (90%)]\tTotal Loss: 2.985765\n",
      "Reconstruction: 1.837614, Regularization: 1.083179, Discriminator: 0.043309; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,004 root         INFO     Train Epoch: 114 [7680/8000 (96%)]\tTotal Loss: 2.370119\n",
      "Reconstruction: 1.542624, Regularization: 0.762493, Discriminator: 0.043346; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,084 root         INFO     ====> Epoch: 114 Average loss: 2.6423\n",
      "2019-04-10 00:00:53,111 root         INFO     Train Epoch: 115 [0/8000 (0%)]\tTotal Loss: 2.548209\n",
      "Reconstruction: 1.610082, Regularization: 0.873126, Discriminator: 0.043340; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,222 root         INFO     Train Epoch: 115 [512/8000 (6%)]\tTotal Loss: 2.516643\n",
      "Reconstruction: 1.628167, Regularization: 0.823464, Discriminator: 0.043342; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,332 root         INFO     Train Epoch: 115 [1024/8000 (13%)]\tTotal Loss: 2.632631\n",
      "Reconstruction: 1.700533, Regularization: 0.867090, Discriminator: 0.043335; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,443 root         INFO     Train Epoch: 115 [1536/8000 (19%)]\tTotal Loss: 2.736817\n",
      "Reconstruction: 1.725668, Regularization: 0.946151, Discriminator: 0.043325; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,554 root         INFO     Train Epoch: 115 [2048/8000 (26%)]\tTotal Loss: 2.728638\n",
      "Reconstruction: 1.710783, Regularization: 0.952857, Discriminator: 0.043327; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,664 root         INFO     Train Epoch: 115 [2560/8000 (32%)]\tTotal Loss: 2.871839\n",
      "Reconstruction: 1.848369, Regularization: 0.958470, Discriminator: 0.043317; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,774 root         INFO     Train Epoch: 115 [3072/8000 (38%)]\tTotal Loss: 2.717485\n",
      "Reconstruction: 1.689014, Regularization: 0.963465, Discriminator: 0.043325; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,884 root         INFO     Train Epoch: 115 [3584/8000 (45%)]\tTotal Loss: 2.595965\n",
      "Reconstruction: 1.678787, Regularization: 0.852158, Discriminator: 0.043347; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:53,995 root         INFO     Train Epoch: 115 [4096/8000 (51%)]\tTotal Loss: 2.584025\n",
      "Reconstruction: 1.666383, Regularization: 0.852649, Discriminator: 0.043326; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,105 root         INFO     Train Epoch: 115 [4608/8000 (58%)]\tTotal Loss: 2.904666\n",
      "Reconstruction: 1.861619, Regularization: 0.978049, Discriminator: 0.043337; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,215 root         INFO     Train Epoch: 115 [5120/8000 (64%)]\tTotal Loss: 2.502874\n",
      "Reconstruction: 1.595238, Regularization: 0.842662, Discriminator: 0.043324; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,325 root         INFO     Train Epoch: 115 [5632/8000 (70%)]\tTotal Loss: 2.889787\n",
      "Reconstruction: 1.867185, Regularization: 0.957643, Discriminator: 0.043322; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,435 root         INFO     Train Epoch: 115 [6144/8000 (77%)]\tTotal Loss: 2.766418\n",
      "Reconstruction: 1.807751, Regularization: 0.893701, Discriminator: 0.043318; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,545 root         INFO     Train Epoch: 115 [6656/8000 (83%)]\tTotal Loss: 2.885158\n",
      "Reconstruction: 1.851702, Regularization: 0.968506, Discriminator: 0.043305; Generator: 0.021644,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,655 root         INFO     Train Epoch: 115 [7168/8000 (90%)]\tTotal Loss: 2.906388\n",
      "Reconstruction: 1.864320, Regularization: 0.977114, Discriminator: 0.043301; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,765 root         INFO     Train Epoch: 115 [7680/8000 (96%)]\tTotal Loss: 2.546113\n",
      "Reconstruction: 1.628911, Regularization: 0.852230, Discriminator: 0.043309; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,845 root         INFO     ====> Epoch: 115 Average loss: 2.6723\n",
      "2019-04-10 00:00:54,873 root         INFO     Train Epoch: 116 [0/8000 (0%)]\tTotal Loss: 2.504886\n",
      "Reconstruction: 1.642689, Regularization: 0.797220, Discriminator: 0.043310; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:54,984 root         INFO     Train Epoch: 116 [512/8000 (6%)]\tTotal Loss: 2.729316\n",
      "Reconstruction: 1.743439, Regularization: 0.920883, Discriminator: 0.043313; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:55,095 root         INFO     Train Epoch: 116 [1024/8000 (13%)]\tTotal Loss: 2.839559\n",
      "Reconstruction: 1.845134, Regularization: 0.929425, Discriminator: 0.043320; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:55,206 root         INFO     Train Epoch: 116 [1536/8000 (19%)]\tTotal Loss: 2.736207\n",
      "Reconstruction: 1.734026, Regularization: 0.937163, Discriminator: 0.043320; Generator: 0.021697,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:00:55,316 root         INFO     Train Epoch: 116 [2048/8000 (26%)]\tTotal Loss: 2.555588\n",
      "Reconstruction: 1.610681, Regularization: 0.879899, Discriminator: 0.043309; Generator: 0.021699,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:55,426 root         INFO     Train Epoch: 116 [2560/8000 (32%)]\tTotal Loss: 2.704604\n",
      "Reconstruction: 1.685492, Regularization: 0.954107, Discriminator: 0.043310; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:55,536 root         INFO     Train Epoch: 116 [3072/8000 (38%)]\tTotal Loss: 2.574328\n",
      "Reconstruction: 1.690849, Regularization: 0.818485, Discriminator: 0.043298; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:55,646 root         INFO     Train Epoch: 116 [3584/8000 (45%)]\tTotal Loss: 3.155886\n",
      "Reconstruction: 2.159729, Regularization: 0.931160, Discriminator: 0.043298; Generator: 0.021700,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:00:55,754 root         INFO     Train Epoch: 116 [4096/8000 (51%)]\tTotal Loss: 2.753967\n",
      "Reconstruction: 1.693213, Regularization: 0.995749, Discriminator: 0.043319; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:55,864 root         INFO     Train Epoch: 116 [4608/8000 (58%)]\tTotal Loss: 2.734149\n",
      "Reconstruction: 1.716038, Regularization: 0.953134, Discriminator: 0.043326; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:55,973 root         INFO     Train Epoch: 116 [5120/8000 (64%)]\tTotal Loss: 2.610294\n",
      "Reconstruction: 1.646381, Regularization: 0.898950, Discriminator: 0.043317; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,084 root         INFO     Train Epoch: 116 [5632/8000 (70%)]\tTotal Loss: 2.559661\n",
      "Reconstruction: 1.519396, Regularization: 0.975287, Discriminator: 0.043327; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,192 root         INFO     Train Epoch: 116 [6144/8000 (77%)]\tTotal Loss: 2.737077\n",
      "Reconstruction: 1.789373, Regularization: 0.882735, Discriminator: 0.043326; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,301 root         INFO     Train Epoch: 116 [6656/8000 (83%)]\tTotal Loss: 2.724199\n",
      "Reconstruction: 1.787548, Regularization: 0.871686, Discriminator: 0.043323; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,411 root         INFO     Train Epoch: 116 [7168/8000 (90%)]\tTotal Loss: 2.384461\n",
      "Reconstruction: 1.503534, Regularization: 0.815981, Discriminator: 0.043307; Generator: 0.021639,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,519 root         INFO     Train Epoch: 116 [7680/8000 (96%)]\tTotal Loss: 2.364536\n",
      "Reconstruction: 1.481404, Regularization: 0.818164, Discriminator: 0.043321; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,598 root         INFO     ====> Epoch: 116 Average loss: 2.6610\n",
      "2019-04-10 00:00:56,626 root         INFO     Train Epoch: 117 [0/8000 (0%)]\tTotal Loss: 2.761838\n",
      "Reconstruction: 1.795055, Regularization: 0.901814, Discriminator: 0.043331; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,736 root         INFO     Train Epoch: 117 [512/8000 (6%)]\tTotal Loss: 2.489475\n",
      "Reconstruction: 1.638404, Regularization: 0.786109, Discriminator: 0.043315; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,847 root         INFO     Train Epoch: 117 [1024/8000 (13%)]\tTotal Loss: 2.726620\n",
      "Reconstruction: 1.706668, Regularization: 0.954953, Discriminator: 0.043343; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:56,958 root         INFO     Train Epoch: 117 [1536/8000 (19%)]\tTotal Loss: 2.882844\n",
      "Reconstruction: 1.791386, Regularization: 1.026440, Discriminator: 0.043356; Generator: 0.021662,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,068 root         INFO     Train Epoch: 117 [2048/8000 (26%)]\tTotal Loss: 2.589046\n",
      "Reconstruction: 1.668833, Regularization: 0.855227, Discriminator: 0.043333; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,177 root         INFO     Train Epoch: 117 [2560/8000 (32%)]\tTotal Loss: 2.625621\n",
      "Reconstruction: 1.695540, Regularization: 0.865082, Discriminator: 0.043331; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,286 root         INFO     Train Epoch: 117 [3072/8000 (38%)]\tTotal Loss: 2.640126\n",
      "Reconstruction: 1.712212, Regularization: 0.862918, Discriminator: 0.043335; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,395 root         INFO     Train Epoch: 117 [3584/8000 (45%)]\tTotal Loss: 2.823410\n",
      "Reconstruction: 1.817880, Regularization: 0.940540, Discriminator: 0.043333; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,505 root         INFO     Train Epoch: 117 [4096/8000 (51%)]\tTotal Loss: 2.881199\n",
      "Reconstruction: 1.812972, Regularization: 1.003229, Discriminator: 0.043341; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,614 root         INFO     Train Epoch: 117 [4608/8000 (58%)]\tTotal Loss: 2.546928\n",
      "Reconstruction: 1.647422, Regularization: 0.834519, Discriminator: 0.043325; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,723 root         INFO     Train Epoch: 117 [5120/8000 (64%)]\tTotal Loss: 2.614710\n",
      "Reconstruction: 1.744435, Regularization: 0.805292, Discriminator: 0.043328; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,832 root         INFO     Train Epoch: 117 [5632/8000 (70%)]\tTotal Loss: 2.680432\n",
      "Reconstruction: 1.745747, Regularization: 0.869694, Discriminator: 0.043329; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:57,942 root         INFO     Train Epoch: 117 [6144/8000 (77%)]\tTotal Loss: 2.451164\n",
      "Reconstruction: 1.567405, Regularization: 0.818776, Discriminator: 0.043315; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,052 root         INFO     Train Epoch: 117 [6656/8000 (83%)]\tTotal Loss: 2.839464\n",
      "Reconstruction: 1.793577, Regularization: 0.980898, Discriminator: 0.043331; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,162 root         INFO     Train Epoch: 117 [7168/8000 (90%)]\tTotal Loss: 2.754131\n",
      "Reconstruction: 1.750873, Regularization: 0.938289, Discriminator: 0.043331; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,272 root         INFO     Train Epoch: 117 [7680/8000 (96%)]\tTotal Loss: 2.760620\n",
      "Reconstruction: 1.689916, Regularization: 1.005731, Discriminator: 0.043320; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,352 root         INFO     ====> Epoch: 117 Average loss: 2.6430\n",
      "2019-04-10 00:00:58,379 root         INFO     Train Epoch: 118 [0/8000 (0%)]\tTotal Loss: 2.665007\n",
      "Reconstruction: 1.776282, Regularization: 0.823749, Discriminator: 0.043321; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,491 root         INFO     Train Epoch: 118 [512/8000 (6%)]\tTotal Loss: 2.760443\n",
      "Reconstruction: 1.763827, Regularization: 0.931648, Discriminator: 0.043324; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,601 root         INFO     Train Epoch: 118 [1024/8000 (13%)]\tTotal Loss: 2.944687\n",
      "Reconstruction: 1.923419, Regularization: 0.956300, Discriminator: 0.043321; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,711 root         INFO     Train Epoch: 118 [1536/8000 (19%)]\tTotal Loss: 2.747507\n",
      "Reconstruction: 1.726274, Regularization: 0.956262, Discriminator: 0.043318; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,822 root         INFO     Train Epoch: 118 [2048/8000 (26%)]\tTotal Loss: 2.865103\n",
      "Reconstruction: 1.816351, Regularization: 0.983788, Discriminator: 0.043314; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:58,932 root         INFO     Train Epoch: 118 [2560/8000 (32%)]\tTotal Loss: 2.764430\n",
      "Reconstruction: 1.787298, Regularization: 0.912169, Discriminator: 0.043313; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,042 root         INFO     Train Epoch: 118 [3072/8000 (38%)]\tTotal Loss: 2.677952\n",
      "Reconstruction: 1.672714, Regularization: 0.940276, Discriminator: 0.043311; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,152 root         INFO     Train Epoch: 118 [3584/8000 (45%)]\tTotal Loss: 2.889202\n",
      "Reconstruction: 1.719097, Regularization: 1.105144, Discriminator: 0.043302; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,262 root         INFO     Train Epoch: 118 [4096/8000 (51%)]\tTotal Loss: 2.715377\n",
      "Reconstruction: 1.746270, Regularization: 0.904134, Discriminator: 0.043308; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,373 root         INFO     Train Epoch: 118 [4608/8000 (58%)]\tTotal Loss: 2.906359\n",
      "Reconstruction: 1.857098, Regularization: 0.984296, Discriminator: 0.043301; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,482 root         INFO     Train Epoch: 118 [5120/8000 (64%)]\tTotal Loss: 2.652778\n",
      "Reconstruction: 1.611204, Regularization: 0.976595, Discriminator: 0.043318; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,591 root         INFO     Train Epoch: 118 [5632/8000 (70%)]\tTotal Loss: 2.443638\n",
      "Reconstruction: 1.536195, Regularization: 0.842459, Discriminator: 0.043335; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,701 root         INFO     Train Epoch: 118 [6144/8000 (77%)]\tTotal Loss: 2.822510\n",
      "Reconstruction: 1.768353, Regularization: 0.989176, Discriminator: 0.043305; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,811 root         INFO     Train Epoch: 118 [6656/8000 (83%)]\tTotal Loss: 2.569272\n",
      "Reconstruction: 1.605281, Regularization: 0.899001, Discriminator: 0.043321; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:00:59,921 root         INFO     Train Epoch: 118 [7168/8000 (90%)]\tTotal Loss: 2.339270\n",
      "Reconstruction: 1.469897, Regularization: 0.804390, Discriminator: 0.043320; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,031 root         INFO     Train Epoch: 118 [7680/8000 (96%)]\tTotal Loss: 2.767515\n",
      "Reconstruction: 1.788605, Regularization: 0.913930, Discriminator: 0.043311; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,112 root         INFO     ====> Epoch: 118 Average loss: 2.6389\n",
      "2019-04-10 00:01:00,139 root         INFO     Train Epoch: 119 [0/8000 (0%)]\tTotal Loss: 2.798144\n",
      "Reconstruction: 1.812212, Regularization: 0.920963, Discriminator: 0.043316; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,250 root         INFO     Train Epoch: 119 [512/8000 (6%)]\tTotal Loss: 2.510097\n",
      "Reconstruction: 1.607884, Regularization: 0.837223, Discriminator: 0.043320; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,360 root         INFO     Train Epoch: 119 [1024/8000 (13%)]\tTotal Loss: 2.804790\n",
      "Reconstruction: 1.755888, Regularization: 0.983923, Discriminator: 0.043329; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,471 root         INFO     Train Epoch: 119 [1536/8000 (19%)]\tTotal Loss: 2.869152\n",
      "Reconstruction: 1.894291, Regularization: 0.909881, Discriminator: 0.043318; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,580 root         INFO     Train Epoch: 119 [2048/8000 (26%)]\tTotal Loss: 3.034513\n",
      "Reconstruction: 2.095897, Regularization: 0.873641, Discriminator: 0.043325; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,688 root         INFO     Train Epoch: 119 [2560/8000 (32%)]\tTotal Loss: 2.833451\n",
      "Reconstruction: 1.775793, Regularization: 0.992703, Discriminator: 0.043321; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,796 root         INFO     Train Epoch: 119 [3072/8000 (38%)]\tTotal Loss: 2.385993\n",
      "Reconstruction: 1.522954, Regularization: 0.798064, Discriminator: 0.043328; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:00,905 root         INFO     Train Epoch: 119 [3584/8000 (45%)]\tTotal Loss: 2.631524\n",
      "Reconstruction: 1.683309, Regularization: 0.883223, Discriminator: 0.043339; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,013 root         INFO     Train Epoch: 119 [4096/8000 (51%)]\tTotal Loss: 2.689339\n",
      "Reconstruction: 1.743735, Regularization: 0.880626, Discriminator: 0.043326; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,122 root         INFO     Train Epoch: 119 [4608/8000 (58%)]\tTotal Loss: 2.369593\n",
      "Reconstruction: 1.485575, Regularization: 0.819019, Discriminator: 0.043351; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,231 root         INFO     Train Epoch: 119 [5120/8000 (64%)]\tTotal Loss: 2.582257\n",
      "Reconstruction: 1.691969, Regularization: 0.825293, Discriminator: 0.043336; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,339 root         INFO     Train Epoch: 119 [5632/8000 (70%)]\tTotal Loss: 2.692726\n",
      "Reconstruction: 1.737501, Regularization: 0.890219, Discriminator: 0.043342; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,448 root         INFO     Train Epoch: 119 [6144/8000 (77%)]\tTotal Loss: 2.584356\n",
      "Reconstruction: 1.605519, Regularization: 0.913840, Discriminator: 0.043331; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,556 root         INFO     Train Epoch: 119 [6656/8000 (83%)]\tTotal Loss: 2.656251\n",
      "Reconstruction: 1.782771, Regularization: 0.808475, Discriminator: 0.043330; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,665 root         INFO     Train Epoch: 119 [7168/8000 (90%)]\tTotal Loss: 2.730873\n",
      "Reconstruction: 1.773992, Regularization: 0.891878, Discriminator: 0.043329; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,773 root         INFO     Train Epoch: 119 [7680/8000 (96%)]\tTotal Loss: 2.638849\n",
      "Reconstruction: 1.609233, Regularization: 0.964615, Discriminator: 0.043323; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,853 root         INFO     ====> Epoch: 119 Average loss: 2.6683\n",
      "2019-04-10 00:01:01,880 root         INFO     Train Epoch: 120 [0/8000 (0%)]\tTotal Loss: 2.832525\n",
      "Reconstruction: 1.821217, Regularization: 0.946311, Discriminator: 0.043320; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:01,991 root         INFO     Train Epoch: 120 [512/8000 (6%)]\tTotal Loss: 2.978916\n",
      "Reconstruction: 1.854228, Regularization: 1.059688, Discriminator: 0.043321; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,102 root         INFO     Train Epoch: 120 [1024/8000 (13%)]\tTotal Loss: 2.813317\n",
      "Reconstruction: 1.798476, Regularization: 0.949860, Discriminator: 0.043320; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,212 root         INFO     Train Epoch: 120 [1536/8000 (19%)]\tTotal Loss: 2.432218\n",
      "Reconstruction: 1.540960, Regularization: 0.826271, Discriminator: 0.043331; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,322 root         INFO     Train Epoch: 120 [2048/8000 (26%)]\tTotal Loss: 2.503797\n",
      "Reconstruction: 1.596538, Regularization: 0.842283, Discriminator: 0.043320; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,433 root         INFO     Train Epoch: 120 [2560/8000 (32%)]\tTotal Loss: 2.503647\n",
      "Reconstruction: 1.603901, Regularization: 0.834773, Discriminator: 0.043321; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,543 root         INFO     Train Epoch: 120 [3072/8000 (38%)]\tTotal Loss: 2.570171\n",
      "Reconstruction: 1.692019, Regularization: 0.813181, Discriminator: 0.043320; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,654 root         INFO     Train Epoch: 120 [3584/8000 (45%)]\tTotal Loss: 2.591179\n",
      "Reconstruction: 1.688446, Regularization: 0.837758, Discriminator: 0.043317; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,764 root         INFO     Train Epoch: 120 [4096/8000 (51%)]\tTotal Loss: 2.592599\n",
      "Reconstruction: 1.585937, Regularization: 0.941690, Discriminator: 0.043316; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,874 root         INFO     Train Epoch: 120 [4608/8000 (58%)]\tTotal Loss: 2.674426\n",
      "Reconstruction: 1.755296, Regularization: 0.854164, Discriminator: 0.043306; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:02,985 root         INFO     Train Epoch: 120 [5120/8000 (64%)]\tTotal Loss: 2.385503\n",
      "Reconstruction: 1.488602, Regularization: 0.831930, Discriminator: 0.043310; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,095 root         INFO     Train Epoch: 120 [5632/8000 (70%)]\tTotal Loss: 3.078756\n",
      "Reconstruction: 2.034475, Regularization: 0.979294, Discriminator: 0.043318; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,205 root         INFO     Train Epoch: 120 [6144/8000 (77%)]\tTotal Loss: 2.526389\n",
      "Reconstruction: 1.574489, Regularization: 0.886906, Discriminator: 0.043322; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,315 root         INFO     Train Epoch: 120 [6656/8000 (83%)]\tTotal Loss: 2.666698\n",
      "Reconstruction: 1.700496, Regularization: 0.901223, Discriminator: 0.043307; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,424 root         INFO     Train Epoch: 120 [7168/8000 (90%)]\tTotal Loss: 2.562236\n",
      "Reconstruction: 1.665631, Regularization: 0.831615, Discriminator: 0.043314; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,535 root         INFO     Train Epoch: 120 [7680/8000 (96%)]\tTotal Loss: 2.714077\n",
      "Reconstruction: 1.824348, Regularization: 0.824734, Discriminator: 0.043313; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,616 root         INFO     ====> Epoch: 120 Average loss: 2.6836\n",
      "2019-04-10 00:01:03,643 root         INFO     Train Epoch: 121 [0/8000 (0%)]\tTotal Loss: 2.584472\n",
      "Reconstruction: 1.649273, Regularization: 0.870201, Discriminator: 0.043311; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,754 root         INFO     Train Epoch: 121 [512/8000 (6%)]\tTotal Loss: 2.708315\n",
      "Reconstruction: 1.738851, Regularization: 0.904471, Discriminator: 0.043316; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,864 root         INFO     Train Epoch: 121 [1024/8000 (13%)]\tTotal Loss: 2.752578\n",
      "Reconstruction: 1.771994, Regularization: 0.915580, Discriminator: 0.043319; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:03,975 root         INFO     Train Epoch: 121 [1536/8000 (19%)]\tTotal Loss: 2.682148\n",
      "Reconstruction: 1.636692, Regularization: 0.980461, Discriminator: 0.043317; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,086 root         INFO     Train Epoch: 121 [2048/8000 (26%)]\tTotal Loss: 2.613931\n",
      "Reconstruction: 1.701079, Regularization: 0.847856, Discriminator: 0.043308; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,196 root         INFO     Train Epoch: 121 [2560/8000 (32%)]\tTotal Loss: 2.644200\n",
      "Reconstruction: 1.710549, Regularization: 0.868675, Discriminator: 0.043311; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,307 root         INFO     Train Epoch: 121 [3072/8000 (38%)]\tTotal Loss: 2.697078\n",
      "Reconstruction: 1.803645, Regularization: 0.828447, Discriminator: 0.043319; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,417 root         INFO     Train Epoch: 121 [3584/8000 (45%)]\tTotal Loss: 2.484299\n",
      "Reconstruction: 1.645428, Regularization: 0.773892, Discriminator: 0.043330; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,528 root         INFO     Train Epoch: 121 [4096/8000 (51%)]\tTotal Loss: 2.833592\n",
      "Reconstruction: 1.803683, Regularization: 0.964929, Discriminator: 0.043340; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,639 root         INFO     Train Epoch: 121 [4608/8000 (58%)]\tTotal Loss: 2.635855\n",
      "Reconstruction: 1.657622, Regularization: 0.913256, Discriminator: 0.043332; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,751 root         INFO     Train Epoch: 121 [5120/8000 (64%)]\tTotal Loss: 2.499606\n",
      "Reconstruction: 1.527483, Regularization: 0.907167, Discriminator: 0.043324; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,863 root         INFO     Train Epoch: 121 [5632/8000 (70%)]\tTotal Loss: 2.617121\n",
      "Reconstruction: 1.624576, Regularization: 0.927566, Discriminator: 0.043336; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:04,975 root         INFO     Train Epoch: 121 [6144/8000 (77%)]\tTotal Loss: 2.969439\n",
      "Reconstruction: 1.975009, Regularization: 0.929452, Discriminator: 0.043340; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,087 root         INFO     Train Epoch: 121 [6656/8000 (83%)]\tTotal Loss: 2.502667\n",
      "Reconstruction: 1.674844, Regularization: 0.762861, Discriminator: 0.043301; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,199 root         INFO     Train Epoch: 121 [7168/8000 (90%)]\tTotal Loss: 2.701242\n",
      "Reconstruction: 1.744201, Regularization: 0.892074, Discriminator: 0.043313; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,311 root         INFO     Train Epoch: 121 [7680/8000 (96%)]\tTotal Loss: 2.738172\n",
      "Reconstruction: 1.768137, Regularization: 0.905060, Discriminator: 0.043325; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,392 root         INFO     ====> Epoch: 121 Average loss: 2.6600\n",
      "2019-04-10 00:01:05,419 root         INFO     Train Epoch: 122 [0/8000 (0%)]\tTotal Loss: 2.584092\n",
      "Reconstruction: 1.643519, Regularization: 0.875583, Discriminator: 0.043331; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,532 root         INFO     Train Epoch: 122 [512/8000 (6%)]\tTotal Loss: 2.671053\n",
      "Reconstruction: 1.799282, Regularization: 0.806795, Discriminator: 0.043318; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,645 root         INFO     Train Epoch: 122 [1024/8000 (13%)]\tTotal Loss: 2.776145\n",
      "Reconstruction: 1.851028, Regularization: 0.860132, Discriminator: 0.043327; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,756 root         INFO     Train Epoch: 122 [1536/8000 (19%)]\tTotal Loss: 2.712607\n",
      "Reconstruction: 1.834996, Regularization: 0.812625, Discriminator: 0.043324; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,867 root         INFO     Train Epoch: 122 [2048/8000 (26%)]\tTotal Loss: 2.719207\n",
      "Reconstruction: 1.702380, Regularization: 0.951826, Discriminator: 0.043333; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:05,976 root         INFO     Train Epoch: 122 [2560/8000 (32%)]\tTotal Loss: 2.520111\n",
      "Reconstruction: 1.634339, Regularization: 0.820786, Discriminator: 0.043322; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,086 root         INFO     Train Epoch: 122 [3072/8000 (38%)]\tTotal Loss: 2.634761\n",
      "Reconstruction: 1.658037, Regularization: 0.911712, Discriminator: 0.043328; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,196 root         INFO     Train Epoch: 122 [3584/8000 (45%)]\tTotal Loss: 2.465947\n",
      "Reconstruction: 1.653331, Regularization: 0.747636, Discriminator: 0.043319; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,305 root         INFO     Train Epoch: 122 [4096/8000 (51%)]\tTotal Loss: 2.754301\n",
      "Reconstruction: 1.727282, Regularization: 0.962016, Discriminator: 0.043334; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,416 root         INFO     Train Epoch: 122 [4608/8000 (58%)]\tTotal Loss: 2.580634\n",
      "Reconstruction: 1.664659, Regularization: 0.850995, Discriminator: 0.043319; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,526 root         INFO     Train Epoch: 122 [5120/8000 (64%)]\tTotal Loss: 2.459766\n",
      "Reconstruction: 1.614878, Regularization: 0.779922, Discriminator: 0.043320; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,637 root         INFO     Train Epoch: 122 [5632/8000 (70%)]\tTotal Loss: 2.460688\n",
      "Reconstruction: 1.608175, Regularization: 0.787530, Discriminator: 0.043319; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,746 root         INFO     Train Epoch: 122 [6144/8000 (77%)]\tTotal Loss: 2.609890\n",
      "Reconstruction: 1.686515, Regularization: 0.858389, Discriminator: 0.043328; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,857 root         INFO     Train Epoch: 122 [6656/8000 (83%)]\tTotal Loss: 2.562675\n",
      "Reconstruction: 1.620526, Regularization: 0.877188, Discriminator: 0.043317; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:06,967 root         INFO     Train Epoch: 122 [7168/8000 (90%)]\tTotal Loss: 2.776258\n",
      "Reconstruction: 1.767099, Regularization: 0.944185, Discriminator: 0.043326; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,077 root         INFO     Train Epoch: 122 [7680/8000 (96%)]\tTotal Loss: 2.280008\n",
      "Reconstruction: 1.488932, Regularization: 0.726090, Discriminator: 0.043321; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,158 root         INFO     ====> Epoch: 122 Average loss: 2.6343\n",
      "2019-04-10 00:01:07,185 root         INFO     Train Epoch: 123 [0/8000 (0%)]\tTotal Loss: 2.594541\n",
      "Reconstruction: 1.689462, Regularization: 0.840118, Discriminator: 0.043318; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,298 root         INFO     Train Epoch: 123 [512/8000 (6%)]\tTotal Loss: 2.373587\n",
      "Reconstruction: 1.520382, Regularization: 0.788243, Discriminator: 0.043323; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,410 root         INFO     Train Epoch: 123 [1024/8000 (13%)]\tTotal Loss: 2.586171\n",
      "Reconstruction: 1.699923, Regularization: 0.821283, Discriminator: 0.043315; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,522 root         INFO     Train Epoch: 123 [1536/8000 (19%)]\tTotal Loss: 2.842245\n",
      "Reconstruction: 1.787654, Regularization: 0.989638, Discriminator: 0.043301; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,634 root         INFO     Train Epoch: 123 [2048/8000 (26%)]\tTotal Loss: 2.627830\n",
      "Reconstruction: 1.697153, Regularization: 0.865712, Discriminator: 0.043303; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,743 root         INFO     Train Epoch: 123 [2560/8000 (32%)]\tTotal Loss: 2.915461\n",
      "Reconstruction: 1.882380, Regularization: 0.968122, Discriminator: 0.043305; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,852 root         INFO     Train Epoch: 123 [3072/8000 (38%)]\tTotal Loss: 2.483010\n",
      "Reconstruction: 1.628914, Regularization: 0.789104, Discriminator: 0.043332; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:07,965 root         INFO     Train Epoch: 123 [3584/8000 (45%)]\tTotal Loss: 2.548025\n",
      "Reconstruction: 1.667603, Regularization: 0.815436, Discriminator: 0.043324; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,077 root         INFO     Train Epoch: 123 [4096/8000 (51%)]\tTotal Loss: 2.584333\n",
      "Reconstruction: 1.723371, Regularization: 0.795970, Discriminator: 0.043323; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,188 root         INFO     Train Epoch: 123 [4608/8000 (58%)]\tTotal Loss: 2.382745\n",
      "Reconstruction: 1.530914, Regularization: 0.786826, Discriminator: 0.043330; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,300 root         INFO     Train Epoch: 123 [5120/8000 (64%)]\tTotal Loss: 2.682801\n",
      "Reconstruction: 1.740126, Regularization: 0.877686, Discriminator: 0.043323; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,412 root         INFO     Train Epoch: 123 [5632/8000 (70%)]\tTotal Loss: 2.699010\n",
      "Reconstruction: 1.755935, Regularization: 0.878081, Discriminator: 0.043311; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,524 root         INFO     Train Epoch: 123 [6144/8000 (77%)]\tTotal Loss: 2.733657\n",
      "Reconstruction: 1.755008, Regularization: 0.913666, Discriminator: 0.043311; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,636 root         INFO     Train Epoch: 123 [6656/8000 (83%)]\tTotal Loss: 2.765638\n",
      "Reconstruction: 1.800482, Regularization: 0.900162, Discriminator: 0.043315; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,743 root         INFO     Train Epoch: 123 [7168/8000 (90%)]\tTotal Loss: 2.552220\n",
      "Reconstruction: 1.613420, Regularization: 0.873807, Discriminator: 0.043321; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,851 root         INFO     Train Epoch: 123 [7680/8000 (96%)]\tTotal Loss: 2.403669\n",
      "Reconstruction: 1.540708, Regularization: 0.797969, Discriminator: 0.043349; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:08,930 root         INFO     ====> Epoch: 123 Average loss: 2.6608\n",
      "2019-04-10 00:01:08,957 root         INFO     Train Epoch: 124 [0/8000 (0%)]\tTotal Loss: 2.470507\n",
      "Reconstruction: 1.587062, Regularization: 0.818471, Discriminator: 0.043330; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,068 root         INFO     Train Epoch: 124 [512/8000 (6%)]\tTotal Loss: 2.667162\n",
      "Reconstruction: 1.779598, Regularization: 0.822566, Discriminator: 0.043349; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,179 root         INFO     Train Epoch: 124 [1024/8000 (13%)]\tTotal Loss: 2.696698\n",
      "Reconstruction: 1.722194, Regularization: 0.909535, Discriminator: 0.043332; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,289 root         INFO     Train Epoch: 124 [1536/8000 (19%)]\tTotal Loss: 3.339522\n",
      "Reconstruction: 2.172238, Regularization: 1.102329, Discriminator: 0.043308; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,400 root         INFO     Train Epoch: 124 [2048/8000 (26%)]\tTotal Loss: 2.485307\n",
      "Reconstruction: 1.634928, Regularization: 0.785389, Discriminator: 0.043342; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,511 root         INFO     Train Epoch: 124 [2560/8000 (32%)]\tTotal Loss: 2.575901\n",
      "Reconstruction: 1.632591, Regularization: 0.878333, Discriminator: 0.043332; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,621 root         INFO     Train Epoch: 124 [3072/8000 (38%)]\tTotal Loss: 2.561535\n",
      "Reconstruction: 1.661662, Regularization: 0.834890, Discriminator: 0.043326; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,732 root         INFO     Train Epoch: 124 [3584/8000 (45%)]\tTotal Loss: 2.760627\n",
      "Reconstruction: 1.757171, Regularization: 0.938475, Discriminator: 0.043325; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,842 root         INFO     Train Epoch: 124 [4096/8000 (51%)]\tTotal Loss: 2.945875\n",
      "Reconstruction: 1.852119, Regularization: 1.028783, Discriminator: 0.043318; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:09,952 root         INFO     Train Epoch: 124 [4608/8000 (58%)]\tTotal Loss: 2.835565\n",
      "Reconstruction: 1.872665, Regularization: 0.897917, Discriminator: 0.043324; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,063 root         INFO     Train Epoch: 124 [5120/8000 (64%)]\tTotal Loss: 2.536719\n",
      "Reconstruction: 1.609414, Regularization: 0.862311, Discriminator: 0.043332; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,172 root         INFO     Train Epoch: 124 [5632/8000 (70%)]\tTotal Loss: 2.792543\n",
      "Reconstruction: 1.780833, Regularization: 0.946718, Discriminator: 0.043331; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,279 root         INFO     Train Epoch: 124 [6144/8000 (77%)]\tTotal Loss: 2.734862\n",
      "Reconstruction: 1.754798, Regularization: 0.915067, Discriminator: 0.043332; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,385 root         INFO     Train Epoch: 124 [6656/8000 (83%)]\tTotal Loss: 2.475743\n",
      "Reconstruction: 1.614081, Regularization: 0.796662, Discriminator: 0.043327; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,491 root         INFO     Train Epoch: 124 [7168/8000 (90%)]\tTotal Loss: 2.601537\n",
      "Reconstruction: 1.667220, Regularization: 0.869313, Discriminator: 0.043325; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,598 root         INFO     Train Epoch: 124 [7680/8000 (96%)]\tTotal Loss: 2.917919\n",
      "Reconstruction: 1.890167, Regularization: 0.962755, Discriminator: 0.043321; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,677 root         INFO     ====> Epoch: 124 Average loss: 2.7002\n",
      "2019-04-10 00:01:10,704 root         INFO     Train Epoch: 125 [0/8000 (0%)]\tTotal Loss: 2.874585\n",
      "Reconstruction: 1.851836, Regularization: 0.957742, Discriminator: 0.043320; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,818 root         INFO     Train Epoch: 125 [512/8000 (6%)]\tTotal Loss: 2.415304\n",
      "Reconstruction: 1.600475, Regularization: 0.749828, Discriminator: 0.043318; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:10,928 root         INFO     Train Epoch: 125 [1024/8000 (13%)]\tTotal Loss: 2.663562\n",
      "Reconstruction: 1.726095, Regularization: 0.872499, Discriminator: 0.043297; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,037 root         INFO     Train Epoch: 125 [1536/8000 (19%)]\tTotal Loss: 2.649333\n",
      "Reconstruction: 1.699063, Regularization: 0.885283, Discriminator: 0.043313; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,146 root         INFO     Train Epoch: 125 [2048/8000 (26%)]\tTotal Loss: 2.513329\n",
      "Reconstruction: 1.664874, Regularization: 0.783478, Discriminator: 0.043313; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,255 root         INFO     Train Epoch: 125 [2560/8000 (32%)]\tTotal Loss: 2.649133\n",
      "Reconstruction: 1.750651, Regularization: 0.833483, Discriminator: 0.043332; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,367 root         INFO     Train Epoch: 125 [3072/8000 (38%)]\tTotal Loss: 2.810735\n",
      "Reconstruction: 1.868826, Regularization: 0.876909, Discriminator: 0.043334; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,477 root         INFO     Train Epoch: 125 [3584/8000 (45%)]\tTotal Loss: 2.915134\n",
      "Reconstruction: 1.903072, Regularization: 0.947079, Discriminator: 0.043327; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,587 root         INFO     Train Epoch: 125 [4096/8000 (51%)]\tTotal Loss: 2.693842\n",
      "Reconstruction: 1.790995, Regularization: 0.837888, Discriminator: 0.043317; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,698 root         INFO     Train Epoch: 125 [4608/8000 (58%)]\tTotal Loss: 2.781726\n",
      "Reconstruction: 1.815232, Regularization: 0.901522, Discriminator: 0.043322; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,808 root         INFO     Train Epoch: 125 [5120/8000 (64%)]\tTotal Loss: 2.502901\n",
      "Reconstruction: 1.581391, Regularization: 0.856541, Discriminator: 0.043314; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:11,918 root         INFO     Train Epoch: 125 [5632/8000 (70%)]\tTotal Loss: 3.047708\n",
      "Reconstruction: 1.893839, Regularization: 1.088894, Discriminator: 0.043330; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,029 root         INFO     Train Epoch: 125 [6144/8000 (77%)]\tTotal Loss: 2.832340\n",
      "Reconstruction: 1.865505, Regularization: 0.901872, Discriminator: 0.043304; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,139 root         INFO     Train Epoch: 125 [6656/8000 (83%)]\tTotal Loss: 2.843042\n",
      "Reconstruction: 1.877801, Regularization: 0.900274, Discriminator: 0.043300; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,249 root         INFO     Train Epoch: 125 [7168/8000 (90%)]\tTotal Loss: 2.843539\n",
      "Reconstruction: 1.832316, Regularization: 0.946252, Discriminator: 0.043309; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,359 root         INFO     Train Epoch: 125 [7680/8000 (96%)]\tTotal Loss: 3.134567\n",
      "Reconstruction: 2.028343, Regularization: 1.041230, Discriminator: 0.043330; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,440 root         INFO     ====> Epoch: 125 Average loss: 2.7019\n",
      "2019-04-10 00:01:12,468 root         INFO     Train Epoch: 126 [0/8000 (0%)]\tTotal Loss: 2.482072\n",
      "Reconstruction: 1.565183, Regularization: 0.851894, Discriminator: 0.043327; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,580 root         INFO     Train Epoch: 126 [512/8000 (6%)]\tTotal Loss: 2.813967\n",
      "Reconstruction: 1.705111, Regularization: 1.043827, Discriminator: 0.043354; Generator: 0.021676,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,692 root         INFO     Train Epoch: 126 [1024/8000 (13%)]\tTotal Loss: 2.852620\n",
      "Reconstruction: 1.878643, Regularization: 0.908982, Discriminator: 0.043327; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,804 root         INFO     Train Epoch: 126 [1536/8000 (19%)]\tTotal Loss: 2.686999\n",
      "Reconstruction: 1.764570, Regularization: 0.857433, Discriminator: 0.043319; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:12,916 root         INFO     Train Epoch: 126 [2048/8000 (26%)]\tTotal Loss: 2.758244\n",
      "Reconstruction: 1.755708, Regularization: 0.937524, Discriminator: 0.043331; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,027 root         INFO     Train Epoch: 126 [2560/8000 (32%)]\tTotal Loss: 2.814564\n",
      "Reconstruction: 1.877424, Regularization: 0.872143, Discriminator: 0.043321; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,137 root         INFO     Train Epoch: 126 [3072/8000 (38%)]\tTotal Loss: 2.403776\n",
      "Reconstruction: 1.548801, Regularization: 0.789988, Discriminator: 0.043310; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,247 root         INFO     Train Epoch: 126 [3584/8000 (45%)]\tTotal Loss: 2.387655\n",
      "Reconstruction: 1.604772, Regularization: 0.717907, Discriminator: 0.043305; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,356 root         INFO     Train Epoch: 126 [4096/8000 (51%)]\tTotal Loss: 2.526574\n",
      "Reconstruction: 1.584757, Regularization: 0.876833, Discriminator: 0.043314; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,465 root         INFO     Train Epoch: 126 [4608/8000 (58%)]\tTotal Loss: 2.746855\n",
      "Reconstruction: 1.790048, Regularization: 0.891799, Discriminator: 0.043337; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,573 root         INFO     Train Epoch: 126 [5120/8000 (64%)]\tTotal Loss: 2.532169\n",
      "Reconstruction: 1.578258, Regularization: 0.888941, Discriminator: 0.043323; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,682 root         INFO     Train Epoch: 126 [5632/8000 (70%)]\tTotal Loss: 2.669713\n",
      "Reconstruction: 1.729045, Regularization: 0.875683, Discriminator: 0.043327; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,791 root         INFO     Train Epoch: 126 [6144/8000 (77%)]\tTotal Loss: 2.841333\n",
      "Reconstruction: 1.836365, Regularization: 0.939964, Discriminator: 0.043352; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:13,901 root         INFO     Train Epoch: 126 [6656/8000 (83%)]\tTotal Loss: 2.765107\n",
      "Reconstruction: 1.738967, Regularization: 0.961167, Discriminator: 0.043331; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,009 root         INFO     Train Epoch: 126 [7168/8000 (90%)]\tTotal Loss: 2.724859\n",
      "Reconstruction: 1.833603, Regularization: 0.826280, Discriminator: 0.043332; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,118 root         INFO     Train Epoch: 126 [7680/8000 (96%)]\tTotal Loss: 2.744573\n",
      "Reconstruction: 1.724276, Regularization: 0.955316, Discriminator: 0.043343; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,197 root         INFO     ====> Epoch: 126 Average loss: 2.6537\n",
      "2019-04-10 00:01:14,225 root         INFO     Train Epoch: 127 [0/8000 (0%)]\tTotal Loss: 2.841622\n",
      "Reconstruction: 1.743679, Regularization: 1.032974, Discriminator: 0.043336; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,334 root         INFO     Train Epoch: 127 [512/8000 (6%)]\tTotal Loss: 2.825113\n",
      "Reconstruction: 1.786510, Regularization: 0.973631, Discriminator: 0.043328; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,443 root         INFO     Train Epoch: 127 [1024/8000 (13%)]\tTotal Loss: 2.624912\n",
      "Reconstruction: 1.692114, Regularization: 0.867839, Discriminator: 0.043323; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,553 root         INFO     Train Epoch: 127 [1536/8000 (19%)]\tTotal Loss: 2.601521\n",
      "Reconstruction: 1.691823, Regularization: 0.844741, Discriminator: 0.043309; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,660 root         INFO     Train Epoch: 127 [2048/8000 (26%)]\tTotal Loss: 2.608150\n",
      "Reconstruction: 1.667036, Regularization: 0.876159, Discriminator: 0.043306; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,768 root         INFO     Train Epoch: 127 [2560/8000 (32%)]\tTotal Loss: 2.777242\n",
      "Reconstruction: 1.798552, Regularization: 0.913736, Discriminator: 0.043302; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,875 root         INFO     Train Epoch: 127 [3072/8000 (38%)]\tTotal Loss: 2.282857\n",
      "Reconstruction: 1.506446, Regularization: 0.711447, Discriminator: 0.043312; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:14,983 root         INFO     Train Epoch: 127 [3584/8000 (45%)]\tTotal Loss: 2.676579\n",
      "Reconstruction: 1.750414, Regularization: 0.861186, Discriminator: 0.043323; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,091 root         INFO     Train Epoch: 127 [4096/8000 (51%)]\tTotal Loss: 2.480953\n",
      "Reconstruction: 1.574200, Regularization: 0.841775, Discriminator: 0.043316; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,198 root         INFO     Train Epoch: 127 [4608/8000 (58%)]\tTotal Loss: 2.459819\n",
      "Reconstruction: 1.633680, Regularization: 0.761161, Discriminator: 0.043314; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,305 root         INFO     Train Epoch: 127 [5120/8000 (64%)]\tTotal Loss: 2.703929\n",
      "Reconstruction: 1.742563, Regularization: 0.896374, Discriminator: 0.043324; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,412 root         INFO     Train Epoch: 127 [5632/8000 (70%)]\tTotal Loss: 2.627044\n",
      "Reconstruction: 1.597126, Regularization: 0.964917, Discriminator: 0.043331; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,518 root         INFO     Train Epoch: 127 [6144/8000 (77%)]\tTotal Loss: 2.336612\n",
      "Reconstruction: 1.568182, Regularization: 0.703433, Discriminator: 0.043329; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,626 root         INFO     Train Epoch: 127 [6656/8000 (83%)]\tTotal Loss: 2.673517\n",
      "Reconstruction: 1.755215, Regularization: 0.853309, Discriminator: 0.043318; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,733 root         INFO     Train Epoch: 127 [7168/8000 (90%)]\tTotal Loss: 2.614104\n",
      "Reconstruction: 1.650278, Regularization: 0.898849, Discriminator: 0.043311; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,840 root         INFO     Train Epoch: 127 [7680/8000 (96%)]\tTotal Loss: 2.554255\n",
      "Reconstruction: 1.651450, Regularization: 0.837811, Discriminator: 0.043316; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:15,918 root         INFO     ====> Epoch: 127 Average loss: 2.6219\n",
      "2019-04-10 00:01:15,945 root         INFO     Train Epoch: 128 [0/8000 (0%)]\tTotal Loss: 2.596832\n",
      "Reconstruction: 1.653828, Regularization: 0.878017, Discriminator: 0.043310; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,055 root         INFO     Train Epoch: 128 [512/8000 (6%)]\tTotal Loss: 2.720825\n",
      "Reconstruction: 1.750982, Regularization: 0.904857, Discriminator: 0.043313; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,164 root         INFO     Train Epoch: 128 [1024/8000 (13%)]\tTotal Loss: 2.419772\n",
      "Reconstruction: 1.517931, Regularization: 0.836890, Discriminator: 0.043287; Generator: 0.021664,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,274 root         INFO     Train Epoch: 128 [1536/8000 (19%)]\tTotal Loss: 2.816830\n",
      "Reconstruction: 1.707436, Regularization: 1.044438, Discriminator: 0.043300; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,383 root         INFO     Train Epoch: 128 [2048/8000 (26%)]\tTotal Loss: 2.725866\n",
      "Reconstruction: 1.672389, Regularization: 0.988497, Discriminator: 0.043314; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,492 root         INFO     Train Epoch: 128 [2560/8000 (32%)]\tTotal Loss: 2.773513\n",
      "Reconstruction: 1.793296, Regularization: 0.915236, Discriminator: 0.043348; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,602 root         INFO     Train Epoch: 128 [3072/8000 (38%)]\tTotal Loss: 2.517454\n",
      "Reconstruction: 1.674381, Regularization: 0.778088, Discriminator: 0.043347; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,712 root         INFO     Train Epoch: 128 [3584/8000 (45%)]\tTotal Loss: 2.814556\n",
      "Reconstruction: 1.753419, Regularization: 0.996186, Discriminator: 0.043311; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,822 root         INFO     Train Epoch: 128 [4096/8000 (51%)]\tTotal Loss: 2.511007\n",
      "Reconstruction: 1.645648, Regularization: 0.800378, Discriminator: 0.043330; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:16,929 root         INFO     Train Epoch: 128 [4608/8000 (58%)]\tTotal Loss: 2.469690\n",
      "Reconstruction: 1.550102, Regularization: 0.854617, Discriminator: 0.043324; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,039 root         INFO     Train Epoch: 128 [5120/8000 (64%)]\tTotal Loss: 2.772291\n",
      "Reconstruction: 1.761269, Regularization: 0.946061, Discriminator: 0.043318; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,147 root         INFO     Train Epoch: 128 [5632/8000 (70%)]\tTotal Loss: 2.595053\n",
      "Reconstruction: 1.615551, Regularization: 0.914534, Discriminator: 0.043317; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,255 root         INFO     Train Epoch: 128 [6144/8000 (77%)]\tTotal Loss: 2.704853\n",
      "Reconstruction: 1.739740, Regularization: 0.900156, Discriminator: 0.043308; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,363 root         INFO     Train Epoch: 128 [6656/8000 (83%)]\tTotal Loss: 2.881779\n",
      "Reconstruction: 1.875023, Regularization: 0.941782, Discriminator: 0.043321; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,471 root         INFO     Train Epoch: 128 [7168/8000 (90%)]\tTotal Loss: 2.661561\n",
      "Reconstruction: 1.716493, Regularization: 0.880087, Discriminator: 0.043319; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,579 root         INFO     Train Epoch: 128 [7680/8000 (96%)]\tTotal Loss: 3.048178\n",
      "Reconstruction: 1.965131, Regularization: 1.018075, Discriminator: 0.043319; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,659 root         INFO     ====> Epoch: 128 Average loss: 2.6629\n",
      "2019-04-10 00:01:17,686 root         INFO     Train Epoch: 129 [0/8000 (0%)]\tTotal Loss: 2.641578\n",
      "Reconstruction: 1.766111, Regularization: 0.810472, Discriminator: 0.043332; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,795 root         INFO     Train Epoch: 129 [512/8000 (6%)]\tTotal Loss: 2.610983\n",
      "Reconstruction: 1.703153, Regularization: 0.842832, Discriminator: 0.043338; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:17,904 root         INFO     Train Epoch: 129 [1024/8000 (13%)]\tTotal Loss: 3.736704\n",
      "Reconstruction: 2.880974, Regularization: 0.790720, Discriminator: 0.043354; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,013 root         INFO     Train Epoch: 129 [1536/8000 (19%)]\tTotal Loss: 2.822372\n",
      "Reconstruction: 1.870984, Regularization: 0.886380, Discriminator: 0.043341; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,123 root         INFO     Train Epoch: 129 [2048/8000 (26%)]\tTotal Loss: 2.519456\n",
      "Reconstruction: 1.599159, Regularization: 0.855289, Discriminator: 0.043344; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,233 root         INFO     Train Epoch: 129 [2560/8000 (32%)]\tTotal Loss: 2.710901\n",
      "Reconstruction: 1.739113, Regularization: 0.906783, Discriminator: 0.043325; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,343 root         INFO     Train Epoch: 129 [3072/8000 (38%)]\tTotal Loss: 2.629484\n",
      "Reconstruction: 1.746439, Regularization: 0.818038, Discriminator: 0.043321; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,452 root         INFO     Train Epoch: 129 [3584/8000 (45%)]\tTotal Loss: 3.001651\n",
      "Reconstruction: 1.956057, Regularization: 0.980598, Discriminator: 0.043314; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,562 root         INFO     Train Epoch: 129 [4096/8000 (51%)]\tTotal Loss: 2.772724\n",
      "Reconstruction: 1.860696, Regularization: 0.847028, Discriminator: 0.043319; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,673 root         INFO     Train Epoch: 129 [4608/8000 (58%)]\tTotal Loss: 2.526687\n",
      "Reconstruction: 1.617036, Regularization: 0.844669, Discriminator: 0.043309; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,784 root         INFO     Train Epoch: 129 [5120/8000 (64%)]\tTotal Loss: 3.037702\n",
      "Reconstruction: 1.963600, Regularization: 1.009093, Discriminator: 0.043329; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:18,894 root         INFO     Train Epoch: 129 [5632/8000 (70%)]\tTotal Loss: 2.954710\n",
      "Reconstruction: 1.938459, Regularization: 0.951262, Discriminator: 0.043323; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,004 root         INFO     Train Epoch: 129 [6144/8000 (77%)]\tTotal Loss: 2.614359\n",
      "Reconstruction: 1.752356, Regularization: 0.797011, Discriminator: 0.043334; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,115 root         INFO     Train Epoch: 129 [6656/8000 (83%)]\tTotal Loss: 2.708773\n",
      "Reconstruction: 1.719599, Regularization: 0.924183, Discriminator: 0.043336; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,225 root         INFO     Train Epoch: 129 [7168/8000 (90%)]\tTotal Loss: 2.577415\n",
      "Reconstruction: 1.641749, Regularization: 0.870674, Discriminator: 0.043333; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,336 root         INFO     Train Epoch: 129 [7680/8000 (96%)]\tTotal Loss: 2.602344\n",
      "Reconstruction: 1.689119, Regularization: 0.848252, Discriminator: 0.043324; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,416 root         INFO     ====> Epoch: 129 Average loss: 2.7166\n",
      "2019-04-10 00:01:19,443 root         INFO     Train Epoch: 130 [0/8000 (0%)]\tTotal Loss: 2.622152\n",
      "Reconstruction: 1.747584, Regularization: 0.809605, Discriminator: 0.043313; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,556 root         INFO     Train Epoch: 130 [512/8000 (6%)]\tTotal Loss: 2.575935\n",
      "Reconstruction: 1.681947, Regularization: 0.829022, Discriminator: 0.043318; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,669 root         INFO     Train Epoch: 130 [1024/8000 (13%)]\tTotal Loss: 2.682903\n",
      "Reconstruction: 1.727975, Regularization: 0.889969, Discriminator: 0.043309; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,781 root         INFO     Train Epoch: 130 [1536/8000 (19%)]\tTotal Loss: 2.871643\n",
      "Reconstruction: 1.924132, Regularization: 0.882553, Discriminator: 0.043301; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:19,892 root         INFO     Train Epoch: 130 [2048/8000 (26%)]\tTotal Loss: 2.680816\n",
      "Reconstruction: 1.811333, Regularization: 0.804530, Discriminator: 0.043293; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,005 root         INFO     Train Epoch: 130 [2560/8000 (32%)]\tTotal Loss: 2.764949\n",
      "Reconstruction: 1.800993, Regularization: 0.899000, Discriminator: 0.043282; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,117 root         INFO     Train Epoch: 130 [3072/8000 (38%)]\tTotal Loss: 2.861664\n",
      "Reconstruction: 1.866680, Regularization: 0.930016, Discriminator: 0.043307; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,229 root         INFO     Train Epoch: 130 [3584/8000 (45%)]\tTotal Loss: 2.894659\n",
      "Reconstruction: 1.860675, Regularization: 0.968991, Discriminator: 0.043326; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,341 root         INFO     Train Epoch: 130 [4096/8000 (51%)]\tTotal Loss: 2.760945\n",
      "Reconstruction: 1.792860, Regularization: 0.903101, Discriminator: 0.043328; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,453 root         INFO     Train Epoch: 130 [4608/8000 (58%)]\tTotal Loss: 2.891233\n",
      "Reconstruction: 1.815044, Regularization: 1.011166, Discriminator: 0.043357; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,565 root         INFO     Train Epoch: 130 [5120/8000 (64%)]\tTotal Loss: 3.427535\n",
      "Reconstruction: 2.511798, Regularization: 0.850727, Discriminator: 0.043338; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,676 root         INFO     Train Epoch: 130 [5632/8000 (70%)]\tTotal Loss: 2.905674\n",
      "Reconstruction: 1.974262, Regularization: 0.866423, Discriminator: 0.043314; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,789 root         INFO     Train Epoch: 130 [6144/8000 (77%)]\tTotal Loss: 2.616305\n",
      "Reconstruction: 1.683120, Regularization: 0.868183, Discriminator: 0.043318; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:20,900 root         INFO     Train Epoch: 130 [6656/8000 (83%)]\tTotal Loss: 2.704048\n",
      "Reconstruction: 1.734145, Regularization: 0.904904, Discriminator: 0.043312; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,011 root         INFO     Train Epoch: 130 [7168/8000 (90%)]\tTotal Loss: 2.487633\n",
      "Reconstruction: 1.596066, Regularization: 0.826573, Discriminator: 0.043306; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,123 root         INFO     Train Epoch: 130 [7680/8000 (96%)]\tTotal Loss: 2.632097\n",
      "Reconstruction: 1.728838, Regularization: 0.838268, Discriminator: 0.043326; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,204 root         INFO     ====> Epoch: 130 Average loss: 2.7150\n",
      "2019-04-10 00:01:21,232 root         INFO     Train Epoch: 131 [0/8000 (0%)]\tTotal Loss: 2.818360\n",
      "Reconstruction: 1.821069, Regularization: 0.932305, Discriminator: 0.043316; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,342 root         INFO     Train Epoch: 131 [512/8000 (6%)]\tTotal Loss: 2.694232\n",
      "Reconstruction: 1.715904, Regularization: 0.913321, Discriminator: 0.043316; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,454 root         INFO     Train Epoch: 131 [1024/8000 (13%)]\tTotal Loss: 2.396625\n",
      "Reconstruction: 1.551472, Regularization: 0.780182, Discriminator: 0.043291; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,564 root         INFO     Train Epoch: 131 [1536/8000 (19%)]\tTotal Loss: 2.833508\n",
      "Reconstruction: 1.838696, Regularization: 0.929833, Discriminator: 0.043319; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,675 root         INFO     Train Epoch: 131 [2048/8000 (26%)]\tTotal Loss: 2.927554\n",
      "Reconstruction: 1.937784, Regularization: 0.924770, Discriminator: 0.043358; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,785 root         INFO     Train Epoch: 131 [2560/8000 (32%)]\tTotal Loss: 3.028629\n",
      "Reconstruction: 1.962749, Regularization: 1.000876, Discriminator: 0.043346; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:21,895 root         INFO     Train Epoch: 131 [3072/8000 (38%)]\tTotal Loss: 2.659400\n",
      "Reconstruction: 1.721863, Regularization: 0.872568, Discriminator: 0.043331; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,005 root         INFO     Train Epoch: 131 [3584/8000 (45%)]\tTotal Loss: 2.840026\n",
      "Reconstruction: 1.725443, Regularization: 1.049590, Discriminator: 0.043348; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,115 root         INFO     Train Epoch: 131 [4096/8000 (51%)]\tTotal Loss: 2.827523\n",
      "Reconstruction: 1.841776, Regularization: 0.920768, Discriminator: 0.043339; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,224 root         INFO     Train Epoch: 131 [4608/8000 (58%)]\tTotal Loss: 2.557888\n",
      "Reconstruction: 1.651481, Regularization: 0.841430, Discriminator: 0.043332; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,334 root         INFO     Train Epoch: 131 [5120/8000 (64%)]\tTotal Loss: 2.604004\n",
      "Reconstruction: 1.701178, Regularization: 0.837855, Discriminator: 0.043326; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,444 root         INFO     Train Epoch: 131 [5632/8000 (70%)]\tTotal Loss: 2.475496\n",
      "Reconstruction: 1.591425, Regularization: 0.819112, Discriminator: 0.043327; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,555 root         INFO     Train Epoch: 131 [6144/8000 (77%)]\tTotal Loss: 2.697791\n",
      "Reconstruction: 1.800580, Regularization: 0.832242, Discriminator: 0.043325; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,665 root         INFO     Train Epoch: 131 [6656/8000 (83%)]\tTotal Loss: 2.819824\n",
      "Reconstruction: 1.834026, Regularization: 0.920825, Discriminator: 0.043326; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,775 root         INFO     Train Epoch: 131 [7168/8000 (90%)]\tTotal Loss: 2.900464\n",
      "Reconstruction: 1.867965, Regularization: 0.967532, Discriminator: 0.043319; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,885 root         INFO     Train Epoch: 131 [7680/8000 (96%)]\tTotal Loss: 2.525331\n",
      "Reconstruction: 1.695449, Regularization: 0.764897, Discriminator: 0.043337; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:22,965 root         INFO     ====> Epoch: 131 Average loss: 2.7019\n",
      "2019-04-10 00:01:22,992 root         INFO     Train Epoch: 132 [0/8000 (0%)]\tTotal Loss: 2.536907\n",
      "Reconstruction: 1.648075, Regularization: 0.823877, Discriminator: 0.043304; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,102 root         INFO     Train Epoch: 132 [512/8000 (6%)]\tTotal Loss: 2.837539\n",
      "Reconstruction: 1.898093, Regularization: 0.874478, Discriminator: 0.043312; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,211 root         INFO     Train Epoch: 132 [1024/8000 (13%)]\tTotal Loss: 2.774075\n",
      "Reconstruction: 1.765056, Regularization: 0.944033, Discriminator: 0.043333; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,321 root         INFO     Train Epoch: 132 [1536/8000 (19%)]\tTotal Loss: 2.843744\n",
      "Reconstruction: 1.891926, Regularization: 0.886829, Discriminator: 0.043331; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,431 root         INFO     Train Epoch: 132 [2048/8000 (26%)]\tTotal Loss: 2.464135\n",
      "Reconstruction: 1.597600, Regularization: 0.801527, Discriminator: 0.043346; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,540 root         INFO     Train Epoch: 132 [2560/8000 (32%)]\tTotal Loss: 2.851533\n",
      "Reconstruction: 1.849581, Regularization: 0.936963, Discriminator: 0.043323; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,650 root         INFO     Train Epoch: 132 [3072/8000 (38%)]\tTotal Loss: 2.958177\n",
      "Reconstruction: 1.976583, Regularization: 0.916599, Discriminator: 0.043327; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,759 root         INFO     Train Epoch: 132 [3584/8000 (45%)]\tTotal Loss: 2.736032\n",
      "Reconstruction: 1.768019, Regularization: 0.903023, Discriminator: 0.043323; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,868 root         INFO     Train Epoch: 132 [4096/8000 (51%)]\tTotal Loss: 2.795624\n",
      "Reconstruction: 1.780621, Regularization: 0.950013, Discriminator: 0.043316; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:23,978 root         INFO     Train Epoch: 132 [4608/8000 (58%)]\tTotal Loss: 2.931541\n",
      "Reconstruction: 1.914709, Regularization: 0.951843, Discriminator: 0.043313; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,087 root         INFO     Train Epoch: 132 [5120/8000 (64%)]\tTotal Loss: 2.713398\n",
      "Reconstruction: 1.769193, Regularization: 0.879218, Discriminator: 0.043308; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,196 root         INFO     Train Epoch: 132 [5632/8000 (70%)]\tTotal Loss: 2.663405\n",
      "Reconstruction: 1.736873, Regularization: 0.861547, Discriminator: 0.043309; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,306 root         INFO     Train Epoch: 132 [6144/8000 (77%)]\tTotal Loss: 2.871438\n",
      "Reconstruction: 1.872238, Regularization: 0.934220, Discriminator: 0.043308; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,415 root         INFO     Train Epoch: 132 [6656/8000 (83%)]\tTotal Loss: 2.677994\n",
      "Reconstruction: 1.760014, Regularization: 0.853010, Discriminator: 0.043301; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,525 root         INFO     Train Epoch: 132 [7168/8000 (90%)]\tTotal Loss: 3.052568\n",
      "Reconstruction: 1.957763, Regularization: 1.029817, Discriminator: 0.043300; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,635 root         INFO     Train Epoch: 132 [7680/8000 (96%)]\tTotal Loss: 2.661916\n",
      "Reconstruction: 1.699449, Regularization: 0.897502, Discriminator: 0.043312; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,716 root         INFO     ====> Epoch: 132 Average loss: 2.7076\n",
      "2019-04-10 00:01:24,743 root         INFO     Train Epoch: 133 [0/8000 (0%)]\tTotal Loss: 2.303681\n",
      "Reconstruction: 1.518240, Regularization: 0.720416, Discriminator: 0.043348; Generator: 0.021676,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,853 root         INFO     Train Epoch: 133 [512/8000 (6%)]\tTotal Loss: 2.631102\n",
      "Reconstruction: 1.790398, Regularization: 0.775696, Discriminator: 0.043361; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:24,963 root         INFO     Train Epoch: 133 [1024/8000 (13%)]\tTotal Loss: 2.633418\n",
      "Reconstruction: 1.713647, Regularization: 0.854808, Discriminator: 0.043325; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,073 root         INFO     Train Epoch: 133 [1536/8000 (19%)]\tTotal Loss: 2.700038\n",
      "Reconstruction: 1.732926, Regularization: 0.902149, Discriminator: 0.043309; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,183 root         INFO     Train Epoch: 133 [2048/8000 (26%)]\tTotal Loss: 2.779447\n",
      "Reconstruction: 1.858475, Regularization: 0.855997, Discriminator: 0.043329; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,292 root         INFO     Train Epoch: 133 [2560/8000 (32%)]\tTotal Loss: 2.701811\n",
      "Reconstruction: 1.782270, Regularization: 0.854579, Discriminator: 0.043326; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,402 root         INFO     Train Epoch: 133 [3072/8000 (38%)]\tTotal Loss: 2.723045\n",
      "Reconstruction: 1.843680, Regularization: 0.814393, Discriminator: 0.043329; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,512 root         INFO     Train Epoch: 133 [3584/8000 (45%)]\tTotal Loss: 2.514757\n",
      "Reconstruction: 1.640714, Regularization: 0.809074, Discriminator: 0.043320; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,621 root         INFO     Train Epoch: 133 [4096/8000 (51%)]\tTotal Loss: 2.675210\n",
      "Reconstruction: 1.741918, Regularization: 0.868323, Discriminator: 0.043314; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,731 root         INFO     Train Epoch: 133 [4608/8000 (58%)]\tTotal Loss: 2.624032\n",
      "Reconstruction: 1.744144, Regularization: 0.814909, Discriminator: 0.043325; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,837 root         INFO     Train Epoch: 133 [5120/8000 (64%)]\tTotal Loss: 2.855111\n",
      "Reconstruction: 1.853962, Regularization: 0.936179, Discriminator: 0.043315; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:25,943 root         INFO     Train Epoch: 133 [5632/8000 (70%)]\tTotal Loss: 2.947099\n",
      "Reconstruction: 1.960777, Regularization: 0.921364, Discriminator: 0.043303; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,049 root         INFO     Train Epoch: 133 [6144/8000 (77%)]\tTotal Loss: 2.861030\n",
      "Reconstruction: 1.904029, Regularization: 0.892030, Discriminator: 0.043314; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,156 root         INFO     Train Epoch: 133 [6656/8000 (83%)]\tTotal Loss: 2.680936\n",
      "Reconstruction: 1.755808, Regularization: 0.860146, Discriminator: 0.043324; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,261 root         INFO     Train Epoch: 133 [7168/8000 (90%)]\tTotal Loss: 3.454923\n",
      "Reconstruction: 2.492998, Regularization: 0.896926, Discriminator: 0.043337; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,372 root         INFO     Train Epoch: 133 [7680/8000 (96%)]\tTotal Loss: 2.980054\n",
      "Reconstruction: 1.920725, Regularization: 0.994332, Discriminator: 0.043330; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,451 root         INFO     ====> Epoch: 133 Average loss: 2.7287\n",
      "2019-04-10 00:01:26,479 root         INFO     Train Epoch: 134 [0/8000 (0%)]\tTotal Loss: 2.339302\n",
      "Reconstruction: 1.556762, Regularization: 0.717508, Discriminator: 0.043368; Generator: 0.021664,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,589 root         INFO     Train Epoch: 134 [512/8000 (6%)]\tTotal Loss: 3.031940\n",
      "Reconstruction: 2.003028, Regularization: 0.963919, Discriminator: 0.043325; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,698 root         INFO     Train Epoch: 134 [1024/8000 (13%)]\tTotal Loss: 2.776759\n",
      "Reconstruction: 1.867532, Regularization: 0.844225, Discriminator: 0.043334; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,808 root         INFO     Train Epoch: 134 [1536/8000 (19%)]\tTotal Loss: 2.892004\n",
      "Reconstruction: 1.990964, Regularization: 0.836035, Discriminator: 0.043336; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:26,917 root         INFO     Train Epoch: 134 [2048/8000 (26%)]\tTotal Loss: 2.606949\n",
      "Reconstruction: 1.656154, Regularization: 0.885803, Discriminator: 0.043314; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,027 root         INFO     Train Epoch: 134 [2560/8000 (32%)]\tTotal Loss: 2.826402\n",
      "Reconstruction: 1.861724, Regularization: 0.899691, Discriminator: 0.043315; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,136 root         INFO     Train Epoch: 134 [3072/8000 (38%)]\tTotal Loss: 3.056756\n",
      "Reconstruction: 2.120009, Regularization: 0.871747, Discriminator: 0.043321; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,246 root         INFO     Train Epoch: 134 [3584/8000 (45%)]\tTotal Loss: 2.506287\n",
      "Reconstruction: 1.653203, Regularization: 0.788092, Discriminator: 0.043334; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,355 root         INFO     Train Epoch: 134 [4096/8000 (51%)]\tTotal Loss: 3.087544\n",
      "Reconstruction: 2.064873, Regularization: 0.957655, Discriminator: 0.043350; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,465 root         INFO     Train Epoch: 134 [4608/8000 (58%)]\tTotal Loss: 2.648661\n",
      "Reconstruction: 1.722632, Regularization: 0.861050, Discriminator: 0.043314; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,574 root         INFO     Train Epoch: 134 [5120/8000 (64%)]\tTotal Loss: 2.696179\n",
      "Reconstruction: 1.779257, Regularization: 0.851941, Discriminator: 0.043335; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,683 root         INFO     Train Epoch: 134 [5632/8000 (70%)]\tTotal Loss: 2.888361\n",
      "Reconstruction: 1.979072, Regularization: 0.844312, Discriminator: 0.043323; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,791 root         INFO     Train Epoch: 134 [6144/8000 (77%)]\tTotal Loss: 2.657666\n",
      "Reconstruction: 1.721944, Regularization: 0.870745, Discriminator: 0.043332; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:27,899 root         INFO     Train Epoch: 134 [6656/8000 (83%)]\tTotal Loss: 2.563227\n",
      "Reconstruction: 1.709139, Regularization: 0.789113, Discriminator: 0.043327; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,008 root         INFO     Train Epoch: 134 [7168/8000 (90%)]\tTotal Loss: 2.832630\n",
      "Reconstruction: 1.878673, Regularization: 0.888984, Discriminator: 0.043320; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,119 root         INFO     Train Epoch: 134 [7680/8000 (96%)]\tTotal Loss: 2.846485\n",
      "Reconstruction: 1.951152, Regularization: 0.830357, Discriminator: 0.043321; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,201 root         INFO     ====> Epoch: 134 Average loss: 2.7501\n",
      "2019-04-10 00:01:28,228 root         INFO     Train Epoch: 135 [0/8000 (0%)]\tTotal Loss: 2.760517\n",
      "Reconstruction: 1.837326, Regularization: 0.858224, Discriminator: 0.043310; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,338 root         INFO     Train Epoch: 135 [512/8000 (6%)]\tTotal Loss: 3.368281\n",
      "Reconstruction: 2.445319, Regularization: 0.857998, Discriminator: 0.043305; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,450 root         INFO     Train Epoch: 135 [1024/8000 (13%)]\tTotal Loss: 2.858567\n",
      "Reconstruction: 1.893430, Regularization: 0.900165, Discriminator: 0.043313; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,563 root         INFO     Train Epoch: 135 [1536/8000 (19%)]\tTotal Loss: 2.519583\n",
      "Reconstruction: 1.676941, Regularization: 0.777660, Discriminator: 0.043324; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,676 root         INFO     Train Epoch: 135 [2048/8000 (26%)]\tTotal Loss: 2.708728\n",
      "Reconstruction: 1.790201, Regularization: 0.853536, Discriminator: 0.043325; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,788 root         INFO     Train Epoch: 135 [2560/8000 (32%)]\tTotal Loss: 2.379240\n",
      "Reconstruction: 1.619056, Regularization: 0.695232, Discriminator: 0.043292; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:28,899 root         INFO     Train Epoch: 135 [3072/8000 (38%)]\tTotal Loss: 2.611364\n",
      "Reconstruction: 1.805925, Regularization: 0.740477, Discriminator: 0.043296; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,010 root         INFO     Train Epoch: 135 [3584/8000 (45%)]\tTotal Loss: 2.807560\n",
      "Reconstruction: 1.949765, Regularization: 0.792804, Discriminator: 0.043314; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,123 root         INFO     Train Epoch: 135 [4096/8000 (51%)]\tTotal Loss: 2.945798\n",
      "Reconstruction: 2.025274, Regularization: 0.855515, Discriminator: 0.043333; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,236 root         INFO     Train Epoch: 135 [4608/8000 (58%)]\tTotal Loss: 2.646262\n",
      "Reconstruction: 1.736393, Regularization: 0.844854, Discriminator: 0.043331; Generator: 0.021685,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,349 root         INFO     Train Epoch: 135 [5120/8000 (64%)]\tTotal Loss: 2.807813\n",
      "Reconstruction: 1.938442, Regularization: 0.804383, Discriminator: 0.043317; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,462 root         INFO     Train Epoch: 135 [5632/8000 (70%)]\tTotal Loss: 2.913866\n",
      "Reconstruction: 2.020855, Regularization: 0.828027, Discriminator: 0.043311; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,574 root         INFO     Train Epoch: 135 [6144/8000 (77%)]\tTotal Loss: 2.740180\n",
      "Reconstruction: 1.831161, Regularization: 0.844033, Discriminator: 0.043309; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,687 root         INFO     Train Epoch: 135 [6656/8000 (83%)]\tTotal Loss: 3.045713\n",
      "Reconstruction: 1.989902, Regularization: 0.990805, Discriminator: 0.043329; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,799 root         INFO     Train Epoch: 135 [7168/8000 (90%)]\tTotal Loss: 2.781158\n",
      "Reconstruction: 1.860909, Regularization: 0.855254, Discriminator: 0.043311; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,905 root         INFO     Train Epoch: 135 [7680/8000 (96%)]\tTotal Loss: 2.708761\n",
      "Reconstruction: 1.772865, Regularization: 0.870900, Discriminator: 0.043319; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:29,984 root         INFO     ====> Epoch: 135 Average loss: 2.7581\n",
      "2019-04-10 00:01:30,011 root         INFO     Train Epoch: 136 [0/8000 (0%)]\tTotal Loss: 2.772352\n",
      "Reconstruction: 1.847183, Regularization: 0.860186, Discriminator: 0.043300; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:30,120 root         INFO     Train Epoch: 136 [512/8000 (6%)]\tTotal Loss: 2.875591\n",
      "Reconstruction: 1.897600, Regularization: 0.912995, Discriminator: 0.043332; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:30,229 root         INFO     Train Epoch: 136 [1024/8000 (13%)]\tTotal Loss: 2.584124\n",
      "Reconstruction: 1.745944, Regularization: 0.773221, Discriminator: 0.043296; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:30,337 root         INFO     Train Epoch: 136 [1536/8000 (19%)]\tTotal Loss: 2.672710\n",
      "Reconstruction: 1.804553, Regularization: 0.803207, Discriminator: 0.043316; Generator: 0.021635,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:30,446 root         INFO     Train Epoch: 136 [2048/8000 (26%)]\tTotal Loss: 2.800518\n",
      "Reconstruction: 1.878208, Regularization: 0.857342, Discriminator: 0.043318; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:30,554 root         INFO     Train Epoch: 136 [2560/8000 (32%)]\tTotal Loss: 2.890552\n",
      "Reconstruction: 1.925572, Regularization: 0.899999, Discriminator: 0.043352; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:01:30,663 root         INFO     Train Epoch: 136 [3072/8000 (38%)]\tTotal Loss: 2.878580\n",
      "Reconstruction: 1.963490, Regularization: 0.850105, Discriminator: 0.043341; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:30,771 root         INFO     Train Epoch: 136 [3584/8000 (45%)]\tTotal Loss: 2.712228\n",
      "Reconstruction: 1.798929, Regularization: 0.848324, Discriminator: 0.043328; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:30,879 root         INFO     Train Epoch: 136 [4096/8000 (51%)]\tTotal Loss: 2.642936\n",
      "Reconstruction: 1.827173, Regularization: 0.750807, Discriminator: 0.043328; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:01:30,988 root         INFO     Train Epoch: 136 [4608/8000 (58%)]\tTotal Loss: 2.810586\n",
      "Reconstruction: 1.832677, Regularization: 0.912932, Discriminator: 0.043337; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,096 root         INFO     Train Epoch: 136 [5120/8000 (64%)]\tTotal Loss: 2.802294\n",
      "Reconstruction: 1.899589, Regularization: 0.837729, Discriminator: 0.043336; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,205 root         INFO     Train Epoch: 136 [5632/8000 (70%)]\tTotal Loss: 2.499810\n",
      "Reconstruction: 1.715464, Regularization: 0.719392, Discriminator: 0.043306; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,313 root         INFO     Train Epoch: 136 [6144/8000 (77%)]\tTotal Loss: 2.955618\n",
      "Reconstruction: 2.100847, Regularization: 0.789800, Discriminator: 0.043321; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,422 root         INFO     Train Epoch: 136 [6656/8000 (83%)]\tTotal Loss: 2.631755\n",
      "Reconstruction: 1.788488, Regularization: 0.778296, Discriminator: 0.043316; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,530 root         INFO     Train Epoch: 136 [7168/8000 (90%)]\tTotal Loss: 2.846456\n",
      "Reconstruction: 1.916741, Regularization: 0.864751, Discriminator: 0.043310; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,638 root         INFO     Train Epoch: 136 [7680/8000 (96%)]\tTotal Loss: 2.967036\n",
      "Reconstruction: 2.008383, Regularization: 0.893685, Discriminator: 0.043315; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,718 root         INFO     ====> Epoch: 136 Average loss: 2.7577\n",
      "2019-04-10 00:01:31,745 root         INFO     Train Epoch: 137 [0/8000 (0%)]\tTotal Loss: 2.789618\n",
      "Reconstruction: 1.864472, Regularization: 0.860189, Discriminator: 0.043300; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,854 root         INFO     Train Epoch: 137 [512/8000 (6%)]\tTotal Loss: 2.999258\n",
      "Reconstruction: 2.061020, Regularization: 0.873261, Discriminator: 0.043321; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:31,962 root         INFO     Train Epoch: 137 [1024/8000 (13%)]\tTotal Loss: 2.446355\n",
      "Reconstruction: 1.665957, Regularization: 0.715401, Discriminator: 0.043335; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,071 root         INFO     Train Epoch: 137 [1536/8000 (19%)]\tTotal Loss: 3.098135\n",
      "Reconstruction: 2.023161, Regularization: 1.009995, Discriminator: 0.043318; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,179 root         INFO     Train Epoch: 137 [2048/8000 (26%)]\tTotal Loss: 3.022931\n",
      "Reconstruction: 2.047900, Regularization: 0.910040, Discriminator: 0.043330; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,288 root         INFO     Train Epoch: 137 [2560/8000 (32%)]\tTotal Loss: 2.931452\n",
      "Reconstruction: 2.013072, Regularization: 0.853396, Discriminator: 0.043320; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,397 root         INFO     Train Epoch: 137 [3072/8000 (38%)]\tTotal Loss: 3.002142\n",
      "Reconstruction: 2.036019, Regularization: 0.901129, Discriminator: 0.043319; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,505 root         INFO     Train Epoch: 137 [3584/8000 (45%)]\tTotal Loss: 2.971452\n",
      "Reconstruction: 1.939668, Regularization: 0.966799, Discriminator: 0.043314; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,614 root         INFO     Train Epoch: 137 [4096/8000 (51%)]\tTotal Loss: 2.661438\n",
      "Reconstruction: 1.895861, Regularization: 0.700577, Discriminator: 0.043316; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,720 root         INFO     Train Epoch: 137 [4608/8000 (58%)]\tTotal Loss: 3.213696\n",
      "Reconstruction: 2.192657, Regularization: 0.956043, Discriminator: 0.043319; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,826 root         INFO     Train Epoch: 137 [5120/8000 (64%)]\tTotal Loss: 2.682642\n",
      "Reconstruction: 1.774860, Regularization: 0.842806, Discriminator: 0.043294; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:32,931 root         INFO     Train Epoch: 137 [5632/8000 (70%)]\tTotal Loss: 2.787484\n",
      "Reconstruction: 1.895842, Regularization: 0.826660, Discriminator: 0.043306; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,037 root         INFO     Train Epoch: 137 [6144/8000 (77%)]\tTotal Loss: 2.297525\n",
      "Reconstruction: 1.581060, Regularization: 0.651457, Discriminator: 0.043326; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,144 root         INFO     Train Epoch: 137 [6656/8000 (83%)]\tTotal Loss: 2.466932\n",
      "Reconstruction: 1.670475, Regularization: 0.731478, Discriminator: 0.043311; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,250 root         INFO     Train Epoch: 137 [7168/8000 (90%)]\tTotal Loss: 2.657165\n",
      "Reconstruction: 1.777154, Regularization: 0.815038, Discriminator: 0.043324; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,356 root         INFO     Train Epoch: 137 [7680/8000 (96%)]\tTotal Loss: 2.638345\n",
      "Reconstruction: 1.726411, Regularization: 0.846969, Discriminator: 0.043318; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,434 root         INFO     ====> Epoch: 137 Average loss: 2.7524\n",
      "2019-04-10 00:01:33,462 root         INFO     Train Epoch: 138 [0/8000 (0%)]\tTotal Loss: 2.532587\n",
      "Reconstruction: 1.705976, Regularization: 0.761648, Discriminator: 0.043325; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,571 root         INFO     Train Epoch: 138 [512/8000 (6%)]\tTotal Loss: 2.542328\n",
      "Reconstruction: 1.751687, Regularization: 0.725657, Discriminator: 0.043331; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,679 root         INFO     Train Epoch: 138 [1024/8000 (13%)]\tTotal Loss: 2.922151\n",
      "Reconstruction: 1.958419, Regularization: 0.898774, Discriminator: 0.043310; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,787 root         INFO     Train Epoch: 138 [1536/8000 (19%)]\tTotal Loss: 2.750851\n",
      "Reconstruction: 1.908157, Regularization: 0.777723, Discriminator: 0.043329; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:33,896 root         INFO     Train Epoch: 138 [2048/8000 (26%)]\tTotal Loss: 2.755101\n",
      "Reconstruction: 1.860562, Regularization: 0.829577, Discriminator: 0.043306; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,004 root         INFO     Train Epoch: 138 [2560/8000 (32%)]\tTotal Loss: 2.841907\n",
      "Reconstruction: 1.901673, Regularization: 0.875278, Discriminator: 0.043308; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,113 root         INFO     Train Epoch: 138 [3072/8000 (38%)]\tTotal Loss: 3.062900\n",
      "Reconstruction: 2.059236, Regularization: 0.938721, Discriminator: 0.043293; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,219 root         INFO     Train Epoch: 138 [3584/8000 (45%)]\tTotal Loss: 2.845673\n",
      "Reconstruction: 1.913045, Regularization: 0.867666, Discriminator: 0.043300; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,326 root         INFO     Train Epoch: 138 [4096/8000 (51%)]\tTotal Loss: 2.765235\n",
      "Reconstruction: 1.853336, Regularization: 0.846936, Discriminator: 0.043302; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,433 root         INFO     Train Epoch: 138 [4608/8000 (58%)]\tTotal Loss: 3.224684\n",
      "Reconstruction: 2.189352, Regularization: 0.970394, Discriminator: 0.043275; Generator: 0.021663,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,540 root         INFO     Train Epoch: 138 [5120/8000 (64%)]\tTotal Loss: 2.797875\n",
      "Reconstruction: 1.969944, Regularization: 0.762962, Discriminator: 0.043311; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,647 root         INFO     Train Epoch: 138 [5632/8000 (70%)]\tTotal Loss: 3.453487\n",
      "Reconstruction: 2.565121, Regularization: 0.823386, Discriminator: 0.043325; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,755 root         INFO     Train Epoch: 138 [6144/8000 (77%)]\tTotal Loss: 2.645942\n",
      "Reconstruction: 1.805615, Regularization: 0.775283, Discriminator: 0.043380; Generator: 0.021664,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,862 root         INFO     Train Epoch: 138 [6656/8000 (83%)]\tTotal Loss: 3.138350\n",
      "Reconstruction: 2.120369, Regularization: 0.952993, Discriminator: 0.043331; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:34,969 root         INFO     Train Epoch: 138 [7168/8000 (90%)]\tTotal Loss: 2.969085\n",
      "Reconstruction: 2.067234, Regularization: 0.836845, Discriminator: 0.043351; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,076 root         INFO     Train Epoch: 138 [7680/8000 (96%)]\tTotal Loss: 2.906152\n",
      "Reconstruction: 1.974702, Regularization: 0.866429, Discriminator: 0.043351; Generator: 0.021670,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,155 root         INFO     ====> Epoch: 138 Average loss: 2.7940\n",
      "2019-04-10 00:01:35,182 root         INFO     Train Epoch: 139 [0/8000 (0%)]\tTotal Loss: 3.058442\n",
      "Reconstruction: 2.127224, Regularization: 0.866212, Discriminator: 0.043340; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,290 root         INFO     Train Epoch: 139 [512/8000 (6%)]\tTotal Loss: 3.095902\n",
      "Reconstruction: 2.175330, Regularization: 0.855566, Discriminator: 0.043340; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,399 root         INFO     Train Epoch: 139 [1024/8000 (13%)]\tTotal Loss: 2.826526\n",
      "Reconstruction: 1.926557, Regularization: 0.834959, Discriminator: 0.043340; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,507 root         INFO     Train Epoch: 139 [1536/8000 (19%)]\tTotal Loss: 2.623472\n",
      "Reconstruction: 1.751698, Regularization: 0.806768, Discriminator: 0.043329; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,616 root         INFO     Train Epoch: 139 [2048/8000 (26%)]\tTotal Loss: 2.834057\n",
      "Reconstruction: 1.936969, Regularization: 0.832088, Discriminator: 0.043322; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,724 root         INFO     Train Epoch: 139 [2560/8000 (32%)]\tTotal Loss: 2.942002\n",
      "Reconstruction: 2.016259, Regularization: 0.860747, Discriminator: 0.043311; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,832 root         INFO     Train Epoch: 139 [3072/8000 (38%)]\tTotal Loss: 2.741195\n",
      "Reconstruction: 1.912886, Regularization: 0.763292, Discriminator: 0.043325; Generator: 0.021692,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:35,940 root         INFO     Train Epoch: 139 [3584/8000 (45%)]\tTotal Loss: 3.096966\n",
      "Reconstruction: 2.121141, Regularization: 0.910806, Discriminator: 0.043336; Generator: 0.021684,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,048 root         INFO     Train Epoch: 139 [4096/8000 (51%)]\tTotal Loss: 3.200562\n",
      "Reconstruction: 2.207646, Regularization: 0.927939, Discriminator: 0.043294; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,156 root         INFO     Train Epoch: 139 [4608/8000 (58%)]\tTotal Loss: 2.704115\n",
      "Reconstruction: 1.880988, Regularization: 0.758138, Discriminator: 0.043321; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,264 root         INFO     Train Epoch: 139 [5120/8000 (64%)]\tTotal Loss: 2.791172\n",
      "Reconstruction: 1.969020, Regularization: 0.757115, Discriminator: 0.043368; Generator: 0.021669,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,372 root         INFO     Train Epoch: 139 [5632/8000 (70%)]\tTotal Loss: 2.664994\n",
      "Reconstruction: 1.863056, Regularization: 0.736948, Discriminator: 0.043335; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,480 root         INFO     Train Epoch: 139 [6144/8000 (77%)]\tTotal Loss: 2.955282\n",
      "Reconstruction: 2.101638, Regularization: 0.788648, Discriminator: 0.043336; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,588 root         INFO     Train Epoch: 139 [6656/8000 (83%)]\tTotal Loss: 2.411410\n",
      "Reconstruction: 1.641871, Regularization: 0.704580, Discriminator: 0.043314; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,696 root         INFO     Train Epoch: 139 [7168/8000 (90%)]\tTotal Loss: 2.796868\n",
      "Reconstruction: 1.927186, Regularization: 0.804709, Discriminator: 0.043323; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,804 root         INFO     Train Epoch: 139 [7680/8000 (96%)]\tTotal Loss: 2.860345\n",
      "Reconstruction: 1.973211, Regularization: 0.822158, Discriminator: 0.043328; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:36,882 root         INFO     ====> Epoch: 139 Average loss: 2.8361\n",
      "2019-04-10 00:01:36,909 root         INFO     Train Epoch: 140 [0/8000 (0%)]\tTotal Loss: 3.016181\n",
      "Reconstruction: 2.068593, Regularization: 0.882602, Discriminator: 0.043338; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,015 root         INFO     Train Epoch: 140 [512/8000 (6%)]\tTotal Loss: 3.048651\n",
      "Reconstruction: 2.183664, Regularization: 0.800006, Discriminator: 0.043329; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,121 root         INFO     Train Epoch: 140 [1024/8000 (13%)]\tTotal Loss: 3.096291\n",
      "Reconstruction: 2.129592, Regularization: 0.901725, Discriminator: 0.043327; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,228 root         INFO     Train Epoch: 140 [1536/8000 (19%)]\tTotal Loss: 2.681220\n",
      "Reconstruction: 1.903163, Regularization: 0.713106, Discriminator: 0.043303; Generator: 0.021648,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,334 root         INFO     Train Epoch: 140 [2048/8000 (26%)]\tTotal Loss: 2.834893\n",
      "Reconstruction: 1.981397, Regularization: 0.788534, Discriminator: 0.043297; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,440 root         INFO     Train Epoch: 140 [2560/8000 (32%)]\tTotal Loss: 2.636154\n",
      "Reconstruction: 1.809341, Regularization: 0.761844, Discriminator: 0.043313; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,545 root         INFO     Train Epoch: 140 [3072/8000 (38%)]\tTotal Loss: 3.200627\n",
      "Reconstruction: 2.283606, Regularization: 0.852073, Discriminator: 0.043289; Generator: 0.021660,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,651 root         INFO     Train Epoch: 140 [3584/8000 (45%)]\tTotal Loss: 2.699840\n",
      "Reconstruction: 1.801132, Regularization: 0.833726, Discriminator: 0.043313; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,757 root         INFO     Train Epoch: 140 [4096/8000 (51%)]\tTotal Loss: 2.688737\n",
      "Reconstruction: 1.853954, Regularization: 0.769815, Discriminator: 0.043304; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,864 root         INFO     Train Epoch: 140 [4608/8000 (58%)]\tTotal Loss: 2.810610\n",
      "Reconstruction: 1.968568, Regularization: 0.777067, Discriminator: 0.043310; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:37,970 root         INFO     Train Epoch: 140 [5120/8000 (64%)]\tTotal Loss: 3.856983\n",
      "Reconstruction: 3.042321, Regularization: 0.749728, Discriminator: 0.043269; Generator: 0.021665,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,076 root         INFO     Train Epoch: 140 [5632/8000 (70%)]\tTotal Loss: 2.979776\n",
      "Reconstruction: 2.045452, Regularization: 0.869326, Discriminator: 0.043326; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,182 root         INFO     Train Epoch: 140 [6144/8000 (77%)]\tTotal Loss: 3.058779\n",
      "Reconstruction: 2.127535, Regularization: 0.866248, Discriminator: 0.043335; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,288 root         INFO     Train Epoch: 140 [6656/8000 (83%)]\tTotal Loss: 3.056405\n",
      "Reconstruction: 2.151759, Regularization: 0.839637, Discriminator: 0.043347; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,394 root         INFO     Train Epoch: 140 [7168/8000 (90%)]\tTotal Loss: 2.899209\n",
      "Reconstruction: 1.928283, Regularization: 0.905905, Discriminator: 0.043345; Generator: 0.021676,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,500 root         INFO     Train Epoch: 140 [7680/8000 (96%)]\tTotal Loss: 3.144598\n",
      "Reconstruction: 2.443909, Regularization: 0.635701, Discriminator: 0.043314; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,579 root         INFO     ====> Epoch: 140 Average loss: 2.8466\n",
      "2019-04-10 00:01:38,606 root         INFO     Train Epoch: 141 [0/8000 (0%)]\tTotal Loss: 2.396929\n",
      "Reconstruction: 1.634147, Regularization: 0.697777, Discriminator: 0.043331; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,714 root         INFO     Train Epoch: 141 [512/8000 (6%)]\tTotal Loss: 3.058392\n",
      "Reconstruction: 2.175606, Regularization: 0.817773, Discriminator: 0.043317; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:01:38,822 root         INFO     Train Epoch: 141 [1024/8000 (13%)]\tTotal Loss: 2.425096\n",
      "Reconstruction: 1.698169, Regularization: 0.661942, Discriminator: 0.043300; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:38,929 root         INFO     Train Epoch: 141 [1536/8000 (19%)]\tTotal Loss: 2.773676\n",
      "Reconstruction: 1.976092, Regularization: 0.732587, Discriminator: 0.043301; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:01:39,037 root         INFO     Train Epoch: 141 [2048/8000 (26%)]\tTotal Loss: 2.604736\n",
      "Reconstruction: 1.794002, Regularization: 0.745757, Discriminator: 0.043293; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,145 root         INFO     Train Epoch: 141 [2560/8000 (32%)]\tTotal Loss: 2.946112\n",
      "Reconstruction: 2.037519, Regularization: 0.843616, Discriminator: 0.043303; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,253 root         INFO     Train Epoch: 141 [3072/8000 (38%)]\tTotal Loss: 2.918725\n",
      "Reconstruction: 2.075451, Regularization: 0.778285, Discriminator: 0.043319; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,360 root         INFO     Train Epoch: 141 [3584/8000 (45%)]\tTotal Loss: 2.701515\n",
      "Reconstruction: 1.828518, Regularization: 0.808029, Discriminator: 0.043288; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,467 root         INFO     Train Epoch: 141 [4096/8000 (51%)]\tTotal Loss: 3.009097\n",
      "Reconstruction: 2.164423, Regularization: 0.779666, Discriminator: 0.043352; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,575 root         INFO     Train Epoch: 141 [4608/8000 (58%)]\tTotal Loss: 2.849573\n",
      "Reconstruction: 1.942629, Regularization: 0.841957, Discriminator: 0.043327; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,681 root         INFO     Train Epoch: 141 [5120/8000 (64%)]\tTotal Loss: 2.869252\n",
      "Reconstruction: 1.986879, Regularization: 0.817345, Discriminator: 0.043363; Generator: 0.021664,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,789 root         INFO     Train Epoch: 141 [5632/8000 (70%)]\tTotal Loss: 2.373335\n",
      "Reconstruction: 1.659340, Regularization: 0.649033, Discriminator: 0.043326; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:39,898 root         INFO     Train Epoch: 141 [6144/8000 (77%)]\tTotal Loss: 2.745867\n",
      "Reconstruction: 1.873382, Regularization: 0.807511, Discriminator: 0.043340; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,005 root         INFO     Train Epoch: 141 [6656/8000 (83%)]\tTotal Loss: 2.768111\n",
      "Reconstruction: 1.971709, Regularization: 0.731423, Discriminator: 0.043343; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,113 root         INFO     Train Epoch: 141 [7168/8000 (90%)]\tTotal Loss: 2.552486\n",
      "Reconstruction: 1.745847, Regularization: 0.741676, Discriminator: 0.043329; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,221 root         INFO     Train Epoch: 141 [7680/8000 (96%)]\tTotal Loss: 2.801394\n",
      "Reconstruction: 1.917983, Regularization: 0.818447, Discriminator: 0.043341; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:01:40,301 root         INFO     ====> Epoch: 141 Average loss: 2.7877\n",
      "2019-04-10 00:01:40,328 root         INFO     Train Epoch: 142 [0/8000 (0%)]\tTotal Loss: 2.822695\n",
      "Reconstruction: 1.961739, Regularization: 0.795994, Discriminator: 0.043339; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:01:40,437 root         INFO     Train Epoch: 142 [512/8000 (6%)]\tTotal Loss: 2.923774\n",
      "Reconstruction: 1.979970, Regularization: 0.878832, Discriminator: 0.043342; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,546 root         INFO     Train Epoch: 142 [1024/8000 (13%)]\tTotal Loss: 2.774070\n",
      "Reconstruction: 1.932722, Regularization: 0.776391, Discriminator: 0.043325; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,654 root         INFO     Train Epoch: 142 [1536/8000 (19%)]\tTotal Loss: 3.029132\n",
      "Reconstruction: 2.114010, Regularization: 0.850163, Discriminator: 0.043314; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,763 root         INFO     Train Epoch: 142 [2048/8000 (26%)]\tTotal Loss: 2.745245\n",
      "Reconstruction: 1.934641, Regularization: 0.745672, Discriminator: 0.043290; Generator: 0.021642,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,871 root         INFO     Train Epoch: 142 [2560/8000 (32%)]\tTotal Loss: 2.779830\n",
      "Reconstruction: 1.924871, Regularization: 0.789995, Discriminator: 0.043316; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:40,980 root         INFO     Train Epoch: 142 [3072/8000 (38%)]\tTotal Loss: 2.574719\n",
      "Reconstruction: 1.753615, Regularization: 0.756142, Discriminator: 0.043313; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,089 root         INFO     Train Epoch: 142 [3584/8000 (45%)]\tTotal Loss: 2.775705\n",
      "Reconstruction: 1.956303, Regularization: 0.754455, Discriminator: 0.043296; Generator: 0.021651,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,197 root         INFO     Train Epoch: 142 [4096/8000 (51%)]\tTotal Loss: 2.831804\n",
      "Reconstruction: 1.984813, Regularization: 0.782038, Discriminator: 0.043301; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,307 root         INFO     Train Epoch: 142 [4608/8000 (58%)]\tTotal Loss: 2.535428\n",
      "Reconstruction: 1.784516, Regularization: 0.685963, Discriminator: 0.043295; Generator: 0.021653,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,415 root         INFO     Train Epoch: 142 [5120/8000 (64%)]\tTotal Loss: 2.758578\n",
      "Reconstruction: 1.977805, Regularization: 0.715815, Discriminator: 0.043301; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,523 root         INFO     Train Epoch: 142 [5632/8000 (70%)]\tTotal Loss: 3.018251\n",
      "Reconstruction: 2.102075, Regularization: 0.851210, Discriminator: 0.043308; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,631 root         INFO     Train Epoch: 142 [6144/8000 (77%)]\tTotal Loss: 2.746083\n",
      "Reconstruction: 1.878264, Regularization: 0.802838, Discriminator: 0.043323; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,739 root         INFO     Train Epoch: 142 [6656/8000 (83%)]\tTotal Loss: 2.771117\n",
      "Reconstruction: 1.910097, Regularization: 0.796027, Discriminator: 0.043330; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,847 root         INFO     Train Epoch: 142 [7168/8000 (90%)]\tTotal Loss: 2.388848\n",
      "Reconstruction: 1.637731, Regularization: 0.686093, Discriminator: 0.043361; Generator: 0.021663,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:41,955 root         INFO     Train Epoch: 142 [7680/8000 (96%)]\tTotal Loss: 2.788762\n",
      "Reconstruction: 1.881084, Regularization: 0.842699, Discriminator: 0.043315; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,034 root         INFO     ====> Epoch: 142 Average loss: 2.7652\n",
      "2019-04-10 00:01:42,061 root         INFO     Train Epoch: 143 [0/8000 (0%)]\tTotal Loss: 2.432246\n",
      "Reconstruction: 1.704167, Regularization: 0.663077, Discriminator: 0.043341; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,170 root         INFO     Train Epoch: 143 [512/8000 (6%)]\tTotal Loss: 2.680116\n",
      "Reconstruction: 1.798777, Regularization: 0.816346, Discriminator: 0.043322; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,278 root         INFO     Train Epoch: 143 [1024/8000 (13%)]\tTotal Loss: 2.610627\n",
      "Reconstruction: 1.831538, Regularization: 0.714086, Discriminator: 0.043325; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,387 root         INFO     Train Epoch: 143 [1536/8000 (19%)]\tTotal Loss: 2.678017\n",
      "Reconstruction: 1.870897, Regularization: 0.742129, Discriminator: 0.043320; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,495 root         INFO     Train Epoch: 143 [2048/8000 (26%)]\tTotal Loss: 2.754301\n",
      "Reconstruction: 1.852915, Regularization: 0.836395, Discriminator: 0.043302; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,603 root         INFO     Train Epoch: 143 [2560/8000 (32%)]\tTotal Loss: 3.103531\n",
      "Reconstruction: 2.161444, Regularization: 0.877099, Discriminator: 0.043299; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,711 root         INFO     Train Epoch: 143 [3072/8000 (38%)]\tTotal Loss: 2.734812\n",
      "Reconstruction: 1.931262, Regularization: 0.738547, Discriminator: 0.043318; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,820 root         INFO     Train Epoch: 143 [3584/8000 (45%)]\tTotal Loss: 2.880966\n",
      "Reconstruction: 2.036729, Regularization: 0.779250, Discriminator: 0.043306; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:42,928 root         INFO     Train Epoch: 143 [4096/8000 (51%)]\tTotal Loss: 2.697734\n",
      "Reconstruction: 1.882775, Regularization: 0.750001, Discriminator: 0.043270; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,034 root         INFO     Train Epoch: 143 [4608/8000 (58%)]\tTotal Loss: 2.632199\n",
      "Reconstruction: 1.801949, Regularization: 0.765290, Discriminator: 0.043277; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,142 root         INFO     Train Epoch: 143 [5120/8000 (64%)]\tTotal Loss: 2.715317\n",
      "Reconstruction: 1.942789, Regularization: 0.707527, Discriminator: 0.043328; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,250 root         INFO     Train Epoch: 143 [5632/8000 (70%)]\tTotal Loss: 2.558926\n",
      "Reconstruction: 1.756014, Regularization: 0.737942, Discriminator: 0.043321; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,358 root         INFO     Train Epoch: 143 [6144/8000 (77%)]\tTotal Loss: 2.894389\n",
      "Reconstruction: 2.046223, Regularization: 0.783189, Discriminator: 0.043333; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,466 root         INFO     Train Epoch: 143 [6656/8000 (83%)]\tTotal Loss: 3.129031\n",
      "Reconstruction: 2.163296, Regularization: 0.900798, Discriminator: 0.043298; Generator: 0.021639,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,574 root         INFO     Train Epoch: 143 [7168/8000 (90%)]\tTotal Loss: 2.859051\n",
      "Reconstruction: 2.031791, Regularization: 0.762264, Discriminator: 0.043359; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,683 root         INFO     Train Epoch: 143 [7680/8000 (96%)]\tTotal Loss: 2.947381\n",
      "Reconstruction: 2.020109, Regularization: 0.862310, Discriminator: 0.043321; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,762 root         INFO     ====> Epoch: 143 Average loss: 2.7858\n",
      "2019-04-10 00:01:43,789 root         INFO     Train Epoch: 144 [0/8000 (0%)]\tTotal Loss: 2.585450\n",
      "Reconstruction: 1.715739, Regularization: 0.804742, Discriminator: 0.043321; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:43,898 root         INFO     Train Epoch: 144 [512/8000 (6%)]\tTotal Loss: 3.151681\n",
      "Reconstruction: 2.172919, Regularization: 0.913801, Discriminator: 0.043331; Generator: 0.021631,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,007 root         INFO     Train Epoch: 144 [1024/8000 (13%)]\tTotal Loss: 2.839649\n",
      "Reconstruction: 2.003688, Regularization: 0.770995, Discriminator: 0.043329; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,116 root         INFO     Train Epoch: 144 [1536/8000 (19%)]\tTotal Loss: 2.669449\n",
      "Reconstruction: 1.846586, Regularization: 0.757889, Discriminator: 0.043326; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,225 root         INFO     Train Epoch: 144 [2048/8000 (26%)]\tTotal Loss: 2.805726\n",
      "Reconstruction: 1.957315, Regularization: 0.783439, Discriminator: 0.043323; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,333 root         INFO     Train Epoch: 144 [2560/8000 (32%)]\tTotal Loss: 2.560774\n",
      "Reconstruction: 1.814158, Regularization: 0.681642, Discriminator: 0.043322; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,442 root         INFO     Train Epoch: 144 [3072/8000 (38%)]\tTotal Loss: 2.625207\n",
      "Reconstruction: 1.834265, Regularization: 0.725958, Discriminator: 0.043331; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,550 root         INFO     Train Epoch: 144 [3584/8000 (45%)]\tTotal Loss: 2.659449\n",
      "Reconstruction: 1.835990, Regularization: 0.758481, Discriminator: 0.043324; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,659 root         INFO     Train Epoch: 144 [4096/8000 (51%)]\tTotal Loss: 2.461563\n",
      "Reconstruction: 1.660873, Regularization: 0.735713, Discriminator: 0.043323; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,768 root         INFO     Train Epoch: 144 [4608/8000 (58%)]\tTotal Loss: 2.894238\n",
      "Reconstruction: 1.991403, Regularization: 0.837878, Discriminator: 0.043299; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,877 root         INFO     Train Epoch: 144 [5120/8000 (64%)]\tTotal Loss: 2.613232\n",
      "Reconstruction: 1.835175, Regularization: 0.713043, Discriminator: 0.043356; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:44,986 root         INFO     Train Epoch: 144 [5632/8000 (70%)]\tTotal Loss: 2.693333\n",
      "Reconstruction: 1.809297, Regularization: 0.819031, Discriminator: 0.043345; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,094 root         INFO     Train Epoch: 144 [6144/8000 (77%)]\tTotal Loss: 2.904391\n",
      "Reconstruction: 2.015220, Regularization: 0.824185, Discriminator: 0.043327; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,203 root         INFO     Train Epoch: 144 [6656/8000 (83%)]\tTotal Loss: 2.804509\n",
      "Reconstruction: 1.962095, Regularization: 0.777405, Discriminator: 0.043346; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,312 root         INFO     Train Epoch: 144 [7168/8000 (90%)]\tTotal Loss: 2.578740\n",
      "Reconstruction: 1.839032, Regularization: 0.674692, Discriminator: 0.043352; Generator: 0.021664,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,420 root         INFO     Train Epoch: 144 [7680/8000 (96%)]\tTotal Loss: 2.628668\n",
      "Reconstruction: 1.764549, Regularization: 0.799103, Discriminator: 0.043348; Generator: 0.021668,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,500 root         INFO     ====> Epoch: 144 Average loss: 2.7978\n",
      "2019-04-10 00:01:45,527 root         INFO     Train Epoch: 145 [0/8000 (0%)]\tTotal Loss: 2.990027\n",
      "Reconstruction: 2.049186, Regularization: 0.875829, Discriminator: 0.043338; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,635 root         INFO     Train Epoch: 145 [512/8000 (6%)]\tTotal Loss: 2.675705\n",
      "Reconstruction: 1.836552, Regularization: 0.774150, Discriminator: 0.043328; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,743 root         INFO     Train Epoch: 145 [1024/8000 (13%)]\tTotal Loss: 2.666896\n",
      "Reconstruction: 1.913863, Regularization: 0.688029, Discriminator: 0.043330; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,851 root         INFO     Train Epoch: 145 [1536/8000 (19%)]\tTotal Loss: 2.961478\n",
      "Reconstruction: 2.052459, Regularization: 0.844015, Discriminator: 0.043317; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:45,959 root         INFO     Train Epoch: 145 [2048/8000 (26%)]\tTotal Loss: 2.783775\n",
      "Reconstruction: 1.917809, Regularization: 0.800972, Discriminator: 0.043312; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,067 root         INFO     Train Epoch: 145 [2560/8000 (32%)]\tTotal Loss: 2.407738\n",
      "Reconstruction: 1.630436, Regularization: 0.712322, Discriminator: 0.043288; Generator: 0.021692,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,175 root         INFO     Train Epoch: 145 [3072/8000 (38%)]\tTotal Loss: 2.960717\n",
      "Reconstruction: 2.110745, Regularization: 0.784975, Discriminator: 0.043314; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,283 root         INFO     Train Epoch: 145 [3584/8000 (45%)]\tTotal Loss: 3.187538\n",
      "Reconstruction: 2.261669, Regularization: 0.860853, Discriminator: 0.043327; Generator: 0.021689,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,391 root         INFO     Train Epoch: 145 [4096/8000 (51%)]\tTotal Loss: 3.362585\n",
      "Reconstruction: 2.363713, Regularization: 0.933839, Discriminator: 0.043347; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,499 root         INFO     Train Epoch: 145 [4608/8000 (58%)]\tTotal Loss: 2.391565\n",
      "Reconstruction: 1.588709, Regularization: 0.737866, Discriminator: 0.043313; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,607 root         INFO     Train Epoch: 145 [5120/8000 (64%)]\tTotal Loss: 2.870119\n",
      "Reconstruction: 2.028320, Regularization: 0.776816, Discriminator: 0.043308; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,715 root         INFO     Train Epoch: 145 [5632/8000 (70%)]\tTotal Loss: 3.007603\n",
      "Reconstruction: 2.041013, Regularization: 0.901609, Discriminator: 0.043317; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,823 root         INFO     Train Epoch: 145 [6144/8000 (77%)]\tTotal Loss: 3.540600\n",
      "Reconstruction: 2.544048, Regularization: 0.931513, Discriminator: 0.043386; Generator: 0.021653,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:46,930 root         INFO     Train Epoch: 145 [6656/8000 (83%)]\tTotal Loss: 2.833696\n",
      "Reconstruction: 2.001698, Regularization: 0.767016, Discriminator: 0.043330; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,037 root         INFO     Train Epoch: 145 [7168/8000 (90%)]\tTotal Loss: 2.416090\n",
      "Reconstruction: 1.734024, Regularization: 0.617111, Discriminator: 0.043306; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,145 root         INFO     Train Epoch: 145 [7680/8000 (96%)]\tTotal Loss: 2.923326\n",
      "Reconstruction: 2.100358, Regularization: 0.758006, Discriminator: 0.043322; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,223 root         INFO     ====> Epoch: 145 Average loss: 2.8390\n",
      "2019-04-10 00:01:47,251 root         INFO     Train Epoch: 146 [0/8000 (0%)]\tTotal Loss: 3.064082\n",
      "Reconstruction: 2.176752, Regularization: 0.822352, Discriminator: 0.043333; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,359 root         INFO     Train Epoch: 146 [512/8000 (6%)]\tTotal Loss: 2.882102\n",
      "Reconstruction: 2.062157, Regularization: 0.754981, Discriminator: 0.043310; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,467 root         INFO     Train Epoch: 146 [1024/8000 (13%)]\tTotal Loss: 2.698744\n",
      "Reconstruction: 1.906839, Regularization: 0.726938, Discriminator: 0.043311; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,575 root         INFO     Train Epoch: 146 [1536/8000 (19%)]\tTotal Loss: 3.081524\n",
      "Reconstruction: 2.155659, Regularization: 0.860885, Discriminator: 0.043326; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,683 root         INFO     Train Epoch: 146 [2048/8000 (26%)]\tTotal Loss: 2.560658\n",
      "Reconstruction: 1.800673, Regularization: 0.695033, Discriminator: 0.043302; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,792 root         INFO     Train Epoch: 146 [2560/8000 (32%)]\tTotal Loss: 2.809519\n",
      "Reconstruction: 1.996554, Regularization: 0.748003, Discriminator: 0.043308; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:47,906 root         INFO     Train Epoch: 146 [3072/8000 (38%)]\tTotal Loss: 2.575913\n",
      "Reconstruction: 1.802765, Regularization: 0.708181, Discriminator: 0.043305; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,017 root         INFO     Train Epoch: 146 [3584/8000 (45%)]\tTotal Loss: 2.866275\n",
      "Reconstruction: 2.000553, Regularization: 0.800772, Discriminator: 0.043291; Generator: 0.021659,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,129 root         INFO     Train Epoch: 146 [4096/8000 (51%)]\tTotal Loss: 2.927458\n",
      "Reconstruction: 2.070277, Regularization: 0.792229, Discriminator: 0.043298; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,241 root         INFO     Train Epoch: 146 [4608/8000 (58%)]\tTotal Loss: 3.240131\n",
      "Reconstruction: 2.389955, Regularization: 0.785199, Discriminator: 0.043344; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,352 root         INFO     Train Epoch: 146 [5120/8000 (64%)]\tTotal Loss: 2.657346\n",
      "Reconstruction: 1.879410, Regularization: 0.712954, Discriminator: 0.043322; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,462 root         INFO     Train Epoch: 146 [5632/8000 (70%)]\tTotal Loss: 2.677393\n",
      "Reconstruction: 1.874025, Regularization: 0.738403, Discriminator: 0.043296; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,571 root         INFO     Train Epoch: 146 [6144/8000 (77%)]\tTotal Loss: 3.015271\n",
      "Reconstruction: 2.091827, Regularization: 0.858431, Discriminator: 0.043356; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,681 root         INFO     Train Epoch: 146 [6656/8000 (83%)]\tTotal Loss: 2.675657\n",
      "Reconstruction: 1.882098, Regularization: 0.728569, Discriminator: 0.043330; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,791 root         INFO     Train Epoch: 146 [7168/8000 (90%)]\tTotal Loss: 2.803226\n",
      "Reconstruction: 1.951920, Regularization: 0.786323, Discriminator: 0.043326; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,901 root         INFO     Train Epoch: 146 [7680/8000 (96%)]\tTotal Loss: 2.894899\n",
      "Reconstruction: 2.093474, Regularization: 0.736451, Discriminator: 0.043318; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:48,981 root         INFO     ====> Epoch: 146 Average loss: 2.8099\n",
      "2019-04-10 00:01:49,008 root         INFO     Train Epoch: 147 [0/8000 (0%)]\tTotal Loss: 2.601894\n",
      "Reconstruction: 1.864062, Regularization: 0.672844, Discriminator: 0.043310; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,120 root         INFO     Train Epoch: 147 [512/8000 (6%)]\tTotal Loss: 2.809059\n",
      "Reconstruction: 1.994158, Regularization: 0.749910, Discriminator: 0.043315; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,231 root         INFO     Train Epoch: 147 [1024/8000 (13%)]\tTotal Loss: 2.921173\n",
      "Reconstruction: 2.032011, Regularization: 0.824150, Discriminator: 0.043333; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,341 root         INFO     Train Epoch: 147 [1536/8000 (19%)]\tTotal Loss: 3.045850\n",
      "Reconstruction: 2.116492, Regularization: 0.864339, Discriminator: 0.043345; Generator: 0.021674,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,449 root         INFO     Train Epoch: 147 [2048/8000 (26%)]\tTotal Loss: 3.122294\n",
      "Reconstruction: 2.200238, Regularization: 0.857048, Discriminator: 0.043333; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,556 root         INFO     Train Epoch: 147 [2560/8000 (32%)]\tTotal Loss: 3.325236\n",
      "Reconstruction: 2.333105, Regularization: 0.927116, Discriminator: 0.043345; Generator: 0.021670,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,664 root         INFO     Train Epoch: 147 [3072/8000 (38%)]\tTotal Loss: 2.903563\n",
      "Reconstruction: 2.066905, Regularization: 0.771672, Discriminator: 0.043317; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,773 root         INFO     Train Epoch: 147 [3584/8000 (45%)]\tTotal Loss: 2.585570\n",
      "Reconstruction: 1.860448, Regularization: 0.660143, Discriminator: 0.043315; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,880 root         INFO     Train Epoch: 147 [4096/8000 (51%)]\tTotal Loss: 2.639840\n",
      "Reconstruction: 1.833099, Regularization: 0.741746, Discriminator: 0.043327; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:49,988 root         INFO     Train Epoch: 147 [4608/8000 (58%)]\tTotal Loss: 2.798946\n",
      "Reconstruction: 1.929557, Regularization: 0.804402, Discriminator: 0.043321; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,095 root         INFO     Train Epoch: 147 [5120/8000 (64%)]\tTotal Loss: 2.671942\n",
      "Reconstruction: 1.869054, Regularization: 0.737929, Discriminator: 0.043304; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,204 root         INFO     Train Epoch: 147 [5632/8000 (70%)]\tTotal Loss: 2.776657\n",
      "Reconstruction: 1.967434, Regularization: 0.744240, Discriminator: 0.043339; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,312 root         INFO     Train Epoch: 147 [6144/8000 (77%)]\tTotal Loss: 2.634517\n",
      "Reconstruction: 1.789277, Regularization: 0.780282, Discriminator: 0.043316; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,420 root         INFO     Train Epoch: 147 [6656/8000 (83%)]\tTotal Loss: 2.574466\n",
      "Reconstruction: 1.804983, Regularization: 0.704539, Discriminator: 0.043297; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,529 root         INFO     Train Epoch: 147 [7168/8000 (90%)]\tTotal Loss: 3.041492\n",
      "Reconstruction: 2.143124, Regularization: 0.833383, Discriminator: 0.043338; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,638 root         INFO     Train Epoch: 147 [7680/8000 (96%)]\tTotal Loss: 2.974894\n",
      "Reconstruction: 2.065455, Regularization: 0.844473, Discriminator: 0.043331; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,718 root         INFO     ====> Epoch: 147 Average loss: 2.8049\n",
      "2019-04-10 00:01:50,745 root         INFO     Train Epoch: 148 [0/8000 (0%)]\tTotal Loss: 2.332959\n",
      "Reconstruction: 1.596742, Regularization: 0.671242, Discriminator: 0.043335; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,855 root         INFO     Train Epoch: 148 [512/8000 (6%)]\tTotal Loss: 2.714910\n",
      "Reconstruction: 1.916930, Regularization: 0.733015, Discriminator: 0.043330; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:50,965 root         INFO     Train Epoch: 148 [1024/8000 (13%)]\tTotal Loss: 2.764289\n",
      "Reconstruction: 1.842574, Regularization: 0.856757, Discriminator: 0.043316; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,074 root         INFO     Train Epoch: 148 [1536/8000 (19%)]\tTotal Loss: 2.492770\n",
      "Reconstruction: 1.793977, Regularization: 0.633821, Discriminator: 0.043327; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,184 root         INFO     Train Epoch: 148 [2048/8000 (26%)]\tTotal Loss: 2.849808\n",
      "Reconstruction: 1.985446, Regularization: 0.799399, Discriminator: 0.043312; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,293 root         INFO     Train Epoch: 148 [2560/8000 (32%)]\tTotal Loss: 2.651473\n",
      "Reconstruction: 1.833355, Regularization: 0.753157, Discriminator: 0.043312; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,402 root         INFO     Train Epoch: 148 [3072/8000 (38%)]\tTotal Loss: 2.442970\n",
      "Reconstruction: 1.653634, Regularization: 0.724368, Discriminator: 0.043316; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,510 root         INFO     Train Epoch: 148 [3584/8000 (45%)]\tTotal Loss: 2.872152\n",
      "Reconstruction: 2.021410, Regularization: 0.785775, Discriminator: 0.043311; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,619 root         INFO     Train Epoch: 148 [4096/8000 (51%)]\tTotal Loss: 2.786747\n",
      "Reconstruction: 2.001185, Regularization: 0.720585, Discriminator: 0.043321; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,728 root         INFO     Train Epoch: 148 [4608/8000 (58%)]\tTotal Loss: 2.972147\n",
      "Reconstruction: 2.040291, Regularization: 0.866897, Discriminator: 0.043302; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,836 root         INFO     Train Epoch: 148 [5120/8000 (64%)]\tTotal Loss: 2.704557\n",
      "Reconstruction: 1.884134, Regularization: 0.755470, Discriminator: 0.043293; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:51,945 root         INFO     Train Epoch: 148 [5632/8000 (70%)]\tTotal Loss: 2.852731\n",
      "Reconstruction: 2.098451, Regularization: 0.689331, Discriminator: 0.043287; Generator: 0.021661,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,054 root         INFO     Train Epoch: 148 [6144/8000 (77%)]\tTotal Loss: 2.749367\n",
      "Reconstruction: 1.886239, Regularization: 0.798120, Discriminator: 0.043341; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,162 root         INFO     Train Epoch: 148 [6656/8000 (83%)]\tTotal Loss: 2.907104\n",
      "Reconstruction: 2.054530, Regularization: 0.787609, Discriminator: 0.043304; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,270 root         INFO     Train Epoch: 148 [7168/8000 (90%)]\tTotal Loss: 2.912555\n",
      "Reconstruction: 2.054934, Regularization: 0.792642, Discriminator: 0.043316; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,378 root         INFO     Train Epoch: 148 [7680/8000 (96%)]\tTotal Loss: 2.847175\n",
      "Reconstruction: 1.995389, Regularization: 0.786799, Discriminator: 0.043321; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,457 root         INFO     ====> Epoch: 148 Average loss: 2.7968\n",
      "2019-04-10 00:01:52,484 root         INFO     Train Epoch: 149 [0/8000 (0%)]\tTotal Loss: 2.431620\n",
      "Reconstruction: 1.660690, Regularization: 0.705925, Discriminator: 0.043339; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,596 root         INFO     Train Epoch: 149 [512/8000 (6%)]\tTotal Loss: 2.622051\n",
      "Reconstruction: 1.818815, Regularization: 0.738245, Discriminator: 0.043331; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,708 root         INFO     Train Epoch: 149 [1024/8000 (13%)]\tTotal Loss: 2.836862\n",
      "Reconstruction: 2.043711, Regularization: 0.728157, Discriminator: 0.043317; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,818 root         INFO     Train Epoch: 149 [1536/8000 (19%)]\tTotal Loss: 2.408746\n",
      "Reconstruction: 1.695443, Regularization: 0.648292, Discriminator: 0.043328; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:52,928 root         INFO     Train Epoch: 149 [2048/8000 (26%)]\tTotal Loss: 2.736587\n",
      "Reconstruction: 1.894594, Regularization: 0.776995, Discriminator: 0.043314; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,038 root         INFO     Train Epoch: 149 [2560/8000 (32%)]\tTotal Loss: 2.727732\n",
      "Reconstruction: 1.950367, Regularization: 0.712373, Discriminator: 0.043320; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,147 root         INFO     Train Epoch: 149 [3072/8000 (38%)]\tTotal Loss: 2.660956\n",
      "Reconstruction: 1.840457, Regularization: 0.755514, Discriminator: 0.043311; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,256 root         INFO     Train Epoch: 149 [3584/8000 (45%)]\tTotal Loss: 2.500233\n",
      "Reconstruction: 1.791576, Regularization: 0.643644, Discriminator: 0.043345; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,365 root         INFO     Train Epoch: 149 [4096/8000 (51%)]\tTotal Loss: 2.628523\n",
      "Reconstruction: 1.806674, Regularization: 0.756858, Discriminator: 0.043318; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,475 root         INFO     Train Epoch: 149 [4608/8000 (58%)]\tTotal Loss: 3.119866\n",
      "Reconstruction: 2.184466, Regularization: 0.870443, Discriminator: 0.043288; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,584 root         INFO     Train Epoch: 149 [5120/8000 (64%)]\tTotal Loss: 3.025826\n",
      "Reconstruction: 2.124660, Regularization: 0.836138, Discriminator: 0.043343; Generator: 0.021685,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,693 root         INFO     Train Epoch: 149 [5632/8000 (70%)]\tTotal Loss: 2.862621\n",
      "Reconstruction: 2.019097, Regularization: 0.778536, Discriminator: 0.043331; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,802 root         INFO     Train Epoch: 149 [6144/8000 (77%)]\tTotal Loss: 2.921293\n",
      "Reconstruction: 2.080341, Regularization: 0.775977, Discriminator: 0.043324; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:53,912 root         INFO     Train Epoch: 149 [6656/8000 (83%)]\tTotal Loss: 3.147212\n",
      "Reconstruction: 2.216425, Regularization: 0.865813, Discriminator: 0.043329; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:54,021 root         INFO     Train Epoch: 149 [7168/8000 (90%)]\tTotal Loss: 2.682254\n",
      "Reconstruction: 1.855833, Regularization: 0.761439, Discriminator: 0.043353; Generator: 0.021629,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:01:54,131 root         INFO     Train Epoch: 149 [7680/8000 (96%)]\tTotal Loss: 2.575654\n",
      "Reconstruction: 1.770915, Regularization: 0.739769, Discriminator: 0.043337; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:54,210 root         INFO     ====> Epoch: 149 Average loss: 2.8433\n",
      "2019-04-10 00:01:54,237 root         INFO     Train Epoch: 150 [0/8000 (0%)]\tTotal Loss: 2.874126\n",
      "Reconstruction: 2.105191, Regularization: 0.703943, Discriminator: 0.043368; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:01:54,349 root         INFO     Train Epoch: 150 [512/8000 (6%)]\tTotal Loss: 2.981302\n",
      "Reconstruction: 2.105609, Regularization: 0.810727, Discriminator: 0.043326; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:54,459 root         INFO     Train Epoch: 150 [1024/8000 (13%)]\tTotal Loss: 2.950514\n",
      "Reconstruction: 2.097940, Regularization: 0.787608, Discriminator: 0.043328; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:54,570 root         INFO     Train Epoch: 150 [1536/8000 (19%)]\tTotal Loss: 3.631674\n",
      "Reconstruction: 2.735986, Regularization: 0.830715, Discriminator: 0.043320; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:54,681 root         INFO     Train Epoch: 150 [2048/8000 (26%)]\tTotal Loss: 2.908477\n",
      "Reconstruction: 2.116055, Regularization: 0.727453, Discriminator: 0.043319; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:54,792 root         INFO     Train Epoch: 150 [2560/8000 (32%)]\tTotal Loss: 3.261307\n",
      "Reconstruction: 2.338266, Regularization: 0.858068, Discriminator: 0.043319; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:54,902 root         INFO     Train Epoch: 150 [3072/8000 (38%)]\tTotal Loss: 2.784064\n",
      "Reconstruction: 1.987658, Regularization: 0.731427, Discriminator: 0.043321; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,013 root         INFO     Train Epoch: 150 [3584/8000 (45%)]\tTotal Loss: 2.876849\n",
      "Reconstruction: 2.020318, Regularization: 0.791563, Discriminator: 0.043310; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,122 root         INFO     Train Epoch: 150 [4096/8000 (51%)]\tTotal Loss: 2.485108\n",
      "Reconstruction: 1.747643, Regularization: 0.672475, Discriminator: 0.043329; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,232 root         INFO     Train Epoch: 150 [4608/8000 (58%)]\tTotal Loss: 3.470476\n",
      "Reconstruction: 2.611098, Regularization: 0.794409, Discriminator: 0.043309; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,342 root         INFO     Train Epoch: 150 [5120/8000 (64%)]\tTotal Loss: 3.011760\n",
      "Reconstruction: 2.117868, Regularization: 0.828890, Discriminator: 0.043338; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,452 root         INFO     Train Epoch: 150 [5632/8000 (70%)]\tTotal Loss: 2.838371\n",
      "Reconstruction: 2.059942, Regularization: 0.713438, Discriminator: 0.043324; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,562 root         INFO     Train Epoch: 150 [6144/8000 (77%)]\tTotal Loss: 2.682360\n",
      "Reconstruction: 1.932083, Regularization: 0.685303, Discriminator: 0.043304; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,671 root         INFO     Train Epoch: 150 [6656/8000 (83%)]\tTotal Loss: 2.812757\n",
      "Reconstruction: 2.053401, Regularization: 0.694352, Discriminator: 0.043337; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,781 root         INFO     Train Epoch: 150 [7168/8000 (90%)]\tTotal Loss: 3.148674\n",
      "Reconstruction: 2.435759, Regularization: 0.647921, Discriminator: 0.043323; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,890 root         INFO     Train Epoch: 150 [7680/8000 (96%)]\tTotal Loss: 2.603488\n",
      "Reconstruction: 1.839682, Regularization: 0.698805, Discriminator: 0.043326; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:55,970 root         INFO     ====> Epoch: 150 Average loss: 2.8830\n",
      "2019-04-10 00:01:55,997 root         INFO     Train Epoch: 151 [0/8000 (0%)]\tTotal Loss: 3.033219\n",
      "Reconstruction: 2.192838, Regularization: 0.775378, Discriminator: 0.043326; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,110 root         INFO     Train Epoch: 151 [512/8000 (6%)]\tTotal Loss: 3.091450\n",
      "Reconstruction: 2.183817, Regularization: 0.842620, Discriminator: 0.043327; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,221 root         INFO     Train Epoch: 151 [1024/8000 (13%)]\tTotal Loss: 2.576601\n",
      "Reconstruction: 1.855623, Regularization: 0.655984, Discriminator: 0.043307; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,332 root         INFO     Train Epoch: 151 [1536/8000 (19%)]\tTotal Loss: 2.692181\n",
      "Reconstruction: 1.957075, Regularization: 0.670112, Discriminator: 0.043311; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,441 root         INFO     Train Epoch: 151 [2048/8000 (26%)]\tTotal Loss: 2.784399\n",
      "Reconstruction: 2.007055, Regularization: 0.712341, Discriminator: 0.043319; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,550 root         INFO     Train Epoch: 151 [2560/8000 (32%)]\tTotal Loss: 2.675483\n",
      "Reconstruction: 1.854759, Regularization: 0.755739, Discriminator: 0.043302; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,659 root         INFO     Train Epoch: 151 [3072/8000 (38%)]\tTotal Loss: 2.717064\n",
      "Reconstruction: 1.926703, Regularization: 0.725369, Discriminator: 0.043297; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:01:56,768 root         INFO     Train Epoch: 151 [3584/8000 (45%)]\tTotal Loss: 2.829922\n",
      "Reconstruction: 2.040137, Regularization: 0.724780, Discriminator: 0.043318; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,876 root         INFO     Train Epoch: 151 [4096/8000 (51%)]\tTotal Loss: 2.923387\n",
      "Reconstruction: 2.114172, Regularization: 0.744203, Discriminator: 0.043333; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:56,985 root         INFO     Train Epoch: 151 [4608/8000 (58%)]\tTotal Loss: 3.037011\n",
      "Reconstruction: 2.221680, Regularization: 0.750328, Discriminator: 0.043332; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,094 root         INFO     Train Epoch: 151 [5120/8000 (64%)]\tTotal Loss: 2.676306\n",
      "Reconstruction: 1.946409, Regularization: 0.664937, Discriminator: 0.043315; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,203 root         INFO     Train Epoch: 151 [5632/8000 (70%)]\tTotal Loss: 2.758210\n",
      "Reconstruction: 1.979904, Regularization: 0.713334, Discriminator: 0.043321; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,312 root         INFO     Train Epoch: 151 [6144/8000 (77%)]\tTotal Loss: 2.796185\n",
      "Reconstruction: 2.001002, Regularization: 0.730203, Discriminator: 0.043328; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,420 root         INFO     Train Epoch: 151 [6656/8000 (83%)]\tTotal Loss: 2.670561\n",
      "Reconstruction: 1.934576, Regularization: 0.671019, Discriminator: 0.043314; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,529 root         INFO     Train Epoch: 151 [7168/8000 (90%)]\tTotal Loss: 2.853102\n",
      "Reconstruction: 2.049642, Regularization: 0.738488, Discriminator: 0.043332; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,638 root         INFO     Train Epoch: 151 [7680/8000 (96%)]\tTotal Loss: 2.588901\n",
      "Reconstruction: 1.855603, Regularization: 0.668336, Discriminator: 0.043313; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,717 root         INFO     ====> Epoch: 151 Average loss: 2.8585\n",
      "2019-04-10 00:01:57,744 root         INFO     Train Epoch: 152 [0/8000 (0%)]\tTotal Loss: 3.241116\n",
      "Reconstruction: 2.344963, Regularization: 0.831165, Discriminator: 0.043336; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,855 root         INFO     Train Epoch: 152 [512/8000 (6%)]\tTotal Loss: 2.719105\n",
      "Reconstruction: 1.935690, Regularization: 0.718442, Discriminator: 0.043320; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:57,965 root         INFO     Train Epoch: 152 [1024/8000 (13%)]\tTotal Loss: 2.954903\n",
      "Reconstruction: 2.150346, Regularization: 0.739593, Discriminator: 0.043319; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,075 root         INFO     Train Epoch: 152 [1536/8000 (19%)]\tTotal Loss: 3.070673\n",
      "Reconstruction: 2.196563, Regularization: 0.809142, Discriminator: 0.043323; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,185 root         INFO     Train Epoch: 152 [2048/8000 (26%)]\tTotal Loss: 2.980818\n",
      "Reconstruction: 2.117045, Regularization: 0.798802, Discriminator: 0.043330; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,294 root         INFO     Train Epoch: 152 [2560/8000 (32%)]\tTotal Loss: 3.063650\n",
      "Reconstruction: 2.196403, Regularization: 0.802265, Discriminator: 0.043331; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,404 root         INFO     Train Epoch: 152 [3072/8000 (38%)]\tTotal Loss: 2.927068\n",
      "Reconstruction: 1.998963, Regularization: 0.863110, Discriminator: 0.043349; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,513 root         INFO     Train Epoch: 152 [3584/8000 (45%)]\tTotal Loss: 2.498893\n",
      "Reconstruction: 1.839074, Regularization: 0.594857, Discriminator: 0.043309; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,622 root         INFO     Train Epoch: 152 [4096/8000 (51%)]\tTotal Loss: 3.205416\n",
      "Reconstruction: 2.313301, Regularization: 0.827143, Discriminator: 0.043315; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,732 root         INFO     Train Epoch: 152 [4608/8000 (58%)]\tTotal Loss: 2.725397\n",
      "Reconstruction: 1.960738, Regularization: 0.699684, Discriminator: 0.043334; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,842 root         INFO     Train Epoch: 152 [5120/8000 (64%)]\tTotal Loss: 2.579948\n",
      "Reconstruction: 1.825761, Regularization: 0.689211, Discriminator: 0.043325; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:58,951 root         INFO     Train Epoch: 152 [5632/8000 (70%)]\tTotal Loss: 3.095860\n",
      "Reconstruction: 2.268555, Regularization: 0.762325, Discriminator: 0.043323; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,061 root         INFO     Train Epoch: 152 [6144/8000 (77%)]\tTotal Loss: 2.767262\n",
      "Reconstruction: 1.997155, Regularization: 0.705096, Discriminator: 0.043351; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,170 root         INFO     Train Epoch: 152 [6656/8000 (83%)]\tTotal Loss: 2.738781\n",
      "Reconstruction: 1.976016, Regularization: 0.697759, Discriminator: 0.043341; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,280 root         INFO     Train Epoch: 152 [7168/8000 (90%)]\tTotal Loss: 3.042416\n",
      "Reconstruction: 2.167120, Regularization: 0.810298, Discriminator: 0.043339; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,390 root         INFO     Train Epoch: 152 [7680/8000 (96%)]\tTotal Loss: 2.805551\n",
      "Reconstruction: 2.014075, Regularization: 0.726470, Discriminator: 0.043346; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,471 root         INFO     ====> Epoch: 152 Average loss: 2.8541\n",
      "2019-04-10 00:01:59,498 root         INFO     Train Epoch: 153 [0/8000 (0%)]\tTotal Loss: 2.994118\n",
      "Reconstruction: 2.173065, Regularization: 0.756047, Discriminator: 0.043332; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,611 root         INFO     Train Epoch: 153 [512/8000 (6%)]\tTotal Loss: 2.907758\n",
      "Reconstruction: 2.131042, Regularization: 0.711721, Discriminator: 0.043330; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,722 root         INFO     Train Epoch: 153 [1024/8000 (13%)]\tTotal Loss: 2.688758\n",
      "Reconstruction: 1.936493, Regularization: 0.687281, Discriminator: 0.043319; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,833 root         INFO     Train Epoch: 153 [1536/8000 (19%)]\tTotal Loss: 2.860802\n",
      "Reconstruction: 2.080479, Regularization: 0.715331, Discriminator: 0.043312; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:01:59,944 root         INFO     Train Epoch: 153 [2048/8000 (26%)]\tTotal Loss: 2.799402\n",
      "Reconstruction: 1.992530, Regularization: 0.741880, Discriminator: 0.043317; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,054 root         INFO     Train Epoch: 153 [2560/8000 (32%)]\tTotal Loss: 2.745297\n",
      "Reconstruction: 1.940597, Regularization: 0.739704, Discriminator: 0.043319; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,165 root         INFO     Train Epoch: 153 [3072/8000 (38%)]\tTotal Loss: 2.696671\n",
      "Reconstruction: 1.941784, Regularization: 0.689923, Discriminator: 0.043292; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,275 root         INFO     Train Epoch: 153 [3584/8000 (45%)]\tTotal Loss: 2.762021\n",
      "Reconstruction: 2.036003, Regularization: 0.661023, Discriminator: 0.043340; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,385 root         INFO     Train Epoch: 153 [4096/8000 (51%)]\tTotal Loss: 2.692965\n",
      "Reconstruction: 1.947388, Regularization: 0.680614, Discriminator: 0.043298; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,495 root         INFO     Train Epoch: 153 [4608/8000 (58%)]\tTotal Loss: 3.190984\n",
      "Reconstruction: 2.329461, Regularization: 0.796534, Discriminator: 0.043339; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,604 root         INFO     Train Epoch: 153 [5120/8000 (64%)]\tTotal Loss: 2.475774\n",
      "Reconstruction: 1.770858, Regularization: 0.639943, Discriminator: 0.043313; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,713 root         INFO     Train Epoch: 153 [5632/8000 (70%)]\tTotal Loss: 2.870369\n",
      "Reconstruction: 2.032801, Regularization: 0.772620, Discriminator: 0.043316; Generator: 0.021632,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,825 root         INFO     Train Epoch: 153 [6144/8000 (77%)]\tTotal Loss: 3.042157\n",
      "Reconstruction: 2.156131, Regularization: 0.821060, Discriminator: 0.043314; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:00,937 root         INFO     Train Epoch: 153 [6656/8000 (83%)]\tTotal Loss: 2.809759\n",
      "Reconstruction: 2.001101, Regularization: 0.743699, Discriminator: 0.043306; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,049 root         INFO     Train Epoch: 153 [7168/8000 (90%)]\tTotal Loss: 2.767919\n",
      "Reconstruction: 1.979517, Regularization: 0.723440, Discriminator: 0.043324; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,161 root         INFO     Train Epoch: 153 [7680/8000 (96%)]\tTotal Loss: 2.486111\n",
      "Reconstruction: 1.765198, Regularization: 0.655942, Discriminator: 0.043314; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,242 root         INFO     ====> Epoch: 153 Average loss: 2.8303\n",
      "2019-04-10 00:02:01,269 root         INFO     Train Epoch: 154 [0/8000 (0%)]\tTotal Loss: 3.023919\n",
      "Reconstruction: 2.183354, Regularization: 0.775610, Discriminator: 0.043318; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,382 root         INFO     Train Epoch: 154 [512/8000 (6%)]\tTotal Loss: 2.688958\n",
      "Reconstruction: 1.926502, Regularization: 0.697493, Discriminator: 0.043316; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,494 root         INFO     Train Epoch: 154 [1024/8000 (13%)]\tTotal Loss: 3.061798\n",
      "Reconstruction: 2.269134, Regularization: 0.727702, Discriminator: 0.043311; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,606 root         INFO     Train Epoch: 154 [1536/8000 (19%)]\tTotal Loss: 2.908595\n",
      "Reconstruction: 2.069638, Regularization: 0.774002, Discriminator: 0.043309; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,718 root         INFO     Train Epoch: 154 [2048/8000 (26%)]\tTotal Loss: 2.769246\n",
      "Reconstruction: 1.933084, Regularization: 0.771191, Discriminator: 0.043314; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,829 root         INFO     Train Epoch: 154 [2560/8000 (32%)]\tTotal Loss: 2.647789\n",
      "Reconstruction: 1.920750, Regularization: 0.662078, Discriminator: 0.043302; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:01,941 root         INFO     Train Epoch: 154 [3072/8000 (38%)]\tTotal Loss: 2.558329\n",
      "Reconstruction: 1.854544, Regularization: 0.638820, Discriminator: 0.043317; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,053 root         INFO     Train Epoch: 154 [3584/8000 (45%)]\tTotal Loss: 2.747415\n",
      "Reconstruction: 1.921262, Regularization: 0.761170, Discriminator: 0.043324; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,165 root         INFO     Train Epoch: 154 [4096/8000 (51%)]\tTotal Loss: 2.717273\n",
      "Reconstruction: 1.954141, Regularization: 0.698142, Discriminator: 0.043325; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,276 root         INFO     Train Epoch: 154 [4608/8000 (58%)]\tTotal Loss: 2.680881\n",
      "Reconstruction: 1.908124, Regularization: 0.707755, Discriminator: 0.043338; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,389 root         INFO     Train Epoch: 154 [5120/8000 (64%)]\tTotal Loss: 2.592674\n",
      "Reconstruction: 1.939895, Regularization: 0.587774, Discriminator: 0.043344; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,500 root         INFO     Train Epoch: 154 [5632/8000 (70%)]\tTotal Loss: 2.383951\n",
      "Reconstruction: 1.683447, Regularization: 0.635454, Discriminator: 0.043386; Generator: 0.021665,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,612 root         INFO     Train Epoch: 154 [6144/8000 (77%)]\tTotal Loss: 2.936152\n",
      "Reconstruction: 2.198309, Regularization: 0.672823, Discriminator: 0.043354; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,724 root         INFO     Train Epoch: 154 [6656/8000 (83%)]\tTotal Loss: 2.901187\n",
      "Reconstruction: 2.066873, Regularization: 0.769320, Discriminator: 0.043329; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,837 root         INFO     Train Epoch: 154 [7168/8000 (90%)]\tTotal Loss: 2.931580\n",
      "Reconstruction: 2.178029, Regularization: 0.688542, Discriminator: 0.043345; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:02,948 root         INFO     Train Epoch: 154 [7680/8000 (96%)]\tTotal Loss: 3.276443\n",
      "Reconstruction: 2.388682, Regularization: 0.822776, Discriminator: 0.043311; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,029 root         INFO     ====> Epoch: 154 Average loss: 2.8480\n",
      "2019-04-10 00:02:03,057 root         INFO     Train Epoch: 155 [0/8000 (0%)]\tTotal Loss: 2.947469\n",
      "Reconstruction: 2.132957, Regularization: 0.749506, Discriminator: 0.043330; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,171 root         INFO     Train Epoch: 155 [512/8000 (6%)]\tTotal Loss: 3.061853\n",
      "Reconstruction: 2.229541, Regularization: 0.767312, Discriminator: 0.043325; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,283 root         INFO     Train Epoch: 155 [1024/8000 (13%)]\tTotal Loss: 3.013073\n",
      "Reconstruction: 2.154393, Regularization: 0.793707, Discriminator: 0.043311; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,394 root         INFO     Train Epoch: 155 [1536/8000 (19%)]\tTotal Loss: 2.510250\n",
      "Reconstruction: 1.811257, Regularization: 0.633997, Discriminator: 0.043328; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,505 root         INFO     Train Epoch: 155 [2048/8000 (26%)]\tTotal Loss: 2.730949\n",
      "Reconstruction: 2.068358, Regularization: 0.597576, Discriminator: 0.043353; Generator: 0.021662,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,616 root         INFO     Train Epoch: 155 [2560/8000 (32%)]\tTotal Loss: 3.111909\n",
      "Reconstruction: 2.284548, Regularization: 0.762362, Discriminator: 0.043325; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,727 root         INFO     Train Epoch: 155 [3072/8000 (38%)]\tTotal Loss: 2.793573\n",
      "Reconstruction: 2.018763, Regularization: 0.709796, Discriminator: 0.043345; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,838 root         INFO     Train Epoch: 155 [3584/8000 (45%)]\tTotal Loss: 2.598344\n",
      "Reconstruction: 1.882755, Regularization: 0.650605, Discriminator: 0.043328; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:03,948 root         INFO     Train Epoch: 155 [4096/8000 (51%)]\tTotal Loss: 3.167031\n",
      "Reconstruction: 2.301003, Regularization: 0.801019, Discriminator: 0.043342; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,059 root         INFO     Train Epoch: 155 [4608/8000 (58%)]\tTotal Loss: 2.943938\n",
      "Reconstruction: 2.158231, Regularization: 0.720739, Discriminator: 0.043318; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,169 root         INFO     Train Epoch: 155 [5120/8000 (64%)]\tTotal Loss: 2.312785\n",
      "Reconstruction: 1.687238, Regularization: 0.560544, Discriminator: 0.043350; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,280 root         INFO     Train Epoch: 155 [5632/8000 (70%)]\tTotal Loss: 2.877020\n",
      "Reconstruction: 2.091162, Regularization: 0.720904, Discriminator: 0.043319; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,390 root         INFO     Train Epoch: 155 [6144/8000 (77%)]\tTotal Loss: 2.737226\n",
      "Reconstruction: 2.035781, Regularization: 0.636472, Discriminator: 0.043330; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,501 root         INFO     Train Epoch: 155 [6656/8000 (83%)]\tTotal Loss: 2.964621\n",
      "Reconstruction: 2.227260, Regularization: 0.672384, Discriminator: 0.043329; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,612 root         INFO     Train Epoch: 155 [7168/8000 (90%)]\tTotal Loss: 4.076933\n",
      "Reconstruction: 3.285032, Regularization: 0.726926, Discriminator: 0.043322; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,722 root         INFO     Train Epoch: 155 [7680/8000 (96%)]\tTotal Loss: 2.965133\n",
      "Reconstruction: 2.214484, Regularization: 0.685683, Discriminator: 0.043312; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,803 root         INFO     ====> Epoch: 155 Average loss: 2.8756\n",
      "2019-04-10 00:02:04,830 root         INFO     Train Epoch: 156 [0/8000 (0%)]\tTotal Loss: 2.828133\n",
      "Reconstruction: 2.044876, Regularization: 0.718290, Discriminator: 0.043311; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:04,941 root         INFO     Train Epoch: 156 [512/8000 (6%)]\tTotal Loss: 3.111235\n",
      "Reconstruction: 2.315622, Regularization: 0.730638, Discriminator: 0.043313; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,052 root         INFO     Train Epoch: 156 [1024/8000 (13%)]\tTotal Loss: 2.860796\n",
      "Reconstruction: 2.081311, Regularization: 0.714530, Discriminator: 0.043295; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,164 root         INFO     Train Epoch: 156 [1536/8000 (19%)]\tTotal Loss: 2.824256\n",
      "Reconstruction: 2.011057, Regularization: 0.748220, Discriminator: 0.043315; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,275 root         INFO     Train Epoch: 156 [2048/8000 (26%)]\tTotal Loss: 3.246462\n",
      "Reconstruction: 2.473571, Regularization: 0.707925, Discriminator: 0.043304; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,386 root         INFO     Train Epoch: 156 [2560/8000 (32%)]\tTotal Loss: 2.960638\n",
      "Reconstruction: 2.149901, Regularization: 0.745757, Discriminator: 0.043309; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,497 root         INFO     Train Epoch: 156 [3072/8000 (38%)]\tTotal Loss: 2.999742\n",
      "Reconstruction: 2.208118, Regularization: 0.726642, Discriminator: 0.043316; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,607 root         INFO     Train Epoch: 156 [3584/8000 (45%)]\tTotal Loss: 3.456122\n",
      "Reconstruction: 2.597186, Regularization: 0.793976, Discriminator: 0.043296; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,718 root         INFO     Train Epoch: 156 [4096/8000 (51%)]\tTotal Loss: 2.414309\n",
      "Reconstruction: 1.740687, Regularization: 0.608622, Discriminator: 0.043332; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,829 root         INFO     Train Epoch: 156 [4608/8000 (58%)]\tTotal Loss: 2.233258\n",
      "Reconstruction: 1.575264, Regularization: 0.592988, Discriminator: 0.043334; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:05,940 root         INFO     Train Epoch: 156 [5120/8000 (64%)]\tTotal Loss: 3.042343\n",
      "Reconstruction: 2.199163, Regularization: 0.778139, Discriminator: 0.043360; Generator: 0.021681,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,050 root         INFO     Train Epoch: 156 [5632/8000 (70%)]\tTotal Loss: 2.897532\n",
      "Reconstruction: 2.136425, Regularization: 0.696107, Discriminator: 0.043320; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,161 root         INFO     Train Epoch: 156 [6144/8000 (77%)]\tTotal Loss: 2.637985\n",
      "Reconstruction: 1.952332, Regularization: 0.620664, Discriminator: 0.043308; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,271 root         INFO     Train Epoch: 156 [6656/8000 (83%)]\tTotal Loss: 2.909450\n",
      "Reconstruction: 2.125286, Regularization: 0.719165, Discriminator: 0.043309; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,380 root         INFO     Train Epoch: 156 [7168/8000 (90%)]\tTotal Loss: 2.775107\n",
      "Reconstruction: 2.051699, Regularization: 0.658416, Discriminator: 0.043306; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,490 root         INFO     Train Epoch: 156 [7680/8000 (96%)]\tTotal Loss: 2.897867\n",
      "Reconstruction: 2.168177, Regularization: 0.664687, Discriminator: 0.043313; Generator: 0.021691,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,570 root         INFO     ====> Epoch: 156 Average loss: 2.8853\n",
      "2019-04-10 00:02:06,597 root         INFO     Train Epoch: 157 [0/8000 (0%)]\tTotal Loss: 2.791476\n",
      "Reconstruction: 2.016007, Regularization: 0.710472, Discriminator: 0.043306; Generator: 0.021691,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,709 root         INFO     Train Epoch: 157 [512/8000 (6%)]\tTotal Loss: 2.737090\n",
      "Reconstruction: 1.990832, Regularization: 0.681271, Discriminator: 0.043311; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,820 root         INFO     Train Epoch: 157 [1024/8000 (13%)]\tTotal Loss: 3.163556\n",
      "Reconstruction: 2.367800, Regularization: 0.730751, Discriminator: 0.043339; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:06,931 root         INFO     Train Epoch: 157 [1536/8000 (19%)]\tTotal Loss: 3.000487\n",
      "Reconstruction: 2.206067, Regularization: 0.729452, Discriminator: 0.043291; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:07,040 root         INFO     Train Epoch: 157 [2048/8000 (26%)]\tTotal Loss: 2.964804\n",
      "Reconstruction: 2.190970, Regularization: 0.708859, Discriminator: 0.043317; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:07,150 root         INFO     Train Epoch: 157 [2560/8000 (32%)]\tTotal Loss: 3.045205\n",
      "Reconstruction: 2.246014, Regularization: 0.734164, Discriminator: 0.043355; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:07,259 root         INFO     Train Epoch: 157 [3072/8000 (38%)]\tTotal Loss: 2.676111\n",
      "Reconstruction: 1.928819, Regularization: 0.682321, Discriminator: 0.043327; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:07,369 root         INFO     Train Epoch: 157 [3584/8000 (45%)]\tTotal Loss: 2.503452\n",
      "Reconstruction: 1.733989, Regularization: 0.704495, Discriminator: 0.043321; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:07,478 root         INFO     Train Epoch: 157 [4096/8000 (51%)]\tTotal Loss: 2.648257\n",
      "Reconstruction: 1.927328, Regularization: 0.655958, Discriminator: 0.043346; Generator: 0.021625,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:07,587 root         INFO     Train Epoch: 157 [4608/8000 (58%)]\tTotal Loss: 3.179749\n",
      "Reconstruction: 2.345139, Regularization: 0.769606, Discriminator: 0.043365; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:07,695 root         INFO     Train Epoch: 157 [5120/8000 (64%)]\tTotal Loss: 2.930758\n",
      "Reconstruction: 2.139465, Regularization: 0.726319, Discriminator: 0.043334; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:07,803 root         INFO     Train Epoch: 157 [5632/8000 (70%)]\tTotal Loss: 2.623160\n",
      "Reconstruction: 1.869038, Regularization: 0.689160, Discriminator: 0.043335; Generator: 0.021627,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:07,913 root         INFO     Train Epoch: 157 [6144/8000 (77%)]\tTotal Loss: 2.882657\n",
      "Reconstruction: 2.094120, Regularization: 0.723557, Discriminator: 0.043334; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,022 root         INFO     Train Epoch: 157 [6656/8000 (83%)]\tTotal Loss: 2.758562\n",
      "Reconstruction: 2.047702, Regularization: 0.645905, Discriminator: 0.043307; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,133 root         INFO     Train Epoch: 157 [7168/8000 (90%)]\tTotal Loss: 2.958477\n",
      "Reconstruction: 2.191707, Regularization: 0.701800, Discriminator: 0.043333; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,246 root         INFO     Train Epoch: 157 [7680/8000 (96%)]\tTotal Loss: 2.641007\n",
      "Reconstruction: 1.966723, Regularization: 0.609337, Discriminator: 0.043312; Generator: 0.021635,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,328 root         INFO     ====> Epoch: 157 Average loss: 2.8483\n",
      "2019-04-10 00:02:08,355 root         INFO     Train Epoch: 158 [0/8000 (0%)]\tTotal Loss: 2.825935\n",
      "Reconstruction: 2.107618, Regularization: 0.653390, Discriminator: 0.043282; Generator: 0.021645,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,467 root         INFO     Train Epoch: 158 [512/8000 (6%)]\tTotal Loss: 2.557589\n",
      "Reconstruction: 1.884185, Regularization: 0.608433, Discriminator: 0.043323; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,578 root         INFO     Train Epoch: 158 [1024/8000 (13%)]\tTotal Loss: 2.814663\n",
      "Reconstruction: 2.080925, Regularization: 0.668805, Discriminator: 0.043284; Generator: 0.021648,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,688 root         INFO     Train Epoch: 158 [1536/8000 (19%)]\tTotal Loss: 2.645648\n",
      "Reconstruction: 1.938536, Regularization: 0.642130, Discriminator: 0.043327; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,797 root         INFO     Train Epoch: 158 [2048/8000 (26%)]\tTotal Loss: 2.338432\n",
      "Reconstruction: 1.630002, Regularization: 0.643399, Discriminator: 0.043377; Generator: 0.021654,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:08,907 root         INFO     Train Epoch: 158 [2560/8000 (32%)]\tTotal Loss: 2.672075\n",
      "Reconstruction: 1.908964, Regularization: 0.698136, Discriminator: 0.043321; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,016 root         INFO     Train Epoch: 158 [3072/8000 (38%)]\tTotal Loss: 3.143091\n",
      "Reconstruction: 2.339678, Regularization: 0.738442, Discriminator: 0.043314; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,125 root         INFO     Train Epoch: 158 [3584/8000 (45%)]\tTotal Loss: 2.539881\n",
      "Reconstruction: 1.860166, Regularization: 0.614708, Discriminator: 0.043349; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,236 root         INFO     Train Epoch: 158 [4096/8000 (51%)]\tTotal Loss: 2.896448\n",
      "Reconstruction: 2.221212, Regularization: 0.610247, Discriminator: 0.043330; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,346 root         INFO     Train Epoch: 158 [4608/8000 (58%)]\tTotal Loss: 3.214935\n",
      "Reconstruction: 2.383632, Regularization: 0.766308, Discriminator: 0.043326; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,454 root         INFO     Train Epoch: 158 [5120/8000 (64%)]\tTotal Loss: 3.051533\n",
      "Reconstruction: 2.234849, Regularization: 0.751689, Discriminator: 0.043321; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,563 root         INFO     Train Epoch: 158 [5632/8000 (70%)]\tTotal Loss: 2.967399\n",
      "Reconstruction: 2.140882, Regularization: 0.761526, Discriminator: 0.043319; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,672 root         INFO     Train Epoch: 158 [6144/8000 (77%)]\tTotal Loss: 3.087458\n",
      "Reconstruction: 2.197746, Regularization: 0.824739, Discriminator: 0.043309; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,782 root         INFO     Train Epoch: 158 [6656/8000 (83%)]\tTotal Loss: 2.627794\n",
      "Reconstruction: 1.870484, Regularization: 0.692331, Discriminator: 0.043309; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:09,891 root         INFO     Train Epoch: 158 [7168/8000 (90%)]\tTotal Loss: 2.783675\n",
      "Reconstruction: 2.056112, Regularization: 0.662583, Discriminator: 0.043301; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,001 root         INFO     Train Epoch: 158 [7680/8000 (96%)]\tTotal Loss: 2.666297\n",
      "Reconstruction: 1.930441, Regularization: 0.670879, Discriminator: 0.043296; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,081 root         INFO     ====> Epoch: 158 Average loss: 2.8227\n",
      "2019-04-10 00:02:10,108 root         INFO     Train Epoch: 159 [0/8000 (0%)]\tTotal Loss: 2.886633\n",
      "Reconstruction: 2.097987, Regularization: 0.723671, Discriminator: 0.043295; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,220 root         INFO     Train Epoch: 159 [512/8000 (6%)]\tTotal Loss: 2.691559\n",
      "Reconstruction: 2.018054, Regularization: 0.608524, Discriminator: 0.043294; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,332 root         INFO     Train Epoch: 159 [1024/8000 (13%)]\tTotal Loss: 3.027344\n",
      "Reconstruction: 2.191358, Regularization: 0.770977, Discriminator: 0.043339; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,445 root         INFO     Train Epoch: 159 [1536/8000 (19%)]\tTotal Loss: 2.975704\n",
      "Reconstruction: 2.153301, Regularization: 0.757446, Discriminator: 0.043294; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,556 root         INFO     Train Epoch: 159 [2048/8000 (26%)]\tTotal Loss: 2.645622\n",
      "Reconstruction: 1.948385, Regularization: 0.632256, Discriminator: 0.043335; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,669 root         INFO     Train Epoch: 159 [2560/8000 (32%)]\tTotal Loss: 2.833105\n",
      "Reconstruction: 2.090373, Regularization: 0.677781, Discriminator: 0.043307; Generator: 0.021644,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,781 root         INFO     Train Epoch: 159 [3072/8000 (38%)]\tTotal Loss: 2.752278\n",
      "Reconstruction: 2.056627, Regularization: 0.630654, Discriminator: 0.043356; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:10,893 root         INFO     Train Epoch: 159 [3584/8000 (45%)]\tTotal Loss: 2.968773\n",
      "Reconstruction: 2.119152, Regularization: 0.784678, Discriminator: 0.043296; Generator: 0.021647,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,004 root         INFO     Train Epoch: 159 [4096/8000 (51%)]\tTotal Loss: 2.890954\n",
      "Reconstruction: 2.154849, Regularization: 0.671125, Discriminator: 0.043330; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,117 root         INFO     Train Epoch: 159 [4608/8000 (58%)]\tTotal Loss: 2.978840\n",
      "Reconstruction: 2.166645, Regularization: 0.747248, Discriminator: 0.043311; Generator: 0.021636,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,229 root         INFO     Train Epoch: 159 [5120/8000 (64%)]\tTotal Loss: 2.848587\n",
      "Reconstruction: 2.064669, Regularization: 0.718961, Discriminator: 0.043303; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,342 root         INFO     Train Epoch: 159 [5632/8000 (70%)]\tTotal Loss: 2.564653\n",
      "Reconstruction: 1.890152, Regularization: 0.609527, Discriminator: 0.043317; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,455 root         INFO     Train Epoch: 159 [6144/8000 (77%)]\tTotal Loss: 2.972666\n",
      "Reconstruction: 2.138289, Regularization: 0.769420, Discriminator: 0.043299; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,567 root         INFO     Train Epoch: 159 [6656/8000 (83%)]\tTotal Loss: 2.664528\n",
      "Reconstruction: 1.928816, Regularization: 0.670751, Discriminator: 0.043305; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,676 root         INFO     Train Epoch: 159 [7168/8000 (90%)]\tTotal Loss: 2.751129\n",
      "Reconstruction: 2.023651, Regularization: 0.662494, Discriminator: 0.043325; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,785 root         INFO     Train Epoch: 159 [7680/8000 (96%)]\tTotal Loss: 2.564124\n",
      "Reconstruction: 1.894588, Regularization: 0.604572, Discriminator: 0.043312; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:11,865 root         INFO     ====> Epoch: 159 Average loss: 2.8255\n",
      "2019-04-10 00:02:11,893 root         INFO     Train Epoch: 160 [0/8000 (0%)]\tTotal Loss: 2.891675\n",
      "Reconstruction: 2.112020, Regularization: 0.714692, Discriminator: 0.043303; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,005 root         INFO     Train Epoch: 160 [512/8000 (6%)]\tTotal Loss: 2.881096\n",
      "Reconstruction: 2.065437, Regularization: 0.750666, Discriminator: 0.043336; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,117 root         INFO     Train Epoch: 160 [1024/8000 (13%)]\tTotal Loss: 3.515293\n",
      "Reconstruction: 2.838682, Regularization: 0.611623, Discriminator: 0.043333; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,229 root         INFO     Train Epoch: 160 [1536/8000 (19%)]\tTotal Loss: 3.148897\n",
      "Reconstruction: 2.360852, Regularization: 0.723086, Discriminator: 0.043298; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,341 root         INFO     Train Epoch: 160 [2048/8000 (26%)]\tTotal Loss: 2.934893\n",
      "Reconstruction: 2.173037, Regularization: 0.696861, Discriminator: 0.043342; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,453 root         INFO     Train Epoch: 160 [2560/8000 (32%)]\tTotal Loss: 2.533756\n",
      "Reconstruction: 1.871658, Regularization: 0.597072, Discriminator: 0.043372; Generator: 0.021654,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,565 root         INFO     Train Epoch: 160 [3072/8000 (38%)]\tTotal Loss: 2.706069\n",
      "Reconstruction: 2.020548, Regularization: 0.620516, Discriminator: 0.043344; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,677 root         INFO     Train Epoch: 160 [3584/8000 (45%)]\tTotal Loss: 2.922067\n",
      "Reconstruction: 2.167165, Regularization: 0.689886, Discriminator: 0.043355; Generator: 0.021661,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,789 root         INFO     Train Epoch: 160 [4096/8000 (51%)]\tTotal Loss: 2.832041\n",
      "Reconstruction: 2.072601, Regularization: 0.694423, Discriminator: 0.043345; Generator: 0.021672,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:12,901 root         INFO     Train Epoch: 160 [4608/8000 (58%)]\tTotal Loss: 2.589727\n",
      "Reconstruction: 1.899346, Regularization: 0.625360, Discriminator: 0.043344; Generator: 0.021677,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:13,013 root         INFO     Train Epoch: 160 [5120/8000 (64%)]\tTotal Loss: 2.491413\n",
      "Reconstruction: 1.847162, Regularization: 0.579227, Discriminator: 0.043329; Generator: 0.021694,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:13,125 root         INFO     Train Epoch: 160 [5632/8000 (70%)]\tTotal Loss: 3.086289\n",
      "Reconstruction: 2.357996, Regularization: 0.663287, Discriminator: 0.043324; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:13,237 root         INFO     Train Epoch: 160 [6144/8000 (77%)]\tTotal Loss: 2.732552\n",
      "Reconstruction: 2.015466, Regularization: 0.652083, Discriminator: 0.043313; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:13,349 root         INFO     Train Epoch: 160 [6656/8000 (83%)]\tTotal Loss: 2.464267\n",
      "Reconstruction: 1.827109, Regularization: 0.572147, Discriminator: 0.043317; Generator: 0.021694,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:13,461 root         INFO     Train Epoch: 160 [7168/8000 (90%)]\tTotal Loss: 2.853069\n",
      "Reconstruction: 2.107441, Regularization: 0.680610, Discriminator: 0.043335; Generator: 0.021683,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:13,572 root         INFO     Train Epoch: 160 [7680/8000 (96%)]\tTotal Loss: 2.833857\n",
      "Reconstruction: 2.145757, Regularization: 0.623086, Discriminator: 0.043337; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:13,653 root         INFO     ====> Epoch: 160 Average loss: 2.8760\n",
      "2019-04-10 00:02:13,680 root         INFO     Train Epoch: 161 [0/8000 (0%)]\tTotal Loss: 2.860824\n",
      "Reconstruction: 2.128340, Regularization: 0.667505, Discriminator: 0.043296; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:13,792 root         INFO     Train Epoch: 161 [512/8000 (6%)]\tTotal Loss: 2.952050\n",
      "Reconstruction: 2.197355, Regularization: 0.689709, Discriminator: 0.043322; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:13,903 root         INFO     Train Epoch: 161 [1024/8000 (13%)]\tTotal Loss: 2.814978\n",
      "Reconstruction: 2.011580, Regularization: 0.738400, Discriminator: 0.043346; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,014 root         INFO     Train Epoch: 161 [1536/8000 (19%)]\tTotal Loss: 2.866344\n",
      "Reconstruction: 2.090838, Regularization: 0.710521, Discriminator: 0.043324; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,125 root         INFO     Train Epoch: 161 [2048/8000 (26%)]\tTotal Loss: 2.820064\n",
      "Reconstruction: 2.138892, Regularization: 0.616204, Discriminator: 0.043323; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,235 root         INFO     Train Epoch: 161 [2560/8000 (32%)]\tTotal Loss: 3.182608\n",
      "Reconstruction: 2.349441, Regularization: 0.768164, Discriminator: 0.043358; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,346 root         INFO     Train Epoch: 161 [3072/8000 (38%)]\tTotal Loss: 3.310222\n",
      "Reconstruction: 2.602034, Regularization: 0.643199, Discriminator: 0.043337; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,457 root         INFO     Train Epoch: 161 [3584/8000 (45%)]\tTotal Loss: 2.619052\n",
      "Reconstruction: 1.892092, Regularization: 0.661987, Discriminator: 0.043331; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,568 root         INFO     Train Epoch: 161 [4096/8000 (51%)]\tTotal Loss: 2.567697\n",
      "Reconstruction: 1.875840, Regularization: 0.626897, Discriminator: 0.043321; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,679 root         INFO     Train Epoch: 161 [4608/8000 (58%)]\tTotal Loss: 2.932771\n",
      "Reconstruction: 2.200017, Regularization: 0.667808, Discriminator: 0.043310; Generator: 0.021637,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,790 root         INFO     Train Epoch: 161 [5120/8000 (64%)]\tTotal Loss: 2.568105\n",
      "Reconstruction: 1.917549, Regularization: 0.585604, Discriminator: 0.043296; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:14,901 root         INFO     Train Epoch: 161 [5632/8000 (70%)]\tTotal Loss: 2.968038\n",
      "Reconstruction: 2.205251, Regularization: 0.697822, Discriminator: 0.043305; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,012 root         INFO     Train Epoch: 161 [6144/8000 (77%)]\tTotal Loss: 2.897766\n",
      "Reconstruction: 2.151261, Regularization: 0.681537, Discriminator: 0.043301; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,122 root         INFO     Train Epoch: 161 [6656/8000 (83%)]\tTotal Loss: 2.964443\n",
      "Reconstruction: 2.240299, Regularization: 0.659203, Discriminator: 0.043267; Generator: 0.021673,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,231 root         INFO     Train Epoch: 161 [7168/8000 (90%)]\tTotal Loss: 3.059867\n",
      "Reconstruction: 2.276869, Regularization: 0.718016, Discriminator: 0.043324; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,340 root         INFO     Train Epoch: 161 [7680/8000 (96%)]\tTotal Loss: 2.970504\n",
      "Reconstruction: 2.193904, Regularization: 0.711609, Discriminator: 0.043323; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,420 root         INFO     ====> Epoch: 161 Average loss: 2.8787\n",
      "2019-04-10 00:02:15,447 root         INFO     Train Epoch: 162 [0/8000 (0%)]\tTotal Loss: 2.782837\n",
      "Reconstruction: 2.038061, Regularization: 0.679799, Discriminator: 0.043303; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,559 root         INFO     Train Epoch: 162 [512/8000 (6%)]\tTotal Loss: 2.760175\n",
      "Reconstruction: 2.059681, Regularization: 0.635495, Discriminator: 0.043323; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,669 root         INFO     Train Epoch: 162 [1024/8000 (13%)]\tTotal Loss: 3.040367\n",
      "Reconstruction: 2.247691, Regularization: 0.727683, Discriminator: 0.043326; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,778 root         INFO     Train Epoch: 162 [1536/8000 (19%)]\tTotal Loss: 2.890792\n",
      "Reconstruction: 2.183100, Regularization: 0.642711, Discriminator: 0.043308; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,888 root         INFO     Train Epoch: 162 [2048/8000 (26%)]\tTotal Loss: 2.603924\n",
      "Reconstruction: 1.859257, Regularization: 0.679632, Discriminator: 0.043374; Generator: 0.021661,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:15,997 root         INFO     Train Epoch: 162 [2560/8000 (32%)]\tTotal Loss: 3.352942\n",
      "Reconstruction: 2.497760, Regularization: 0.790149, Discriminator: 0.043362; Generator: 0.021672,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,107 root         INFO     Train Epoch: 162 [3072/8000 (38%)]\tTotal Loss: 2.484509\n",
      "Reconstruction: 1.794808, Regularization: 0.624696, Discriminator: 0.043332; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,216 root         INFO     Train Epoch: 162 [3584/8000 (45%)]\tTotal Loss: 2.696416\n",
      "Reconstruction: 1.998647, Regularization: 0.632753, Discriminator: 0.043330; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,325 root         INFO     Train Epoch: 162 [4096/8000 (51%)]\tTotal Loss: 2.453207\n",
      "Reconstruction: 1.829709, Regularization: 0.558505, Discriminator: 0.043324; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,435 root         INFO     Train Epoch: 162 [4608/8000 (58%)]\tTotal Loss: 2.768342\n",
      "Reconstruction: 2.106988, Regularization: 0.596349, Discriminator: 0.043318; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,544 root         INFO     Train Epoch: 162 [5120/8000 (64%)]\tTotal Loss: 2.952252\n",
      "Reconstruction: 2.182769, Regularization: 0.704477, Discriminator: 0.043318; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,654 root         INFO     Train Epoch: 162 [5632/8000 (70%)]\tTotal Loss: 2.759197\n",
      "Reconstruction: 2.012752, Regularization: 0.681448, Discriminator: 0.043311; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,763 root         INFO     Train Epoch: 162 [6144/8000 (77%)]\tTotal Loss: 2.831298\n",
      "Reconstruction: 2.125806, Regularization: 0.640499, Discriminator: 0.043305; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,873 root         INFO     Train Epoch: 162 [6656/8000 (83%)]\tTotal Loss: 3.119571\n",
      "Reconstruction: 2.273284, Regularization: 0.781301, Discriminator: 0.043321; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:16,982 root         INFO     Train Epoch: 162 [7168/8000 (90%)]\tTotal Loss: 2.443633\n",
      "Reconstruction: 1.846477, Regularization: 0.532164, Discriminator: 0.043314; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,091 root         INFO     Train Epoch: 162 [7680/8000 (96%)]\tTotal Loss: 2.990834\n",
      "Reconstruction: 2.261935, Regularization: 0.663904, Discriminator: 0.043323; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,171 root         INFO     ====> Epoch: 162 Average loss: 2.8490\n",
      "2019-04-10 00:02:17,199 root         INFO     Train Epoch: 163 [0/8000 (0%)]\tTotal Loss: 2.663449\n",
      "Reconstruction: 2.008639, Regularization: 0.589797, Discriminator: 0.043332; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,311 root         INFO     Train Epoch: 163 [512/8000 (6%)]\tTotal Loss: 2.931374\n",
      "Reconstruction: 2.246189, Regularization: 0.620232, Discriminator: 0.043288; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,421 root         INFO     Train Epoch: 163 [1024/8000 (13%)]\tTotal Loss: 2.527174\n",
      "Reconstruction: 1.907988, Regularization: 0.554166, Discriminator: 0.043372; Generator: 0.021648,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,531 root         INFO     Train Epoch: 163 [1536/8000 (19%)]\tTotal Loss: 2.941596\n",
      "Reconstruction: 2.247873, Regularization: 0.628736, Discriminator: 0.043351; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,642 root         INFO     Train Epoch: 163 [2048/8000 (26%)]\tTotal Loss: 2.981487\n",
      "Reconstruction: 2.306685, Regularization: 0.609829, Discriminator: 0.043342; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,752 root         INFO     Train Epoch: 163 [2560/8000 (32%)]\tTotal Loss: 2.806433\n",
      "Reconstruction: 2.029526, Regularization: 0.711949, Discriminator: 0.043328; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,862 root         INFO     Train Epoch: 163 [3072/8000 (38%)]\tTotal Loss: 2.689734\n",
      "Reconstruction: 2.011789, Regularization: 0.612972, Discriminator: 0.043341; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:17,972 root         INFO     Train Epoch: 163 [3584/8000 (45%)]\tTotal Loss: 2.607615\n",
      "Reconstruction: 1.934949, Regularization: 0.607705, Discriminator: 0.043322; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:18,083 root         INFO     Train Epoch: 163 [4096/8000 (51%)]\tTotal Loss: 2.733839\n",
      "Reconstruction: 2.065177, Regularization: 0.603696, Discriminator: 0.043345; Generator: 0.021621,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:18,194 root         INFO     Train Epoch: 163 [4608/8000 (58%)]\tTotal Loss: 2.950880\n",
      "Reconstruction: 2.229115, Regularization: 0.656801, Discriminator: 0.043337; Generator: 0.021627,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:18,304 root         INFO     Train Epoch: 163 [5120/8000 (64%)]\tTotal Loss: 2.826681\n",
      "Reconstruction: 2.108199, Regularization: 0.653524, Discriminator: 0.043330; Generator: 0.021627,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:18,415 root         INFO     Train Epoch: 163 [5632/8000 (70%)]\tTotal Loss: 3.178428\n",
      "Reconstruction: 2.382060, Regularization: 0.731405, Discriminator: 0.043328; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:18,526 root         INFO     Train Epoch: 163 [6144/8000 (77%)]\tTotal Loss: 2.929597\n",
      "Reconstruction: 2.226436, Regularization: 0.638202, Discriminator: 0.043316; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:18,635 root         INFO     Train Epoch: 163 [6656/8000 (83%)]\tTotal Loss: 2.847713\n",
      "Reconstruction: 2.081676, Regularization: 0.701070, Discriminator: 0.043326; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:18,743 root         INFO     Train Epoch: 163 [7168/8000 (90%)]\tTotal Loss: 2.817035\n",
      "Reconstruction: 2.080425, Regularization: 0.671657, Discriminator: 0.043306; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:18,850 root         INFO     Train Epoch: 163 [7680/8000 (96%)]\tTotal Loss: 3.231822\n",
      "Reconstruction: 2.467656, Regularization: 0.699238, Discriminator: 0.043282; Generator: 0.021646,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:18,929 root         INFO     ====> Epoch: 163 Average loss: 2.8105\n",
      "2019-04-10 00:02:18,957 root         INFO     Train Epoch: 164 [0/8000 (0%)]\tTotal Loss: 2.919811\n",
      "Reconstruction: 2.185885, Regularization: 0.668952, Discriminator: 0.043324; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,068 root         INFO     Train Epoch: 164 [512/8000 (6%)]\tTotal Loss: 2.581756\n",
      "Reconstruction: 1.948386, Regularization: 0.568428, Discriminator: 0.043290; Generator: 0.021651,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,179 root         INFO     Train Epoch: 164 [1024/8000 (13%)]\tTotal Loss: 2.832896\n",
      "Reconstruction: 2.155967, Regularization: 0.611999, Discriminator: 0.043276; Generator: 0.021653,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,290 root         INFO     Train Epoch: 164 [1536/8000 (19%)]\tTotal Loss: 2.560338\n",
      "Reconstruction: 1.911928, Regularization: 0.583431, Discriminator: 0.043325; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,401 root         INFO     Train Epoch: 164 [2048/8000 (26%)]\tTotal Loss: 3.090433\n",
      "Reconstruction: 2.286160, Regularization: 0.739302, Discriminator: 0.043313; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,512 root         INFO     Train Epoch: 164 [2560/8000 (32%)]\tTotal Loss: 2.813282\n",
      "Reconstruction: 2.076213, Regularization: 0.672089, Discriminator: 0.043322; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,623 root         INFO     Train Epoch: 164 [3072/8000 (38%)]\tTotal Loss: 2.578956\n",
      "Reconstruction: 1.892186, Regularization: 0.621783, Discriminator: 0.043330; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,734 root         INFO     Train Epoch: 164 [3584/8000 (45%)]\tTotal Loss: 3.029086\n",
      "Reconstruction: 2.308199, Regularization: 0.655920, Discriminator: 0.043302; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,844 root         INFO     Train Epoch: 164 [4096/8000 (51%)]\tTotal Loss: 2.882629\n",
      "Reconstruction: 2.195317, Regularization: 0.622324, Discriminator: 0.043325; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:19,956 root         INFO     Train Epoch: 164 [4608/8000 (58%)]\tTotal Loss: 2.578514\n",
      "Reconstruction: 1.928673, Regularization: 0.584840, Discriminator: 0.043334; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,069 root         INFO     Train Epoch: 164 [5120/8000 (64%)]\tTotal Loss: 2.830353\n",
      "Reconstruction: 2.061348, Regularization: 0.704005, Discriminator: 0.043325; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,181 root         INFO     Train Epoch: 164 [5632/8000 (70%)]\tTotal Loss: 2.666816\n",
      "Reconstruction: 1.961478, Regularization: 0.640343, Discriminator: 0.043319; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,294 root         INFO     Train Epoch: 164 [6144/8000 (77%)]\tTotal Loss: 2.449681\n",
      "Reconstruction: 1.777085, Regularization: 0.607589, Discriminator: 0.043323; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,407 root         INFO     Train Epoch: 164 [6656/8000 (83%)]\tTotal Loss: 2.627158\n",
      "Reconstruction: 1.900968, Regularization: 0.661206, Discriminator: 0.043309; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,518 root         INFO     Train Epoch: 164 [7168/8000 (90%)]\tTotal Loss: 2.853381\n",
      "Reconstruction: 2.130133, Regularization: 0.658260, Discriminator: 0.043308; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,628 root         INFO     Train Epoch: 164 [7680/8000 (96%)]\tTotal Loss: 2.490687\n",
      "Reconstruction: 1.807415, Regularization: 0.618299, Discriminator: 0.043297; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,708 root         INFO     ====> Epoch: 164 Average loss: 2.8136\n",
      "2019-04-10 00:02:20,735 root         INFO     Train Epoch: 165 [0/8000 (0%)]\tTotal Loss: 2.973138\n",
      "Reconstruction: 2.241094, Regularization: 0.667047, Discriminator: 0.043302; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:20,847 root         INFO     Train Epoch: 165 [512/8000 (6%)]\tTotal Loss: 2.858280\n",
      "Reconstruction: 2.139634, Regularization: 0.653664, Discriminator: 0.043299; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:20,956 root         INFO     Train Epoch: 165 [1024/8000 (13%)]\tTotal Loss: 2.764271\n",
      "Reconstruction: 2.075155, Regularization: 0.624125, Discriminator: 0.043301; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,066 root         INFO     Train Epoch: 165 [1536/8000 (19%)]\tTotal Loss: 3.248442\n",
      "Reconstruction: 2.581664, Regularization: 0.601762, Discriminator: 0.043335; Generator: 0.021681,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,176 root         INFO     Train Epoch: 165 [2048/8000 (26%)]\tTotal Loss: 3.163718\n",
      "Reconstruction: 2.423382, Regularization: 0.675340, Discriminator: 0.043321; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,286 root         INFO     Train Epoch: 165 [2560/8000 (32%)]\tTotal Loss: 2.941720\n",
      "Reconstruction: 2.203970, Regularization: 0.672802, Discriminator: 0.043291; Generator: 0.021658,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,396 root         INFO     Train Epoch: 165 [3072/8000 (38%)]\tTotal Loss: 2.540407\n",
      "Reconstruction: 1.851121, Regularization: 0.624322, Discriminator: 0.043287; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,506 root         INFO     Train Epoch: 165 [3584/8000 (45%)]\tTotal Loss: 2.590781\n",
      "Reconstruction: 1.951749, Regularization: 0.574051, Discriminator: 0.043329; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,616 root         INFO     Train Epoch: 165 [4096/8000 (51%)]\tTotal Loss: 2.894177\n",
      "Reconstruction: 2.177679, Regularization: 0.651513, Discriminator: 0.043344; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,728 root         INFO     Train Epoch: 165 [4608/8000 (58%)]\tTotal Loss: 2.544906\n",
      "Reconstruction: 1.897639, Regularization: 0.582275, Discriminator: 0.043368; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:21,839 root         INFO     Train Epoch: 165 [5120/8000 (64%)]\tTotal Loss: 2.614648\n",
      "Reconstruction: 1.943483, Regularization: 0.606198, Discriminator: 0.043325; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:21,950 root         INFO     Train Epoch: 165 [5632/8000 (70%)]\tTotal Loss: 2.692457\n",
      "Reconstruction: 1.973091, Regularization: 0.654398, Discriminator: 0.043338; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:22,061 root         INFO     Train Epoch: 165 [6144/8000 (77%)]\tTotal Loss: 2.822809\n",
      "Reconstruction: 2.126167, Regularization: 0.631659, Discriminator: 0.043337; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,171 root         INFO     Train Epoch: 165 [6656/8000 (83%)]\tTotal Loss: 2.799468\n",
      "Reconstruction: 2.101929, Regularization: 0.632569, Discriminator: 0.043330; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,282 root         INFO     Train Epoch: 165 [7168/8000 (90%)]\tTotal Loss: 2.741748\n",
      "Reconstruction: 2.048081, Regularization: 0.628691, Discriminator: 0.043330; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,393 root         INFO     Train Epoch: 165 [7680/8000 (96%)]\tTotal Loss: 2.764841\n",
      "Reconstruction: 2.047911, Regularization: 0.651966, Discriminator: 0.043317; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,473 root         INFO     ====> Epoch: 165 Average loss: 2.8261\n",
      "2019-04-10 00:02:22,501 root         INFO     Train Epoch: 166 [0/8000 (0%)]\tTotal Loss: 2.944802\n",
      "Reconstruction: 2.203838, Regularization: 0.676002, Discriminator: 0.043312; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,611 root         INFO     Train Epoch: 166 [512/8000 (6%)]\tTotal Loss: 2.867904\n",
      "Reconstruction: 2.175650, Regularization: 0.627313, Discriminator: 0.043288; Generator: 0.021653,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,721 root         INFO     Train Epoch: 166 [1024/8000 (13%)]\tTotal Loss: 2.703259\n",
      "Reconstruction: 2.012831, Regularization: 0.625446, Discriminator: 0.043330; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,831 root         INFO     Train Epoch: 166 [1536/8000 (19%)]\tTotal Loss: 2.833998\n",
      "Reconstruction: 2.135780, Regularization: 0.633278, Discriminator: 0.043283; Generator: 0.021656,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:22,942 root         INFO     Train Epoch: 166 [2048/8000 (26%)]\tTotal Loss: 2.961066\n",
      "Reconstruction: 2.195229, Regularization: 0.700855, Discriminator: 0.043325; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,052 root         INFO     Train Epoch: 166 [2560/8000 (32%)]\tTotal Loss: 2.955559\n",
      "Reconstruction: 2.200871, Regularization: 0.689716, Discriminator: 0.043313; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,162 root         INFO     Train Epoch: 166 [3072/8000 (38%)]\tTotal Loss: 3.234912\n",
      "Reconstruction: 2.506867, Regularization: 0.663063, Discriminator: 0.043325; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,272 root         INFO     Train Epoch: 166 [3584/8000 (45%)]\tTotal Loss: 2.671403\n",
      "Reconstruction: 1.933399, Regularization: 0.672976, Discriminator: 0.043370; Generator: 0.021658,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,382 root         INFO     Train Epoch: 166 [4096/8000 (51%)]\tTotal Loss: 3.187659\n",
      "Reconstruction: 2.383825, Regularization: 0.738841, Discriminator: 0.043332; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,492 root         INFO     Train Epoch: 166 [4608/8000 (58%)]\tTotal Loss: 2.977281\n",
      "Reconstruction: 2.233140, Regularization: 0.679137, Discriminator: 0.043338; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,601 root         INFO     Train Epoch: 166 [5120/8000 (64%)]\tTotal Loss: 3.036257\n",
      "Reconstruction: 2.300725, Regularization: 0.670538, Discriminator: 0.043329; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,711 root         INFO     Train Epoch: 166 [5632/8000 (70%)]\tTotal Loss: 2.438687\n",
      "Reconstruction: 1.829609, Regularization: 0.544066, Discriminator: 0.043339; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,820 root         INFO     Train Epoch: 166 [6144/8000 (77%)]\tTotal Loss: 2.256058\n",
      "Reconstruction: 1.661677, Regularization: 0.529356, Discriminator: 0.043346; Generator: 0.021680,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:23,930 root         INFO     Train Epoch: 166 [6656/8000 (83%)]\tTotal Loss: 2.728114\n",
      "Reconstruction: 2.081078, Regularization: 0.582021, Discriminator: 0.043331; Generator: 0.021685,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:24,040 root         INFO     Train Epoch: 166 [7168/8000 (90%)]\tTotal Loss: 2.913223\n",
      "Reconstruction: 2.241952, Regularization: 0.606263, Discriminator: 0.043324; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:24,149 root         INFO     Train Epoch: 166 [7680/8000 (96%)]\tTotal Loss: 3.213378\n",
      "Reconstruction: 2.492521, Regularization: 0.655847, Discriminator: 0.043313; Generator: 0.021697,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:24,229 root         INFO     ====> Epoch: 166 Average loss: 2.8743\n",
      "2019-04-10 00:02:24,256 root         INFO     Train Epoch: 167 [0/8000 (0%)]\tTotal Loss: 3.061937\n",
      "Reconstruction: 2.384067, Regularization: 0.612843, Discriminator: 0.043317; Generator: 0.021710,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:24,367 root         INFO     Train Epoch: 167 [512/8000 (6%)]\tTotal Loss: 3.389929\n",
      "Reconstruction: 2.626960, Regularization: 0.697958, Discriminator: 0.043318; Generator: 0.021693,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:24,476 root         INFO     Train Epoch: 167 [1024/8000 (13%)]\tTotal Loss: 2.746383\n",
      "Reconstruction: 2.116333, Regularization: 0.565038, Discriminator: 0.043307; Generator: 0.021705,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:24,586 root         INFO     Train Epoch: 167 [1536/8000 (19%)]\tTotal Loss: 3.129983\n",
      "Reconstruction: 2.428083, Regularization: 0.636859, Discriminator: 0.043339; Generator: 0.021701,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:24,696 root         INFO     Train Epoch: 167 [2048/8000 (26%)]\tTotal Loss: 3.183225\n",
      "Reconstruction: 2.456676, Regularization: 0.661529, Discriminator: 0.043321; Generator: 0.021699,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:24,805 root         INFO     Train Epoch: 167 [2560/8000 (32%)]\tTotal Loss: 2.670825\n",
      "Reconstruction: 2.026224, Regularization: 0.579621, Discriminator: 0.043305; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:24,914 root         INFO     Train Epoch: 167 [3072/8000 (38%)]\tTotal Loss: 2.802174\n",
      "Reconstruction: 2.135317, Regularization: 0.601859, Discriminator: 0.043319; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,024 root         INFO     Train Epoch: 167 [3584/8000 (45%)]\tTotal Loss: 2.926990\n",
      "Reconstruction: 2.179608, Regularization: 0.682414, Discriminator: 0.043297; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,133 root         INFO     Train Epoch: 167 [4096/8000 (51%)]\tTotal Loss: 2.982441\n",
      "Reconstruction: 2.254825, Regularization: 0.662636, Discriminator: 0.043320; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,242 root         INFO     Train Epoch: 167 [4608/8000 (58%)]\tTotal Loss: 2.743280\n",
      "Reconstruction: 2.094707, Regularization: 0.583539, Discriminator: 0.043374; Generator: 0.021659,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,354 root         INFO     Train Epoch: 167 [5120/8000 (64%)]\tTotal Loss: 3.146321\n",
      "Reconstruction: 2.404350, Regularization: 0.676995, Discriminator: 0.043345; Generator: 0.021631,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,466 root         INFO     Train Epoch: 167 [5632/8000 (70%)]\tTotal Loss: 2.928322\n",
      "Reconstruction: 2.224423, Regularization: 0.638927, Discriminator: 0.043342; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,577 root         INFO     Train Epoch: 167 [6144/8000 (77%)]\tTotal Loss: 3.074830\n",
      "Reconstruction: 2.368180, Regularization: 0.641655, Discriminator: 0.043352; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,689 root         INFO     Train Epoch: 167 [6656/8000 (83%)]\tTotal Loss: 2.871619\n",
      "Reconstruction: 2.243122, Regularization: 0.563530, Discriminator: 0.043326; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,800 root         INFO     Train Epoch: 167 [7168/8000 (90%)]\tTotal Loss: 2.818857\n",
      "Reconstruction: 2.187616, Regularization: 0.566275, Discriminator: 0.043319; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,910 root         INFO     Train Epoch: 167 [7680/8000 (96%)]\tTotal Loss: 3.029841\n",
      "Reconstruction: 2.364640, Regularization: 0.600242, Discriminator: 0.043301; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:25,991 root         INFO     ====> Epoch: 167 Average loss: 2.8949\n",
      "2019-04-10 00:02:26,019 root         INFO     Train Epoch: 168 [0/8000 (0%)]\tTotal Loss: 2.856297\n",
      "Reconstruction: 2.166923, Regularization: 0.624399, Discriminator: 0.043297; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,130 root         INFO     Train Epoch: 168 [512/8000 (6%)]\tTotal Loss: 2.824639\n",
      "Reconstruction: 2.132567, Regularization: 0.627105, Discriminator: 0.043304; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,240 root         INFO     Train Epoch: 168 [1024/8000 (13%)]\tTotal Loss: 2.898420\n",
      "Reconstruction: 2.211553, Regularization: 0.621905, Discriminator: 0.043304; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,352 root         INFO     Train Epoch: 168 [1536/8000 (19%)]\tTotal Loss: 2.962532\n",
      "Reconstruction: 2.249681, Regularization: 0.647858, Discriminator: 0.043328; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,461 root         INFO     Train Epoch: 168 [2048/8000 (26%)]\tTotal Loss: 3.367174\n",
      "Reconstruction: 2.590925, Regularization: 0.711287, Discriminator: 0.043313; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,571 root         INFO     Train Epoch: 168 [2560/8000 (32%)]\tTotal Loss: 2.712625\n",
      "Reconstruction: 2.061406, Regularization: 0.586258, Discriminator: 0.043320; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,680 root         INFO     Train Epoch: 168 [3072/8000 (38%)]\tTotal Loss: 3.196289\n",
      "Reconstruction: 2.407562, Regularization: 0.723696, Discriminator: 0.043374; Generator: 0.021657,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,790 root         INFO     Train Epoch: 168 [3584/8000 (45%)]\tTotal Loss: 2.613223\n",
      "Reconstruction: 1.967918, Regularization: 0.580323, Discriminator: 0.043326; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:26,900 root         INFO     Train Epoch: 168 [4096/8000 (51%)]\tTotal Loss: 3.024397\n",
      "Reconstruction: 2.321325, Regularization: 0.638078, Discriminator: 0.043339; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,010 root         INFO     Train Epoch: 168 [4608/8000 (58%)]\tTotal Loss: 2.496712\n",
      "Reconstruction: 1.923102, Regularization: 0.508645, Discriminator: 0.043315; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,120 root         INFO     Train Epoch: 168 [5120/8000 (64%)]\tTotal Loss: 2.611962\n",
      "Reconstruction: 1.980812, Regularization: 0.566160, Discriminator: 0.043331; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,230 root         INFO     Train Epoch: 168 [5632/8000 (70%)]\tTotal Loss: 2.891283\n",
      "Reconstruction: 2.227560, Regularization: 0.598751, Discriminator: 0.043318; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,339 root         INFO     Train Epoch: 168 [6144/8000 (77%)]\tTotal Loss: 3.058228\n",
      "Reconstruction: 2.403888, Regularization: 0.589375, Discriminator: 0.043316; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,449 root         INFO     Train Epoch: 168 [6656/8000 (83%)]\tTotal Loss: 2.575619\n",
      "Reconstruction: 1.907905, Regularization: 0.602690, Discriminator: 0.043372; Generator: 0.021653,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,559 root         INFO     Train Epoch: 168 [7168/8000 (90%)]\tTotal Loss: 2.632170\n",
      "Reconstruction: 1.990594, Regularization: 0.576563, Discriminator: 0.043353; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,669 root         INFO     Train Epoch: 168 [7680/8000 (96%)]\tTotal Loss: 2.905299\n",
      "Reconstruction: 2.266586, Regularization: 0.573708, Discriminator: 0.043341; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,749 root         INFO     ====> Epoch: 168 Average loss: 2.8550\n",
      "2019-04-10 00:02:27,777 root         INFO     Train Epoch: 169 [0/8000 (0%)]\tTotal Loss: 2.783787\n",
      "Reconstruction: 2.109549, Regularization: 0.609224, Discriminator: 0.043348; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:27,889 root         INFO     Train Epoch: 169 [512/8000 (6%)]\tTotal Loss: 2.107687\n",
      "Reconstruction: 1.584040, Regularization: 0.458653, Discriminator: 0.043324; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,001 root         INFO     Train Epoch: 169 [1024/8000 (13%)]\tTotal Loss: 2.668566\n",
      "Reconstruction: 2.001976, Regularization: 0.601586, Discriminator: 0.043329; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,112 root         INFO     Train Epoch: 169 [1536/8000 (19%)]\tTotal Loss: 2.516868\n",
      "Reconstruction: 1.935218, Regularization: 0.516658, Discriminator: 0.043319; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,224 root         INFO     Train Epoch: 169 [2048/8000 (26%)]\tTotal Loss: 2.982149\n",
      "Reconstruction: 2.262648, Regularization: 0.654496, Discriminator: 0.043330; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,335 root         INFO     Train Epoch: 169 [2560/8000 (32%)]\tTotal Loss: 2.683621\n",
      "Reconstruction: 2.047878, Regularization: 0.570737, Discriminator: 0.043326; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,447 root         INFO     Train Epoch: 169 [3072/8000 (38%)]\tTotal Loss: 3.133204\n",
      "Reconstruction: 2.413820, Regularization: 0.654368, Discriminator: 0.043336; Generator: 0.021680,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,559 root         INFO     Train Epoch: 169 [3584/8000 (45%)]\tTotal Loss: 2.414356\n",
      "Reconstruction: 1.811651, Regularization: 0.537726, Discriminator: 0.043308; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,671 root         INFO     Train Epoch: 169 [4096/8000 (51%)]\tTotal Loss: 2.608906\n",
      "Reconstruction: 2.031555, Regularization: 0.512372, Discriminator: 0.043312; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,782 root         INFO     Train Epoch: 169 [4608/8000 (58%)]\tTotal Loss: 2.638162\n",
      "Reconstruction: 2.030995, Regularization: 0.542203, Discriminator: 0.043300; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:28,894 root         INFO     Train Epoch: 169 [5120/8000 (64%)]\tTotal Loss: 2.722318\n",
      "Reconstruction: 2.084577, Regularization: 0.572772, Discriminator: 0.043306; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,005 root         INFO     Train Epoch: 169 [5632/8000 (70%)]\tTotal Loss: 2.666063\n",
      "Reconstruction: 2.054997, Regularization: 0.546118, Discriminator: 0.043286; Generator: 0.021662,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,117 root         INFO     Train Epoch: 169 [6144/8000 (77%)]\tTotal Loss: 2.724306\n",
      "Reconstruction: 2.077597, Regularization: 0.581742, Discriminator: 0.043321; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,228 root         INFO     Train Epoch: 169 [6656/8000 (83%)]\tTotal Loss: 2.778581\n",
      "Reconstruction: 2.170746, Regularization: 0.542858, Discriminator: 0.043335; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,337 root         INFO     Train Epoch: 169 [7168/8000 (90%)]\tTotal Loss: 2.706185\n",
      "Reconstruction: 2.094874, Regularization: 0.546350, Discriminator: 0.043329; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,449 root         INFO     Train Epoch: 169 [7680/8000 (96%)]\tTotal Loss: 2.920730\n",
      "Reconstruction: 2.251113, Regularization: 0.604634, Discriminator: 0.043340; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,529 root         INFO     ====> Epoch: 169 Average loss: 2.8353\n",
      "2019-04-10 00:02:29,557 root         INFO     Train Epoch: 170 [0/8000 (0%)]\tTotal Loss: 2.660181\n",
      "Reconstruction: 2.025443, Regularization: 0.569754, Discriminator: 0.043331; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,669 root         INFO     Train Epoch: 170 [512/8000 (6%)]\tTotal Loss: 3.129139\n",
      "Reconstruction: 2.475436, Regularization: 0.588707, Discriminator: 0.043356; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,779 root         INFO     Train Epoch: 170 [1024/8000 (13%)]\tTotal Loss: 2.650486\n",
      "Reconstruction: 1.966918, Regularization: 0.618612, Discriminator: 0.043318; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,888 root         INFO     Train Epoch: 170 [1536/8000 (19%)]\tTotal Loss: 2.906386\n",
      "Reconstruction: 2.232806, Regularization: 0.608621, Discriminator: 0.043319; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:29,999 root         INFO     Train Epoch: 170 [2048/8000 (26%)]\tTotal Loss: 2.815397\n",
      "Reconstruction: 2.181645, Regularization: 0.568791, Discriminator: 0.043318; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,109 root         INFO     Train Epoch: 170 [2560/8000 (32%)]\tTotal Loss: 2.407552\n",
      "Reconstruction: 1.827501, Regularization: 0.515089, Discriminator: 0.043310; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,219 root         INFO     Train Epoch: 170 [3072/8000 (38%)]\tTotal Loss: 3.191937\n",
      "Reconstruction: 2.505634, Regularization: 0.621360, Discriminator: 0.043292; Generator: 0.021651,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,329 root         INFO     Train Epoch: 170 [3584/8000 (45%)]\tTotal Loss: 3.153000\n",
      "Reconstruction: 2.478497, Regularization: 0.609571, Discriminator: 0.043280; Generator: 0.021652,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,439 root         INFO     Train Epoch: 170 [4096/8000 (51%)]\tTotal Loss: 2.816988\n",
      "Reconstruction: 2.199689, Regularization: 0.552346, Discriminator: 0.043296; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,549 root         INFO     Train Epoch: 170 [4608/8000 (58%)]\tTotal Loss: 2.559227\n",
      "Reconstruction: 1.972999, Regularization: 0.521262, Discriminator: 0.043305; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,659 root         INFO     Train Epoch: 170 [5120/8000 (64%)]\tTotal Loss: 2.972078\n",
      "Reconstruction: 2.376200, Regularization: 0.530910, Discriminator: 0.043309; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,768 root         INFO     Train Epoch: 170 [5632/8000 (70%)]\tTotal Loss: 3.261352\n",
      "Reconstruction: 2.587866, Regularization: 0.608545, Discriminator: 0.043280; Generator: 0.021662,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,878 root         INFO     Train Epoch: 170 [6144/8000 (77%)]\tTotal Loss: 2.740199\n",
      "Reconstruction: 2.112680, Regularization: 0.562554, Discriminator: 0.043302; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:30,988 root         INFO     Train Epoch: 170 [6656/8000 (83%)]\tTotal Loss: 2.519807\n",
      "Reconstruction: 1.927112, Regularization: 0.527713, Discriminator: 0.043322; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,098 root         INFO     Train Epoch: 170 [7168/8000 (90%)]\tTotal Loss: 3.121873\n",
      "Reconstruction: 2.423998, Regularization: 0.632903, Discriminator: 0.043310; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,208 root         INFO     Train Epoch: 170 [7680/8000 (96%)]\tTotal Loss: 3.123798\n",
      "Reconstruction: 2.427348, Regularization: 0.631472, Discriminator: 0.043315; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,289 root         INFO     ====> Epoch: 170 Average loss: 2.8520\n",
      "2019-04-10 00:02:31,316 root         INFO     Train Epoch: 171 [0/8000 (0%)]\tTotal Loss: 3.045070\n",
      "Reconstruction: 2.336321, Regularization: 0.643766, Discriminator: 0.043319; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,429 root         INFO     Train Epoch: 171 [512/8000 (6%)]\tTotal Loss: 3.179202\n",
      "Reconstruction: 2.447223, Regularization: 0.666992, Discriminator: 0.043331; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,540 root         INFO     Train Epoch: 171 [1024/8000 (13%)]\tTotal Loss: 2.562244\n",
      "Reconstruction: 2.027479, Regularization: 0.469748, Discriminator: 0.043349; Generator: 0.021667,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,653 root         INFO     Train Epoch: 171 [1536/8000 (19%)]\tTotal Loss: 2.598572\n",
      "Reconstruction: 1.990943, Regularization: 0.542611, Discriminator: 0.043349; Generator: 0.021668,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,761 root         INFO     Train Epoch: 171 [2048/8000 (26%)]\tTotal Loss: 2.745039\n",
      "Reconstruction: 2.134041, Regularization: 0.545989, Discriminator: 0.043333; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,871 root         INFO     Train Epoch: 171 [2560/8000 (32%)]\tTotal Loss: 2.845131\n",
      "Reconstruction: 2.251623, Regularization: 0.528492, Discriminator: 0.043332; Generator: 0.021683,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:31,979 root         INFO     Train Epoch: 171 [3072/8000 (38%)]\tTotal Loss: 2.928410\n",
      "Reconstruction: 2.229745, Regularization: 0.633676, Discriminator: 0.043312; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,087 root         INFO     Train Epoch: 171 [3584/8000 (45%)]\tTotal Loss: 2.978403\n",
      "Reconstruction: 2.322099, Regularization: 0.591315, Discriminator: 0.043299; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,196 root         INFO     Train Epoch: 171 [4096/8000 (51%)]\tTotal Loss: 2.761392\n",
      "Reconstruction: 2.151100, Regularization: 0.545286, Discriminator: 0.043325; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,305 root         INFO     Train Epoch: 171 [4608/8000 (58%)]\tTotal Loss: 2.644746\n",
      "Reconstruction: 2.048926, Regularization: 0.530815, Discriminator: 0.043312; Generator: 0.021694,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:32,413 root         INFO     Train Epoch: 171 [5120/8000 (64%)]\tTotal Loss: 2.943366\n",
      "Reconstruction: 2.320004, Regularization: 0.558357, Discriminator: 0.043319; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,522 root         INFO     Train Epoch: 171 [5632/8000 (70%)]\tTotal Loss: 2.992963\n",
      "Reconstruction: 2.368747, Regularization: 0.559228, Discriminator: 0.043310; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,631 root         INFO     Train Epoch: 171 [6144/8000 (77%)]\tTotal Loss: 2.895227\n",
      "Reconstruction: 2.268928, Regularization: 0.561313, Discriminator: 0.043314; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,740 root         INFO     Train Epoch: 171 [6656/8000 (83%)]\tTotal Loss: 2.554536\n",
      "Reconstruction: 1.993047, Regularization: 0.496504, Discriminator: 0.043345; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,850 root         INFO     Train Epoch: 171 [7168/8000 (90%)]\tTotal Loss: 3.392732\n",
      "Reconstruction: 2.749485, Regularization: 0.578260, Discriminator: 0.043330; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:32,959 root         INFO     Train Epoch: 171 [7680/8000 (96%)]\tTotal Loss: 2.749259\n",
      "Reconstruction: 2.216967, Regularization: 0.467265, Discriminator: 0.043384; Generator: 0.021642,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,038 root         INFO     ====> Epoch: 171 Average loss: 2.8989\n",
      "2019-04-10 00:02:33,065 root         INFO     Train Epoch: 172 [0/8000 (0%)]\tTotal Loss: 2.766291\n",
      "Reconstruction: 2.177710, Regularization: 0.523579, Discriminator: 0.043344; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,177 root         INFO     Train Epoch: 172 [512/8000 (6%)]\tTotal Loss: 2.751934\n",
      "Reconstruction: 2.208543, Regularization: 0.478385, Discriminator: 0.043358; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,288 root         INFO     Train Epoch: 172 [1024/8000 (13%)]\tTotal Loss: 2.869633\n",
      "Reconstruction: 2.236312, Regularization: 0.568333, Discriminator: 0.043340; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,398 root         INFO     Train Epoch: 172 [1536/8000 (19%)]\tTotal Loss: 2.402215\n",
      "Reconstruction: 1.844284, Regularization: 0.492967, Discriminator: 0.043323; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,509 root         INFO     Train Epoch: 172 [2048/8000 (26%)]\tTotal Loss: 3.156263\n",
      "Reconstruction: 2.505949, Regularization: 0.585346, Discriminator: 0.043326; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,619 root         INFO     Train Epoch: 172 [2560/8000 (32%)]\tTotal Loss: 3.511826\n",
      "Reconstruction: 2.842333, Regularization: 0.604511, Discriminator: 0.043330; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,728 root         INFO     Train Epoch: 172 [3072/8000 (38%)]\tTotal Loss: 2.686784\n",
      "Reconstruction: 2.157007, Regularization: 0.464797, Discriminator: 0.043331; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,836 root         INFO     Train Epoch: 172 [3584/8000 (45%)]\tTotal Loss: 2.676522\n",
      "Reconstruction: 2.104318, Regularization: 0.507226, Discriminator: 0.043323; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:33,945 root         INFO     Train Epoch: 172 [4096/8000 (51%)]\tTotal Loss: 4.026611\n",
      "Reconstruction: 3.464655, Regularization: 0.496983, Discriminator: 0.043319; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,054 root         INFO     Train Epoch: 172 [4608/8000 (58%)]\tTotal Loss: 3.009938\n",
      "Reconstruction: 2.419906, Regularization: 0.525072, Discriminator: 0.043306; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,162 root         INFO     Train Epoch: 172 [5120/8000 (64%)]\tTotal Loss: 2.454897\n",
      "Reconstruction: 1.923205, Regularization: 0.466730, Discriminator: 0.043303; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,271 root         INFO     Train Epoch: 172 [5632/8000 (70%)]\tTotal Loss: 2.897641\n",
      "Reconstruction: 2.329725, Regularization: 0.502971, Discriminator: 0.043284; Generator: 0.021661,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,379 root         INFO     Train Epoch: 172 [6144/8000 (77%)]\tTotal Loss: 2.798813\n",
      "Reconstruction: 2.178731, Regularization: 0.555093, Discriminator: 0.043326; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,488 root         INFO     Train Epoch: 172 [6656/8000 (83%)]\tTotal Loss: 2.815649\n",
      "Reconstruction: 2.226386, Regularization: 0.524281, Discriminator: 0.043319; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,597 root         INFO     Train Epoch: 172 [7168/8000 (90%)]\tTotal Loss: 3.029012\n",
      "Reconstruction: 2.414649, Regularization: 0.549386, Discriminator: 0.043311; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,707 root         INFO     Train Epoch: 172 [7680/8000 (96%)]\tTotal Loss: 3.112704\n",
      "Reconstruction: 2.511037, Regularization: 0.536692, Discriminator: 0.043308; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,789 root         INFO     ====> Epoch: 172 Average loss: 2.9228\n",
      "2019-04-10 00:02:34,816 root         INFO     Train Epoch: 173 [0/8000 (0%)]\tTotal Loss: 3.878513\n",
      "Reconstruction: 3.245273, Regularization: 0.568238, Discriminator: 0.043333; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:34,928 root         INFO     Train Epoch: 173 [512/8000 (6%)]\tTotal Loss: 2.725713\n",
      "Reconstruction: 2.204880, Regularization: 0.455834, Discriminator: 0.043326; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:35,038 root         INFO     Train Epoch: 173 [1024/8000 (13%)]\tTotal Loss: 2.892660\n",
      "Reconstruction: 2.319281, Regularization: 0.508381, Discriminator: 0.043333; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:35,149 root         INFO     Train Epoch: 173 [1536/8000 (19%)]\tTotal Loss: 2.923960\n",
      "Reconstruction: 2.352937, Regularization: 0.506035, Discriminator: 0.043307; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:35,260 root         INFO     Train Epoch: 173 [2048/8000 (26%)]\tTotal Loss: 2.992251\n",
      "Reconstruction: 2.401547, Regularization: 0.525687, Discriminator: 0.043337; Generator: 0.021681,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:35,371 root         INFO     Train Epoch: 173 [2560/8000 (32%)]\tTotal Loss: 3.769160\n",
      "Reconstruction: 3.161729, Regularization: 0.542419, Discriminator: 0.043330; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:35,481 root         INFO     Train Epoch: 173 [3072/8000 (38%)]\tTotal Loss: 2.962750\n",
      "Reconstruction: 2.400660, Regularization: 0.497094, Discriminator: 0.043307; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:35,591 root         INFO     Train Epoch: 173 [3584/8000 (45%)]\tTotal Loss: 3.211840\n",
      "Reconstruction: 2.635071, Regularization: 0.511761, Discriminator: 0.043319; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:35,700 root         INFO     Train Epoch: 173 [4096/8000 (51%)]\tTotal Loss: 3.016589\n",
      "Reconstruction: 2.444931, Regularization: 0.506640, Discriminator: 0.043312; Generator: 0.021706,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:35,809 root         INFO     Train Epoch: 173 [4608/8000 (58%)]\tTotal Loss: 2.977013\n",
      "Reconstruction: 2.375679, Regularization: 0.536325, Discriminator: 0.043302; Generator: 0.021707,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:35,919 root         INFO     Train Epoch: 173 [5120/8000 (64%)]\tTotal Loss: 3.159870\n",
      "Reconstruction: 2.545982, Regularization: 0.548892, Discriminator: 0.043303; Generator: 0.021693,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:36,028 root         INFO     Train Epoch: 173 [5632/8000 (70%)]\tTotal Loss: 3.040617\n",
      "Reconstruction: 2.520321, Regularization: 0.455342, Discriminator: 0.043272; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:36,138 root         INFO     Train Epoch: 173 [6144/8000 (77%)]\tTotal Loss: 3.207133\n",
      "Reconstruction: 2.623046, Regularization: 0.519052, Discriminator: 0.043339; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:36,247 root         INFO     Train Epoch: 173 [6656/8000 (83%)]\tTotal Loss: 2.999820\n",
      "Reconstruction: 2.432714, Regularization: 0.502125, Discriminator: 0.043317; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:36,357 root         INFO     Train Epoch: 173 [7168/8000 (90%)]\tTotal Loss: 3.675300\n",
      "Reconstruction: 3.126372, Regularization: 0.483974, Discriminator: 0.043301; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:36,467 root         INFO     Train Epoch: 173 [7680/8000 (96%)]\tTotal Loss: 2.924455\n",
      "Reconstruction: 2.353949, Regularization: 0.505502, Discriminator: 0.043356; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:36,548 root         INFO     ====> Epoch: 173 Average loss: 2.9146\n",
      "2019-04-10 00:02:36,575 root         INFO     Train Epoch: 174 [0/8000 (0%)]\tTotal Loss: 3.181237\n",
      "Reconstruction: 2.581477, Regularization: 0.534773, Discriminator: 0.043342; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:36,686 root         INFO     Train Epoch: 174 [512/8000 (6%)]\tTotal Loss: 2.517184\n",
      "Reconstruction: 2.015850, Regularization: 0.436360, Discriminator: 0.043327; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:36,797 root         INFO     Train Epoch: 174 [1024/8000 (13%)]\tTotal Loss: 3.057782\n",
      "Reconstruction: 2.485598, Regularization: 0.507192, Discriminator: 0.043344; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:36,907 root         INFO     Train Epoch: 174 [1536/8000 (19%)]\tTotal Loss: 2.845262\n",
      "Reconstruction: 2.326489, Regularization: 0.453793, Discriminator: 0.043347; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,017 root         INFO     Train Epoch: 174 [2048/8000 (26%)]\tTotal Loss: 2.766603\n",
      "Reconstruction: 2.239638, Regularization: 0.462006, Discriminator: 0.043322; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,126 root         INFO     Train Epoch: 174 [2560/8000 (32%)]\tTotal Loss: 2.672626\n",
      "Reconstruction: 2.136318, Regularization: 0.471344, Discriminator: 0.043328; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,235 root         INFO     Train Epoch: 174 [3072/8000 (38%)]\tTotal Loss: 3.542212\n",
      "Reconstruction: 2.913212, Regularization: 0.564006, Discriminator: 0.043355; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,344 root         INFO     Train Epoch: 174 [3584/8000 (45%)]\tTotal Loss: 2.947022\n",
      "Reconstruction: 2.384578, Regularization: 0.497475, Discriminator: 0.043339; Generator: 0.021630,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,454 root         INFO     Train Epoch: 174 [4096/8000 (51%)]\tTotal Loss: 2.794093\n",
      "Reconstruction: 2.260100, Regularization: 0.469029, Discriminator: 0.043322; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,563 root         INFO     Train Epoch: 174 [4608/8000 (58%)]\tTotal Loss: 2.963986\n",
      "Reconstruction: 2.393047, Regularization: 0.505953, Discriminator: 0.043345; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,672 root         INFO     Train Epoch: 174 [5120/8000 (64%)]\tTotal Loss: 2.681847\n",
      "Reconstruction: 2.159207, Regularization: 0.457668, Discriminator: 0.043330; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,781 root         INFO     Train Epoch: 174 [5632/8000 (70%)]\tTotal Loss: 3.319973\n",
      "Reconstruction: 2.741735, Regularization: 0.513269, Discriminator: 0.043318; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,890 root         INFO     Train Epoch: 174 [6144/8000 (77%)]\tTotal Loss: 2.526988\n",
      "Reconstruction: 2.000112, Regularization: 0.461931, Discriminator: 0.043298; Generator: 0.021648,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:37,999 root         INFO     Train Epoch: 174 [6656/8000 (83%)]\tTotal Loss: 3.029347\n",
      "Reconstruction: 2.528893, Regularization: 0.435497, Discriminator: 0.043313; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,108 root         INFO     Train Epoch: 174 [7168/8000 (90%)]\tTotal Loss: 3.134832\n",
      "Reconstruction: 2.594092, Regularization: 0.475807, Discriminator: 0.043279; Generator: 0.021653,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,217 root         INFO     Train Epoch: 174 [7680/8000 (96%)]\tTotal Loss: 2.976865\n",
      "Reconstruction: 2.411367, Regularization: 0.500521, Discriminator: 0.043323; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,298 root         INFO     ====> Epoch: 174 Average loss: 2.8821\n",
      "2019-04-10 00:02:38,325 root         INFO     Train Epoch: 175 [0/8000 (0%)]\tTotal Loss: 2.851584\n",
      "Reconstruction: 2.312330, Regularization: 0.474261, Discriminator: 0.043344; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,436 root         INFO     Train Epoch: 175 [512/8000 (6%)]\tTotal Loss: 2.564745\n",
      "Reconstruction: 2.054524, Regularization: 0.445236, Discriminator: 0.043331; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,547 root         INFO     Train Epoch: 175 [1024/8000 (13%)]\tTotal Loss: 2.918060\n",
      "Reconstruction: 2.361576, Regularization: 0.491474, Discriminator: 0.043351; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,657 root         INFO     Train Epoch: 175 [1536/8000 (19%)]\tTotal Loss: 2.463716\n",
      "Reconstruction: 1.989426, Regularization: 0.409304, Discriminator: 0.043335; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,766 root         INFO     Train Epoch: 175 [2048/8000 (26%)]\tTotal Loss: 2.911589\n",
      "Reconstruction: 2.387197, Regularization: 0.459387, Discriminator: 0.043341; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,875 root         INFO     Train Epoch: 175 [2560/8000 (32%)]\tTotal Loss: 2.652722\n",
      "Reconstruction: 2.161981, Regularization: 0.425746, Discriminator: 0.043328; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:38,983 root         INFO     Train Epoch: 175 [3072/8000 (38%)]\tTotal Loss: 3.177683\n",
      "Reconstruction: 2.632169, Regularization: 0.480530, Discriminator: 0.043321; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,091 root         INFO     Train Epoch: 175 [3584/8000 (45%)]\tTotal Loss: 2.188920\n",
      "Reconstruction: 1.727108, Regularization: 0.396802, Discriminator: 0.043340; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,200 root         INFO     Train Epoch: 175 [4096/8000 (51%)]\tTotal Loss: 2.977147\n",
      "Reconstruction: 2.468251, Regularization: 0.443901, Discriminator: 0.043320; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,309 root         INFO     Train Epoch: 175 [4608/8000 (58%)]\tTotal Loss: 2.992106\n",
      "Reconstruction: 2.452117, Regularization: 0.474995, Discriminator: 0.043315; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,417 root         INFO     Train Epoch: 175 [5120/8000 (64%)]\tTotal Loss: 2.243776\n",
      "Reconstruction: 1.778922, Regularization: 0.399858, Discriminator: 0.043301; Generator: 0.021694,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:39,526 root         INFO     Train Epoch: 175 [5632/8000 (70%)]\tTotal Loss: 2.946156\n",
      "Reconstruction: 2.408603, Regularization: 0.472577, Discriminator: 0.043288; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,637 root         INFO     Train Epoch: 175 [6144/8000 (77%)]\tTotal Loss: 2.651744\n",
      "Reconstruction: 2.188979, Regularization: 0.397767, Discriminator: 0.043320; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,748 root         INFO     Train Epoch: 175 [6656/8000 (83%)]\tTotal Loss: 2.849060\n",
      "Reconstruction: 2.341723, Regularization: 0.442342, Discriminator: 0.043310; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,858 root         INFO     Train Epoch: 175 [7168/8000 (90%)]\tTotal Loss: 2.830541\n",
      "Reconstruction: 2.309057, Regularization: 0.456523, Discriminator: 0.043272; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:39,967 root         INFO     Train Epoch: 175 [7680/8000 (96%)]\tTotal Loss: 2.729357\n",
      "Reconstruction: 2.232168, Regularization: 0.432256, Discriminator: 0.043262; Generator: 0.021671,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,046 root         INFO     ====> Epoch: 175 Average loss: 2.9063\n",
      "2019-04-10 00:02:40,073 root         INFO     Train Epoch: 176 [0/8000 (0%)]\tTotal Loss: 3.410291\n",
      "Reconstruction: 2.894103, Regularization: 0.451244, Discriminator: 0.043275; Generator: 0.021670,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,185 root         INFO     Train Epoch: 176 [512/8000 (6%)]\tTotal Loss: 3.260635\n",
      "Reconstruction: 2.694358, Regularization: 0.501330, Discriminator: 0.043300; Generator: 0.021648,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,297 root         INFO     Train Epoch: 176 [1024/8000 (13%)]\tTotal Loss: 2.611282\n",
      "Reconstruction: 2.074437, Regularization: 0.471923, Discriminator: 0.043264; Generator: 0.021657,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,409 root         INFO     Train Epoch: 176 [1536/8000 (19%)]\tTotal Loss: 2.861961\n",
      "Reconstruction: 2.343548, Regularization: 0.453418, Discriminator: 0.043326; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,519 root         INFO     Train Epoch: 176 [2048/8000 (26%)]\tTotal Loss: 3.127446\n",
      "Reconstruction: 2.622991, Regularization: 0.439494, Discriminator: 0.043312; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,629 root         INFO     Train Epoch: 176 [2560/8000 (32%)]\tTotal Loss: 3.127384\n",
      "Reconstruction: 2.595551, Regularization: 0.466896, Discriminator: 0.043311; Generator: 0.021627,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:02:40,739 root         INFO     Train Epoch: 176 [3072/8000 (38%)]\tTotal Loss: 2.495583\n",
      "Reconstruction: 2.031942, Regularization: 0.398658, Discriminator: 0.043343; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,849 root         INFO     Train Epoch: 176 [3584/8000 (45%)]\tTotal Loss: 2.762364\n",
      "Reconstruction: 2.307375, Regularization: 0.390010, Discriminator: 0.043333; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:40,960 root         INFO     Train Epoch: 176 [4096/8000 (51%)]\tTotal Loss: 2.969024\n",
      "Reconstruction: 2.483116, Regularization: 0.420939, Discriminator: 0.043334; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,071 root         INFO     Train Epoch: 176 [4608/8000 (58%)]\tTotal Loss: 2.964018\n",
      "Reconstruction: 2.484490, Regularization: 0.414566, Discriminator: 0.043325; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,182 root         INFO     Train Epoch: 176 [5120/8000 (64%)]\tTotal Loss: 2.431023\n",
      "Reconstruction: 2.004632, Regularization: 0.361420, Discriminator: 0.043325; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,292 root         INFO     Train Epoch: 176 [5632/8000 (70%)]\tTotal Loss: 2.937731\n",
      "Reconstruction: 2.448472, Regularization: 0.424304, Discriminator: 0.043304; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,405 root         INFO     Train Epoch: 176 [6144/8000 (77%)]\tTotal Loss: 2.824848\n",
      "Reconstruction: 2.355417, Regularization: 0.404489, Discriminator: 0.043284; Generator: 0.021658,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,516 root         INFO     Train Epoch: 176 [6656/8000 (83%)]\tTotal Loss: 3.394374\n",
      "Reconstruction: 2.845738, Regularization: 0.483706, Discriminator: 0.043274; Generator: 0.021655,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,628 root         INFO     Train Epoch: 176 [7168/8000 (90%)]\tTotal Loss: 2.861086\n",
      "Reconstruction: 2.367860, Regularization: 0.428266, Discriminator: 0.043301; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,739 root         INFO     Train Epoch: 176 [7680/8000 (96%)]\tTotal Loss: 3.203292\n",
      "Reconstruction: 2.679227, Regularization: 0.459122, Discriminator: 0.043291; Generator: 0.021652,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,820 root         INFO     ====> Epoch: 176 Average loss: 2.8950\n",
      "2019-04-10 00:02:41,848 root         INFO     Train Epoch: 177 [0/8000 (0%)]\tTotal Loss: 3.719407\n",
      "Reconstruction: 3.188638, Regularization: 0.465811, Discriminator: 0.043301; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:41,960 root         INFO     Train Epoch: 177 [512/8000 (6%)]\tTotal Loss: 3.220581\n",
      "Reconstruction: 2.750859, Regularization: 0.404756, Discriminator: 0.043305; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,072 root         INFO     Train Epoch: 177 [1024/8000 (13%)]\tTotal Loss: 3.183001\n",
      "Reconstruction: 2.718793, Regularization: 0.399254, Discriminator: 0.043296; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,185 root         INFO     Train Epoch: 177 [1536/8000 (19%)]\tTotal Loss: 2.974467\n",
      "Reconstruction: 2.476200, Regularization: 0.433280, Discriminator: 0.043334; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,297 root         INFO     Train Epoch: 177 [2048/8000 (26%)]\tTotal Loss: 2.717688\n",
      "Reconstruction: 2.204089, Regularization: 0.448580, Discriminator: 0.043364; Generator: 0.021655,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,409 root         INFO     Train Epoch: 177 [2560/8000 (32%)]\tTotal Loss: 2.793571\n",
      "Reconstruction: 2.253628, Regularization: 0.474943, Discriminator: 0.043340; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,521 root         INFO     Train Epoch: 177 [3072/8000 (38%)]\tTotal Loss: 2.334839\n",
      "Reconstruction: 1.885014, Regularization: 0.384781, Discriminator: 0.043385; Generator: 0.021659,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,632 root         INFO     Train Epoch: 177 [3584/8000 (45%)]\tTotal Loss: 2.838006\n",
      "Reconstruction: 2.338041, Regularization: 0.434945, Discriminator: 0.043358; Generator: 0.021661,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,744 root         INFO     Train Epoch: 177 [4096/8000 (51%)]\tTotal Loss: 2.836094\n",
      "Reconstruction: 2.341511, Regularization: 0.429563, Discriminator: 0.043352; Generator: 0.021669,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,853 root         INFO     Train Epoch: 177 [4608/8000 (58%)]\tTotal Loss: 2.410776\n",
      "Reconstruction: 1.984042, Regularization: 0.361712, Discriminator: 0.043349; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:42,962 root         INFO     Train Epoch: 177 [5120/8000 (64%)]\tTotal Loss: 2.789570\n",
      "Reconstruction: 2.325114, Regularization: 0.399437, Discriminator: 0.043331; Generator: 0.021688,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:43,072 root         INFO     Train Epoch: 177 [5632/8000 (70%)]\tTotal Loss: 2.888141\n",
      "Reconstruction: 2.419168, Regularization: 0.403952, Discriminator: 0.043330; Generator: 0.021692,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:43,181 root         INFO     Train Epoch: 177 [6144/8000 (77%)]\tTotal Loss: 2.699521\n",
      "Reconstruction: 2.237949, Regularization: 0.396556, Discriminator: 0.043322; Generator: 0.021693,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:43,290 root         INFO     Train Epoch: 177 [6656/8000 (83%)]\tTotal Loss: 2.691332\n",
      "Reconstruction: 2.229785, Regularization: 0.396515, Discriminator: 0.043325; Generator: 0.021707,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:43,398 root         INFO     Train Epoch: 177 [7168/8000 (90%)]\tTotal Loss: 2.557261\n",
      "Reconstruction: 2.103393, Regularization: 0.388870, Discriminator: 0.043295; Generator: 0.021704,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:43,508 root         INFO     Train Epoch: 177 [7680/8000 (96%)]\tTotal Loss: 2.422448\n",
      "Reconstruction: 2.002869, Regularization: 0.354554, Discriminator: 0.043331; Generator: 0.021695,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:43,588 root         INFO     ====> Epoch: 177 Average loss: 2.9052\n",
      "2019-04-10 00:02:43,615 root         INFO     Train Epoch: 178 [0/8000 (0%)]\tTotal Loss: 2.626387\n",
      "Reconstruction: 2.203489, Regularization: 0.357894, Discriminator: 0.043316; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:43,726 root         INFO     Train Epoch: 178 [512/8000 (6%)]\tTotal Loss: 3.447478\n",
      "Reconstruction: 2.968931, Regularization: 0.413508, Discriminator: 0.043350; Generator: 0.021689,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:43,836 root         INFO     Train Epoch: 178 [1024/8000 (13%)]\tTotal Loss: 2.915060\n",
      "Reconstruction: 2.448761, Regularization: 0.401279, Discriminator: 0.043336; Generator: 0.021683,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:43,945 root         INFO     Train Epoch: 178 [1536/8000 (19%)]\tTotal Loss: 2.717819\n",
      "Reconstruction: 2.247057, Regularization: 0.405744, Discriminator: 0.043348; Generator: 0.021670,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,055 root         INFO     Train Epoch: 178 [2048/8000 (26%)]\tTotal Loss: 2.923167\n",
      "Reconstruction: 2.490317, Regularization: 0.367881, Discriminator: 0.043322; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,164 root         INFO     Train Epoch: 178 [2560/8000 (32%)]\tTotal Loss: 2.889282\n",
      "Reconstruction: 2.399487, Regularization: 0.424826, Discriminator: 0.043313; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,273 root         INFO     Train Epoch: 178 [3072/8000 (38%)]\tTotal Loss: 2.595681\n",
      "Reconstruction: 2.159081, Regularization: 0.371621, Discriminator: 0.043327; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,382 root         INFO     Train Epoch: 178 [3584/8000 (45%)]\tTotal Loss: 2.537338\n",
      "Reconstruction: 2.119733, Regularization: 0.352616, Discriminator: 0.043347; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,492 root         INFO     Train Epoch: 178 [4096/8000 (51%)]\tTotal Loss: 3.069097\n",
      "Reconstruction: 2.606575, Regularization: 0.397554, Discriminator: 0.043330; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,602 root         INFO     Train Epoch: 178 [4608/8000 (58%)]\tTotal Loss: 2.583353\n",
      "Reconstruction: 2.134140, Regularization: 0.384256, Discriminator: 0.043333; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:44,711 root         INFO     Train Epoch: 178 [5120/8000 (64%)]\tTotal Loss: 2.804629\n",
      "Reconstruction: 2.338826, Regularization: 0.400845, Discriminator: 0.043321; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,820 root         INFO     Train Epoch: 178 [5632/8000 (70%)]\tTotal Loss: 2.412671\n",
      "Reconstruction: 1.981307, Regularization: 0.366398, Discriminator: 0.043311; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:44,930 root         INFO     Train Epoch: 178 [6144/8000 (77%)]\tTotal Loss: 2.759942\n",
      "Reconstruction: 2.330648, Regularization: 0.364341, Discriminator: 0.043309; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,041 root         INFO     Train Epoch: 178 [6656/8000 (83%)]\tTotal Loss: 2.235337\n",
      "Reconstruction: 1.800483, Regularization: 0.369910, Discriminator: 0.043304; Generator: 0.021641,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,152 root         INFO     Train Epoch: 178 [7168/8000 (90%)]\tTotal Loss: 2.452785\n",
      "Reconstruction: 2.032963, Regularization: 0.354865, Discriminator: 0.043302; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,262 root         INFO     Train Epoch: 178 [7680/8000 (96%)]\tTotal Loss: 2.946366\n",
      "Reconstruction: 2.500715, Regularization: 0.380707, Discriminator: 0.043283; Generator: 0.021662,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,343 root         INFO     ====> Epoch: 178 Average loss: 2.9012\n",
      "2019-04-10 00:02:45,370 root         INFO     Train Epoch: 179 [0/8000 (0%)]\tTotal Loss: 2.824000\n",
      "Reconstruction: 2.351387, Regularization: 0.407646, Discriminator: 0.043304; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,483 root         INFO     Train Epoch: 179 [512/8000 (6%)]\tTotal Loss: 3.371507\n",
      "Reconstruction: 2.930772, Regularization: 0.375770, Discriminator: 0.043308; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,595 root         INFO     Train Epoch: 179 [1024/8000 (13%)]\tTotal Loss: 2.684756\n",
      "Reconstruction: 2.254907, Regularization: 0.364898, Discriminator: 0.043289; Generator: 0.021663,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,706 root         INFO     Train Epoch: 179 [1536/8000 (19%)]\tTotal Loss: 2.760673\n",
      "Reconstruction: 2.326197, Regularization: 0.369534, Discriminator: 0.043271; Generator: 0.021671,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,816 root         INFO     Train Epoch: 179 [2048/8000 (26%)]\tTotal Loss: 3.149193\n",
      "Reconstruction: 2.677250, Regularization: 0.406945, Discriminator: 0.043333; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:45,927 root         INFO     Train Epoch: 179 [2560/8000 (32%)]\tTotal Loss: 2.804098\n",
      "Reconstruction: 2.353449, Regularization: 0.385669, Discriminator: 0.043313; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,037 root         INFO     Train Epoch: 179 [3072/8000 (38%)]\tTotal Loss: 2.572258\n",
      "Reconstruction: 2.120810, Regularization: 0.386451, Discriminator: 0.043319; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,147 root         INFO     Train Epoch: 179 [3584/8000 (45%)]\tTotal Loss: 2.809994\n",
      "Reconstruction: 2.350634, Regularization: 0.394349, Discriminator: 0.043350; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,258 root         INFO     Train Epoch: 179 [4096/8000 (51%)]\tTotal Loss: 3.446393\n",
      "Reconstruction: 2.956755, Regularization: 0.424619, Discriminator: 0.043354; Generator: 0.021664,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,368 root         INFO     Train Epoch: 179 [4608/8000 (58%)]\tTotal Loss: 3.023482\n",
      "Reconstruction: 2.552253, Regularization: 0.406249, Discriminator: 0.043313; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,478 root         INFO     Train Epoch: 179 [5120/8000 (64%)]\tTotal Loss: 3.205869\n",
      "Reconstruction: 2.747396, Regularization: 0.393480, Discriminator: 0.043319; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,589 root         INFO     Train Epoch: 179 [5632/8000 (70%)]\tTotal Loss: 3.053477\n",
      "Reconstruction: 2.550535, Regularization: 0.437877, Discriminator: 0.043394; Generator: 0.021670,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,700 root         INFO     Train Epoch: 179 [6144/8000 (77%)]\tTotal Loss: 2.821101\n",
      "Reconstruction: 2.357676, Regularization: 0.398399, Discriminator: 0.043351; Generator: 0.021675,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,811 root         INFO     Train Epoch: 179 [6656/8000 (83%)]\tTotal Loss: 2.629123\n",
      "Reconstruction: 2.233559, Regularization: 0.330569, Discriminator: 0.043318; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:46,921 root         INFO     Train Epoch: 179 [7168/8000 (90%)]\tTotal Loss: 2.867225\n",
      "Reconstruction: 2.392998, Regularization: 0.409193, Discriminator: 0.043355; Generator: 0.021680,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:47,032 root         INFO     Train Epoch: 179 [7680/8000 (96%)]\tTotal Loss: 3.042176\n",
      "Reconstruction: 2.600317, Regularization: 0.376837, Discriminator: 0.043326; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:47,113 root         INFO     ====> Epoch: 179 Average loss: 2.8693\n",
      "2019-04-10 00:02:47,140 root         INFO     Train Epoch: 180 [0/8000 (0%)]\tTotal Loss: 3.295609\n",
      "Reconstruction: 2.864056, Regularization: 0.366536, Discriminator: 0.043319; Generator: 0.021698,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:47,251 root         INFO     Train Epoch: 180 [512/8000 (6%)]\tTotal Loss: 2.853178\n",
      "Reconstruction: 2.420136, Regularization: 0.368038, Discriminator: 0.043327; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:47,361 root         INFO     Train Epoch: 180 [1024/8000 (13%)]\tTotal Loss: 2.580587\n",
      "Reconstruction: 2.161283, Regularization: 0.354301, Discriminator: 0.043302; Generator: 0.021701,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:47,471 root         INFO     Train Epoch: 180 [1536/8000 (19%)]\tTotal Loss: 3.165056\n",
      "Reconstruction: 2.753999, Regularization: 0.346067, Discriminator: 0.043310; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:47,580 root         INFO     Train Epoch: 180 [2048/8000 (26%)]\tTotal Loss: 2.583937\n",
      "Reconstruction: 2.139895, Regularization: 0.379090, Discriminator: 0.043288; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:47,689 root         INFO     Train Epoch: 180 [2560/8000 (32%)]\tTotal Loss: 3.164214\n",
      "Reconstruction: 2.760716, Regularization: 0.338477, Discriminator: 0.043333; Generator: 0.021688,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:47,798 root         INFO     Train Epoch: 180 [3072/8000 (38%)]\tTotal Loss: 2.741256\n",
      "Reconstruction: 2.329648, Regularization: 0.346601, Discriminator: 0.043341; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:47,907 root         INFO     Train Epoch: 180 [3584/8000 (45%)]\tTotal Loss: 2.618741\n",
      "Reconstruction: 2.252695, Regularization: 0.301098, Discriminator: 0.043288; Generator: 0.021660,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:48,017 root         INFO     Train Epoch: 180 [4096/8000 (51%)]\tTotal Loss: 2.380058\n",
      "Reconstruction: 1.998540, Regularization: 0.316529, Discriminator: 0.043339; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:48,126 root         INFO     Train Epoch: 180 [4608/8000 (58%)]\tTotal Loss: 2.905020\n",
      "Reconstruction: 2.487051, Regularization: 0.352962, Discriminator: 0.043362; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:48,235 root         INFO     Train Epoch: 180 [5120/8000 (64%)]\tTotal Loss: 2.308369\n",
      "Reconstruction: 1.950915, Regularization: 0.292470, Discriminator: 0.043347; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:48,344 root         INFO     Train Epoch: 180 [5632/8000 (70%)]\tTotal Loss: 2.361337\n",
      "Reconstruction: 1.996064, Regularization: 0.300296, Discriminator: 0.043348; Generator: 0.021629,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:48,453 root         INFO     Train Epoch: 180 [6144/8000 (77%)]\tTotal Loss: 3.283792\n",
      "Reconstruction: 2.802734, Regularization: 0.416087, Discriminator: 0.043351; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:48,563 root         INFO     Train Epoch: 180 [6656/8000 (83%)]\tTotal Loss: 2.768209\n",
      "Reconstruction: 2.354878, Regularization: 0.348352, Discriminator: 0.043356; Generator: 0.021623,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:48,669 root         INFO     Train Epoch: 180 [7168/8000 (90%)]\tTotal Loss: 2.783460\n",
      "Reconstruction: 2.369102, Regularization: 0.349387, Discriminator: 0.043335; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:48,774 root         INFO     Train Epoch: 180 [7680/8000 (96%)]\tTotal Loss: 3.082970\n",
      "Reconstruction: 2.703294, Regularization: 0.314709, Discriminator: 0.043334; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:48,852 root         INFO     ====> Epoch: 180 Average loss: 2.8550\n",
      "2019-04-10 00:02:48,880 root         INFO     Train Epoch: 181 [0/8000 (0%)]\tTotal Loss: 3.215242\n",
      "Reconstruction: 2.791593, Regularization: 0.358694, Discriminator: 0.043341; Generator: 0.021615,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:48,992 root         INFO     Train Epoch: 181 [512/8000 (6%)]\tTotal Loss: 2.625164\n",
      "Reconstruction: 2.241206, Regularization: 0.319002, Discriminator: 0.043328; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:49,103 root         INFO     Train Epoch: 181 [1024/8000 (13%)]\tTotal Loss: 2.454164\n",
      "Reconstruction: 2.076002, Regularization: 0.313203, Discriminator: 0.043322; Generator: 0.021637,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,213 root         INFO     Train Epoch: 181 [1536/8000 (19%)]\tTotal Loss: 2.806334\n",
      "Reconstruction: 2.432651, Regularization: 0.308733, Discriminator: 0.043316; Generator: 0.021634,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,323 root         INFO     Train Epoch: 181 [2048/8000 (26%)]\tTotal Loss: 3.564088\n",
      "Reconstruction: 3.130347, Regularization: 0.368810, Discriminator: 0.043294; Generator: 0.021637,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,433 root         INFO     Train Epoch: 181 [2560/8000 (32%)]\tTotal Loss: 2.640741\n",
      "Reconstruction: 2.244040, Regularization: 0.331744, Discriminator: 0.043310; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,545 root         INFO     Train Epoch: 181 [3072/8000 (38%)]\tTotal Loss: 3.572783\n",
      "Reconstruction: 3.122525, Regularization: 0.385319, Discriminator: 0.043290; Generator: 0.021650,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,657 root         INFO     Train Epoch: 181 [3584/8000 (45%)]\tTotal Loss: 2.499994\n",
      "Reconstruction: 2.133468, Regularization: 0.301572, Discriminator: 0.043302; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,768 root         INFO     Train Epoch: 181 [4096/8000 (51%)]\tTotal Loss: 3.004945\n",
      "Reconstruction: 2.592062, Regularization: 0.347930, Discriminator: 0.043298; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,880 root         INFO     Train Epoch: 181 [4608/8000 (58%)]\tTotal Loss: 2.987895\n",
      "Reconstruction: 2.585377, Regularization: 0.337592, Discriminator: 0.043272; Generator: 0.021654,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:49,992 root         INFO     Train Epoch: 181 [5120/8000 (64%)]\tTotal Loss: 2.584520\n",
      "Reconstruction: 2.205092, Regularization: 0.314449, Discriminator: 0.043323; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,103 root         INFO     Train Epoch: 181 [5632/8000 (70%)]\tTotal Loss: 2.898837\n",
      "Reconstruction: 2.545676, Regularization: 0.288170, Discriminator: 0.043333; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,215 root         INFO     Train Epoch: 181 [6144/8000 (77%)]\tTotal Loss: 3.045283\n",
      "Reconstruction: 2.611954, Regularization: 0.368372, Discriminator: 0.043299; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,326 root         INFO     Train Epoch: 181 [6656/8000 (83%)]\tTotal Loss: 2.735247\n",
      "Reconstruction: 2.372172, Regularization: 0.298073, Discriminator: 0.043343; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,438 root         INFO     Train Epoch: 181 [7168/8000 (90%)]\tTotal Loss: 2.447659\n",
      "Reconstruction: 2.092012, Regularization: 0.290640, Discriminator: 0.043348; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,549 root         INFO     Train Epoch: 181 [7680/8000 (96%)]\tTotal Loss: 2.713808\n",
      "Reconstruction: 2.342469, Regularization: 0.306349, Discriminator: 0.043325; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,628 root         INFO     ====> Epoch: 181 Average loss: 2.8770\n",
      "2019-04-10 00:02:50,654 root         INFO     Train Epoch: 182 [0/8000 (0%)]\tTotal Loss: 2.479997\n",
      "Reconstruction: 2.128495, Regularization: 0.286503, Discriminator: 0.043332; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,761 root         INFO     Train Epoch: 182 [512/8000 (6%)]\tTotal Loss: 3.209102\n",
      "Reconstruction: 2.816179, Regularization: 0.327952, Discriminator: 0.043303; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,867 root         INFO     Train Epoch: 182 [1024/8000 (13%)]\tTotal Loss: 3.126027\n",
      "Reconstruction: 2.713639, Regularization: 0.347393, Discriminator: 0.043320; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:50,973 root         INFO     Train Epoch: 182 [1536/8000 (19%)]\tTotal Loss: 3.216844\n",
      "Reconstruction: 2.783799, Regularization: 0.368057, Discriminator: 0.043305; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,079 root         INFO     Train Epoch: 182 [2048/8000 (26%)]\tTotal Loss: 3.558725\n",
      "Reconstruction: 3.103039, Regularization: 0.390712, Discriminator: 0.043289; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,185 root         INFO     Train Epoch: 182 [2560/8000 (32%)]\tTotal Loss: 2.812691\n",
      "Reconstruction: 2.426361, Regularization: 0.321338, Discriminator: 0.043306; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,290 root         INFO     Train Epoch: 182 [3072/8000 (38%)]\tTotal Loss: 3.135398\n",
      "Reconstruction: 2.730588, Regularization: 0.339829, Discriminator: 0.043294; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,397 root         INFO     Train Epoch: 182 [3584/8000 (45%)]\tTotal Loss: 3.087871\n",
      "Reconstruction: 2.678964, Regularization: 0.343937, Discriminator: 0.043287; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,503 root         INFO     Train Epoch: 182 [4096/8000 (51%)]\tTotal Loss: 2.526107\n",
      "Reconstruction: 2.181912, Regularization: 0.279178, Discriminator: 0.043314; Generator: 0.021703,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:51,610 root         INFO     Train Epoch: 182 [4608/8000 (58%)]\tTotal Loss: 2.323392\n",
      "Reconstruction: 1.966971, Regularization: 0.291427, Discriminator: 0.043309; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,719 root         INFO     Train Epoch: 182 [5120/8000 (64%)]\tTotal Loss: 2.868523\n",
      "Reconstruction: 2.488280, Regularization: 0.315270, Discriminator: 0.043292; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,828 root         INFO     Train Epoch: 182 [5632/8000 (70%)]\tTotal Loss: 2.776688\n",
      "Reconstruction: 2.434167, Regularization: 0.277551, Discriminator: 0.043313; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:51,935 root         INFO     Train Epoch: 182 [6144/8000 (77%)]\tTotal Loss: 3.042707\n",
      "Reconstruction: 2.651809, Regularization: 0.325889, Discriminator: 0.043330; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,044 root         INFO     Train Epoch: 182 [6656/8000 (83%)]\tTotal Loss: 3.265024\n",
      "Reconstruction: 2.838408, Regularization: 0.361654, Discriminator: 0.043315; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,152 root         INFO     Train Epoch: 182 [7168/8000 (90%)]\tTotal Loss: 2.953256\n",
      "Reconstruction: 2.536907, Regularization: 0.351392, Discriminator: 0.043304; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,260 root         INFO     Train Epoch: 182 [7680/8000 (96%)]\tTotal Loss: 2.593776\n",
      "Reconstruction: 2.230791, Regularization: 0.298009, Discriminator: 0.043334; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,339 root         INFO     ====> Epoch: 182 Average loss: 2.8446\n",
      "2019-04-10 00:02:52,366 root         INFO     Train Epoch: 183 [0/8000 (0%)]\tTotal Loss: 2.314156\n",
      "Reconstruction: 1.961791, Regularization: 0.287376, Discriminator: 0.043351; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,475 root         INFO     Train Epoch: 183 [512/8000 (6%)]\tTotal Loss: 2.530921\n",
      "Reconstruction: 2.154912, Regularization: 0.311037, Discriminator: 0.043330; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,582 root         INFO     Train Epoch: 183 [1024/8000 (13%)]\tTotal Loss: 2.667096\n",
      "Reconstruction: 2.304909, Regularization: 0.297234, Discriminator: 0.043328; Generator: 0.021625,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:52,690 root         INFO     Train Epoch: 183 [1536/8000 (19%)]\tTotal Loss: 3.346329\n",
      "Reconstruction: 2.945856, Regularization: 0.335523, Discriminator: 0.043316; Generator: 0.021633,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,797 root         INFO     Train Epoch: 183 [2048/8000 (26%)]\tTotal Loss: 3.134192\n",
      "Reconstruction: 2.752632, Regularization: 0.316597, Discriminator: 0.043322; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:52,905 root         INFO     Train Epoch: 183 [2560/8000 (32%)]\tTotal Loss: 3.916839\n",
      "Reconstruction: 3.532385, Regularization: 0.319481, Discriminator: 0.043330; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,012 root         INFO     Train Epoch: 183 [3072/8000 (38%)]\tTotal Loss: 2.240230\n",
      "Reconstruction: 1.905143, Regularization: 0.270108, Discriminator: 0.043341; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,120 root         INFO     Train Epoch: 183 [3584/8000 (45%)]\tTotal Loss: 2.096364\n",
      "Reconstruction: 1.769465, Regularization: 0.261913, Discriminator: 0.043342; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,228 root         INFO     Train Epoch: 183 [4096/8000 (51%)]\tTotal Loss: 2.687539\n",
      "Reconstruction: 2.340715, Regularization: 0.281865, Discriminator: 0.043313; Generator: 0.021646,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,336 root         INFO     Train Epoch: 183 [4608/8000 (58%)]\tTotal Loss: 2.224358\n",
      "Reconstruction: 1.882172, Regularization: 0.277213, Discriminator: 0.043325; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,443 root         INFO     Train Epoch: 183 [5120/8000 (64%)]\tTotal Loss: 2.602134\n",
      "Reconstruction: 2.250613, Regularization: 0.286550, Discriminator: 0.043320; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,550 root         INFO     Train Epoch: 183 [5632/8000 (70%)]\tTotal Loss: 3.090461\n",
      "Reconstruction: 2.752759, Regularization: 0.272723, Discriminator: 0.043323; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,657 root         INFO     Train Epoch: 183 [6144/8000 (77%)]\tTotal Loss: 2.831245\n",
      "Reconstruction: 2.478824, Regularization: 0.287440, Discriminator: 0.043323; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,764 root         INFO     Train Epoch: 183 [6656/8000 (83%)]\tTotal Loss: 3.143835\n",
      "Reconstruction: 2.772065, Regularization: 0.306793, Discriminator: 0.043320; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,871 root         INFO     Train Epoch: 183 [7168/8000 (90%)]\tTotal Loss: 2.854297\n",
      "Reconstruction: 2.487572, Regularization: 0.301743, Discriminator: 0.043324; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:53,978 root         INFO     Train Epoch: 183 [7680/8000 (96%)]\tTotal Loss: 2.366685\n",
      "Reconstruction: 2.035511, Regularization: 0.266178, Discriminator: 0.043336; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,057 root         INFO     ====> Epoch: 183 Average loss: 2.8593\n",
      "2019-04-10 00:02:54,084 root         INFO     Train Epoch: 184 [0/8000 (0%)]\tTotal Loss: 2.447463\n",
      "Reconstruction: 2.112627, Regularization: 0.269819, Discriminator: 0.043355; Generator: 0.021661,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,194 root         INFO     Train Epoch: 184 [512/8000 (6%)]\tTotal Loss: 3.151394\n",
      "Reconstruction: 2.773776, Regularization: 0.312624, Discriminator: 0.043331; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,302 root         INFO     Train Epoch: 184 [1024/8000 (13%)]\tTotal Loss: 3.083366\n",
      "Reconstruction: 2.732324, Regularization: 0.286037, Discriminator: 0.043339; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,411 root         INFO     Train Epoch: 184 [1536/8000 (19%)]\tTotal Loss: 2.428744\n",
      "Reconstruction: 2.088647, Regularization: 0.275094, Discriminator: 0.043334; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,520 root         INFO     Train Epoch: 184 [2048/8000 (26%)]\tTotal Loss: 2.521512\n",
      "Reconstruction: 2.142803, Regularization: 0.313683, Discriminator: 0.043355; Generator: 0.021671,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,629 root         INFO     Train Epoch: 184 [2560/8000 (32%)]\tTotal Loss: 3.524943\n",
      "Reconstruction: 3.131670, Regularization: 0.328264, Discriminator: 0.043330; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,737 root         INFO     Train Epoch: 184 [3072/8000 (38%)]\tTotal Loss: 3.161957\n",
      "Reconstruction: 2.772828, Regularization: 0.324122, Discriminator: 0.043324; Generator: 0.021682,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,847 root         INFO     Train Epoch: 184 [3584/8000 (45%)]\tTotal Loss: 2.495148\n",
      "Reconstruction: 2.112778, Regularization: 0.317356, Discriminator: 0.043325; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:54,955 root         INFO     Train Epoch: 184 [4096/8000 (51%)]\tTotal Loss: 2.372362\n",
      "Reconstruction: 2.033391, Regularization: 0.273963, Discriminator: 0.043313; Generator: 0.021696,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:55,064 root         INFO     Train Epoch: 184 [4608/8000 (58%)]\tTotal Loss: 2.199618\n",
      "Reconstruction: 1.851198, Regularization: 0.283405, Discriminator: 0.043312; Generator: 0.021703,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:55,171 root         INFO     Train Epoch: 184 [5120/8000 (64%)]\tTotal Loss: 3.108703\n",
      "Reconstruction: 2.701234, Regularization: 0.342490, Discriminator: 0.043290; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:55,278 root         INFO     Train Epoch: 184 [5632/8000 (70%)]\tTotal Loss: 2.705885\n",
      "Reconstruction: 2.383883, Regularization: 0.257009, Discriminator: 0.043293; Generator: 0.021699,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:55,386 root         INFO     Train Epoch: 184 [6144/8000 (77%)]\tTotal Loss: 3.449475\n",
      "Reconstruction: 3.096682, Regularization: 0.287776, Discriminator: 0.043316; Generator: 0.021701,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:02:55,495 root         INFO     Train Epoch: 184 [6656/8000 (83%)]\tTotal Loss: 2.881552\n",
      "Reconstruction: 2.582816, Regularization: 0.233781, Discriminator: 0.043256; Generator: 0.021698,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:02:55,602 root         INFO     Train Epoch: 184 [7168/8000 (90%)]\tTotal Loss: 1.874007\n",
      "Reconstruction: 1.574183, Regularization: 0.234870, Discriminator: 0.043265; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:55,709 root         INFO     Train Epoch: 184 [7680/8000 (96%)]\tTotal Loss: 2.406258\n",
      "Reconstruction: 2.074774, Regularization: 0.266495, Discriminator: 0.043297; Generator: 0.021692,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:55,789 root         INFO     ====> Epoch: 184 Average loss: 2.8635\n",
      "2019-04-10 00:02:55,816 root         INFO     Train Epoch: 185 [0/8000 (0%)]\tTotal Loss: 2.971050\n",
      "Reconstruction: 2.651582, Regularization: 0.254470, Discriminator: 0.043331; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:55,924 root         INFO     Train Epoch: 185 [512/8000 (6%)]\tTotal Loss: 2.456462\n",
      "Reconstruction: 2.132013, Regularization: 0.259482, Discriminator: 0.043302; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:56,032 root         INFO     Train Epoch: 185 [1024/8000 (13%)]\tTotal Loss: 4.240285\n",
      "Reconstruction: 3.860220, Regularization: 0.315072, Discriminator: 0.043355; Generator: 0.021639,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:56,140 root         INFO     Train Epoch: 185 [1536/8000 (19%)]\tTotal Loss: 2.726438\n",
      "Reconstruction: 2.367380, Regularization: 0.294070, Discriminator: 0.043334; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:56,248 root         INFO     Train Epoch: 185 [2048/8000 (26%)]\tTotal Loss: 2.650646\n",
      "Reconstruction: 2.335072, Regularization: 0.250598, Discriminator: 0.043330; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:56,357 root         INFO     Train Epoch: 185 [2560/8000 (32%)]\tTotal Loss: 3.053948\n",
      "Reconstruction: 2.705003, Regularization: 0.283935, Discriminator: 0.043373; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:56,465 root         INFO     Train Epoch: 185 [3072/8000 (38%)]\tTotal Loss: 2.545944\n",
      "Reconstruction: 2.232178, Regularization: 0.248820, Discriminator: 0.043318; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:02:56,573 root         INFO     Train Epoch: 185 [3584/8000 (45%)]\tTotal Loss: 2.272053\n",
      "Reconstruction: 1.945914, Regularization: 0.261191, Discriminator: 0.043311; Generator: 0.021638,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:56,680 root         INFO     Train Epoch: 185 [4096/8000 (51%)]\tTotal Loss: 2.972661\n",
      "Reconstruction: 2.611330, Regularization: 0.296368, Discriminator: 0.043336; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:02:56,788 root         INFO     Train Epoch: 185 [4608/8000 (58%)]\tTotal Loss: 2.556136\n",
      "Reconstruction: 2.219543, Regularization: 0.271644, Discriminator: 0.043303; Generator: 0.021645,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:56,896 root         INFO     Train Epoch: 185 [5120/8000 (64%)]\tTotal Loss: 2.738105\n",
      "Reconstruction: 2.387475, Regularization: 0.285662, Discriminator: 0.043327; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,002 root         INFO     Train Epoch: 185 [5632/8000 (70%)]\tTotal Loss: 3.121907\n",
      "Reconstruction: 2.751435, Regularization: 0.305519, Discriminator: 0.043309; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,111 root         INFO     Train Epoch: 185 [6144/8000 (77%)]\tTotal Loss: 2.321545\n",
      "Reconstruction: 2.025549, Regularization: 0.231052, Discriminator: 0.043289; Generator: 0.021655,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,219 root         INFO     Train Epoch: 185 [6656/8000 (83%)]\tTotal Loss: 3.041528\n",
      "Reconstruction: 2.718043, Regularization: 0.258549, Discriminator: 0.043279; Generator: 0.021657,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,327 root         INFO     Train Epoch: 185 [7168/8000 (90%)]\tTotal Loss: 2.663311\n",
      "Reconstruction: 2.344227, Regularization: 0.254134, Discriminator: 0.043294; Generator: 0.021656,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,435 root         INFO     Train Epoch: 185 [7680/8000 (96%)]\tTotal Loss: 3.017705\n",
      "Reconstruction: 2.698851, Regularization: 0.253885, Discriminator: 0.043318; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,513 root         INFO     ====> Epoch: 185 Average loss: 2.8447\n",
      "2019-04-10 00:02:57,540 root         INFO     Train Epoch: 186 [0/8000 (0%)]\tTotal Loss: 3.424263\n",
      "Reconstruction: 3.099089, Regularization: 0.260197, Discriminator: 0.043321; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,648 root         INFO     Train Epoch: 186 [512/8000 (6%)]\tTotal Loss: 2.726983\n",
      "Reconstruction: 2.402450, Regularization: 0.259548, Discriminator: 0.043338; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,755 root         INFO     Train Epoch: 186 [1024/8000 (13%)]\tTotal Loss: 2.478920\n",
      "Reconstruction: 2.158035, Regularization: 0.255927, Discriminator: 0.043316; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,861 root         INFO     Train Epoch: 186 [1536/8000 (19%)]\tTotal Loss: 3.154465\n",
      "Reconstruction: 2.826126, Regularization: 0.263377, Discriminator: 0.043313; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:57,969 root         INFO     Train Epoch: 186 [2048/8000 (26%)]\tTotal Loss: 2.580919\n",
      "Reconstruction: 2.260641, Regularization: 0.255283, Discriminator: 0.043338; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,077 root         INFO     Train Epoch: 186 [2560/8000 (32%)]\tTotal Loss: 2.324974\n",
      "Reconstruction: 2.020106, Regularization: 0.239866, Discriminator: 0.043343; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,184 root         INFO     Train Epoch: 186 [3072/8000 (38%)]\tTotal Loss: 3.366478\n",
      "Reconstruction: 3.019681, Regularization: 0.281808, Discriminator: 0.043339; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,290 root         INFO     Train Epoch: 186 [3584/8000 (45%)]\tTotal Loss: 2.796794\n",
      "Reconstruction: 2.442310, Regularization: 0.289465, Discriminator: 0.043366; Generator: 0.021653,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,397 root         INFO     Train Epoch: 186 [4096/8000 (51%)]\tTotal Loss: 1.754896\n",
      "Reconstruction: 1.487145, Regularization: 0.202729, Discriminator: 0.043358; Generator: 0.021663,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,505 root         INFO     Train Epoch: 186 [4608/8000 (58%)]\tTotal Loss: 2.707341\n",
      "Reconstruction: 2.369509, Regularization: 0.272812, Discriminator: 0.043346; Generator: 0.021674,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,611 root         INFO     Train Epoch: 186 [5120/8000 (64%)]\tTotal Loss: 2.541824\n",
      "Reconstruction: 2.213275, Regularization: 0.263532, Discriminator: 0.043343; Generator: 0.021674,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,719 root         INFO     Train Epoch: 186 [5632/8000 (70%)]\tTotal Loss: 3.182250\n",
      "Reconstruction: 2.837611, Regularization: 0.279627, Discriminator: 0.043335; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,826 root         INFO     Train Epoch: 186 [6144/8000 (77%)]\tTotal Loss: 3.064942\n",
      "Reconstruction: 2.754267, Regularization: 0.245673, Discriminator: 0.043315; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:58,932 root         INFO     Train Epoch: 186 [6656/8000 (83%)]\tTotal Loss: 2.154631\n",
      "Reconstruction: 1.864061, Regularization: 0.225582, Discriminator: 0.043299; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,039 root         INFO     Train Epoch: 186 [7168/8000 (90%)]\tTotal Loss: 3.279006\n",
      "Reconstruction: 2.948489, Regularization: 0.265528, Discriminator: 0.043299; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,146 root         INFO     Train Epoch: 186 [7680/8000 (96%)]\tTotal Loss: 2.537507\n",
      "Reconstruction: 2.240899, Regularization: 0.231587, Discriminator: 0.043329; Generator: 0.021692,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,225 root         INFO     ====> Epoch: 186 Average loss: 2.8334\n",
      "2019-04-10 00:02:59,252 root         INFO     Train Epoch: 187 [0/8000 (0%)]\tTotal Loss: 3.096515\n",
      "Reconstruction: 2.770325, Regularization: 0.261176, Discriminator: 0.043340; Generator: 0.021676,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,358 root         INFO     Train Epoch: 187 [512/8000 (6%)]\tTotal Loss: 3.037621\n",
      "Reconstruction: 2.704058, Regularization: 0.268555, Discriminator: 0.043324; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,464 root         INFO     Train Epoch: 187 [1024/8000 (13%)]\tTotal Loss: 2.405347\n",
      "Reconstruction: 2.120262, Regularization: 0.220124, Discriminator: 0.043298; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,572 root         INFO     Train Epoch: 187 [1536/8000 (19%)]\tTotal Loss: 2.637451\n",
      "Reconstruction: 2.353439, Regularization: 0.219033, Discriminator: 0.043326; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,683 root         INFO     Train Epoch: 187 [2048/8000 (26%)]\tTotal Loss: 2.320083\n",
      "Reconstruction: 2.032650, Regularization: 0.222439, Discriminator: 0.043337; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,791 root         INFO     Train Epoch: 187 [2560/8000 (32%)]\tTotal Loss: 2.196009\n",
      "Reconstruction: 1.918912, Regularization: 0.212101, Discriminator: 0.043349; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:02:59,900 root         INFO     Train Epoch: 187 [3072/8000 (38%)]\tTotal Loss: 2.934514\n",
      "Reconstruction: 2.634754, Regularization: 0.234809, Discriminator: 0.043325; Generator: 0.021626,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:03:00,008 root         INFO     Train Epoch: 187 [3584/8000 (45%)]\tTotal Loss: 2.904412\n",
      "Reconstruction: 2.596371, Regularization: 0.243085, Discriminator: 0.043320; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:00,116 root         INFO     Train Epoch: 187 [4096/8000 (51%)]\tTotal Loss: 2.385177\n",
      "Reconstruction: 2.114707, Regularization: 0.205494, Discriminator: 0.043351; Generator: 0.021626,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:00,225 root         INFO     Train Epoch: 187 [4608/8000 (58%)]\tTotal Loss: 2.653186\n",
      "Reconstruction: 2.361985, Regularization: 0.226235, Discriminator: 0.043335; Generator: 0.021631,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:00,334 root         INFO     Train Epoch: 187 [5120/8000 (64%)]\tTotal Loss: 4.265000\n",
      "Reconstruction: 3.985167, Regularization: 0.214888, Discriminator: 0.043316; Generator: 0.021629,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:03:00,441 root         INFO     Train Epoch: 187 [5632/8000 (70%)]\tTotal Loss: 2.856989\n",
      "Reconstruction: 2.562520, Regularization: 0.229503, Discriminator: 0.043334; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:00,550 root         INFO     Train Epoch: 187 [6144/8000 (77%)]\tTotal Loss: 2.872114\n",
      "Reconstruction: 2.556528, Regularization: 0.250637, Discriminator: 0.043310; Generator: 0.021640,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:00,659 root         INFO     Train Epoch: 187 [6656/8000 (83%)]\tTotal Loss: 2.837299\n",
      "Reconstruction: 2.505192, Regularization: 0.267158, Discriminator: 0.043313; Generator: 0.021635,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:00,769 root         INFO     Train Epoch: 187 [7168/8000 (90%)]\tTotal Loss: 2.643237\n",
      "Reconstruction: 2.363937, Regularization: 0.214348, Discriminator: 0.043305; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:00,879 root         INFO     Train Epoch: 187 [7680/8000 (96%)]\tTotal Loss: 3.030340\n",
      "Reconstruction: 2.714132, Regularization: 0.251261, Discriminator: 0.043296; Generator: 0.021651,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:00,960 root         INFO     ====> Epoch: 187 Average loss: 2.8397\n",
      "2019-04-10 00:03:00,986 root         INFO     Train Epoch: 188 [0/8000 (0%)]\tTotal Loss: 2.832095\n",
      "Reconstruction: 2.546138, Regularization: 0.221011, Discriminator: 0.043291; Generator: 0.021654,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,098 root         INFO     Train Epoch: 188 [512/8000 (6%)]\tTotal Loss: 2.581681\n",
      "Reconstruction: 2.283749, Regularization: 0.232967, Discriminator: 0.043310; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,209 root         INFO     Train Epoch: 188 [1024/8000 (13%)]\tTotal Loss: 2.709925\n",
      "Reconstruction: 2.427131, Regularization: 0.217846, Discriminator: 0.043293; Generator: 0.021656,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,320 root         INFO     Train Epoch: 188 [1536/8000 (19%)]\tTotal Loss: 2.820540\n",
      "Reconstruction: 2.507910, Regularization: 0.247664, Discriminator: 0.043309; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,432 root         INFO     Train Epoch: 188 [2048/8000 (26%)]\tTotal Loss: 2.253156\n",
      "Reconstruction: 1.981749, Regularization: 0.206436, Discriminator: 0.043314; Generator: 0.021657,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,543 root         INFO     Train Epoch: 188 [2560/8000 (32%)]\tTotal Loss: 2.635016\n",
      "Reconstruction: 2.344418, Regularization: 0.225633, Discriminator: 0.043302; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,655 root         INFO     Train Epoch: 188 [3072/8000 (38%)]\tTotal Loss: 3.209739\n",
      "Reconstruction: 2.901039, Regularization: 0.243727, Discriminator: 0.043314; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,766 root         INFO     Train Epoch: 188 [3584/8000 (45%)]\tTotal Loss: 3.171054\n",
      "Reconstruction: 2.855064, Regularization: 0.251042, Discriminator: 0.043288; Generator: 0.021661,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,878 root         INFO     Train Epoch: 188 [4096/8000 (51%)]\tTotal Loss: 3.391787\n",
      "Reconstruction: 3.091483, Regularization: 0.235336, Discriminator: 0.043313; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:01,991 root         INFO     Train Epoch: 188 [4608/8000 (58%)]\tTotal Loss: 2.274613\n",
      "Reconstruction: 2.001414, Regularization: 0.208188, Discriminator: 0.043350; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,101 root         INFO     Train Epoch: 188 [5120/8000 (64%)]\tTotal Loss: 3.065868\n",
      "Reconstruction: 2.760863, Regularization: 0.240041, Discriminator: 0.043299; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,210 root         INFO     Train Epoch: 188 [5632/8000 (70%)]\tTotal Loss: 2.693359\n",
      "Reconstruction: 2.408605, Regularization: 0.219773, Discriminator: 0.043314; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,320 root         INFO     Train Epoch: 188 [6144/8000 (77%)]\tTotal Loss: 2.968038\n",
      "Reconstruction: 2.659945, Regularization: 0.243094, Discriminator: 0.043329; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,430 root         INFO     Train Epoch: 188 [6656/8000 (83%)]\tTotal Loss: 2.643492\n",
      "Reconstruction: 2.380682, Regularization: 0.197778, Discriminator: 0.043359; Generator: 0.021673,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,539 root         INFO     Train Epoch: 188 [7168/8000 (90%)]\tTotal Loss: 2.910046\n",
      "Reconstruction: 2.601182, Regularization: 0.243867, Discriminator: 0.043329; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,648 root         INFO     Train Epoch: 188 [7680/8000 (96%)]\tTotal Loss: 2.584737\n",
      "Reconstruction: 2.291830, Regularization: 0.227904, Discriminator: 0.043320; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,729 root         INFO     ====> Epoch: 188 Average loss: 2.8104\n",
      "2019-04-10 00:03:02,756 root         INFO     Train Epoch: 189 [0/8000 (0%)]\tTotal Loss: 4.068624\n",
      "Reconstruction: 3.748570, Regularization: 0.255057, Discriminator: 0.043310; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,869 root         INFO     Train Epoch: 189 [512/8000 (6%)]\tTotal Loss: 2.835765\n",
      "Reconstruction: 2.532524, Regularization: 0.238244, Discriminator: 0.043320; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:02,982 root         INFO     Train Epoch: 189 [1024/8000 (13%)]\tTotal Loss: 2.146513\n",
      "Reconstruction: 1.880393, Regularization: 0.201111, Discriminator: 0.043320; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,094 root         INFO     Train Epoch: 189 [1536/8000 (19%)]\tTotal Loss: 3.229717\n",
      "Reconstruction: 2.927815, Regularization: 0.236928, Discriminator: 0.043275; Generator: 0.021700,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:03:03,206 root         INFO     Train Epoch: 189 [2048/8000 (26%)]\tTotal Loss: 2.250704\n",
      "Reconstruction: 1.982719, Regularization: 0.202963, Discriminator: 0.043337; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,318 root         INFO     Train Epoch: 189 [2560/8000 (32%)]\tTotal Loss: 2.693542\n",
      "Reconstruction: 2.399268, Regularization: 0.229304, Discriminator: 0.043298; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,427 root         INFO     Train Epoch: 189 [3072/8000 (38%)]\tTotal Loss: 2.890718\n",
      "Reconstruction: 2.580445, Regularization: 0.245302, Discriminator: 0.043286; Generator: 0.021685,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,535 root         INFO     Train Epoch: 189 [3584/8000 (45%)]\tTotal Loss: 2.555033\n",
      "Reconstruction: 2.267808, Regularization: 0.222263, Discriminator: 0.043314; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,642 root         INFO     Train Epoch: 189 [4096/8000 (51%)]\tTotal Loss: 2.627806\n",
      "Reconstruction: 2.355375, Regularization: 0.207411, Discriminator: 0.043360; Generator: 0.021660,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,749 root         INFO     Train Epoch: 189 [4608/8000 (58%)]\tTotal Loss: 2.905109\n",
      "Reconstruction: 2.622262, Regularization: 0.217883, Discriminator: 0.043311; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,856 root         INFO     Train Epoch: 189 [5120/8000 (64%)]\tTotal Loss: 3.368008\n",
      "Reconstruction: 3.076031, Regularization: 0.227004, Discriminator: 0.043318; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:03,962 root         INFO     Train Epoch: 189 [5632/8000 (70%)]\tTotal Loss: 3.354592\n",
      "Reconstruction: 3.049862, Regularization: 0.239764, Discriminator: 0.043328; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,069 root         INFO     Train Epoch: 189 [6144/8000 (77%)]\tTotal Loss: 2.731419\n",
      "Reconstruction: 2.460565, Regularization: 0.205898, Discriminator: 0.043328; Generator: 0.021628,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:04,176 root         INFO     Train Epoch: 189 [6656/8000 (83%)]\tTotal Loss: 3.301569\n",
      "Reconstruction: 2.998303, Regularization: 0.238280, Discriminator: 0.043354; Generator: 0.021633,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,282 root         INFO     Train Epoch: 189 [7168/8000 (90%)]\tTotal Loss: 2.644600\n",
      "Reconstruction: 2.376738, Regularization: 0.202893, Discriminator: 0.043334; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,389 root         INFO     Train Epoch: 189 [7680/8000 (96%)]\tTotal Loss: 2.847649\n",
      "Reconstruction: 2.558736, Regularization: 0.223947, Discriminator: 0.043330; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,469 root         INFO     ====> Epoch: 189 Average loss: 2.8334\n",
      "2019-04-10 00:03:04,496 root         INFO     Train Epoch: 190 [0/8000 (0%)]\tTotal Loss: 2.377116\n",
      "Reconstruction: 2.103253, Regularization: 0.208895, Discriminator: 0.043330; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,607 root         INFO     Train Epoch: 190 [512/8000 (6%)]\tTotal Loss: 3.223066\n",
      "Reconstruction: 2.956591, Regularization: 0.201511, Discriminator: 0.043321; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,719 root         INFO     Train Epoch: 190 [1024/8000 (13%)]\tTotal Loss: 2.527556\n",
      "Reconstruction: 2.247954, Regularization: 0.214638, Discriminator: 0.043316; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,829 root         INFO     Train Epoch: 190 [1536/8000 (19%)]\tTotal Loss: 2.974325\n",
      "Reconstruction: 2.688535, Regularization: 0.220814, Discriminator: 0.043320; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:04,939 root         INFO     Train Epoch: 190 [2048/8000 (26%)]\tTotal Loss: 3.129147\n",
      "Reconstruction: 2.832681, Regularization: 0.231509, Discriminator: 0.043298; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,049 root         INFO     Train Epoch: 190 [2560/8000 (32%)]\tTotal Loss: 3.198250\n",
      "Reconstruction: 2.918680, Regularization: 0.214607, Discriminator: 0.043301; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,158 root         INFO     Train Epoch: 190 [3072/8000 (38%)]\tTotal Loss: 2.607103\n",
      "Reconstruction: 2.337510, Regularization: 0.204633, Discriminator: 0.043300; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,268 root         INFO     Train Epoch: 190 [3584/8000 (45%)]\tTotal Loss: 2.443021\n",
      "Reconstruction: 2.173958, Regularization: 0.204095, Discriminator: 0.043297; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,378 root         INFO     Train Epoch: 190 [4096/8000 (51%)]\tTotal Loss: 2.857822\n",
      "Reconstruction: 2.587450, Regularization: 0.205381, Discriminator: 0.043328; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,488 root         INFO     Train Epoch: 190 [4608/8000 (58%)]\tTotal Loss: 4.376820\n",
      "Reconstruction: 4.110830, Regularization: 0.201005, Discriminator: 0.043314; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,598 root         INFO     Train Epoch: 190 [5120/8000 (64%)]\tTotal Loss: 2.262166\n",
      "Reconstruction: 1.998237, Regularization: 0.198947, Discriminator: 0.043309; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,706 root         INFO     Train Epoch: 190 [5632/8000 (70%)]\tTotal Loss: 3.371163\n",
      "Reconstruction: 3.086574, Regularization: 0.219547, Discriminator: 0.043373; Generator: 0.021669,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,815 root         INFO     Train Epoch: 190 [6144/8000 (77%)]\tTotal Loss: 2.000546\n",
      "Reconstruction: 1.759787, Regularization: 0.175766, Discriminator: 0.043321; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:05,923 root         INFO     Train Epoch: 190 [6656/8000 (83%)]\tTotal Loss: 2.981623\n",
      "Reconstruction: 2.678404, Regularization: 0.238219, Discriminator: 0.043328; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:06,032 root         INFO     Train Epoch: 190 [7168/8000 (90%)]\tTotal Loss: 2.729889\n",
      "Reconstruction: 2.458673, Regularization: 0.206217, Discriminator: 0.043322; Generator: 0.021677,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:06,140 root         INFO     Train Epoch: 190 [7680/8000 (96%)]\tTotal Loss: 2.460288\n",
      "Reconstruction: 2.196755, Regularization: 0.198536, Discriminator: 0.043311; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:06,220 root         INFO     ====> Epoch: 190 Average loss: 2.8028\n",
      "2019-04-10 00:03:06,247 root         INFO     Train Epoch: 191 [0/8000 (0%)]\tTotal Loss: 2.471488\n",
      "Reconstruction: 2.232306, Regularization: 0.174177, Discriminator: 0.043318; Generator: 0.021687,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:06,358 root         INFO     Train Epoch: 191 [512/8000 (6%)]\tTotal Loss: 2.919079\n",
      "Reconstruction: 2.639220, Regularization: 0.214841, Discriminator: 0.043322; Generator: 0.021696,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:03:06,469 root         INFO     Train Epoch: 191 [1024/8000 (13%)]\tTotal Loss: 2.616808\n",
      "Reconstruction: 2.379327, Regularization: 0.172487, Discriminator: 0.043300; Generator: 0.021695,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:03:06,582 root         INFO     Train Epoch: 191 [1536/8000 (19%)]\tTotal Loss: 2.767211\n",
      "Reconstruction: 2.488657, Regularization: 0.213540, Discriminator: 0.043314; Generator: 0.021700,\n",
      "D(x): 0.499, D(G(z)): 0.499\n",
      "2019-04-10 00:03:06,693 root         INFO     Train Epoch: 191 [2048/8000 (26%)]\tTotal Loss: 2.625182\n",
      "Reconstruction: 2.362055, Regularization: 0.198128, Discriminator: 0.043314; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:06,805 root         INFO     Train Epoch: 191 [2560/8000 (32%)]\tTotal Loss: 2.686178\n",
      "Reconstruction: 2.438212, Regularization: 0.182983, Discriminator: 0.043288; Generator: 0.021694,\n",
      "D(x): 0.500, D(G(z)): 0.499\n",
      "2019-04-10 00:03:06,916 root         INFO     Train Epoch: 191 [3072/8000 (38%)]\tTotal Loss: 2.419129\n",
      "Reconstruction: 2.176464, Regularization: 0.177696, Discriminator: 0.043284; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,029 root         INFO     Train Epoch: 191 [3584/8000 (45%)]\tTotal Loss: 3.404700\n",
      "Reconstruction: 3.120030, Regularization: 0.219670, Discriminator: 0.043321; Generator: 0.021678,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,140 root         INFO     Train Epoch: 191 [4096/8000 (51%)]\tTotal Loss: 2.996448\n",
      "Reconstruction: 2.726398, Regularization: 0.205053, Discriminator: 0.043313; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,250 root         INFO     Train Epoch: 191 [4608/8000 (58%)]\tTotal Loss: 2.685797\n",
      "Reconstruction: 2.416866, Regularization: 0.203964, Discriminator: 0.043290; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,359 root         INFO     Train Epoch: 191 [5120/8000 (64%)]\tTotal Loss: 2.743038\n",
      "Reconstruction: 2.476633, Regularization: 0.201409, Discriminator: 0.043339; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,468 root         INFO     Train Epoch: 191 [5632/8000 (70%)]\tTotal Loss: 2.388011\n",
      "Reconstruction: 2.135603, Regularization: 0.187436, Discriminator: 0.043323; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,578 root         INFO     Train Epoch: 191 [6144/8000 (77%)]\tTotal Loss: 2.335679\n",
      "Reconstruction: 2.088059, Regularization: 0.182644, Discriminator: 0.043341; Generator: 0.021634,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,687 root         INFO     Train Epoch: 191 [6656/8000 (83%)]\tTotal Loss: 3.527669\n",
      "Reconstruction: 3.228297, Regularization: 0.234358, Discriminator: 0.043373; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,796 root         INFO     Train Epoch: 191 [7168/8000 (90%)]\tTotal Loss: 2.880913\n",
      "Reconstruction: 2.628804, Regularization: 0.187122, Discriminator: 0.043360; Generator: 0.021626,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:07,905 root         INFO     Train Epoch: 191 [7680/8000 (96%)]\tTotal Loss: 3.264698\n",
      "Reconstruction: 2.990280, Regularization: 0.209447, Discriminator: 0.043333; Generator: 0.021638,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:07,985 root         INFO     ====> Epoch: 191 Average loss: 2.7596\n",
      "2019-04-10 00:03:08,012 root         INFO     Train Epoch: 192 [0/8000 (0%)]\tTotal Loss: 2.762404\n",
      "Reconstruction: 2.500504, Regularization: 0.196940, Discriminator: 0.043340; Generator: 0.021620,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:08,122 root         INFO     Train Epoch: 192 [512/8000 (6%)]\tTotal Loss: 2.536849\n",
      "Reconstruction: 2.295722, Regularization: 0.176177, Discriminator: 0.043322; Generator: 0.021628,\n",
      "D(x): 0.501, D(G(z)): 0.501\n",
      "2019-04-10 00:03:08,234 root         INFO     Train Epoch: 192 [1024/8000 (13%)]\tTotal Loss: 3.226747\n",
      "Reconstruction: 2.974856, Regularization: 0.186925, Discriminator: 0.043321; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:08,345 root         INFO     Train Epoch: 192 [1536/8000 (19%)]\tTotal Loss: 2.713544\n",
      "Reconstruction: 2.467289, Regularization: 0.181294, Discriminator: 0.043328; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:08,457 root         INFO     Train Epoch: 192 [2048/8000 (26%)]\tTotal Loss: 3.232876\n",
      "Reconstruction: 2.982431, Regularization: 0.185491, Discriminator: 0.043327; Generator: 0.021627,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:08,568 root         INFO     Train Epoch: 192 [2560/8000 (32%)]\tTotal Loss: 2.596328\n",
      "Reconstruction: 2.344304, Regularization: 0.187051, Discriminator: 0.043325; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:08,680 root         INFO     Train Epoch: 192 [3072/8000 (38%)]\tTotal Loss: 2.366643\n",
      "Reconstruction: 2.130142, Regularization: 0.171545, Discriminator: 0.043316; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:08,792 root         INFO     Train Epoch: 192 [3584/8000 (45%)]\tTotal Loss: 3.697282\n",
      "Reconstruction: 3.416036, Regularization: 0.216279, Discriminator: 0.043327; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:08,901 root         INFO     Train Epoch: 192 [4096/8000 (51%)]\tTotal Loss: 2.647537\n",
      "Reconstruction: 2.411998, Regularization: 0.170567, Discriminator: 0.043329; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,010 root         INFO     Train Epoch: 192 [4608/8000 (58%)]\tTotal Loss: 2.326040\n",
      "Reconstruction: 2.103307, Regularization: 0.157767, Discriminator: 0.043316; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,119 root         INFO     Train Epoch: 192 [5120/8000 (64%)]\tTotal Loss: 2.247350\n",
      "Reconstruction: 2.012885, Regularization: 0.169479, Discriminator: 0.043337; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,228 root         INFO     Train Epoch: 192 [5632/8000 (70%)]\tTotal Loss: 2.461679\n",
      "Reconstruction: 2.225462, Regularization: 0.171240, Discriminator: 0.043327; Generator: 0.021649,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,336 root         INFO     Train Epoch: 192 [6144/8000 (77%)]\tTotal Loss: 2.620264\n",
      "Reconstruction: 2.375778, Regularization: 0.179450, Discriminator: 0.043388; Generator: 0.021648,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,445 root         INFO     Train Epoch: 192 [6656/8000 (83%)]\tTotal Loss: 2.249007\n",
      "Reconstruction: 2.019616, Regularization: 0.164398, Discriminator: 0.043334; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,554 root         INFO     Train Epoch: 192 [7168/8000 (90%)]\tTotal Loss: 2.592052\n",
      "Reconstruction: 2.352720, Regularization: 0.174339, Discriminator: 0.043330; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,664 root         INFO     Train Epoch: 192 [7680/8000 (96%)]\tTotal Loss: 2.939528\n",
      "Reconstruction: 2.684399, Regularization: 0.190130, Discriminator: 0.043337; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,743 root         INFO     ====> Epoch: 192 Average loss: 2.7975\n",
      "2019-04-10 00:03:09,771 root         INFO     Train Epoch: 193 [0/8000 (0%)]\tTotal Loss: 2.601859\n",
      "Reconstruction: 2.373142, Regularization: 0.163711, Discriminator: 0.043347; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,882 root         INFO     Train Epoch: 193 [512/8000 (6%)]\tTotal Loss: 2.780126\n",
      "Reconstruction: 2.543056, Regularization: 0.172067, Discriminator: 0.043334; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:09,991 root         INFO     Train Epoch: 193 [1024/8000 (13%)]\tTotal Loss: 3.136044\n",
      "Reconstruction: 2.894578, Regularization: 0.176466, Discriminator: 0.043330; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,099 root         INFO     Train Epoch: 193 [1536/8000 (19%)]\tTotal Loss: 2.918996\n",
      "Reconstruction: 2.673865, Regularization: 0.180136, Discriminator: 0.043322; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,208 root         INFO     Train Epoch: 193 [2048/8000 (26%)]\tTotal Loss: 2.626689\n",
      "Reconstruction: 2.406411, Regularization: 0.155288, Discriminator: 0.043320; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,320 root         INFO     Train Epoch: 193 [2560/8000 (32%)]\tTotal Loss: 1.954099\n",
      "Reconstruction: 1.740250, Regularization: 0.148859, Discriminator: 0.043315; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,432 root         INFO     Train Epoch: 193 [3072/8000 (38%)]\tTotal Loss: 2.464512\n",
      "Reconstruction: 2.254138, Regularization: 0.145386, Discriminator: 0.043309; Generator: 0.021679,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,545 root         INFO     Train Epoch: 193 [3584/8000 (45%)]\tTotal Loss: 3.416945\n",
      "Reconstruction: 3.160730, Regularization: 0.191227, Discriminator: 0.043313; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,658 root         INFO     Train Epoch: 193 [4096/8000 (51%)]\tTotal Loss: 3.206384\n",
      "Reconstruction: 2.959170, Regularization: 0.182239, Discriminator: 0.043303; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,772 root         INFO     Train Epoch: 193 [4608/8000 (58%)]\tTotal Loss: 2.267501\n",
      "Reconstruction: 2.050644, Regularization: 0.151881, Discriminator: 0.043291; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,885 root         INFO     Train Epoch: 193 [5120/8000 (64%)]\tTotal Loss: 2.554508\n",
      "Reconstruction: 2.330765, Regularization: 0.158757, Discriminator: 0.043306; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:10,998 root         INFO     Train Epoch: 193 [5632/8000 (70%)]\tTotal Loss: 2.847101\n",
      "Reconstruction: 2.618761, Regularization: 0.163367, Discriminator: 0.043312; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,111 root         INFO     Train Epoch: 193 [6144/8000 (77%)]\tTotal Loss: 3.068672\n",
      "Reconstruction: 2.849158, Regularization: 0.154577, Discriminator: 0.043279; Generator: 0.021658,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,225 root         INFO     Train Epoch: 193 [6656/8000 (83%)]\tTotal Loss: 2.856240\n",
      "Reconstruction: 2.625031, Regularization: 0.166224, Discriminator: 0.043314; Generator: 0.021670,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,339 root         INFO     Train Epoch: 193 [7168/8000 (90%)]\tTotal Loss: 2.697914\n",
      "Reconstruction: 2.461970, Regularization: 0.170984, Discriminator: 0.043293; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,453 root         INFO     Train Epoch: 193 [7680/8000 (96%)]\tTotal Loss: 2.673829\n",
      "Reconstruction: 2.455597, Regularization: 0.153237, Discriminator: 0.043345; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,535 root         INFO     ====> Epoch: 193 Average loss: 2.7948\n",
      "2019-04-10 00:03:11,563 root         INFO     Train Epoch: 194 [0/8000 (0%)]\tTotal Loss: 2.342159\n",
      "Reconstruction: 2.140685, Regularization: 0.136521, Discriminator: 0.043320; Generator: 0.021632,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,674 root         INFO     Train Epoch: 194 [512/8000 (6%)]\tTotal Loss: 2.039647\n",
      "Reconstruction: 1.830504, Regularization: 0.144184, Discriminator: 0.043323; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,785 root         INFO     Train Epoch: 194 [1024/8000 (13%)]\tTotal Loss: 1.800279\n",
      "Reconstruction: 1.605756, Regularization: 0.129556, Discriminator: 0.043308; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:11,896 root         INFO     Train Epoch: 194 [1536/8000 (19%)]\tTotal Loss: 3.079939\n",
      "Reconstruction: 2.852260, Regularization: 0.162720, Discriminator: 0.043318; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,007 root         INFO     Train Epoch: 194 [2048/8000 (26%)]\tTotal Loss: 2.181954\n",
      "Reconstruction: 1.975918, Regularization: 0.141072, Discriminator: 0.043324; Generator: 0.021640,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,117 root         INFO     Train Epoch: 194 [2560/8000 (32%)]\tTotal Loss: 2.454100\n",
      "Reconstruction: 2.246585, Regularization: 0.142537, Discriminator: 0.043336; Generator: 0.021642,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,228 root         INFO     Train Epoch: 194 [3072/8000 (38%)]\tTotal Loss: 2.672399\n",
      "Reconstruction: 2.457829, Regularization: 0.149610, Discriminator: 0.043310; Generator: 0.021650,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,338 root         INFO     Train Epoch: 194 [3584/8000 (45%)]\tTotal Loss: 2.548120\n",
      "Reconstruction: 2.348258, Regularization: 0.134877, Discriminator: 0.043330; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,449 root         INFO     Train Epoch: 194 [4096/8000 (51%)]\tTotal Loss: 2.552436\n",
      "Reconstruction: 2.334388, Regularization: 0.153071, Discriminator: 0.043325; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,560 root         INFO     Train Epoch: 194 [4608/8000 (58%)]\tTotal Loss: 2.025720\n",
      "Reconstruction: 1.825853, Regularization: 0.134892, Discriminator: 0.043321; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,671 root         INFO     Train Epoch: 194 [5120/8000 (64%)]\tTotal Loss: 2.780640\n",
      "Reconstruction: 2.579715, Regularization: 0.135944, Discriminator: 0.043326; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,780 root         INFO     Train Epoch: 194 [5632/8000 (70%)]\tTotal Loss: 2.140499\n",
      "Reconstruction: 1.927315, Regularization: 0.148206, Discriminator: 0.043325; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,889 root         INFO     Train Epoch: 194 [6144/8000 (77%)]\tTotal Loss: 2.994234\n",
      "Reconstruction: 2.789644, Regularization: 0.139596, Discriminator: 0.043339; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:12,998 root         INFO     Train Epoch: 194 [6656/8000 (83%)]\tTotal Loss: 3.932033\n",
      "Reconstruction: 3.719818, Regularization: 0.147259, Discriminator: 0.043300; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,108 root         INFO     Train Epoch: 194 [7168/8000 (90%)]\tTotal Loss: 1.883523\n",
      "Reconstruction: 1.687023, Regularization: 0.131524, Discriminator: 0.043323; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,217 root         INFO     Train Epoch: 194 [7680/8000 (96%)]\tTotal Loss: 3.076296\n",
      "Reconstruction: 2.835649, Regularization: 0.175656, Discriminator: 0.043338; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,297 root         INFO     ====> Epoch: 194 Average loss: 2.7441\n",
      "2019-04-10 00:03:13,324 root         INFO     Train Epoch: 195 [0/8000 (0%)]\tTotal Loss: 3.056277\n",
      "Reconstruction: 2.839998, Regularization: 0.151279, Discriminator: 0.043345; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,437 root         INFO     Train Epoch: 195 [512/8000 (6%)]\tTotal Loss: 3.630632\n",
      "Reconstruction: 3.411385, Regularization: 0.154255, Discriminator: 0.043327; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,549 root         INFO     Train Epoch: 195 [1024/8000 (13%)]\tTotal Loss: 2.725473\n",
      "Reconstruction: 2.508067, Regularization: 0.152392, Discriminator: 0.043348; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,662 root         INFO     Train Epoch: 195 [1536/8000 (19%)]\tTotal Loss: 3.036242\n",
      "Reconstruction: 2.820242, Regularization: 0.151000, Discriminator: 0.043333; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,771 root         INFO     Train Epoch: 195 [2048/8000 (26%)]\tTotal Loss: 2.369046\n",
      "Reconstruction: 2.165505, Regularization: 0.138527, Discriminator: 0.043341; Generator: 0.021672,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,880 root         INFO     Train Epoch: 195 [2560/8000 (32%)]\tTotal Loss: 2.242733\n",
      "Reconstruction: 2.032596, Regularization: 0.145126, Discriminator: 0.043330; Generator: 0.021681,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:13,990 root         INFO     Train Epoch: 195 [3072/8000 (38%)]\tTotal Loss: 2.667971\n",
      "Reconstruction: 2.447978, Regularization: 0.154988, Discriminator: 0.043321; Generator: 0.021684,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,099 root         INFO     Train Epoch: 195 [3584/8000 (45%)]\tTotal Loss: 2.084512\n",
      "Reconstruction: 1.871573, Regularization: 0.147930, Discriminator: 0.043322; Generator: 0.021686,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,208 root         INFO     Train Epoch: 195 [4096/8000 (51%)]\tTotal Loss: 3.146250\n",
      "Reconstruction: 2.963139, Regularization: 0.118076, Discriminator: 0.043345; Generator: 0.021690,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,316 root         INFO     Train Epoch: 195 [4608/8000 (58%)]\tTotal Loss: 3.068435\n",
      "Reconstruction: 2.837387, Regularization: 0.166035, Discriminator: 0.043324; Generator: 0.021690,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,425 root         INFO     Train Epoch: 195 [5120/8000 (64%)]\tTotal Loss: 2.645341\n",
      "Reconstruction: 2.469548, Regularization: 0.110798, Discriminator: 0.043307; Generator: 0.021688,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,535 root         INFO     Train Epoch: 195 [5632/8000 (70%)]\tTotal Loss: 4.599097\n",
      "Reconstruction: 4.401857, Regularization: 0.132201, Discriminator: 0.043352; Generator: 0.021686,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,644 root         INFO     Train Epoch: 195 [6144/8000 (77%)]\tTotal Loss: 2.143452\n",
      "Reconstruction: 1.947863, Regularization: 0.130583, Discriminator: 0.043323; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,753 root         INFO     Train Epoch: 195 [6656/8000 (83%)]\tTotal Loss: 2.797504\n",
      "Reconstruction: 2.602621, Regularization: 0.129892, Discriminator: 0.043325; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,862 root         INFO     Train Epoch: 195 [7168/8000 (90%)]\tTotal Loss: 2.229625\n",
      "Reconstruction: 2.026512, Regularization: 0.138124, Discriminator: 0.043330; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:14,971 root         INFO     Train Epoch: 195 [7680/8000 (96%)]\tTotal Loss: 2.686281\n",
      "Reconstruction: 2.478166, Regularization: 0.143128, Discriminator: 0.043332; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,050 root         INFO     ====> Epoch: 195 Average loss: 2.7934\n",
      "2019-04-10 00:03:15,077 root         INFO     Train Epoch: 196 [0/8000 (0%)]\tTotal Loss: 2.585519\n",
      "Reconstruction: 2.376574, Regularization: 0.143957, Discriminator: 0.043345; Generator: 0.021644,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,188 root         INFO     Train Epoch: 196 [512/8000 (6%)]\tTotal Loss: 2.769558\n",
      "Reconstruction: 2.551231, Regularization: 0.153328, Discriminator: 0.043341; Generator: 0.021658,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,297 root         INFO     Train Epoch: 196 [1024/8000 (13%)]\tTotal Loss: 2.863897\n",
      "Reconstruction: 2.636952, Regularization: 0.161964, Discriminator: 0.043329; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,408 root         INFO     Train Epoch: 196 [1536/8000 (19%)]\tTotal Loss: 2.173561\n",
      "Reconstruction: 1.971348, Regularization: 0.137240, Discriminator: 0.043320; Generator: 0.021652,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,518 root         INFO     Train Epoch: 196 [2048/8000 (26%)]\tTotal Loss: 3.660455\n",
      "Reconstruction: 3.437421, Regularization: 0.158064, Discriminator: 0.043325; Generator: 0.021645,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,628 root         INFO     Train Epoch: 196 [2560/8000 (32%)]\tTotal Loss: 3.401209\n",
      "Reconstruction: 3.204830, Regularization: 0.131420, Discriminator: 0.043306; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,739 root         INFO     Train Epoch: 196 [3072/8000 (38%)]\tTotal Loss: 3.323519\n",
      "Reconstruction: 3.128003, Regularization: 0.130571, Discriminator: 0.043290; Generator: 0.021655,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,849 root         INFO     Train Epoch: 196 [3584/8000 (45%)]\tTotal Loss: 2.251673\n",
      "Reconstruction: 2.054057, Regularization: 0.132648, Discriminator: 0.043302; Generator: 0.021666,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:15,958 root         INFO     Train Epoch: 196 [4096/8000 (51%)]\tTotal Loss: 2.933882\n",
      "Reconstruction: 2.720700, Regularization: 0.148208, Discriminator: 0.043310; Generator: 0.021664,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,069 root         INFO     Train Epoch: 196 [4608/8000 (58%)]\tTotal Loss: 2.649781\n",
      "Reconstruction: 2.450735, Regularization: 0.134082, Discriminator: 0.043301; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,179 root         INFO     Train Epoch: 196 [5120/8000 (64%)]\tTotal Loss: 2.353271\n",
      "Reconstruction: 2.156691, Regularization: 0.131614, Discriminator: 0.043304; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,289 root         INFO     Train Epoch: 196 [5632/8000 (70%)]\tTotal Loss: 3.014049\n",
      "Reconstruction: 2.814602, Regularization: 0.134484, Discriminator: 0.043297; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,399 root         INFO     Train Epoch: 196 [6144/8000 (77%)]\tTotal Loss: 3.113149\n",
      "Reconstruction: 2.881468, Regularization: 0.166720, Discriminator: 0.043292; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,509 root         INFO     Train Epoch: 196 [6656/8000 (83%)]\tTotal Loss: 2.862602\n",
      "Reconstruction: 2.661708, Regularization: 0.135909, Discriminator: 0.043319; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,619 root         INFO     Train Epoch: 196 [7168/8000 (90%)]\tTotal Loss: 2.603821\n",
      "Reconstruction: 2.395850, Regularization: 0.143000, Discriminator: 0.043297; Generator: 0.021674,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,729 root         INFO     Train Epoch: 196 [7680/8000 (96%)]\tTotal Loss: 3.060605\n",
      "Reconstruction: 2.856570, Regularization: 0.139020, Discriminator: 0.043354; Generator: 0.021661,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,809 root         INFO     ====> Epoch: 196 Average loss: 2.7521\n",
      "2019-04-10 00:03:16,836 root         INFO     Train Epoch: 197 [0/8000 (0%)]\tTotal Loss: 3.136431\n",
      "Reconstruction: 2.925248, Regularization: 0.146138, Discriminator: 0.043379; Generator: 0.021666,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:16,948 root         INFO     Train Epoch: 197 [512/8000 (6%)]\tTotal Loss: 2.821830\n",
      "Reconstruction: 2.617164, Regularization: 0.139666, Discriminator: 0.043333; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,059 root         INFO     Train Epoch: 197 [1024/8000 (13%)]\tTotal Loss: 2.370390\n",
      "Reconstruction: 2.175411, Regularization: 0.129970, Discriminator: 0.043341; Generator: 0.021667,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,171 root         INFO     Train Epoch: 197 [1536/8000 (19%)]\tTotal Loss: 3.929128\n",
      "Reconstruction: 3.728546, Regularization: 0.135576, Discriminator: 0.043323; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,280 root         INFO     Train Epoch: 197 [2048/8000 (26%)]\tTotal Loss: 3.050090\n",
      "Reconstruction: 2.847447, Regularization: 0.137640, Discriminator: 0.043322; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,392 root         INFO     Train Epoch: 197 [2560/8000 (32%)]\tTotal Loss: 2.528778\n",
      "Reconstruction: 2.327651, Regularization: 0.136118, Discriminator: 0.043330; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,504 root         INFO     Train Epoch: 197 [3072/8000 (38%)]\tTotal Loss: 2.405488\n",
      "Reconstruction: 2.202288, Regularization: 0.138201, Discriminator: 0.043339; Generator: 0.021660,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,613 root         INFO     Train Epoch: 197 [3584/8000 (45%)]\tTotal Loss: 2.870838\n",
      "Reconstruction: 2.678845, Regularization: 0.126990, Discriminator: 0.043330; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,720 root         INFO     Train Epoch: 197 [4096/8000 (51%)]\tTotal Loss: 2.506282\n",
      "Reconstruction: 2.311774, Regularization: 0.129518, Discriminator: 0.043319; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,828 root         INFO     Train Epoch: 197 [4608/8000 (58%)]\tTotal Loss: 2.396171\n",
      "Reconstruction: 2.205861, Regularization: 0.125328, Discriminator: 0.043302; Generator: 0.021680,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:17,935 root         INFO     Train Epoch: 197 [5120/8000 (64%)]\tTotal Loss: 3.172178\n",
      "Reconstruction: 2.955393, Regularization: 0.151769, Discriminator: 0.043360; Generator: 0.021657,\n",
      "D(x): 0.499, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,043 root         INFO     Train Epoch: 197 [5632/8000 (70%)]\tTotal Loss: 3.327597\n",
      "Reconstruction: 3.108681, Regularization: 0.153902, Discriminator: 0.043346; Generator: 0.021668,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,151 root         INFO     Train Epoch: 197 [6144/8000 (77%)]\tTotal Loss: 2.653612\n",
      "Reconstruction: 2.468556, Regularization: 0.120079, Discriminator: 0.043314; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,258 root         INFO     Train Epoch: 197 [6656/8000 (83%)]\tTotal Loss: 2.324946\n",
      "Reconstruction: 2.140176, Regularization: 0.119821, Discriminator: 0.043310; Generator: 0.021639,\n",
      "D(x): 0.501, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,365 root         INFO     Train Epoch: 197 [7168/8000 (90%)]\tTotal Loss: 2.402760\n",
      "Reconstruction: 2.207443, Regularization: 0.130329, Discriminator: 0.043345; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,471 root         INFO     Train Epoch: 197 [7680/8000 (96%)]\tTotal Loss: 2.369964\n",
      "Reconstruction: 2.195521, Regularization: 0.109464, Discriminator: 0.043336; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,549 root         INFO     ====> Epoch: 197 Average loss: 2.7116\n",
      "2019-04-10 00:03:18,577 root         INFO     Train Epoch: 198 [0/8000 (0%)]\tTotal Loss: 2.529747\n",
      "Reconstruction: 2.344076, Regularization: 0.120672, Discriminator: 0.043359; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,687 root         INFO     Train Epoch: 198 [512/8000 (6%)]\tTotal Loss: 2.913158\n",
      "Reconstruction: 2.714647, Regularization: 0.133536, Discriminator: 0.043339; Generator: 0.021636,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:18,798 root         INFO     Train Epoch: 198 [1024/8000 (13%)]\tTotal Loss: 2.497539\n",
      "Reconstruction: 2.329618, Regularization: 0.102941, Discriminator: 0.043350; Generator: 0.021629,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:18,909 root         INFO     Train Epoch: 198 [1536/8000 (19%)]\tTotal Loss: 2.507003\n",
      "Reconstruction: 2.325301, Regularization: 0.116742, Discriminator: 0.043336; Generator: 0.021624,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:19,021 root         INFO     Train Epoch: 198 [2048/8000 (26%)]\tTotal Loss: 1.874627\n",
      "Reconstruction: 1.705186, Regularization: 0.104485, Discriminator: 0.043330; Generator: 0.021627,\n",
      "D(x): 0.500, D(G(z)): 0.501\n",
      "2019-04-10 00:03:19,132 root         INFO     Train Epoch: 198 [2560/8000 (32%)]\tTotal Loss: 2.751066\n",
      "Reconstruction: 2.560004, Regularization: 0.126095, Discriminator: 0.043332; Generator: 0.021635,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:19,243 root         INFO     Train Epoch: 198 [3072/8000 (38%)]\tTotal Loss: 2.141107\n",
      "Reconstruction: 1.971243, Regularization: 0.104901, Discriminator: 0.043321; Generator: 0.021641,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:19,354 root         INFO     Train Epoch: 198 [3584/8000 (45%)]\tTotal Loss: 2.506920\n",
      "Reconstruction: 2.325237, Regularization: 0.116726, Discriminator: 0.043314; Generator: 0.021643,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:19,465 root         INFO     Train Epoch: 198 [4096/8000 (51%)]\tTotal Loss: 2.895658\n",
      "Reconstruction: 2.708907, Regularization: 0.121781, Discriminator: 0.043323; Generator: 0.021648,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:19,576 root         INFO     Train Epoch: 198 [4608/8000 (58%)]\tTotal Loss: 3.394338\n",
      "Reconstruction: 3.200717, Regularization: 0.128662, Discriminator: 0.043311; Generator: 0.021647,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:19,685 root         INFO     Train Epoch: 198 [5120/8000 (64%)]\tTotal Loss: 2.841699\n",
      "Reconstruction: 2.669210, Regularization: 0.107528, Discriminator: 0.043307; Generator: 0.021655,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:19,793 root         INFO     Train Epoch: 198 [5632/8000 (70%)]\tTotal Loss: 2.251203\n",
      "Reconstruction: 2.085010, Regularization: 0.101220, Discriminator: 0.043323; Generator: 0.021651,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:19,902 root         INFO     Train Epoch: 198 [6144/8000 (77%)]\tTotal Loss: 2.657656\n",
      "Reconstruction: 2.478148, Regularization: 0.114549, Discriminator: 0.043306; Generator: 0.021654,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,011 root         INFO     Train Epoch: 198 [6656/8000 (83%)]\tTotal Loss: 2.254006\n",
      "Reconstruction: 2.088733, Regularization: 0.100298, Discriminator: 0.043314; Generator: 0.021661,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,120 root         INFO     Train Epoch: 198 [7168/8000 (90%)]\tTotal Loss: 2.510724\n",
      "Reconstruction: 2.336036, Regularization: 0.109708, Discriminator: 0.043324; Generator: 0.021656,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,229 root         INFO     Train Epoch: 198 [7680/8000 (96%)]\tTotal Loss: 3.370006\n",
      "Reconstruction: 3.184396, Regularization: 0.120658, Discriminator: 0.043290; Generator: 0.021662,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,308 root         INFO     ====> Epoch: 198 Average loss: 2.7250\n",
      "2019-04-10 00:03:20,335 root         INFO     Train Epoch: 199 [0/8000 (0%)]\tTotal Loss: 3.235593\n",
      "Reconstruction: 3.049320, Regularization: 0.121279, Discriminator: 0.043336; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,446 root         INFO     Train Epoch: 199 [512/8000 (6%)]\tTotal Loss: 2.369171\n",
      "Reconstruction: 2.190839, Regularization: 0.113330, Discriminator: 0.043338; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,555 root         INFO     Train Epoch: 199 [1024/8000 (13%)]\tTotal Loss: 2.825711\n",
      "Reconstruction: 2.659120, Regularization: 0.101607, Discriminator: 0.043319; Generator: 0.021665,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,665 root         INFO     Train Epoch: 199 [1536/8000 (19%)]\tTotal Loss: 2.463475\n",
      "Reconstruction: 2.288133, Regularization: 0.110342, Discriminator: 0.043328; Generator: 0.021673,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,774 root         INFO     Train Epoch: 199 [2048/8000 (26%)]\tTotal Loss: 2.326043\n",
      "Reconstruction: 2.158633, Regularization: 0.102419, Discriminator: 0.043320; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,884 root         INFO     Train Epoch: 199 [2560/8000 (32%)]\tTotal Loss: 3.341968\n",
      "Reconstruction: 3.174708, Regularization: 0.102274, Discriminator: 0.043315; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:20,993 root         INFO     Train Epoch: 199 [3072/8000 (38%)]\tTotal Loss: 2.647513\n",
      "Reconstruction: 2.498825, Regularization: 0.083681, Discriminator: 0.043332; Generator: 0.021676,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,103 root         INFO     Train Epoch: 199 [3584/8000 (45%)]\tTotal Loss: 2.531785\n",
      "Reconstruction: 2.378581, Regularization: 0.088201, Discriminator: 0.043335; Generator: 0.021669,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,211 root         INFO     Train Epoch: 199 [4096/8000 (51%)]\tTotal Loss: 2.038715\n",
      "Reconstruction: 1.881706, Regularization: 0.092013, Discriminator: 0.043308; Generator: 0.021689,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,319 root         INFO     Train Epoch: 199 [4608/8000 (58%)]\tTotal Loss: 2.533064\n",
      "Reconstruction: 2.376866, Regularization: 0.091186, Discriminator: 0.043329; Generator: 0.021683,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,428 root         INFO     Train Epoch: 199 [5120/8000 (64%)]\tTotal Loss: 2.730062\n",
      "Reconstruction: 2.558456, Regularization: 0.106618, Discriminator: 0.043313; Generator: 0.021675,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,537 root         INFO     Train Epoch: 199 [5632/8000 (70%)]\tTotal Loss: 2.921235\n",
      "Reconstruction: 2.756214, Regularization: 0.100023, Discriminator: 0.043326; Generator: 0.021672,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,646 root         INFO     Train Epoch: 199 [6144/8000 (77%)]\tTotal Loss: 2.987544\n",
      "Reconstruction: 2.821824, Regularization: 0.100738, Discriminator: 0.043318; Generator: 0.021663,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,755 root         INFO     Train Epoch: 199 [6656/8000 (83%)]\tTotal Loss: 2.019557\n",
      "Reconstruction: 1.864917, Regularization: 0.089628, Discriminator: 0.043341; Generator: 0.021671,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,864 root         INFO     Train Epoch: 199 [7168/8000 (90%)]\tTotal Loss: 2.281894\n",
      "Reconstruction: 2.118204, Regularization: 0.098724, Discriminator: 0.043307; Generator: 0.021659,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:21,973 root         INFO     Train Epoch: 199 [7680/8000 (96%)]\tTotal Loss: 1.851262\n",
      "Reconstruction: 1.697014, Regularization: 0.089248, Discriminator: 0.043348; Generator: 0.021653,\n",
      "D(x): 0.500, D(G(z)): 0.500\n",
      "2019-04-10 00:03:22,053 root         INFO     ====> Epoch: 199 Average loss: 2.6485\n",
      "2019-04-10 00:03:22,068 luigi-interface INFO     [pid 26441] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26441) done      TrainVEM()\n",
      "2019-04-10 00:03:22,069 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 00:03:22,069 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   DONE\n",
      "2019-04-10 00:03:22,069 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 00:03:22,069 luigi-interface DEBUG    Pending tasks: 2\n",
      "2019-04-10 00:03:22,069 luigi-interface INFO     [pid 26441] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26441) running   TrainVAE()\n",
      "2019-04-10 00:03:22,070 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-10 00:03:22,070 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-10 00:03:22,071 root         INFO     Values of VAE's decoder parameters before training:\n",
      "2019-04-10 00:03:22,071 root         INFO     layers.0.weight\n",
      "2019-04-10 00:03:22,072 root         INFO     tensor([[ 0.8124],\n",
      "        [-0.8515]], device='cuda:0')\n",
      "2019-04-10 00:03:22,073 root         INFO     layers.0.bias\n",
      "2019-04-10 00:03:22,073 root         INFO     tensor([ 0.8447, -0.9972], device='cuda:0')\n",
      "2019-04-10 00:03:22,074 root         INFO     layers.1.weight\n",
      "2019-04-10 00:03:22,074 root         INFO     tensor([[0.3859, 0.0856],\n",
      "        [0.4315, 0.0529]], device='cuda:0')\n",
      "2019-04-10 00:03:22,075 root         INFO     layers.1.bias\n",
      "2019-04-10 00:03:22,075 root         INFO     tensor([ 0.6930, -0.3029], device='cuda:0')\n",
      "2019-04-10 00:03:22,076 root         INFO     layers.2.weight\n",
      "2019-04-10 00:03:22,076 root         INFO     tensor([[ 0.6785, -0.3599],\n",
      "        [ 0.5108, -0.3066]], device='cuda:0')\n",
      "2019-04-10 00:03:22,077 root         INFO     layers.2.bias\n",
      "2019-04-10 00:03:22,077 root         INFO     tensor([ 0.2088, -0.1693], device='cuda:0')\n",
      "2019-04-10 00:03:22,078 root         INFO     layers.3.weight\n",
      "2019-04-10 00:03:22,078 root         INFO     tensor([[ 0.4634, -0.6727],\n",
      "        [ 0.2699,  0.4862]], device='cuda:0')\n",
      "2019-04-10 00:03:22,079 root         INFO     layers.3.bias\n",
      "2019-04-10 00:03:22,079 root         INFO     tensor([-0.0108,  0.3020], device='cuda:0')\n",
      "2019-04-10 00:03:22,080 root         INFO     layers.4.weight\n",
      "2019-04-10 00:03:22,080 root         INFO     tensor([[-0.2539, -0.3689],\n",
      "        [ 0.0538,  0.6629]], device='cuda:0')\n",
      "2019-04-10 00:03:22,081 root         INFO     layers.4.bias\n",
      "2019-04-10 00:03:22,081 root         INFO     tensor([ 0.1679, -0.1252], device='cuda:0')\n",
      "2019-04-10 00:03:22,082 root         INFO     layers.5.weight\n",
      "2019-04-10 00:03:22,082 root         INFO     tensor([[ 0.2158,  0.5553],\n",
      "        [ 0.6396, -0.0919]], device='cuda:0')\n",
      "2019-04-10 00:03:22,084 root         INFO     layers.5.bias\n",
      "2019-04-10 00:03:22,084 root         INFO     tensor([ 0.6798, -0.1397], device='cuda:0')\n",
      "2019-04-10 00:03:22,108 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 5.544777\n",
      "Reconstruction: 0.637713, Regularization: 4.907064\n",
      "2019-04-10 00:03:22,169 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: 4.792115\n",
      "Reconstruction: 0.567529, Regularization: 4.224586\n",
      "2019-04-10 00:03:22,230 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: 5.011720\n",
      "Reconstruction: 0.583561, Regularization: 4.428159\n",
      "2019-04-10 00:03:22,290 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: 4.419728\n",
      "Reconstruction: 0.522533, Regularization: 3.897195\n",
      "2019-04-10 00:03:22,351 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: 5.350024\n",
      "Reconstruction: 0.600737, Regularization: 4.749288\n",
      "2019-04-10 00:03:22,411 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: 4.914319\n",
      "Reconstruction: 0.559526, Regularization: 4.354794\n",
      "2019-04-10 00:03:22,472 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: 4.944205\n",
      "Reconstruction: 0.557412, Regularization: 4.386793\n",
      "2019-04-10 00:03:22,533 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: 5.149256\n",
      "Reconstruction: 0.555604, Regularization: 4.593652\n",
      "2019-04-10 00:03:22,594 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: 4.543803\n",
      "Reconstruction: 0.509587, Regularization: 4.034216\n",
      "2019-04-10 00:03:22,655 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: 4.494501\n",
      "Reconstruction: 0.493292, Regularization: 4.001208\n",
      "2019-04-10 00:03:22,717 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: 5.092978\n",
      "Reconstruction: 0.544454, Regularization: 4.548525\n",
      "2019-04-10 00:03:22,778 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: 5.019686\n",
      "Reconstruction: 0.519144, Regularization: 4.500542\n",
      "2019-04-10 00:03:22,840 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: 4.932837\n",
      "Reconstruction: 0.514218, Regularization: 4.418619\n",
      "2019-04-10 00:03:22,902 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: 4.600794\n",
      "Reconstruction: 0.487620, Regularization: 4.113174\n",
      "2019-04-10 00:03:22,963 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: 4.511919\n",
      "Reconstruction: 0.483303, Regularization: 4.028617\n",
      "2019-04-10 00:03:23,025 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: 4.481489\n",
      "Reconstruction: 0.471131, Regularization: 4.010358\n",
      "2019-04-10 00:03:23,077 root         INFO     ====> Epoch: 0 Average loss: 4.8235\n",
      "2019-04-10 00:03:23,102 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: 4.522566\n",
      "Reconstruction: 0.465447, Regularization: 4.057119\n",
      "2019-04-10 00:03:23,166 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: 4.630674\n",
      "Reconstruction: 0.474072, Regularization: 4.156602\n",
      "2019-04-10 00:03:23,229 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: 4.694016\n",
      "Reconstruction: 0.475196, Regularization: 4.218820\n",
      "2019-04-10 00:03:23,292 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: 4.161846\n",
      "Reconstruction: 0.429639, Regularization: 3.732207\n",
      "2019-04-10 00:03:23,356 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: 4.569580\n",
      "Reconstruction: 0.446213, Regularization: 4.123366\n",
      "2019-04-10 00:03:23,421 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: 4.473982\n",
      "Reconstruction: 0.437485, Regularization: 4.036497\n",
      "2019-04-10 00:03:23,484 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: 4.929551\n",
      "Reconstruction: 0.462294, Regularization: 4.467257\n",
      "2019-04-10 00:03:23,547 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: 5.366850\n",
      "Reconstruction: 0.475592, Regularization: 4.891258\n",
      "2019-04-10 00:03:23,608 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: 4.678688\n",
      "Reconstruction: 0.439353, Regularization: 4.239336\n",
      "2019-04-10 00:03:23,668 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: 4.798764\n",
      "Reconstruction: 0.437092, Regularization: 4.361672\n",
      "2019-04-10 00:03:23,729 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: 4.831593\n",
      "Reconstruction: 0.436894, Regularization: 4.394698\n",
      "2019-04-10 00:03:23,790 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: 4.436132\n",
      "Reconstruction: 0.417599, Regularization: 4.018534\n",
      "2019-04-10 00:03:23,850 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: 5.118167\n",
      "Reconstruction: 0.438683, Regularization: 4.679484\n",
      "2019-04-10 00:03:23,911 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: 4.651052\n",
      "Reconstruction: 0.412221, Regularization: 4.238831\n",
      "2019-04-10 00:03:23,972 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: 3.708754\n",
      "Reconstruction: 0.366885, Regularization: 3.341869\n",
      "2019-04-10 00:03:24,032 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: 4.079641\n",
      "Reconstruction: 0.372647, Regularization: 3.706995\n",
      "2019-04-10 00:03:24,085 root         INFO     ====> Epoch: 1 Average loss: 4.5706\n",
      "2019-04-10 00:03:24,109 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: 4.784131\n",
      "Reconstruction: 0.403476, Regularization: 4.380654\n",
      "2019-04-10 00:03:24,173 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: 4.068775\n",
      "Reconstruction: 0.374806, Regularization: 3.693969\n",
      "2019-04-10 00:03:24,237 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: 4.760883\n",
      "Reconstruction: 0.395145, Regularization: 4.365738\n",
      "2019-04-10 00:03:24,299 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: 4.431792\n",
      "Reconstruction: 0.376803, Regularization: 4.054988\n",
      "2019-04-10 00:03:24,361 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: 4.187810\n",
      "Reconstruction: 0.357704, Regularization: 3.830106\n",
      "2019-04-10 00:03:24,423 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: 4.775930\n",
      "Reconstruction: 0.389123, Regularization: 4.386806\n",
      "2019-04-10 00:03:24,486 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: 4.508687\n",
      "Reconstruction: 0.363991, Regularization: 4.144696\n",
      "2019-04-10 00:03:24,548 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: 4.129860\n",
      "Reconstruction: 0.354710, Regularization: 3.775151\n",
      "2019-04-10 00:03:24,610 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: 4.583139\n",
      "Reconstruction: 0.354770, Regularization: 4.228370\n",
      "2019-04-10 00:03:24,672 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: 4.261834\n",
      "Reconstruction: 0.345947, Regularization: 3.915887\n",
      "2019-04-10 00:03:24,736 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: 4.422284\n",
      "Reconstruction: 0.346931, Regularization: 4.075353\n",
      "2019-04-10 00:03:24,799 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: 4.109608\n",
      "Reconstruction: 0.327039, Regularization: 3.782569\n",
      "2019-04-10 00:03:24,863 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: 3.765684\n",
      "Reconstruction: 0.317390, Regularization: 3.448293\n",
      "2019-04-10 00:03:24,926 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: 4.015062\n",
      "Reconstruction: 0.322927, Regularization: 3.692135\n",
      "2019-04-10 00:03:24,990 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: 4.307190\n",
      "Reconstruction: 0.329655, Regularization: 3.977535\n",
      "2019-04-10 00:03:25,053 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: 4.407271\n",
      "Reconstruction: 0.325964, Regularization: 4.081306\n",
      "2019-04-10 00:03:25,107 root         INFO     ====> Epoch: 2 Average loss: 4.3434\n",
      "2019-04-10 00:03:25,130 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: 3.706270\n",
      "Reconstruction: 0.308648, Regularization: 3.397622\n",
      "2019-04-10 00:03:25,194 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: 4.268856\n",
      "Reconstruction: 0.321711, Regularization: 3.947145\n",
      "2019-04-10 00:03:25,257 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: 4.015128\n",
      "Reconstruction: 0.307005, Regularization: 3.708123\n",
      "2019-04-10 00:03:25,320 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: 4.481364\n",
      "Reconstruction: 0.323437, Regularization: 4.157928\n",
      "2019-04-10 00:03:25,384 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: 3.765715\n",
      "Reconstruction: 0.296043, Regularization: 3.469672\n",
      "2019-04-10 00:03:25,448 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: 4.435995\n",
      "Reconstruction: 0.315470, Regularization: 4.120525\n",
      "2019-04-10 00:03:25,513 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: 3.779989\n",
      "Reconstruction: 0.294495, Regularization: 3.485494\n",
      "2019-04-10 00:03:25,577 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: 4.067974\n",
      "Reconstruction: 0.296662, Regularization: 3.771312\n",
      "2019-04-10 00:03:25,641 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: 3.928894\n",
      "Reconstruction: 0.291526, Regularization: 3.637368\n",
      "2019-04-10 00:03:25,705 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: 4.131960\n",
      "Reconstruction: 0.295435, Regularization: 3.836525\n",
      "2019-04-10 00:03:25,769 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: 4.455464\n",
      "Reconstruction: 0.300850, Regularization: 4.154614\n",
      "2019-04-10 00:03:25,832 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: 4.190395\n",
      "Reconstruction: 0.292412, Regularization: 3.897983\n",
      "2019-04-10 00:03:25,896 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: 4.158764\n",
      "Reconstruction: 0.291372, Regularization: 3.867392\n",
      "2019-04-10 00:03:25,960 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: 4.512398\n",
      "Reconstruction: 0.293934, Regularization: 4.218464\n",
      "2019-04-10 00:03:26,025 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: 4.241834\n",
      "Reconstruction: 0.285128, Regularization: 3.956706\n",
      "2019-04-10 00:03:26,089 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: 4.269578\n",
      "Reconstruction: 0.283195, Regularization: 3.986383\n",
      "2019-04-10 00:03:26,144 root         INFO     ====> Epoch: 3 Average loss: 4.1416\n",
      "2019-04-10 00:03:26,168 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: 4.369788\n",
      "Reconstruction: 0.281837, Regularization: 4.087951\n",
      "2019-04-10 00:03:26,231 root         INFO     Train Epoch: 4 [512/8000 (6%)]\tTotal Loss: 4.264059\n",
      "Reconstruction: 0.282141, Regularization: 3.981919\n",
      "2019-04-10 00:03:26,294 root         INFO     Train Epoch: 4 [1024/8000 (13%)]\tTotal Loss: 3.754130\n",
      "Reconstruction: 0.268002, Regularization: 3.486128\n",
      "2019-04-10 00:03:26,359 root         INFO     Train Epoch: 4 [1536/8000 (19%)]\tTotal Loss: 3.695152\n",
      "Reconstruction: 0.264956, Regularization: 3.430196\n",
      "2019-04-10 00:03:26,423 root         INFO     Train Epoch: 4 [2048/8000 (26%)]\tTotal Loss: 4.135266\n",
      "Reconstruction: 0.269605, Regularization: 3.865661\n",
      "2019-04-10 00:03:26,486 root         INFO     Train Epoch: 4 [2560/8000 (32%)]\tTotal Loss: 3.862968\n",
      "Reconstruction: 0.262207, Regularization: 3.600761\n",
      "2019-04-10 00:03:26,549 root         INFO     Train Epoch: 4 [3072/8000 (38%)]\tTotal Loss: 3.925252\n",
      "Reconstruction: 0.261492, Regularization: 3.663760\n",
      "2019-04-10 00:03:26,613 root         INFO     Train Epoch: 4 [3584/8000 (45%)]\tTotal Loss: 3.688387\n",
      "Reconstruction: 0.249921, Regularization: 3.438466\n",
      "2019-04-10 00:03:26,678 root         INFO     Train Epoch: 4 [4096/8000 (51%)]\tTotal Loss: 3.505939\n",
      "Reconstruction: 0.249601, Regularization: 3.256338\n",
      "2019-04-10 00:03:26,744 root         INFO     Train Epoch: 4 [4608/8000 (58%)]\tTotal Loss: 4.118244\n",
      "Reconstruction: 0.257830, Regularization: 3.860414\n",
      "2019-04-10 00:03:26,808 root         INFO     Train Epoch: 4 [5120/8000 (64%)]\tTotal Loss: 3.801773\n",
      "Reconstruction: 0.252933, Regularization: 3.548840\n",
      "2019-04-10 00:03:26,871 root         INFO     Train Epoch: 4 [5632/8000 (70%)]\tTotal Loss: 3.793063\n",
      "Reconstruction: 0.254756, Regularization: 3.538307\n",
      "2019-04-10 00:03:26,934 root         INFO     Train Epoch: 4 [6144/8000 (77%)]\tTotal Loss: 3.809104\n",
      "Reconstruction: 0.250942, Regularization: 3.558162\n",
      "2019-04-10 00:03:26,996 root         INFO     Train Epoch: 4 [6656/8000 (83%)]\tTotal Loss: 3.864724\n",
      "Reconstruction: 0.247038, Regularization: 3.617685\n",
      "2019-04-10 00:03:27,059 root         INFO     Train Epoch: 4 [7168/8000 (90%)]\tTotal Loss: 4.044535\n",
      "Reconstruction: 0.252552, Regularization: 3.791983\n",
      "2019-04-10 00:03:27,122 root         INFO     Train Epoch: 4 [7680/8000 (96%)]\tTotal Loss: 4.089710\n",
      "Reconstruction: 0.251631, Regularization: 3.838079\n",
      "2019-04-10 00:03:27,176 root         INFO     ====> Epoch: 4 Average loss: 3.9607\n",
      "2019-04-10 00:03:27,199 root         INFO     Train Epoch: 5 [0/8000 (0%)]\tTotal Loss: 3.991980\n",
      "Reconstruction: 0.245799, Regularization: 3.746181\n",
      "2019-04-10 00:03:27,263 root         INFO     Train Epoch: 5 [512/8000 (6%)]\tTotal Loss: 3.580968\n",
      "Reconstruction: 0.241645, Regularization: 3.339323\n",
      "2019-04-10 00:03:27,325 root         INFO     Train Epoch: 5 [1024/8000 (13%)]\tTotal Loss: 3.373808\n",
      "Reconstruction: 0.234570, Regularization: 3.139238\n",
      "2019-04-10 00:03:27,389 root         INFO     Train Epoch: 5 [1536/8000 (19%)]\tTotal Loss: 4.123785\n",
      "Reconstruction: 0.243686, Regularization: 3.880099\n",
      "2019-04-10 00:03:27,452 root         INFO     Train Epoch: 5 [2048/8000 (26%)]\tTotal Loss: 3.809265\n",
      "Reconstruction: 0.237290, Regularization: 3.571975\n",
      "2019-04-10 00:03:27,516 root         INFO     Train Epoch: 5 [2560/8000 (32%)]\tTotal Loss: 3.403698\n",
      "Reconstruction: 0.232377, Regularization: 3.171321\n",
      "2019-04-10 00:03:27,578 root         INFO     Train Epoch: 5 [3072/8000 (38%)]\tTotal Loss: 3.785821\n",
      "Reconstruction: 0.238119, Regularization: 3.547702\n",
      "2019-04-10 00:03:27,641 root         INFO     Train Epoch: 5 [3584/8000 (45%)]\tTotal Loss: 4.074844\n",
      "Reconstruction: 0.235818, Regularization: 3.839026\n",
      "2019-04-10 00:03:27,705 root         INFO     Train Epoch: 5 [4096/8000 (51%)]\tTotal Loss: 3.493261\n",
      "Reconstruction: 0.231083, Regularization: 3.262178\n",
      "2019-04-10 00:03:27,770 root         INFO     Train Epoch: 5 [4608/8000 (58%)]\tTotal Loss: 3.529269\n",
      "Reconstruction: 0.225812, Regularization: 3.303457\n",
      "2019-04-10 00:03:27,834 root         INFO     Train Epoch: 5 [5120/8000 (64%)]\tTotal Loss: 4.043408\n",
      "Reconstruction: 0.232637, Regularization: 3.810771\n",
      "2019-04-10 00:03:27,896 root         INFO     Train Epoch: 5 [5632/8000 (70%)]\tTotal Loss: 3.981135\n",
      "Reconstruction: 0.231965, Regularization: 3.749171\n",
      "2019-04-10 00:03:27,959 root         INFO     Train Epoch: 5 [6144/8000 (77%)]\tTotal Loss: 3.514679\n",
      "Reconstruction: 0.225140, Regularization: 3.289539\n",
      "2019-04-10 00:03:28,022 root         INFO     Train Epoch: 5 [6656/8000 (83%)]\tTotal Loss: 3.601330\n",
      "Reconstruction: 0.225210, Regularization: 3.376119\n",
      "2019-04-10 00:03:28,086 root         INFO     Train Epoch: 5 [7168/8000 (90%)]\tTotal Loss: 3.320495\n",
      "Reconstruction: 0.220697, Regularization: 3.099798\n",
      "2019-04-10 00:03:28,149 root         INFO     Train Epoch: 5 [7680/8000 (96%)]\tTotal Loss: 3.562166\n",
      "Reconstruction: 0.220567, Regularization: 3.341599\n",
      "2019-04-10 00:03:28,204 root         INFO     ====> Epoch: 5 Average loss: 3.7960\n",
      "2019-04-10 00:03:28,228 root         INFO     Train Epoch: 6 [0/8000 (0%)]\tTotal Loss: 3.842774\n",
      "Reconstruction: 0.223176, Regularization: 3.619597\n",
      "2019-04-10 00:03:28,291 root         INFO     Train Epoch: 6 [512/8000 (6%)]\tTotal Loss: 3.944745\n",
      "Reconstruction: 0.223054, Regularization: 3.721691\n",
      "2019-04-10 00:03:28,354 root         INFO     Train Epoch: 6 [1024/8000 (13%)]\tTotal Loss: 3.872531\n",
      "Reconstruction: 0.223163, Regularization: 3.649368\n",
      "2019-04-10 00:03:28,417 root         INFO     Train Epoch: 6 [1536/8000 (19%)]\tTotal Loss: 3.968135\n",
      "Reconstruction: 0.220180, Regularization: 3.747956\n",
      "2019-04-10 00:03:28,481 root         INFO     Train Epoch: 6 [2048/8000 (26%)]\tTotal Loss: 3.449006\n",
      "Reconstruction: 0.216701, Regularization: 3.232305\n",
      "2019-04-10 00:03:28,543 root         INFO     Train Epoch: 6 [2560/8000 (32%)]\tTotal Loss: 3.449092\n",
      "Reconstruction: 0.218541, Regularization: 3.230551\n",
      "2019-04-10 00:03:28,606 root         INFO     Train Epoch: 6 [3072/8000 (38%)]\tTotal Loss: 3.640997\n",
      "Reconstruction: 0.214052, Regularization: 3.426945\n",
      "2019-04-10 00:03:28,670 root         INFO     Train Epoch: 6 [3584/8000 (45%)]\tTotal Loss: 3.219060\n",
      "Reconstruction: 0.206668, Regularization: 3.012392\n",
      "2019-04-10 00:03:28,733 root         INFO     Train Epoch: 6 [4096/8000 (51%)]\tTotal Loss: 3.682214\n",
      "Reconstruction: 0.216685, Regularization: 3.465529\n",
      "2019-04-10 00:03:28,796 root         INFO     Train Epoch: 6 [4608/8000 (58%)]\tTotal Loss: 3.697826\n",
      "Reconstruction: 0.215919, Regularization: 3.481906\n",
      "2019-04-10 00:03:28,859 root         INFO     Train Epoch: 6 [5120/8000 (64%)]\tTotal Loss: 3.446451\n",
      "Reconstruction: 0.212039, Regularization: 3.234412\n",
      "2019-04-10 00:03:28,922 root         INFO     Train Epoch: 6 [5632/8000 (70%)]\tTotal Loss: 3.892791\n",
      "Reconstruction: 0.215898, Regularization: 3.676892\n",
      "2019-04-10 00:03:28,984 root         INFO     Train Epoch: 6 [6144/8000 (77%)]\tTotal Loss: 3.881846\n",
      "Reconstruction: 0.216808, Regularization: 3.665037\n",
      "2019-04-10 00:03:29,047 root         INFO     Train Epoch: 6 [6656/8000 (83%)]\tTotal Loss: 3.627563\n",
      "Reconstruction: 0.211715, Regularization: 3.415849\n",
      "2019-04-10 00:03:29,109 root         INFO     Train Epoch: 6 [7168/8000 (90%)]\tTotal Loss: 3.382766\n",
      "Reconstruction: 0.205494, Regularization: 3.177271\n",
      "2019-04-10 00:03:29,171 root         INFO     Train Epoch: 6 [7680/8000 (96%)]\tTotal Loss: 3.672097\n",
      "Reconstruction: 0.207909, Regularization: 3.464188\n",
      "2019-04-10 00:03:29,225 root         INFO     ====> Epoch: 6 Average loss: 3.6432\n",
      "2019-04-10 00:03:29,249 root         INFO     Train Epoch: 7 [0/8000 (0%)]\tTotal Loss: 3.562412\n",
      "Reconstruction: 0.208987, Regularization: 3.353425\n",
      "2019-04-10 00:03:29,313 root         INFO     Train Epoch: 7 [512/8000 (6%)]\tTotal Loss: 3.218004\n",
      "Reconstruction: 0.206401, Regularization: 3.011604\n",
      "2019-04-10 00:03:29,377 root         INFO     Train Epoch: 7 [1024/8000 (13%)]\tTotal Loss: 3.692136\n",
      "Reconstruction: 0.210871, Regularization: 3.481265\n",
      "2019-04-10 00:03:29,440 root         INFO     Train Epoch: 7 [1536/8000 (19%)]\tTotal Loss: 3.701214\n",
      "Reconstruction: 0.208326, Regularization: 3.492888\n",
      "2019-04-10 00:03:29,503 root         INFO     Train Epoch: 7 [2048/8000 (26%)]\tTotal Loss: 3.504440\n",
      "Reconstruction: 0.206395, Regularization: 3.298045\n",
      "2019-04-10 00:03:29,565 root         INFO     Train Epoch: 7 [2560/8000 (32%)]\tTotal Loss: 3.282134\n",
      "Reconstruction: 0.205310, Regularization: 3.076824\n",
      "2019-04-10 00:03:29,627 root         INFO     Train Epoch: 7 [3072/8000 (38%)]\tTotal Loss: 3.621672\n",
      "Reconstruction: 0.205918, Regularization: 3.415754\n",
      "2019-04-10 00:03:29,690 root         INFO     Train Epoch: 7 [3584/8000 (45%)]\tTotal Loss: 3.602988\n",
      "Reconstruction: 0.205259, Regularization: 3.397730\n",
      "2019-04-10 00:03:29,752 root         INFO     Train Epoch: 7 [4096/8000 (51%)]\tTotal Loss: 3.434243\n",
      "Reconstruction: 0.202343, Regularization: 3.231900\n",
      "2019-04-10 00:03:29,814 root         INFO     Train Epoch: 7 [4608/8000 (58%)]\tTotal Loss: 4.172588\n",
      "Reconstruction: 0.209012, Regularization: 3.963576\n",
      "2019-04-10 00:03:29,876 root         INFO     Train Epoch: 7 [5120/8000 (64%)]\tTotal Loss: 3.564083\n",
      "Reconstruction: 0.206044, Regularization: 3.358039\n",
      "2019-04-10 00:03:29,939 root         INFO     Train Epoch: 7 [5632/8000 (70%)]\tTotal Loss: 3.424865\n",
      "Reconstruction: 0.203034, Regularization: 3.221831\n",
      "2019-04-10 00:03:30,000 root         INFO     Train Epoch: 7 [6144/8000 (77%)]\tTotal Loss: 3.527719\n",
      "Reconstruction: 0.205648, Regularization: 3.322071\n",
      "2019-04-10 00:03:30,062 root         INFO     Train Epoch: 7 [6656/8000 (83%)]\tTotal Loss: 3.749433\n",
      "Reconstruction: 0.203022, Regularization: 3.546410\n",
      "2019-04-10 00:03:30,124 root         INFO     Train Epoch: 7 [7168/8000 (90%)]\tTotal Loss: 3.108829\n",
      "Reconstruction: 0.196923, Regularization: 2.911906\n",
      "2019-04-10 00:03:30,186 root         INFO     Train Epoch: 7 [7680/8000 (96%)]\tTotal Loss: 3.218930\n",
      "Reconstruction: 0.196691, Regularization: 3.022239\n",
      "2019-04-10 00:03:30,239 root         INFO     ====> Epoch: 7 Average loss: 3.4989\n",
      "2019-04-10 00:03:30,263 root         INFO     Train Epoch: 8 [0/8000 (0%)]\tTotal Loss: 3.984398\n",
      "Reconstruction: 0.204971, Regularization: 3.779427\n",
      "2019-04-10 00:03:30,327 root         INFO     Train Epoch: 8 [512/8000 (6%)]\tTotal Loss: 3.403322\n",
      "Reconstruction: 0.200313, Regularization: 3.203008\n",
      "2019-04-10 00:03:30,389 root         INFO     Train Epoch: 8 [1024/8000 (13%)]\tTotal Loss: 3.018662\n",
      "Reconstruction: 0.196010, Regularization: 2.822653\n",
      "2019-04-10 00:03:30,452 root         INFO     Train Epoch: 8 [1536/8000 (19%)]\tTotal Loss: 3.229357\n",
      "Reconstruction: 0.198227, Regularization: 3.031130\n",
      "2019-04-10 00:03:30,514 root         INFO     Train Epoch: 8 [2048/8000 (26%)]\tTotal Loss: 3.345804\n",
      "Reconstruction: 0.198641, Regularization: 3.147163\n",
      "2019-04-10 00:03:30,577 root         INFO     Train Epoch: 8 [2560/8000 (32%)]\tTotal Loss: 3.671164\n",
      "Reconstruction: 0.198882, Regularization: 3.472282\n",
      "2019-04-10 00:03:30,640 root         INFO     Train Epoch: 8 [3072/8000 (38%)]\tTotal Loss: 3.625135\n",
      "Reconstruction: 0.200935, Regularization: 3.424200\n",
      "2019-04-10 00:03:30,703 root         INFO     Train Epoch: 8 [3584/8000 (45%)]\tTotal Loss: 3.056813\n",
      "Reconstruction: 0.193121, Regularization: 2.863693\n",
      "2019-04-10 00:03:30,766 root         INFO     Train Epoch: 8 [4096/8000 (51%)]\tTotal Loss: 3.562566\n",
      "Reconstruction: 0.198014, Regularization: 3.364552\n",
      "2019-04-10 00:03:30,829 root         INFO     Train Epoch: 8 [4608/8000 (58%)]\tTotal Loss: 3.594683\n",
      "Reconstruction: 0.198541, Regularization: 3.396142\n",
      "2019-04-10 00:03:30,893 root         INFO     Train Epoch: 8 [5120/8000 (64%)]\tTotal Loss: 3.282210\n",
      "Reconstruction: 0.195403, Regularization: 3.086807\n",
      "2019-04-10 00:03:30,957 root         INFO     Train Epoch: 8 [5632/8000 (70%)]\tTotal Loss: 3.459808\n",
      "Reconstruction: 0.195360, Regularization: 3.264448\n",
      "2019-04-10 00:03:31,020 root         INFO     Train Epoch: 8 [6144/8000 (77%)]\tTotal Loss: 3.509828\n",
      "Reconstruction: 0.196520, Regularization: 3.313308\n",
      "2019-04-10 00:03:31,082 root         INFO     Train Epoch: 8 [6656/8000 (83%)]\tTotal Loss: 3.309480\n",
      "Reconstruction: 0.192811, Regularization: 3.116669\n",
      "2019-04-10 00:03:31,145 root         INFO     Train Epoch: 8 [7168/8000 (90%)]\tTotal Loss: 3.402502\n",
      "Reconstruction: 0.194844, Regularization: 3.207658\n",
      "2019-04-10 00:03:31,209 root         INFO     Train Epoch: 8 [7680/8000 (96%)]\tTotal Loss: 3.444503\n",
      "Reconstruction: 0.195835, Regularization: 3.248668\n",
      "2019-04-10 00:03:31,262 root         INFO     ====> Epoch: 8 Average loss: 3.3613\n",
      "2019-04-10 00:03:31,286 root         INFO     Train Epoch: 9 [0/8000 (0%)]\tTotal Loss: 3.314296\n",
      "Reconstruction: 0.189801, Regularization: 3.124495\n",
      "2019-04-10 00:03:31,350 root         INFO     Train Epoch: 9 [512/8000 (6%)]\tTotal Loss: 3.506750\n",
      "Reconstruction: 0.193419, Regularization: 3.313331\n",
      "2019-04-10 00:03:31,413 root         INFO     Train Epoch: 9 [1024/8000 (13%)]\tTotal Loss: 3.026393\n",
      "Reconstruction: 0.191772, Regularization: 2.834622\n",
      "2019-04-10 00:03:31,476 root         INFO     Train Epoch: 9 [1536/8000 (19%)]\tTotal Loss: 3.404675\n",
      "Reconstruction: 0.193694, Regularization: 3.210981\n",
      "2019-04-10 00:03:31,539 root         INFO     Train Epoch: 9 [2048/8000 (26%)]\tTotal Loss: 3.506298\n",
      "Reconstruction: 0.191711, Regularization: 3.314587\n",
      "2019-04-10 00:03:31,601 root         INFO     Train Epoch: 9 [2560/8000 (32%)]\tTotal Loss: 3.020189\n",
      "Reconstruction: 0.188281, Regularization: 2.831908\n",
      "2019-04-10 00:03:31,662 root         INFO     Train Epoch: 9 [3072/8000 (38%)]\tTotal Loss: 2.987873\n",
      "Reconstruction: 0.187842, Regularization: 2.800030\n",
      "2019-04-10 00:03:31,722 root         INFO     Train Epoch: 9 [3584/8000 (45%)]\tTotal Loss: 3.243090\n",
      "Reconstruction: 0.191336, Regularization: 3.051754\n",
      "2019-04-10 00:03:31,783 root         INFO     Train Epoch: 9 [4096/8000 (51%)]\tTotal Loss: 3.353192\n",
      "Reconstruction: 0.191392, Regularization: 3.161800\n",
      "2019-04-10 00:03:31,843 root         INFO     Train Epoch: 9 [4608/8000 (58%)]\tTotal Loss: 3.213115\n",
      "Reconstruction: 0.191181, Regularization: 3.021934\n",
      "2019-04-10 00:03:31,905 root         INFO     Train Epoch: 9 [5120/8000 (64%)]\tTotal Loss: 3.104542\n",
      "Reconstruction: 0.186972, Regularization: 2.917570\n",
      "2019-04-10 00:03:31,966 root         INFO     Train Epoch: 9 [5632/8000 (70%)]\tTotal Loss: 3.505067\n",
      "Reconstruction: 0.191934, Regularization: 3.313133\n",
      "2019-04-10 00:03:32,027 root         INFO     Train Epoch: 9 [6144/8000 (77%)]\tTotal Loss: 3.084342\n",
      "Reconstruction: 0.189561, Regularization: 2.894780\n",
      "2019-04-10 00:03:32,089 root         INFO     Train Epoch: 9 [6656/8000 (83%)]\tTotal Loss: 3.260314\n",
      "Reconstruction: 0.188794, Regularization: 3.071520\n",
      "2019-04-10 00:03:32,150 root         INFO     Train Epoch: 9 [7168/8000 (90%)]\tTotal Loss: 3.231771\n",
      "Reconstruction: 0.188654, Regularization: 3.043116\n",
      "2019-04-10 00:03:32,211 root         INFO     Train Epoch: 9 [7680/8000 (96%)]\tTotal Loss: 3.314476\n",
      "Reconstruction: 0.189815, Regularization: 3.124661\n",
      "2019-04-10 00:03:32,263 root         INFO     ====> Epoch: 9 Average loss: 3.2289\n",
      "2019-04-10 00:03:32,287 root         INFO     Train Epoch: 10 [0/8000 (0%)]\tTotal Loss: 3.291342\n",
      "Reconstruction: 0.190904, Regularization: 3.100438\n",
      "2019-04-10 00:03:32,350 root         INFO     Train Epoch: 10 [512/8000 (6%)]\tTotal Loss: 3.171082\n",
      "Reconstruction: 0.186470, Regularization: 2.984612\n",
      "2019-04-10 00:03:32,412 root         INFO     Train Epoch: 10 [1024/8000 (13%)]\tTotal Loss: 2.987452\n",
      "Reconstruction: 0.184274, Regularization: 2.803177\n",
      "2019-04-10 00:03:32,474 root         INFO     Train Epoch: 10 [1536/8000 (19%)]\tTotal Loss: 3.095703\n",
      "Reconstruction: 0.184295, Regularization: 2.911408\n",
      "2019-04-10 00:03:32,536 root         INFO     Train Epoch: 10 [2048/8000 (26%)]\tTotal Loss: 2.534696\n",
      "Reconstruction: 0.178536, Regularization: 2.356160\n",
      "2019-04-10 00:03:32,597 root         INFO     Train Epoch: 10 [2560/8000 (32%)]\tTotal Loss: 2.894854\n",
      "Reconstruction: 0.182494, Regularization: 2.712360\n",
      "2019-04-10 00:03:32,659 root         INFO     Train Epoch: 10 [3072/8000 (38%)]\tTotal Loss: 3.124429\n",
      "Reconstruction: 0.185840, Regularization: 2.938589\n",
      "2019-04-10 00:03:32,721 root         INFO     Train Epoch: 10 [3584/8000 (45%)]\tTotal Loss: 3.007975\n",
      "Reconstruction: 0.186606, Regularization: 2.821369\n",
      "2019-04-10 00:03:32,784 root         INFO     Train Epoch: 10 [4096/8000 (51%)]\tTotal Loss: 3.101853\n",
      "Reconstruction: 0.184817, Regularization: 2.917036\n",
      "2019-04-10 00:03:32,845 root         INFO     Train Epoch: 10 [4608/8000 (58%)]\tTotal Loss: 3.406043\n",
      "Reconstruction: 0.187408, Regularization: 3.218634\n",
      "2019-04-10 00:03:32,907 root         INFO     Train Epoch: 10 [5120/8000 (64%)]\tTotal Loss: 3.201618\n",
      "Reconstruction: 0.186625, Regularization: 3.014994\n",
      "2019-04-10 00:03:32,969 root         INFO     Train Epoch: 10 [5632/8000 (70%)]\tTotal Loss: 3.229065\n",
      "Reconstruction: 0.189018, Regularization: 3.040046\n",
      "2019-04-10 00:03:33,030 root         INFO     Train Epoch: 10 [6144/8000 (77%)]\tTotal Loss: 3.002381\n",
      "Reconstruction: 0.185545, Regularization: 2.816836\n",
      "2019-04-10 00:03:33,092 root         INFO     Train Epoch: 10 [6656/8000 (83%)]\tTotal Loss: 3.110626\n",
      "Reconstruction: 0.184452, Regularization: 2.926174\n",
      "2019-04-10 00:03:33,153 root         INFO     Train Epoch: 10 [7168/8000 (90%)]\tTotal Loss: 2.949425\n",
      "Reconstruction: 0.184573, Regularization: 2.764853\n",
      "2019-04-10 00:03:33,215 root         INFO     Train Epoch: 10 [7680/8000 (96%)]\tTotal Loss: 3.311798\n",
      "Reconstruction: 0.188282, Regularization: 3.123516\n",
      "2019-04-10 00:03:33,268 root         INFO     ====> Epoch: 10 Average loss: 3.1009\n",
      "2019-04-10 00:03:33,292 root         INFO     Train Epoch: 11 [0/8000 (0%)]\tTotal Loss: 2.804031\n",
      "Reconstruction: 0.181518, Regularization: 2.622513\n",
      "2019-04-10 00:03:33,354 root         INFO     Train Epoch: 11 [512/8000 (6%)]\tTotal Loss: 3.384312\n",
      "Reconstruction: 0.187290, Regularization: 3.197023\n",
      "2019-04-10 00:03:33,416 root         INFO     Train Epoch: 11 [1024/8000 (13%)]\tTotal Loss: 2.822148\n",
      "Reconstruction: 0.179621, Regularization: 2.642527\n",
      "2019-04-10 00:03:33,477 root         INFO     Train Epoch: 11 [1536/8000 (19%)]\tTotal Loss: 2.914158\n",
      "Reconstruction: 0.180411, Regularization: 2.733747\n",
      "2019-04-10 00:03:33,538 root         INFO     Train Epoch: 11 [2048/8000 (26%)]\tTotal Loss: 3.279043\n",
      "Reconstruction: 0.186841, Regularization: 3.092202\n",
      "2019-04-10 00:03:33,600 root         INFO     Train Epoch: 11 [2560/8000 (32%)]\tTotal Loss: 2.904683\n",
      "Reconstruction: 0.182611, Regularization: 2.722072\n",
      "2019-04-10 00:03:33,661 root         INFO     Train Epoch: 11 [3072/8000 (38%)]\tTotal Loss: 2.695181\n",
      "Reconstruction: 0.179966, Regularization: 2.515214\n",
      "2019-04-10 00:03:33,723 root         INFO     Train Epoch: 11 [3584/8000 (45%)]\tTotal Loss: 3.096661\n",
      "Reconstruction: 0.183110, Regularization: 2.913550\n",
      "2019-04-10 00:03:33,785 root         INFO     Train Epoch: 11 [4096/8000 (51%)]\tTotal Loss: 2.872932\n",
      "Reconstruction: 0.180712, Regularization: 2.692220\n",
      "2019-04-10 00:03:33,847 root         INFO     Train Epoch: 11 [4608/8000 (58%)]\tTotal Loss: 2.834706\n",
      "Reconstruction: 0.179443, Regularization: 2.655263\n",
      "2019-04-10 00:03:33,909 root         INFO     Train Epoch: 11 [5120/8000 (64%)]\tTotal Loss: 3.162154\n",
      "Reconstruction: 0.185922, Regularization: 2.976232\n",
      "2019-04-10 00:03:33,971 root         INFO     Train Epoch: 11 [5632/8000 (70%)]\tTotal Loss: 2.896319\n",
      "Reconstruction: 0.182345, Regularization: 2.713975\n",
      "2019-04-10 00:03:34,032 root         INFO     Train Epoch: 11 [6144/8000 (77%)]\tTotal Loss: 3.404698\n",
      "Reconstruction: 0.187825, Regularization: 3.216873\n",
      "2019-04-10 00:03:34,094 root         INFO     Train Epoch: 11 [6656/8000 (83%)]\tTotal Loss: 3.041082\n",
      "Reconstruction: 0.182006, Regularization: 2.859076\n",
      "2019-04-10 00:03:34,156 root         INFO     Train Epoch: 11 [7168/8000 (90%)]\tTotal Loss: 2.801465\n",
      "Reconstruction: 0.178944, Regularization: 2.622521\n",
      "2019-04-10 00:03:34,218 root         INFO     Train Epoch: 11 [7680/8000 (96%)]\tTotal Loss: 2.861929\n",
      "Reconstruction: 0.181113, Regularization: 2.680815\n",
      "2019-04-10 00:03:34,272 root         INFO     ====> Epoch: 11 Average loss: 2.9768\n",
      "2019-04-10 00:03:34,296 root         INFO     Train Epoch: 12 [0/8000 (0%)]\tTotal Loss: 2.796901\n",
      "Reconstruction: 0.178163, Regularization: 2.618737\n",
      "2019-04-10 00:03:34,358 root         INFO     Train Epoch: 12 [512/8000 (6%)]\tTotal Loss: 2.906636\n",
      "Reconstruction: 0.177714, Regularization: 2.728921\n",
      "2019-04-10 00:03:34,421 root         INFO     Train Epoch: 12 [1024/8000 (13%)]\tTotal Loss: 2.910834\n",
      "Reconstruction: 0.183694, Regularization: 2.727139\n",
      "2019-04-10 00:03:34,482 root         INFO     Train Epoch: 12 [1536/8000 (19%)]\tTotal Loss: 2.918833\n",
      "Reconstruction: 0.180615, Regularization: 2.738218\n",
      "2019-04-10 00:03:34,544 root         INFO     Train Epoch: 12 [2048/8000 (26%)]\tTotal Loss: 2.905106\n",
      "Reconstruction: 0.179448, Regularization: 2.725658\n",
      "2019-04-10 00:03:34,606 root         INFO     Train Epoch: 12 [2560/8000 (32%)]\tTotal Loss: 2.920832\n",
      "Reconstruction: 0.181686, Regularization: 2.739146\n",
      "2019-04-10 00:03:34,670 root         INFO     Train Epoch: 12 [3072/8000 (38%)]\tTotal Loss: 2.750580\n",
      "Reconstruction: 0.177500, Regularization: 2.573080\n",
      "2019-04-10 00:03:34,733 root         INFO     Train Epoch: 12 [3584/8000 (45%)]\tTotal Loss: 2.651063\n",
      "Reconstruction: 0.176932, Regularization: 2.474131\n",
      "2019-04-10 00:03:34,796 root         INFO     Train Epoch: 12 [4096/8000 (51%)]\tTotal Loss: 2.727572\n",
      "Reconstruction: 0.176560, Regularization: 2.551012\n",
      "2019-04-10 00:03:34,859 root         INFO     Train Epoch: 12 [4608/8000 (58%)]\tTotal Loss: 2.937896\n",
      "Reconstruction: 0.181203, Regularization: 2.756692\n",
      "2019-04-10 00:03:34,921 root         INFO     Train Epoch: 12 [5120/8000 (64%)]\tTotal Loss: 2.863867\n",
      "Reconstruction: 0.179258, Regularization: 2.684609\n",
      "2019-04-10 00:03:34,984 root         INFO     Train Epoch: 12 [5632/8000 (70%)]\tTotal Loss: 2.738827\n",
      "Reconstruction: 0.177782, Regularization: 2.561045\n",
      "2019-04-10 00:03:35,048 root         INFO     Train Epoch: 12 [6144/8000 (77%)]\tTotal Loss: 2.925750\n",
      "Reconstruction: 0.180637, Regularization: 2.745113\n",
      "2019-04-10 00:03:35,111 root         INFO     Train Epoch: 12 [6656/8000 (83%)]\tTotal Loss: 3.134197\n",
      "Reconstruction: 0.185014, Regularization: 2.949183\n",
      "2019-04-10 00:03:35,173 root         INFO     Train Epoch: 12 [7168/8000 (90%)]\tTotal Loss: 2.834689\n",
      "Reconstruction: 0.178137, Regularization: 2.656552\n",
      "2019-04-10 00:03:35,237 root         INFO     Train Epoch: 12 [7680/8000 (96%)]\tTotal Loss: 2.709395\n",
      "Reconstruction: 0.177805, Regularization: 2.531590\n",
      "2019-04-10 00:03:35,291 root         INFO     ====> Epoch: 12 Average loss: 2.8560\n",
      "2019-04-10 00:03:35,314 root         INFO     Train Epoch: 13 [0/8000 (0%)]\tTotal Loss: 2.902944\n",
      "Reconstruction: 0.180355, Regularization: 2.722589\n",
      "2019-04-10 00:03:35,379 root         INFO     Train Epoch: 13 [512/8000 (6%)]\tTotal Loss: 2.489607\n",
      "Reconstruction: 0.175201, Regularization: 2.314406\n",
      "2019-04-10 00:03:35,443 root         INFO     Train Epoch: 13 [1024/8000 (13%)]\tTotal Loss: 2.553465\n",
      "Reconstruction: 0.173541, Regularization: 2.379924\n",
      "2019-04-10 00:03:35,507 root         INFO     Train Epoch: 13 [1536/8000 (19%)]\tTotal Loss: 2.439633\n",
      "Reconstruction: 0.171604, Regularization: 2.268029\n",
      "2019-04-10 00:03:35,572 root         INFO     Train Epoch: 13 [2048/8000 (26%)]\tTotal Loss: 2.953396\n",
      "Reconstruction: 0.176400, Regularization: 2.776997\n",
      "2019-04-10 00:03:35,638 root         INFO     Train Epoch: 13 [2560/8000 (32%)]\tTotal Loss: 2.791421\n",
      "Reconstruction: 0.173409, Regularization: 2.618012\n",
      "2019-04-10 00:03:35,704 root         INFO     Train Epoch: 13 [3072/8000 (38%)]\tTotal Loss: 2.814191\n",
      "Reconstruction: 0.178280, Regularization: 2.635911\n",
      "2019-04-10 00:03:35,769 root         INFO     Train Epoch: 13 [3584/8000 (45%)]\tTotal Loss: 2.545333\n",
      "Reconstruction: 0.176389, Regularization: 2.368944\n",
      "2019-04-10 00:03:35,835 root         INFO     Train Epoch: 13 [4096/8000 (51%)]\tTotal Loss: 2.875999\n",
      "Reconstruction: 0.179699, Regularization: 2.696300\n",
      "2019-04-10 00:03:35,900 root         INFO     Train Epoch: 13 [4608/8000 (58%)]\tTotal Loss: 2.495525\n",
      "Reconstruction: 0.169685, Regularization: 2.325840\n",
      "2019-04-10 00:03:35,964 root         INFO     Train Epoch: 13 [5120/8000 (64%)]\tTotal Loss: 2.726987\n",
      "Reconstruction: 0.174619, Regularization: 2.552367\n",
      "2019-04-10 00:03:36,028 root         INFO     Train Epoch: 13 [5632/8000 (70%)]\tTotal Loss: 2.893676\n",
      "Reconstruction: 0.178273, Regularization: 2.715403\n",
      "2019-04-10 00:03:36,092 root         INFO     Train Epoch: 13 [6144/8000 (77%)]\tTotal Loss: 2.961951\n",
      "Reconstruction: 0.178064, Regularization: 2.783888\n",
      "2019-04-10 00:03:36,156 root         INFO     Train Epoch: 13 [6656/8000 (83%)]\tTotal Loss: 2.439438\n",
      "Reconstruction: 0.173635, Regularization: 2.265803\n",
      "2019-04-10 00:03:36,221 root         INFO     Train Epoch: 13 [7168/8000 (90%)]\tTotal Loss: 2.718885\n",
      "Reconstruction: 0.177258, Regularization: 2.541627\n",
      "2019-04-10 00:03:36,285 root         INFO     Train Epoch: 13 [7680/8000 (96%)]\tTotal Loss: 2.420361\n",
      "Reconstruction: 0.172260, Regularization: 2.248101\n",
      "2019-04-10 00:03:36,341 root         INFO     ====> Epoch: 13 Average loss: 2.7384\n",
      "2019-04-10 00:03:36,365 root         INFO     Train Epoch: 14 [0/8000 (0%)]\tTotal Loss: 2.511004\n",
      "Reconstruction: 0.172157, Regularization: 2.338847\n",
      "2019-04-10 00:03:36,429 root         INFO     Train Epoch: 14 [512/8000 (6%)]\tTotal Loss: 2.520320\n",
      "Reconstruction: 0.171033, Regularization: 2.349287\n",
      "2019-04-10 00:03:36,492 root         INFO     Train Epoch: 14 [1024/8000 (13%)]\tTotal Loss: 2.522051\n",
      "Reconstruction: 0.172471, Regularization: 2.349580\n",
      "2019-04-10 00:03:36,555 root         INFO     Train Epoch: 14 [1536/8000 (19%)]\tTotal Loss: 2.811761\n",
      "Reconstruction: 0.173817, Regularization: 2.637944\n",
      "2019-04-10 00:03:36,618 root         INFO     Train Epoch: 14 [2048/8000 (26%)]\tTotal Loss: 2.770321\n",
      "Reconstruction: 0.174218, Regularization: 2.596103\n",
      "2019-04-10 00:03:36,681 root         INFO     Train Epoch: 14 [2560/8000 (32%)]\tTotal Loss: 2.412968\n",
      "Reconstruction: 0.169110, Regularization: 2.243858\n",
      "2019-04-10 00:03:36,743 root         INFO     Train Epoch: 14 [3072/8000 (38%)]\tTotal Loss: 2.413095\n",
      "Reconstruction: 0.168153, Regularization: 2.244942\n",
      "2019-04-10 00:03:36,806 root         INFO     Train Epoch: 14 [3584/8000 (45%)]\tTotal Loss: 2.442168\n",
      "Reconstruction: 0.175555, Regularization: 2.266612\n",
      "2019-04-10 00:03:36,869 root         INFO     Train Epoch: 14 [4096/8000 (51%)]\tTotal Loss: 2.393438\n",
      "Reconstruction: 0.163467, Regularization: 2.229971\n",
      "2019-04-10 00:03:36,931 root         INFO     Train Epoch: 14 [4608/8000 (58%)]\tTotal Loss: 2.534564\n",
      "Reconstruction: 0.170082, Regularization: 2.364482\n",
      "2019-04-10 00:03:36,994 root         INFO     Train Epoch: 14 [5120/8000 (64%)]\tTotal Loss: 2.415793\n",
      "Reconstruction: 0.169483, Regularization: 2.246310\n",
      "2019-04-10 00:03:37,057 root         INFO     Train Epoch: 14 [5632/8000 (70%)]\tTotal Loss: 2.787952\n",
      "Reconstruction: 0.174789, Regularization: 2.613163\n",
      "2019-04-10 00:03:37,120 root         INFO     Train Epoch: 14 [6144/8000 (77%)]\tTotal Loss: 2.395418\n",
      "Reconstruction: 0.169148, Regularization: 2.226271\n",
      "2019-04-10 00:03:37,183 root         INFO     Train Epoch: 14 [6656/8000 (83%)]\tTotal Loss: 2.394025\n",
      "Reconstruction: 0.169074, Regularization: 2.224951\n",
      "2019-04-10 00:03:37,245 root         INFO     Train Epoch: 14 [7168/8000 (90%)]\tTotal Loss: 2.326819\n",
      "Reconstruction: 0.170938, Regularization: 2.155881\n",
      "2019-04-10 00:03:37,308 root         INFO     Train Epoch: 14 [7680/8000 (96%)]\tTotal Loss: 2.619802\n",
      "Reconstruction: 0.169573, Regularization: 2.450229\n",
      "2019-04-10 00:03:37,361 root         INFO     ====> Epoch: 14 Average loss: 2.6238\n",
      "2019-04-10 00:03:37,385 root         INFO     Train Epoch: 15 [0/8000 (0%)]\tTotal Loss: 2.659425\n",
      "Reconstruction: 0.174347, Regularization: 2.485078\n",
      "2019-04-10 00:03:37,448 root         INFO     Train Epoch: 15 [512/8000 (6%)]\tTotal Loss: 2.403486\n",
      "Reconstruction: 0.169263, Regularization: 2.234223\n",
      "2019-04-10 00:03:37,511 root         INFO     Train Epoch: 15 [1024/8000 (13%)]\tTotal Loss: 2.475321\n",
      "Reconstruction: 0.170903, Regularization: 2.304418\n",
      "2019-04-10 00:03:37,573 root         INFO     Train Epoch: 15 [1536/8000 (19%)]\tTotal Loss: 2.826139\n",
      "Reconstruction: 0.175524, Regularization: 2.650615\n",
      "2019-04-10 00:03:37,635 root         INFO     Train Epoch: 15 [2048/8000 (26%)]\tTotal Loss: 2.652350\n",
      "Reconstruction: 0.173198, Regularization: 2.479153\n",
      "2019-04-10 00:03:37,697 root         INFO     Train Epoch: 15 [2560/8000 (32%)]\tTotal Loss: 2.698129\n",
      "Reconstruction: 0.172420, Regularization: 2.525709\n",
      "2019-04-10 00:03:37,757 root         INFO     Train Epoch: 15 [3072/8000 (38%)]\tTotal Loss: 2.463230\n",
      "Reconstruction: 0.168598, Regularization: 2.294632\n",
      "2019-04-10 00:03:37,818 root         INFO     Train Epoch: 15 [3584/8000 (45%)]\tTotal Loss: 2.393421\n",
      "Reconstruction: 0.167749, Regularization: 2.225672\n",
      "2019-04-10 00:03:37,879 root         INFO     Train Epoch: 15 [4096/8000 (51%)]\tTotal Loss: 2.882904\n",
      "Reconstruction: 0.175073, Regularization: 2.707831\n",
      "2019-04-10 00:03:37,940 root         INFO     Train Epoch: 15 [4608/8000 (58%)]\tTotal Loss: 2.802213\n",
      "Reconstruction: 0.172851, Regularization: 2.629362\n",
      "2019-04-10 00:03:38,001 root         INFO     Train Epoch: 15 [5120/8000 (64%)]\tTotal Loss: 2.251061\n",
      "Reconstruction: 0.166768, Regularization: 2.084292\n",
      "2019-04-10 00:03:38,061 root         INFO     Train Epoch: 15 [5632/8000 (70%)]\tTotal Loss: 2.489668\n",
      "Reconstruction: 0.168885, Regularization: 2.320782\n",
      "2019-04-10 00:03:38,122 root         INFO     Train Epoch: 15 [6144/8000 (77%)]\tTotal Loss: 2.978855\n",
      "Reconstruction: 0.177195, Regularization: 2.801660\n",
      "2019-04-10 00:03:38,183 root         INFO     Train Epoch: 15 [6656/8000 (83%)]\tTotal Loss: 2.416907\n",
      "Reconstruction: 0.169819, Regularization: 2.247088\n",
      "2019-04-10 00:03:38,244 root         INFO     Train Epoch: 15 [7168/8000 (90%)]\tTotal Loss: 2.184753\n",
      "Reconstruction: 0.161096, Regularization: 2.023657\n",
      "2019-04-10 00:03:38,305 root         INFO     Train Epoch: 15 [7680/8000 (96%)]\tTotal Loss: 2.822698\n",
      "Reconstruction: 0.174260, Regularization: 2.648439\n",
      "2019-04-10 00:03:38,358 root         INFO     ====> Epoch: 15 Average loss: 2.5120\n",
      "2019-04-10 00:03:38,381 root         INFO     Train Epoch: 16 [0/8000 (0%)]\tTotal Loss: 2.568200\n",
      "Reconstruction: 0.167722, Regularization: 2.400478\n",
      "2019-04-10 00:03:38,443 root         INFO     Train Epoch: 16 [512/8000 (6%)]\tTotal Loss: 2.507644\n",
      "Reconstruction: 0.169580, Regularization: 2.338064\n",
      "2019-04-10 00:03:38,504 root         INFO     Train Epoch: 16 [1024/8000 (13%)]\tTotal Loss: 2.569780\n",
      "Reconstruction: 0.170685, Regularization: 2.399096\n",
      "2019-04-10 00:03:38,565 root         INFO     Train Epoch: 16 [1536/8000 (19%)]\tTotal Loss: 2.606743\n",
      "Reconstruction: 0.173909, Regularization: 2.432833\n",
      "2019-04-10 00:03:38,626 root         INFO     Train Epoch: 16 [2048/8000 (26%)]\tTotal Loss: 2.166886\n",
      "Reconstruction: 0.162209, Regularization: 2.004677\n",
      "2019-04-10 00:03:38,688 root         INFO     Train Epoch: 16 [2560/8000 (32%)]\tTotal Loss: 2.484356\n",
      "Reconstruction: 0.167297, Regularization: 2.317058\n",
      "2019-04-10 00:03:38,748 root         INFO     Train Epoch: 16 [3072/8000 (38%)]\tTotal Loss: 2.342482\n",
      "Reconstruction: 0.165707, Regularization: 2.176775\n",
      "2019-04-10 00:03:38,809 root         INFO     Train Epoch: 16 [3584/8000 (45%)]\tTotal Loss: 2.241723\n",
      "Reconstruction: 0.164433, Regularization: 2.077290\n",
      "2019-04-10 00:03:38,871 root         INFO     Train Epoch: 16 [4096/8000 (51%)]\tTotal Loss: 2.565436\n",
      "Reconstruction: 0.170887, Regularization: 2.394549\n",
      "2019-04-10 00:03:38,932 root         INFO     Train Epoch: 16 [4608/8000 (58%)]\tTotal Loss: 2.495549\n",
      "Reconstruction: 0.170193, Regularization: 2.325356\n",
      "2019-04-10 00:03:38,993 root         INFO     Train Epoch: 16 [5120/8000 (64%)]\tTotal Loss: 2.203113\n",
      "Reconstruction: 0.163424, Regularization: 2.039689\n",
      "2019-04-10 00:03:39,054 root         INFO     Train Epoch: 16 [5632/8000 (70%)]\tTotal Loss: 2.383110\n",
      "Reconstruction: 0.164012, Regularization: 2.219098\n",
      "2019-04-10 00:03:39,116 root         INFO     Train Epoch: 16 [6144/8000 (77%)]\tTotal Loss: 2.289811\n",
      "Reconstruction: 0.166043, Regularization: 2.123768\n",
      "2019-04-10 00:03:39,178 root         INFO     Train Epoch: 16 [6656/8000 (83%)]\tTotal Loss: 2.212123\n",
      "Reconstruction: 0.164094, Regularization: 2.048029\n",
      "2019-04-10 00:03:39,241 root         INFO     Train Epoch: 16 [7168/8000 (90%)]\tTotal Loss: 2.252984\n",
      "Reconstruction: 0.162383, Regularization: 2.090601\n",
      "2019-04-10 00:03:39,303 root         INFO     Train Epoch: 16 [7680/8000 (96%)]\tTotal Loss: 2.565740\n",
      "Reconstruction: 0.166124, Regularization: 2.399616\n",
      "2019-04-10 00:03:39,358 root         INFO     ====> Epoch: 16 Average loss: 2.4031\n",
      "2019-04-10 00:03:39,381 root         INFO     Train Epoch: 17 [0/8000 (0%)]\tTotal Loss: 2.335044\n",
      "Reconstruction: 0.164341, Regularization: 2.170703\n",
      "2019-04-10 00:03:39,445 root         INFO     Train Epoch: 17 [512/8000 (6%)]\tTotal Loss: 2.331160\n",
      "Reconstruction: 0.167039, Regularization: 2.164122\n",
      "2019-04-10 00:03:39,508 root         INFO     Train Epoch: 17 [1024/8000 (13%)]\tTotal Loss: 2.327500\n",
      "Reconstruction: 0.166957, Regularization: 2.160542\n",
      "2019-04-10 00:03:39,572 root         INFO     Train Epoch: 17 [1536/8000 (19%)]\tTotal Loss: 2.448433\n",
      "Reconstruction: 0.168126, Regularization: 2.280308\n",
      "2019-04-10 00:03:39,635 root         INFO     Train Epoch: 17 [2048/8000 (26%)]\tTotal Loss: 2.237478\n",
      "Reconstruction: 0.164434, Regularization: 2.073043\n",
      "2019-04-10 00:03:39,698 root         INFO     Train Epoch: 17 [2560/8000 (32%)]\tTotal Loss: 2.430910\n",
      "Reconstruction: 0.164286, Regularization: 2.266624\n",
      "2019-04-10 00:03:39,761 root         INFO     Train Epoch: 17 [3072/8000 (38%)]\tTotal Loss: 2.270259\n",
      "Reconstruction: 0.164535, Regularization: 2.105724\n",
      "2019-04-10 00:03:39,823 root         INFO     Train Epoch: 17 [3584/8000 (45%)]\tTotal Loss: 2.359345\n",
      "Reconstruction: 0.163935, Regularization: 2.195410\n",
      "2019-04-10 00:03:39,886 root         INFO     Train Epoch: 17 [4096/8000 (51%)]\tTotal Loss: 2.347569\n",
      "Reconstruction: 0.167177, Regularization: 2.180392\n",
      "2019-04-10 00:03:39,948 root         INFO     Train Epoch: 17 [4608/8000 (58%)]\tTotal Loss: 2.397731\n",
      "Reconstruction: 0.165082, Regularization: 2.232649\n",
      "2019-04-10 00:03:40,011 root         INFO     Train Epoch: 17 [5120/8000 (64%)]\tTotal Loss: 1.997556\n",
      "Reconstruction: 0.158184, Regularization: 1.839372\n",
      "2019-04-10 00:03:40,073 root         INFO     Train Epoch: 17 [5632/8000 (70%)]\tTotal Loss: 2.243755\n",
      "Reconstruction: 0.164225, Regularization: 2.079531\n",
      "2019-04-10 00:03:40,136 root         INFO     Train Epoch: 17 [6144/8000 (77%)]\tTotal Loss: 2.328535\n",
      "Reconstruction: 0.166398, Regularization: 2.162138\n",
      "2019-04-10 00:03:40,198 root         INFO     Train Epoch: 17 [6656/8000 (83%)]\tTotal Loss: 2.145356\n",
      "Reconstruction: 0.159024, Regularization: 1.986331\n",
      "2019-04-10 00:03:40,259 root         INFO     Train Epoch: 17 [7168/8000 (90%)]\tTotal Loss: 2.299774\n",
      "Reconstruction: 0.164141, Regularization: 2.135634\n",
      "2019-04-10 00:03:40,320 root         INFO     Train Epoch: 17 [7680/8000 (96%)]\tTotal Loss: 2.068033\n",
      "Reconstruction: 0.159474, Regularization: 1.908560\n",
      "2019-04-10 00:03:40,373 root         INFO     ====> Epoch: 17 Average loss: 2.2969\n",
      "2019-04-10 00:03:40,397 root         INFO     Train Epoch: 18 [0/8000 (0%)]\tTotal Loss: 2.669907\n",
      "Reconstruction: 0.168001, Regularization: 2.501906\n",
      "2019-04-10 00:03:40,461 root         INFO     Train Epoch: 18 [512/8000 (6%)]\tTotal Loss: 2.259298\n",
      "Reconstruction: 0.161926, Regularization: 2.097373\n",
      "2019-04-10 00:03:40,524 root         INFO     Train Epoch: 18 [1024/8000 (13%)]\tTotal Loss: 2.207354\n",
      "Reconstruction: 0.163413, Regularization: 2.043940\n",
      "2019-04-10 00:03:40,586 root         INFO     Train Epoch: 18 [1536/8000 (19%)]\tTotal Loss: 2.345888\n",
      "Reconstruction: 0.163762, Regularization: 2.182126\n",
      "2019-04-10 00:03:40,649 root         INFO     Train Epoch: 18 [2048/8000 (26%)]\tTotal Loss: 2.184651\n",
      "Reconstruction: 0.163410, Regularization: 2.021241\n",
      "2019-04-10 00:03:40,712 root         INFO     Train Epoch: 18 [2560/8000 (32%)]\tTotal Loss: 2.234370\n",
      "Reconstruction: 0.159731, Regularization: 2.074638\n",
      "2019-04-10 00:03:40,775 root         INFO     Train Epoch: 18 [3072/8000 (38%)]\tTotal Loss: 2.334802\n",
      "Reconstruction: 0.163694, Regularization: 2.171108\n",
      "2019-04-10 00:03:40,837 root         INFO     Train Epoch: 18 [3584/8000 (45%)]\tTotal Loss: 2.081482\n",
      "Reconstruction: 0.158034, Regularization: 1.923448\n",
      "2019-04-10 00:03:40,899 root         INFO     Train Epoch: 18 [4096/8000 (51%)]\tTotal Loss: 2.336065\n",
      "Reconstruction: 0.164439, Regularization: 2.171626\n",
      "2019-04-10 00:03:40,962 root         INFO     Train Epoch: 18 [4608/8000 (58%)]\tTotal Loss: 2.226423\n",
      "Reconstruction: 0.160129, Regularization: 2.066294\n",
      "2019-04-10 00:03:41,025 root         INFO     Train Epoch: 18 [5120/8000 (64%)]\tTotal Loss: 2.400164\n",
      "Reconstruction: 0.165677, Regularization: 2.234487\n",
      "2019-04-10 00:03:41,088 root         INFO     Train Epoch: 18 [5632/8000 (70%)]\tTotal Loss: 2.103610\n",
      "Reconstruction: 0.158829, Regularization: 1.944781\n",
      "2019-04-10 00:03:41,151 root         INFO     Train Epoch: 18 [6144/8000 (77%)]\tTotal Loss: 2.334334\n",
      "Reconstruction: 0.162315, Regularization: 2.172019\n",
      "2019-04-10 00:03:41,214 root         INFO     Train Epoch: 18 [6656/8000 (83%)]\tTotal Loss: 1.990864\n",
      "Reconstruction: 0.158735, Regularization: 1.832129\n",
      "2019-04-10 00:03:41,278 root         INFO     Train Epoch: 18 [7168/8000 (90%)]\tTotal Loss: 2.108792\n",
      "Reconstruction: 0.156480, Regularization: 1.952312\n",
      "2019-04-10 00:03:41,341 root         INFO     Train Epoch: 18 [7680/8000 (96%)]\tTotal Loss: 2.224875\n",
      "Reconstruction: 0.157422, Regularization: 2.067453\n",
      "2019-04-10 00:03:41,394 root         INFO     ====> Epoch: 18 Average loss: 2.1934\n",
      "2019-04-10 00:03:41,417 root         INFO     Train Epoch: 19 [0/8000 (0%)]\tTotal Loss: 1.897668\n",
      "Reconstruction: 0.157757, Regularization: 1.739911\n",
      "2019-04-10 00:03:41,481 root         INFO     Train Epoch: 19 [512/8000 (6%)]\tTotal Loss: 2.352510\n",
      "Reconstruction: 0.162804, Regularization: 2.189705\n",
      "2019-04-10 00:03:41,545 root         INFO     Train Epoch: 19 [1024/8000 (13%)]\tTotal Loss: 2.278218\n",
      "Reconstruction: 0.159803, Regularization: 2.118415\n",
      "2019-04-10 00:03:41,609 root         INFO     Train Epoch: 19 [1536/8000 (19%)]\tTotal Loss: 2.343796\n",
      "Reconstruction: 0.160445, Regularization: 2.183351\n",
      "2019-04-10 00:03:41,673 root         INFO     Train Epoch: 19 [2048/8000 (26%)]\tTotal Loss: 2.061750\n",
      "Reconstruction: 0.159822, Regularization: 1.901928\n",
      "2019-04-10 00:03:41,736 root         INFO     Train Epoch: 19 [2560/8000 (32%)]\tTotal Loss: 2.026595\n",
      "Reconstruction: 0.155175, Regularization: 1.871419\n",
      "2019-04-10 00:03:41,798 root         INFO     Train Epoch: 19 [3072/8000 (38%)]\tTotal Loss: 2.010054\n",
      "Reconstruction: 0.153422, Regularization: 1.856632\n",
      "2019-04-10 00:03:41,860 root         INFO     Train Epoch: 19 [3584/8000 (45%)]\tTotal Loss: 2.234494\n",
      "Reconstruction: 0.160147, Regularization: 2.074347\n",
      "2019-04-10 00:03:41,921 root         INFO     Train Epoch: 19 [4096/8000 (51%)]\tTotal Loss: 1.911829\n",
      "Reconstruction: 0.156705, Regularization: 1.755124\n",
      "2019-04-10 00:03:41,983 root         INFO     Train Epoch: 19 [4608/8000 (58%)]\tTotal Loss: 2.016245\n",
      "Reconstruction: 0.155300, Regularization: 1.860945\n",
      "2019-04-10 00:03:42,046 root         INFO     Train Epoch: 19 [5120/8000 (64%)]\tTotal Loss: 1.985668\n",
      "Reconstruction: 0.153845, Regularization: 1.831823\n",
      "2019-04-10 00:03:42,110 root         INFO     Train Epoch: 19 [5632/8000 (70%)]\tTotal Loss: 2.311771\n",
      "Reconstruction: 0.163171, Regularization: 2.148600\n",
      "2019-04-10 00:03:42,174 root         INFO     Train Epoch: 19 [6144/8000 (77%)]\tTotal Loss: 2.267567\n",
      "Reconstruction: 0.163190, Regularization: 2.104377\n",
      "2019-04-10 00:03:42,238 root         INFO     Train Epoch: 19 [6656/8000 (83%)]\tTotal Loss: 2.118108\n",
      "Reconstruction: 0.156572, Regularization: 1.961536\n",
      "2019-04-10 00:03:42,301 root         INFO     Train Epoch: 19 [7168/8000 (90%)]\tTotal Loss: 2.143110\n",
      "Reconstruction: 0.158068, Regularization: 1.985043\n",
      "2019-04-10 00:03:42,365 root         INFO     Train Epoch: 19 [7680/8000 (96%)]\tTotal Loss: 2.067297\n",
      "Reconstruction: 0.159995, Regularization: 1.907302\n",
      "2019-04-10 00:03:42,419 root         INFO     ====> Epoch: 19 Average loss: 2.0925\n",
      "2019-04-10 00:03:42,442 root         INFO     Train Epoch: 20 [0/8000 (0%)]\tTotal Loss: 2.050135\n",
      "Reconstruction: 0.158484, Regularization: 1.891650\n",
      "2019-04-10 00:03:42,506 root         INFO     Train Epoch: 20 [512/8000 (6%)]\tTotal Loss: 2.014113\n",
      "Reconstruction: 0.152174, Regularization: 1.861938\n",
      "2019-04-10 00:03:42,569 root         INFO     Train Epoch: 20 [1024/8000 (13%)]\tTotal Loss: 2.118037\n",
      "Reconstruction: 0.159192, Regularization: 1.958845\n",
      "2019-04-10 00:03:42,632 root         INFO     Train Epoch: 20 [1536/8000 (19%)]\tTotal Loss: 1.982582\n",
      "Reconstruction: 0.156944, Regularization: 1.825638\n",
      "2019-04-10 00:03:42,695 root         INFO     Train Epoch: 20 [2048/8000 (26%)]\tTotal Loss: 2.238423\n",
      "Reconstruction: 0.161668, Regularization: 2.076755\n",
      "2019-04-10 00:03:42,758 root         INFO     Train Epoch: 20 [2560/8000 (32%)]\tTotal Loss: 2.057379\n",
      "Reconstruction: 0.155084, Regularization: 1.902295\n",
      "2019-04-10 00:03:42,822 root         INFO     Train Epoch: 20 [3072/8000 (38%)]\tTotal Loss: 2.050904\n",
      "Reconstruction: 0.153300, Regularization: 1.897604\n",
      "2019-04-10 00:03:42,885 root         INFO     Train Epoch: 20 [3584/8000 (45%)]\tTotal Loss: 1.851041\n",
      "Reconstruction: 0.152412, Regularization: 1.698630\n",
      "2019-04-10 00:03:42,948 root         INFO     Train Epoch: 20 [4096/8000 (51%)]\tTotal Loss: 1.817265\n",
      "Reconstruction: 0.154041, Regularization: 1.663223\n",
      "2019-04-10 00:03:43,011 root         INFO     Train Epoch: 20 [4608/8000 (58%)]\tTotal Loss: 1.802064\n",
      "Reconstruction: 0.150503, Regularization: 1.651561\n",
      "2019-04-10 00:03:43,074 root         INFO     Train Epoch: 20 [5120/8000 (64%)]\tTotal Loss: 2.223943\n",
      "Reconstruction: 0.159805, Regularization: 2.064138\n",
      "2019-04-10 00:03:43,137 root         INFO     Train Epoch: 20 [5632/8000 (70%)]\tTotal Loss: 1.840923\n",
      "Reconstruction: 0.152149, Regularization: 1.688774\n",
      "2019-04-10 00:03:43,201 root         INFO     Train Epoch: 20 [6144/8000 (77%)]\tTotal Loss: 2.037025\n",
      "Reconstruction: 0.155860, Regularization: 1.881166\n",
      "2019-04-10 00:03:43,264 root         INFO     Train Epoch: 20 [6656/8000 (83%)]\tTotal Loss: 2.078711\n",
      "Reconstruction: 0.158513, Regularization: 1.920198\n",
      "2019-04-10 00:03:43,326 root         INFO     Train Epoch: 20 [7168/8000 (90%)]\tTotal Loss: 1.920989\n",
      "Reconstruction: 0.149151, Regularization: 1.771839\n",
      "2019-04-10 00:03:43,389 root         INFO     Train Epoch: 20 [7680/8000 (96%)]\tTotal Loss: 1.647378\n",
      "Reconstruction: 0.148290, Regularization: 1.499088\n",
      "2019-04-10 00:03:43,443 root         INFO     ====> Epoch: 20 Average loss: 1.9944\n",
      "2019-04-10 00:03:43,467 root         INFO     Train Epoch: 21 [0/8000 (0%)]\tTotal Loss: 1.861577\n",
      "Reconstruction: 0.151828, Regularization: 1.709750\n",
      "2019-04-10 00:03:43,531 root         INFO     Train Epoch: 21 [512/8000 (6%)]\tTotal Loss: 1.718322\n",
      "Reconstruction: 0.145397, Regularization: 1.572926\n",
      "2019-04-10 00:03:43,592 root         INFO     Train Epoch: 21 [1024/8000 (13%)]\tTotal Loss: 1.858531\n",
      "Reconstruction: 0.150792, Regularization: 1.707738\n",
      "2019-04-10 00:03:43,653 root         INFO     Train Epoch: 21 [1536/8000 (19%)]\tTotal Loss: 1.873568\n",
      "Reconstruction: 0.151190, Regularization: 1.722378\n",
      "2019-04-10 00:03:43,715 root         INFO     Train Epoch: 21 [2048/8000 (26%)]\tTotal Loss: 1.836579\n",
      "Reconstruction: 0.148788, Regularization: 1.687791\n",
      "2019-04-10 00:03:43,778 root         INFO     Train Epoch: 21 [2560/8000 (32%)]\tTotal Loss: 1.770424\n",
      "Reconstruction: 0.150980, Regularization: 1.619444\n",
      "2019-04-10 00:03:43,840 root         INFO     Train Epoch: 21 [3072/8000 (38%)]\tTotal Loss: 1.744789\n",
      "Reconstruction: 0.145988, Regularization: 1.598800\n",
      "2019-04-10 00:03:43,903 root         INFO     Train Epoch: 21 [3584/8000 (45%)]\tTotal Loss: 1.668699\n",
      "Reconstruction: 0.146567, Regularization: 1.522133\n",
      "2019-04-10 00:03:43,965 root         INFO     Train Epoch: 21 [4096/8000 (51%)]\tTotal Loss: 1.792889\n",
      "Reconstruction: 0.150528, Regularization: 1.642361\n",
      "2019-04-10 00:03:44,027 root         INFO     Train Epoch: 21 [4608/8000 (58%)]\tTotal Loss: 1.785082\n",
      "Reconstruction: 0.152194, Regularization: 1.632888\n",
      "2019-04-10 00:03:44,090 root         INFO     Train Epoch: 21 [5120/8000 (64%)]\tTotal Loss: 1.883320\n",
      "Reconstruction: 0.148940, Regularization: 1.734380\n",
      "2019-04-10 00:03:44,152 root         INFO     Train Epoch: 21 [5632/8000 (70%)]\tTotal Loss: 1.984143\n",
      "Reconstruction: 0.150023, Regularization: 1.834120\n",
      "2019-04-10 00:03:44,215 root         INFO     Train Epoch: 21 [6144/8000 (77%)]\tTotal Loss: 1.653522\n",
      "Reconstruction: 0.147822, Regularization: 1.505700\n",
      "2019-04-10 00:03:44,278 root         INFO     Train Epoch: 21 [6656/8000 (83%)]\tTotal Loss: 1.838019\n",
      "Reconstruction: 0.149303, Regularization: 1.688715\n",
      "2019-04-10 00:03:44,340 root         INFO     Train Epoch: 21 [7168/8000 (90%)]\tTotal Loss: 1.931828\n",
      "Reconstruction: 0.148217, Regularization: 1.783611\n",
      "2019-04-10 00:03:44,403 root         INFO     Train Epoch: 21 [7680/8000 (96%)]\tTotal Loss: 1.814396\n",
      "Reconstruction: 0.146760, Regularization: 1.667636\n",
      "2019-04-10 00:03:44,457 root         INFO     ====> Epoch: 21 Average loss: 1.8990\n",
      "2019-04-10 00:03:44,481 root         INFO     Train Epoch: 22 [0/8000 (0%)]\tTotal Loss: 1.873796\n",
      "Reconstruction: 0.150283, Regularization: 1.723513\n",
      "2019-04-10 00:03:44,545 root         INFO     Train Epoch: 22 [512/8000 (6%)]\tTotal Loss: 1.705992\n",
      "Reconstruction: 0.144322, Regularization: 1.561670\n",
      "2019-04-10 00:03:44,609 root         INFO     Train Epoch: 22 [1024/8000 (13%)]\tTotal Loss: 1.863798\n",
      "Reconstruction: 0.148965, Regularization: 1.714833\n",
      "2019-04-10 00:03:44,672 root         INFO     Train Epoch: 22 [1536/8000 (19%)]\tTotal Loss: 1.853758\n",
      "Reconstruction: 0.151211, Regularization: 1.702546\n",
      "2019-04-10 00:03:44,737 root         INFO     Train Epoch: 22 [2048/8000 (26%)]\tTotal Loss: 1.713664\n",
      "Reconstruction: 0.144327, Regularization: 1.569337\n",
      "2019-04-10 00:03:44,801 root         INFO     Train Epoch: 22 [2560/8000 (32%)]\tTotal Loss: 1.714183\n",
      "Reconstruction: 0.146984, Regularization: 1.567199\n",
      "2019-04-10 00:03:44,864 root         INFO     Train Epoch: 22 [3072/8000 (38%)]\tTotal Loss: 1.920345\n",
      "Reconstruction: 0.150897, Regularization: 1.769448\n",
      "2019-04-10 00:03:44,929 root         INFO     Train Epoch: 22 [3584/8000 (45%)]\tTotal Loss: 1.734066\n",
      "Reconstruction: 0.144725, Regularization: 1.589341\n",
      "2019-04-10 00:03:44,992 root         INFO     Train Epoch: 22 [4096/8000 (51%)]\tTotal Loss: 1.835281\n",
      "Reconstruction: 0.149421, Regularization: 1.685860\n",
      "2019-04-10 00:03:45,055 root         INFO     Train Epoch: 22 [4608/8000 (58%)]\tTotal Loss: 1.924928\n",
      "Reconstruction: 0.149943, Regularization: 1.774986\n",
      "2019-04-10 00:03:45,118 root         INFO     Train Epoch: 22 [5120/8000 (64%)]\tTotal Loss: 1.732892\n",
      "Reconstruction: 0.144270, Regularization: 1.588622\n",
      "2019-04-10 00:03:45,181 root         INFO     Train Epoch: 22 [5632/8000 (70%)]\tTotal Loss: 1.717659\n",
      "Reconstruction: 0.143151, Regularization: 1.574508\n",
      "2019-04-10 00:03:45,244 root         INFO     Train Epoch: 22 [6144/8000 (77%)]\tTotal Loss: 1.730667\n",
      "Reconstruction: 0.151852, Regularization: 1.578815\n",
      "2019-04-10 00:03:45,307 root         INFO     Train Epoch: 22 [6656/8000 (83%)]\tTotal Loss: 1.728469\n",
      "Reconstruction: 0.144414, Regularization: 1.584055\n",
      "2019-04-10 00:03:45,368 root         INFO     Train Epoch: 22 [7168/8000 (90%)]\tTotal Loss: 1.906396\n",
      "Reconstruction: 0.149168, Regularization: 1.757228\n",
      "2019-04-10 00:03:45,430 root         INFO     Train Epoch: 22 [7680/8000 (96%)]\tTotal Loss: 1.806012\n",
      "Reconstruction: 0.145553, Regularization: 1.660459\n",
      "2019-04-10 00:03:45,482 root         INFO     ====> Epoch: 22 Average loss: 1.8067\n",
      "2019-04-10 00:03:45,507 root         INFO     Train Epoch: 23 [0/8000 (0%)]\tTotal Loss: 1.542595\n",
      "Reconstruction: 0.138690, Regularization: 1.403905\n",
      "2019-04-10 00:03:45,570 root         INFO     Train Epoch: 23 [512/8000 (6%)]\tTotal Loss: 1.566162\n",
      "Reconstruction: 0.141676, Regularization: 1.424486\n",
      "2019-04-10 00:03:45,634 root         INFO     Train Epoch: 23 [1024/8000 (13%)]\tTotal Loss: 1.697701\n",
      "Reconstruction: 0.142989, Regularization: 1.554713\n",
      "2019-04-10 00:03:45,697 root         INFO     Train Epoch: 23 [1536/8000 (19%)]\tTotal Loss: 1.829232\n",
      "Reconstruction: 0.143290, Regularization: 1.685942\n",
      "2019-04-10 00:03:45,760 root         INFO     Train Epoch: 23 [2048/8000 (26%)]\tTotal Loss: 1.702696\n",
      "Reconstruction: 0.149254, Regularization: 1.553442\n",
      "2019-04-10 00:03:45,823 root         INFO     Train Epoch: 23 [2560/8000 (32%)]\tTotal Loss: 1.772405\n",
      "Reconstruction: 0.144401, Regularization: 1.628004\n",
      "2019-04-10 00:03:45,886 root         INFO     Train Epoch: 23 [3072/8000 (38%)]\tTotal Loss: 1.853975\n",
      "Reconstruction: 0.143931, Regularization: 1.710044\n",
      "2019-04-10 00:03:45,948 root         INFO     Train Epoch: 23 [3584/8000 (45%)]\tTotal Loss: 1.765058\n",
      "Reconstruction: 0.149465, Regularization: 1.615593\n",
      "2019-04-10 00:03:46,011 root         INFO     Train Epoch: 23 [4096/8000 (51%)]\tTotal Loss: 1.756733\n",
      "Reconstruction: 0.143009, Regularization: 1.613724\n",
      "2019-04-10 00:03:46,073 root         INFO     Train Epoch: 23 [4608/8000 (58%)]\tTotal Loss: 1.859974\n",
      "Reconstruction: 0.146992, Regularization: 1.712982\n",
      "2019-04-10 00:03:46,136 root         INFO     Train Epoch: 23 [5120/8000 (64%)]\tTotal Loss: 1.601061\n",
      "Reconstruction: 0.144417, Regularization: 1.456643\n",
      "2019-04-10 00:03:46,198 root         INFO     Train Epoch: 23 [5632/8000 (70%)]\tTotal Loss: 1.566546\n",
      "Reconstruction: 0.139057, Regularization: 1.427489\n",
      "2019-04-10 00:03:46,262 root         INFO     Train Epoch: 23 [6144/8000 (77%)]\tTotal Loss: 1.628271\n",
      "Reconstruction: 0.144452, Regularization: 1.483818\n",
      "2019-04-10 00:03:46,326 root         INFO     Train Epoch: 23 [6656/8000 (83%)]\tTotal Loss: 1.801019\n",
      "Reconstruction: 0.145406, Regularization: 1.655613\n",
      "2019-04-10 00:03:46,389 root         INFO     Train Epoch: 23 [7168/8000 (90%)]\tTotal Loss: 1.704204\n",
      "Reconstruction: 0.144142, Regularization: 1.560062\n",
      "2019-04-10 00:03:46,453 root         INFO     Train Epoch: 23 [7680/8000 (96%)]\tTotal Loss: 1.562808\n",
      "Reconstruction: 0.143898, Regularization: 1.418910\n",
      "2019-04-10 00:03:46,507 root         INFO     ====> Epoch: 23 Average loss: 1.7178\n",
      "2019-04-10 00:03:46,531 root         INFO     Train Epoch: 24 [0/8000 (0%)]\tTotal Loss: 1.539931\n",
      "Reconstruction: 0.141582, Regularization: 1.398349\n",
      "2019-04-10 00:03:46,593 root         INFO     Train Epoch: 24 [512/8000 (6%)]\tTotal Loss: 1.738427\n",
      "Reconstruction: 0.142666, Regularization: 1.595761\n",
      "2019-04-10 00:03:46,655 root         INFO     Train Epoch: 24 [1024/8000 (13%)]\tTotal Loss: 1.630220\n",
      "Reconstruction: 0.143297, Regularization: 1.486924\n",
      "2019-04-10 00:03:46,718 root         INFO     Train Epoch: 24 [1536/8000 (19%)]\tTotal Loss: 1.512211\n",
      "Reconstruction: 0.139107, Regularization: 1.373104\n",
      "2019-04-10 00:03:46,780 root         INFO     Train Epoch: 24 [2048/8000 (26%)]\tTotal Loss: 1.489201\n",
      "Reconstruction: 0.145286, Regularization: 1.343915\n",
      "2019-04-10 00:03:46,842 root         INFO     Train Epoch: 24 [2560/8000 (32%)]\tTotal Loss: 1.571561\n",
      "Reconstruction: 0.141975, Regularization: 1.429586\n",
      "2019-04-10 00:03:46,905 root         INFO     Train Epoch: 24 [3072/8000 (38%)]\tTotal Loss: 1.650618\n",
      "Reconstruction: 0.146733, Regularization: 1.503884\n",
      "2019-04-10 00:03:46,967 root         INFO     Train Epoch: 24 [3584/8000 (45%)]\tTotal Loss: 1.567106\n",
      "Reconstruction: 0.142319, Regularization: 1.424787\n",
      "2019-04-10 00:03:47,029 root         INFO     Train Epoch: 24 [4096/8000 (51%)]\tTotal Loss: 1.743658\n",
      "Reconstruction: 0.145738, Regularization: 1.597920\n",
      "2019-04-10 00:03:47,091 root         INFO     Train Epoch: 24 [4608/8000 (58%)]\tTotal Loss: 1.794995\n",
      "Reconstruction: 0.141167, Regularization: 1.653828\n",
      "2019-04-10 00:03:47,154 root         INFO     Train Epoch: 24 [5120/8000 (64%)]\tTotal Loss: 1.626500\n",
      "Reconstruction: 0.140684, Regularization: 1.485816\n",
      "2019-04-10 00:03:47,215 root         INFO     Train Epoch: 24 [5632/8000 (70%)]\tTotal Loss: 1.492346\n",
      "Reconstruction: 0.137834, Regularization: 1.354512\n",
      "2019-04-10 00:03:47,278 root         INFO     Train Epoch: 24 [6144/8000 (77%)]\tTotal Loss: 1.964297\n",
      "Reconstruction: 0.144681, Regularization: 1.819617\n",
      "2019-04-10 00:03:47,340 root         INFO     Train Epoch: 24 [6656/8000 (83%)]\tTotal Loss: 1.723950\n",
      "Reconstruction: 0.145683, Regularization: 1.578267\n",
      "2019-04-10 00:03:47,402 root         INFO     Train Epoch: 24 [7168/8000 (90%)]\tTotal Loss: 1.576660\n",
      "Reconstruction: 0.143216, Regularization: 1.433444\n",
      "2019-04-10 00:03:47,465 root         INFO     Train Epoch: 24 [7680/8000 (96%)]\tTotal Loss: 1.395955\n",
      "Reconstruction: 0.135426, Regularization: 1.260530\n",
      "2019-04-10 00:03:47,518 root         INFO     ====> Epoch: 24 Average loss: 1.6323\n",
      "2019-04-10 00:03:47,542 root         INFO     Train Epoch: 25 [0/8000 (0%)]\tTotal Loss: 1.425301\n",
      "Reconstruction: 0.135795, Regularization: 1.289506\n",
      "2019-04-10 00:03:47,606 root         INFO     Train Epoch: 25 [512/8000 (6%)]\tTotal Loss: 1.507375\n",
      "Reconstruction: 0.144455, Regularization: 1.362920\n",
      "2019-04-10 00:03:47,669 root         INFO     Train Epoch: 25 [1024/8000 (13%)]\tTotal Loss: 1.656844\n",
      "Reconstruction: 0.140888, Regularization: 1.515956\n",
      "2019-04-10 00:03:47,733 root         INFO     Train Epoch: 25 [1536/8000 (19%)]\tTotal Loss: 1.534726\n",
      "Reconstruction: 0.151483, Regularization: 1.383243\n",
      "2019-04-10 00:03:47,796 root         INFO     Train Epoch: 25 [2048/8000 (26%)]\tTotal Loss: 1.743585\n",
      "Reconstruction: 0.140606, Regularization: 1.602979\n",
      "2019-04-10 00:03:47,859 root         INFO     Train Epoch: 25 [2560/8000 (32%)]\tTotal Loss: 1.664144\n",
      "Reconstruction: 0.149602, Regularization: 1.514541\n",
      "2019-04-10 00:03:47,922 root         INFO     Train Epoch: 25 [3072/8000 (38%)]\tTotal Loss: 1.749183\n",
      "Reconstruction: 0.145238, Regularization: 1.603945\n",
      "2019-04-10 00:03:47,985 root         INFO     Train Epoch: 25 [3584/8000 (45%)]\tTotal Loss: 1.667340\n",
      "Reconstruction: 0.146856, Regularization: 1.520484\n",
      "2019-04-10 00:03:48,048 root         INFO     Train Epoch: 25 [4096/8000 (51%)]\tTotal Loss: 1.721471\n",
      "Reconstruction: 0.142982, Regularization: 1.578490\n",
      "2019-04-10 00:03:48,110 root         INFO     Train Epoch: 25 [4608/8000 (58%)]\tTotal Loss: 1.535937\n",
      "Reconstruction: 0.144621, Regularization: 1.391316\n",
      "2019-04-10 00:03:48,172 root         INFO     Train Epoch: 25 [5120/8000 (64%)]\tTotal Loss: 1.652309\n",
      "Reconstruction: 0.142845, Regularization: 1.509464\n",
      "2019-04-10 00:03:48,235 root         INFO     Train Epoch: 25 [5632/8000 (70%)]\tTotal Loss: 1.363313\n",
      "Reconstruction: 0.142474, Regularization: 1.220840\n",
      "2019-04-10 00:03:48,297 root         INFO     Train Epoch: 25 [6144/8000 (77%)]\tTotal Loss: 1.556748\n",
      "Reconstruction: 0.145590, Regularization: 1.411158\n",
      "2019-04-10 00:03:48,360 root         INFO     Train Epoch: 25 [6656/8000 (83%)]\tTotal Loss: 1.654714\n",
      "Reconstruction: 0.144146, Regularization: 1.510569\n",
      "2019-04-10 00:03:48,423 root         INFO     Train Epoch: 25 [7168/8000 (90%)]\tTotal Loss: 1.409796\n",
      "Reconstruction: 0.136593, Regularization: 1.273202\n",
      "2019-04-10 00:03:48,484 root         INFO     Train Epoch: 25 [7680/8000 (96%)]\tTotal Loss: 1.480928\n",
      "Reconstruction: 0.135457, Regularization: 1.345471\n",
      "2019-04-10 00:03:48,537 root         INFO     ====> Epoch: 25 Average loss: 1.5498\n",
      "2019-04-10 00:03:48,561 root         INFO     Train Epoch: 26 [0/8000 (0%)]\tTotal Loss: 1.677868\n",
      "Reconstruction: 0.143152, Regularization: 1.534716\n",
      "2019-04-10 00:03:48,625 root         INFO     Train Epoch: 26 [512/8000 (6%)]\tTotal Loss: 1.504004\n",
      "Reconstruction: 0.142749, Regularization: 1.361256\n",
      "2019-04-10 00:03:48,688 root         INFO     Train Epoch: 26 [1024/8000 (13%)]\tTotal Loss: 1.592988\n",
      "Reconstruction: 0.137450, Regularization: 1.455538\n",
      "2019-04-10 00:03:48,751 root         INFO     Train Epoch: 26 [1536/8000 (19%)]\tTotal Loss: 1.411344\n",
      "Reconstruction: 0.140952, Regularization: 1.270392\n",
      "2019-04-10 00:03:48,815 root         INFO     Train Epoch: 26 [2048/8000 (26%)]\tTotal Loss: 1.369209\n",
      "Reconstruction: 0.140867, Regularization: 1.228342\n",
      "2019-04-10 00:03:48,878 root         INFO     Train Epoch: 26 [2560/8000 (32%)]\tTotal Loss: 1.518004\n",
      "Reconstruction: 0.141572, Regularization: 1.376431\n",
      "2019-04-10 00:03:48,941 root         INFO     Train Epoch: 26 [3072/8000 (38%)]\tTotal Loss: 1.497148\n",
      "Reconstruction: 0.136941, Regularization: 1.360207\n",
      "2019-04-10 00:03:49,003 root         INFO     Train Epoch: 26 [3584/8000 (45%)]\tTotal Loss: 1.382872\n",
      "Reconstruction: 0.146724, Regularization: 1.236148\n",
      "2019-04-10 00:03:49,066 root         INFO     Train Epoch: 26 [4096/8000 (51%)]\tTotal Loss: 1.698464\n",
      "Reconstruction: 0.143059, Regularization: 1.555405\n",
      "2019-04-10 00:03:49,129 root         INFO     Train Epoch: 26 [4608/8000 (58%)]\tTotal Loss: 1.314735\n",
      "Reconstruction: 0.132181, Regularization: 1.182554\n",
      "2019-04-10 00:03:49,191 root         INFO     Train Epoch: 26 [5120/8000 (64%)]\tTotal Loss: 1.479043\n",
      "Reconstruction: 0.134526, Regularization: 1.344518\n",
      "2019-04-10 00:03:49,253 root         INFO     Train Epoch: 26 [5632/8000 (70%)]\tTotal Loss: 1.481503\n",
      "Reconstruction: 0.138949, Regularization: 1.342553\n",
      "2019-04-10 00:03:49,314 root         INFO     Train Epoch: 26 [6144/8000 (77%)]\tTotal Loss: 1.542883\n",
      "Reconstruction: 0.142184, Regularization: 1.400699\n",
      "2019-04-10 00:03:49,376 root         INFO     Train Epoch: 26 [6656/8000 (83%)]\tTotal Loss: 1.448428\n",
      "Reconstruction: 0.138438, Regularization: 1.309989\n",
      "2019-04-10 00:03:49,438 root         INFO     Train Epoch: 26 [7168/8000 (90%)]\tTotal Loss: 1.329636\n",
      "Reconstruction: 0.130421, Regularization: 1.199216\n",
      "2019-04-10 00:03:49,499 root         INFO     Train Epoch: 26 [7680/8000 (96%)]\tTotal Loss: 1.388830\n",
      "Reconstruction: 0.139915, Regularization: 1.248915\n",
      "2019-04-10 00:03:49,552 root         INFO     ====> Epoch: 26 Average loss: 1.4703\n",
      "2019-04-10 00:03:49,577 root         INFO     Train Epoch: 27 [0/8000 (0%)]\tTotal Loss: 1.401347\n",
      "Reconstruction: 0.138976, Regularization: 1.262370\n",
      "2019-04-10 00:03:49,640 root         INFO     Train Epoch: 27 [512/8000 (6%)]\tTotal Loss: 1.410782\n",
      "Reconstruction: 0.144377, Regularization: 1.266405\n",
      "2019-04-10 00:03:49,703 root         INFO     Train Epoch: 27 [1024/8000 (13%)]\tTotal Loss: 1.292074\n",
      "Reconstruction: 0.140614, Regularization: 1.151460\n",
      "2019-04-10 00:03:49,767 root         INFO     Train Epoch: 27 [1536/8000 (19%)]\tTotal Loss: 1.406062\n",
      "Reconstruction: 0.138162, Regularization: 1.267901\n",
      "2019-04-10 00:03:49,829 root         INFO     Train Epoch: 27 [2048/8000 (26%)]\tTotal Loss: 1.345531\n",
      "Reconstruction: 0.134857, Regularization: 1.210674\n",
      "2019-04-10 00:03:49,892 root         INFO     Train Epoch: 27 [2560/8000 (32%)]\tTotal Loss: 1.598799\n",
      "Reconstruction: 0.142209, Regularization: 1.456590\n",
      "2019-04-10 00:03:49,954 root         INFO     Train Epoch: 27 [3072/8000 (38%)]\tTotal Loss: 1.533181\n",
      "Reconstruction: 0.136988, Regularization: 1.396193\n",
      "2019-04-10 00:03:50,017 root         INFO     Train Epoch: 27 [3584/8000 (45%)]\tTotal Loss: 1.512752\n",
      "Reconstruction: 0.133084, Regularization: 1.379668\n",
      "2019-04-10 00:03:50,079 root         INFO     Train Epoch: 27 [4096/8000 (51%)]\tTotal Loss: 1.567951\n",
      "Reconstruction: 0.139868, Regularization: 1.428082\n",
      "2019-04-10 00:03:50,142 root         INFO     Train Epoch: 27 [4608/8000 (58%)]\tTotal Loss: 1.344660\n",
      "Reconstruction: 0.134182, Regularization: 1.210478\n",
      "2019-04-10 00:03:50,204 root         INFO     Train Epoch: 27 [5120/8000 (64%)]\tTotal Loss: 1.388953\n",
      "Reconstruction: 0.134748, Regularization: 1.254205\n",
      "2019-04-10 00:03:50,267 root         INFO     Train Epoch: 27 [5632/8000 (70%)]\tTotal Loss: 1.448242\n",
      "Reconstruction: 0.134812, Regularization: 1.313430\n",
      "2019-04-10 00:03:50,329 root         INFO     Train Epoch: 27 [6144/8000 (77%)]\tTotal Loss: 1.472919\n",
      "Reconstruction: 0.134491, Regularization: 1.338427\n",
      "2019-04-10 00:03:50,391 root         INFO     Train Epoch: 27 [6656/8000 (83%)]\tTotal Loss: 1.346407\n",
      "Reconstruction: 0.135648, Regularization: 1.210758\n",
      "2019-04-10 00:03:50,454 root         INFO     Train Epoch: 27 [7168/8000 (90%)]\tTotal Loss: 1.412058\n",
      "Reconstruction: 0.136418, Regularization: 1.275640\n",
      "2019-04-10 00:03:50,517 root         INFO     Train Epoch: 27 [7680/8000 (96%)]\tTotal Loss: 1.382929\n",
      "Reconstruction: 0.132668, Regularization: 1.250260\n",
      "2019-04-10 00:03:50,571 root         INFO     ====> Epoch: 27 Average loss: 1.3936\n",
      "2019-04-10 00:03:50,595 root         INFO     Train Epoch: 28 [0/8000 (0%)]\tTotal Loss: 1.297653\n",
      "Reconstruction: 0.134848, Regularization: 1.162805\n",
      "2019-04-10 00:03:50,659 root         INFO     Train Epoch: 28 [512/8000 (6%)]\tTotal Loss: 1.406018\n",
      "Reconstruction: 0.136534, Regularization: 1.269485\n",
      "2019-04-10 00:03:50,722 root         INFO     Train Epoch: 28 [1024/8000 (13%)]\tTotal Loss: 1.327329\n",
      "Reconstruction: 0.132437, Regularization: 1.194892\n",
      "2019-04-10 00:03:50,786 root         INFO     Train Epoch: 28 [1536/8000 (19%)]\tTotal Loss: 1.280980\n",
      "Reconstruction: 0.134279, Regularization: 1.146701\n",
      "2019-04-10 00:03:50,849 root         INFO     Train Epoch: 28 [2048/8000 (26%)]\tTotal Loss: 1.245004\n",
      "Reconstruction: 0.137403, Regularization: 1.107600\n",
      "2019-04-10 00:03:50,912 root         INFO     Train Epoch: 28 [2560/8000 (32%)]\tTotal Loss: 1.269380\n",
      "Reconstruction: 0.128461, Regularization: 1.140918\n",
      "2019-04-10 00:03:50,975 root         INFO     Train Epoch: 28 [3072/8000 (38%)]\tTotal Loss: 1.562382\n",
      "Reconstruction: 0.146015, Regularization: 1.416367\n",
      "2019-04-10 00:03:51,038 root         INFO     Train Epoch: 28 [3584/8000 (45%)]\tTotal Loss: 1.233729\n",
      "Reconstruction: 0.141934, Regularization: 1.091795\n",
      "2019-04-10 00:03:51,101 root         INFO     Train Epoch: 28 [4096/8000 (51%)]\tTotal Loss: 1.194786\n",
      "Reconstruction: 0.131247, Regularization: 1.063539\n",
      "2019-04-10 00:03:51,164 root         INFO     Train Epoch: 28 [4608/8000 (58%)]\tTotal Loss: 1.176119\n",
      "Reconstruction: 0.135919, Regularization: 1.040200\n",
      "2019-04-10 00:03:51,226 root         INFO     Train Epoch: 28 [5120/8000 (64%)]\tTotal Loss: 1.342002\n",
      "Reconstruction: 0.133071, Regularization: 1.208931\n",
      "2019-04-10 00:03:51,289 root         INFO     Train Epoch: 28 [5632/8000 (70%)]\tTotal Loss: 1.222766\n",
      "Reconstruction: 0.132394, Regularization: 1.090372\n",
      "2019-04-10 00:03:51,351 root         INFO     Train Epoch: 28 [6144/8000 (77%)]\tTotal Loss: 1.136601\n",
      "Reconstruction: 0.132043, Regularization: 1.004558\n",
      "2019-04-10 00:03:51,414 root         INFO     Train Epoch: 28 [6656/8000 (83%)]\tTotal Loss: 1.387338\n",
      "Reconstruction: 0.135028, Regularization: 1.252310\n",
      "2019-04-10 00:03:51,475 root         INFO     Train Epoch: 28 [7168/8000 (90%)]\tTotal Loss: 1.418572\n",
      "Reconstruction: 0.136960, Regularization: 1.281612\n",
      "2019-04-10 00:03:51,537 root         INFO     Train Epoch: 28 [7680/8000 (96%)]\tTotal Loss: 1.270276\n",
      "Reconstruction: 0.131687, Regularization: 1.138590\n",
      "2019-04-10 00:03:51,592 root         INFO     ====> Epoch: 28 Average loss: 1.3191\n",
      "2019-04-10 00:03:51,616 root         INFO     Train Epoch: 29 [0/8000 (0%)]\tTotal Loss: 1.152256\n",
      "Reconstruction: 0.134098, Regularization: 1.018158\n",
      "2019-04-10 00:03:51,678 root         INFO     Train Epoch: 29 [512/8000 (6%)]\tTotal Loss: 1.314930\n",
      "Reconstruction: 0.135564, Regularization: 1.179366\n",
      "2019-04-10 00:03:51,742 root         INFO     Train Epoch: 29 [1024/8000 (13%)]\tTotal Loss: 1.291780\n",
      "Reconstruction: 0.129374, Regularization: 1.162406\n",
      "2019-04-10 00:03:51,806 root         INFO     Train Epoch: 29 [1536/8000 (19%)]\tTotal Loss: 1.275095\n",
      "Reconstruction: 0.136851, Regularization: 1.138245\n",
      "2019-04-10 00:03:51,869 root         INFO     Train Epoch: 29 [2048/8000 (26%)]\tTotal Loss: 1.158085\n",
      "Reconstruction: 0.134650, Regularization: 1.023435\n",
      "2019-04-10 00:03:51,933 root         INFO     Train Epoch: 29 [2560/8000 (32%)]\tTotal Loss: 1.240927\n",
      "Reconstruction: 0.133895, Regularization: 1.107032\n",
      "2019-04-10 00:03:51,997 root         INFO     Train Epoch: 29 [3072/8000 (38%)]\tTotal Loss: 1.206621\n",
      "Reconstruction: 0.130590, Regularization: 1.076031\n",
      "2019-04-10 00:03:52,061 root         INFO     Train Epoch: 29 [3584/8000 (45%)]\tTotal Loss: 1.122490\n",
      "Reconstruction: 0.136354, Regularization: 0.986136\n",
      "2019-04-10 00:03:52,125 root         INFO     Train Epoch: 29 [4096/8000 (51%)]\tTotal Loss: 1.221080\n",
      "Reconstruction: 0.131639, Regularization: 1.089441\n",
      "2019-04-10 00:03:52,189 root         INFO     Train Epoch: 29 [4608/8000 (58%)]\tTotal Loss: 1.156377\n",
      "Reconstruction: 0.133954, Regularization: 1.022424\n",
      "2019-04-10 00:03:52,253 root         INFO     Train Epoch: 29 [5120/8000 (64%)]\tTotal Loss: 1.139492\n",
      "Reconstruction: 0.128765, Regularization: 1.010727\n",
      "2019-04-10 00:03:52,317 root         INFO     Train Epoch: 29 [5632/8000 (70%)]\tTotal Loss: 1.260116\n",
      "Reconstruction: 0.132650, Regularization: 1.127466\n",
      "2019-04-10 00:03:52,381 root         INFO     Train Epoch: 29 [6144/8000 (77%)]\tTotal Loss: 1.192151\n",
      "Reconstruction: 0.135890, Regularization: 1.056261\n",
      "2019-04-10 00:03:52,445 root         INFO     Train Epoch: 29 [6656/8000 (83%)]\tTotal Loss: 1.091798\n",
      "Reconstruction: 0.126520, Regularization: 0.965278\n",
      "2019-04-10 00:03:52,509 root         INFO     Train Epoch: 29 [7168/8000 (90%)]\tTotal Loss: 1.374970\n",
      "Reconstruction: 0.128291, Regularization: 1.246679\n",
      "2019-04-10 00:03:52,573 root         INFO     Train Epoch: 29 [7680/8000 (96%)]\tTotal Loss: 1.339550\n",
      "Reconstruction: 0.132640, Regularization: 1.206911\n",
      "2019-04-10 00:03:52,628 root         INFO     ====> Epoch: 29 Average loss: 1.2465\n",
      "2019-04-10 00:03:52,652 root         INFO     Train Epoch: 30 [0/8000 (0%)]\tTotal Loss: 1.139516\n",
      "Reconstruction: 0.135921, Regularization: 1.003595\n",
      "2019-04-10 00:03:52,716 root         INFO     Train Epoch: 30 [512/8000 (6%)]\tTotal Loss: 1.152528\n",
      "Reconstruction: 0.127902, Regularization: 1.024626\n",
      "2019-04-10 00:03:52,778 root         INFO     Train Epoch: 30 [1024/8000 (13%)]\tTotal Loss: 1.254949\n",
      "Reconstruction: 0.130717, Regularization: 1.124233\n",
      "2019-04-10 00:03:52,841 root         INFO     Train Epoch: 30 [1536/8000 (19%)]\tTotal Loss: 1.239833\n",
      "Reconstruction: 0.132733, Regularization: 1.107100\n",
      "2019-04-10 00:03:52,903 root         INFO     Train Epoch: 30 [2048/8000 (26%)]\tTotal Loss: 1.264592\n",
      "Reconstruction: 0.137454, Regularization: 1.127138\n",
      "2019-04-10 00:03:52,966 root         INFO     Train Epoch: 30 [2560/8000 (32%)]\tTotal Loss: 1.283160\n",
      "Reconstruction: 0.128793, Regularization: 1.154367\n",
      "2019-04-10 00:03:53,029 root         INFO     Train Epoch: 30 [3072/8000 (38%)]\tTotal Loss: 1.123855\n",
      "Reconstruction: 0.123251, Regularization: 1.000604\n",
      "2019-04-10 00:03:53,091 root         INFO     Train Epoch: 30 [3584/8000 (45%)]\tTotal Loss: 1.208489\n",
      "Reconstruction: 0.133626, Regularization: 1.074863\n",
      "2019-04-10 00:03:53,154 root         INFO     Train Epoch: 30 [4096/8000 (51%)]\tTotal Loss: 1.085456\n",
      "Reconstruction: 0.126733, Regularization: 0.958723\n",
      "2019-04-10 00:03:53,216 root         INFO     Train Epoch: 30 [4608/8000 (58%)]\tTotal Loss: 1.028856\n",
      "Reconstruction: 0.126081, Regularization: 0.902775\n",
      "2019-04-10 00:03:53,279 root         INFO     Train Epoch: 30 [5120/8000 (64%)]\tTotal Loss: 1.089333\n",
      "Reconstruction: 0.121568, Regularization: 0.967765\n",
      "2019-04-10 00:03:53,341 root         INFO     Train Epoch: 30 [5632/8000 (70%)]\tTotal Loss: 1.194069\n",
      "Reconstruction: 0.126777, Regularization: 1.067292\n",
      "2019-04-10 00:03:53,404 root         INFO     Train Epoch: 30 [6144/8000 (77%)]\tTotal Loss: 1.156080\n",
      "Reconstruction: 0.125733, Regularization: 1.030347\n",
      "2019-04-10 00:03:53,467 root         INFO     Train Epoch: 30 [6656/8000 (83%)]\tTotal Loss: 1.024078\n",
      "Reconstruction: 0.127824, Regularization: 0.896254\n",
      "2019-04-10 00:03:53,530 root         INFO     Train Epoch: 30 [7168/8000 (90%)]\tTotal Loss: 1.246688\n",
      "Reconstruction: 0.128160, Regularization: 1.118528\n",
      "2019-04-10 00:03:53,593 root         INFO     Train Epoch: 30 [7680/8000 (96%)]\tTotal Loss: 1.039130\n",
      "Reconstruction: 0.124032, Regularization: 0.915097\n",
      "2019-04-10 00:03:53,646 root         INFO     ====> Epoch: 30 Average loss: 1.1756\n",
      "2019-04-10 00:03:53,670 root         INFO     Train Epoch: 31 [0/8000 (0%)]\tTotal Loss: 1.143762\n",
      "Reconstruction: 0.124212, Regularization: 1.019551\n",
      "2019-04-10 00:03:53,734 root         INFO     Train Epoch: 31 [512/8000 (6%)]\tTotal Loss: 1.307803\n",
      "Reconstruction: 0.131650, Regularization: 1.176152\n",
      "2019-04-10 00:03:53,798 root         INFO     Train Epoch: 31 [1024/8000 (13%)]\tTotal Loss: 1.140864\n",
      "Reconstruction: 0.126558, Regularization: 1.014306\n",
      "2019-04-10 00:03:53,862 root         INFO     Train Epoch: 31 [1536/8000 (19%)]\tTotal Loss: 1.134749\n",
      "Reconstruction: 0.124582, Regularization: 1.010167\n",
      "2019-04-10 00:03:53,926 root         INFO     Train Epoch: 31 [2048/8000 (26%)]\tTotal Loss: 1.139111\n",
      "Reconstruction: 0.127101, Regularization: 1.012011\n",
      "2019-04-10 00:03:53,989 root         INFO     Train Epoch: 31 [2560/8000 (32%)]\tTotal Loss: 1.155884\n",
      "Reconstruction: 0.121405, Regularization: 1.034478\n",
      "2019-04-10 00:03:54,053 root         INFO     Train Epoch: 31 [3072/8000 (38%)]\tTotal Loss: 1.069542\n",
      "Reconstruction: 0.123017, Regularization: 0.946526\n",
      "2019-04-10 00:03:54,116 root         INFO     Train Epoch: 31 [3584/8000 (45%)]\tTotal Loss: 1.108812\n",
      "Reconstruction: 0.127415, Regularization: 0.981397\n",
      "2019-04-10 00:03:54,180 root         INFO     Train Epoch: 31 [4096/8000 (51%)]\tTotal Loss: 1.042929\n",
      "Reconstruction: 0.119074, Regularization: 0.923856\n",
      "2019-04-10 00:03:54,243 root         INFO     Train Epoch: 31 [4608/8000 (58%)]\tTotal Loss: 1.021172\n",
      "Reconstruction: 0.119644, Regularization: 0.901528\n",
      "2019-04-10 00:03:54,308 root         INFO     Train Epoch: 31 [5120/8000 (64%)]\tTotal Loss: 0.964966\n",
      "Reconstruction: 0.118250, Regularization: 0.846717\n",
      "2019-04-10 00:03:54,371 root         INFO     Train Epoch: 31 [5632/8000 (70%)]\tTotal Loss: 1.121644\n",
      "Reconstruction: 0.124875, Regularization: 0.996769\n",
      "2019-04-10 00:03:54,435 root         INFO     Train Epoch: 31 [6144/8000 (77%)]\tTotal Loss: 1.271088\n",
      "Reconstruction: 0.128843, Regularization: 1.142245\n",
      "2019-04-10 00:03:54,499 root         INFO     Train Epoch: 31 [6656/8000 (83%)]\tTotal Loss: 1.166535\n",
      "Reconstruction: 0.119829, Regularization: 1.046706\n",
      "2019-04-10 00:03:54,563 root         INFO     Train Epoch: 31 [7168/8000 (90%)]\tTotal Loss: 1.127787\n",
      "Reconstruction: 0.123312, Regularization: 1.004475\n",
      "2019-04-10 00:03:54,627 root         INFO     Train Epoch: 31 [7680/8000 (96%)]\tTotal Loss: 1.153303\n",
      "Reconstruction: 0.126556, Regularization: 1.026747\n",
      "2019-04-10 00:03:54,681 root         INFO     ====> Epoch: 31 Average loss: 1.1058\n",
      "2019-04-10 00:03:54,706 root         INFO     Train Epoch: 32 [0/8000 (0%)]\tTotal Loss: 1.063226\n",
      "Reconstruction: 0.120598, Regularization: 0.942628\n",
      "2019-04-10 00:03:54,770 root         INFO     Train Epoch: 32 [512/8000 (6%)]\tTotal Loss: 1.133001\n",
      "Reconstruction: 0.126804, Regularization: 1.006197\n",
      "2019-04-10 00:03:54,833 root         INFO     Train Epoch: 32 [1024/8000 (13%)]\tTotal Loss: 1.187154\n",
      "Reconstruction: 0.119259, Regularization: 1.067896\n",
      "2019-04-10 00:03:54,897 root         INFO     Train Epoch: 32 [1536/8000 (19%)]\tTotal Loss: 0.904057\n",
      "Reconstruction: 0.118318, Regularization: 0.785739\n",
      "2019-04-10 00:03:54,960 root         INFO     Train Epoch: 32 [2048/8000 (26%)]\tTotal Loss: 0.978261\n",
      "Reconstruction: 0.116059, Regularization: 0.862202\n",
      "2019-04-10 00:03:55,024 root         INFO     Train Epoch: 32 [2560/8000 (32%)]\tTotal Loss: 1.062591\n",
      "Reconstruction: 0.126984, Regularization: 0.935607\n",
      "2019-04-10 00:03:55,088 root         INFO     Train Epoch: 32 [3072/8000 (38%)]\tTotal Loss: 1.218339\n",
      "Reconstruction: 0.118003, Regularization: 1.100336\n",
      "2019-04-10 00:03:55,151 root         INFO     Train Epoch: 32 [3584/8000 (45%)]\tTotal Loss: 1.152322\n",
      "Reconstruction: 0.117814, Regularization: 1.034508\n",
      "2019-04-10 00:03:55,215 root         INFO     Train Epoch: 32 [4096/8000 (51%)]\tTotal Loss: 1.064570\n",
      "Reconstruction: 0.118697, Regularization: 0.945873\n",
      "2019-04-10 00:03:55,278 root         INFO     Train Epoch: 32 [4608/8000 (58%)]\tTotal Loss: 0.981092\n",
      "Reconstruction: 0.116097, Regularization: 0.864995\n",
      "2019-04-10 00:03:55,342 root         INFO     Train Epoch: 32 [5120/8000 (64%)]\tTotal Loss: 1.017528\n",
      "Reconstruction: 0.114841, Regularization: 0.902686\n",
      "2019-04-10 00:03:55,406 root         INFO     Train Epoch: 32 [5632/8000 (70%)]\tTotal Loss: 1.049848\n",
      "Reconstruction: 0.118839, Regularization: 0.931009\n",
      "2019-04-10 00:03:55,469 root         INFO     Train Epoch: 32 [6144/8000 (77%)]\tTotal Loss: 1.000071\n",
      "Reconstruction: 0.110548, Regularization: 0.889523\n",
      "2019-04-10 00:03:55,533 root         INFO     Train Epoch: 32 [6656/8000 (83%)]\tTotal Loss: 1.001229\n",
      "Reconstruction: 0.117723, Regularization: 0.883506\n",
      "2019-04-10 00:03:55,596 root         INFO     Train Epoch: 32 [7168/8000 (90%)]\tTotal Loss: 0.963854\n",
      "Reconstruction: 0.108253, Regularization: 0.855602\n",
      "2019-04-10 00:03:55,659 root         INFO     Train Epoch: 32 [7680/8000 (96%)]\tTotal Loss: 1.046261\n",
      "Reconstruction: 0.115410, Regularization: 0.930851\n",
      "2019-04-10 00:03:55,712 root         INFO     ====> Epoch: 32 Average loss: 1.0368\n",
      "2019-04-10 00:03:55,736 root         INFO     Train Epoch: 33 [0/8000 (0%)]\tTotal Loss: 0.997248\n",
      "Reconstruction: 0.110230, Regularization: 0.887017\n",
      "2019-04-10 00:03:55,800 root         INFO     Train Epoch: 33 [512/8000 (6%)]\tTotal Loss: 0.965433\n",
      "Reconstruction: 0.110767, Regularization: 0.854666\n",
      "2019-04-10 00:03:55,864 root         INFO     Train Epoch: 33 [1024/8000 (13%)]\tTotal Loss: 0.960929\n",
      "Reconstruction: 0.110659, Regularization: 0.850270\n",
      "2019-04-10 00:03:55,928 root         INFO     Train Epoch: 33 [1536/8000 (19%)]\tTotal Loss: 1.088402\n",
      "Reconstruction: 0.114748, Regularization: 0.973655\n",
      "2019-04-10 00:03:55,991 root         INFO     Train Epoch: 33 [2048/8000 (26%)]\tTotal Loss: 0.974737\n",
      "Reconstruction: 0.116349, Regularization: 0.858388\n",
      "2019-04-10 00:03:56,055 root         INFO     Train Epoch: 33 [2560/8000 (32%)]\tTotal Loss: 0.906339\n",
      "Reconstruction: 0.112480, Regularization: 0.793859\n",
      "2019-04-10 00:03:56,118 root         INFO     Train Epoch: 33 [3072/8000 (38%)]\tTotal Loss: 1.000077\n",
      "Reconstruction: 0.116391, Regularization: 0.883686\n",
      "2019-04-10 00:03:56,181 root         INFO     Train Epoch: 33 [3584/8000 (45%)]\tTotal Loss: 0.881457\n",
      "Reconstruction: 0.108393, Regularization: 0.773064\n",
      "2019-04-10 00:03:56,244 root         INFO     Train Epoch: 33 [4096/8000 (51%)]\tTotal Loss: 0.929472\n",
      "Reconstruction: 0.107624, Regularization: 0.821848\n",
      "2019-04-10 00:03:56,310 root         INFO     Train Epoch: 33 [4608/8000 (58%)]\tTotal Loss: 0.976689\n",
      "Reconstruction: 0.109198, Regularization: 0.867490\n",
      "2019-04-10 00:03:56,373 root         INFO     Train Epoch: 33 [5120/8000 (64%)]\tTotal Loss: 1.100125\n",
      "Reconstruction: 0.112788, Regularization: 0.987337\n",
      "2019-04-10 00:03:56,436 root         INFO     Train Epoch: 33 [5632/8000 (70%)]\tTotal Loss: 1.015733\n",
      "Reconstruction: 0.108507, Regularization: 0.907226\n",
      "2019-04-10 00:03:56,498 root         INFO     Train Epoch: 33 [6144/8000 (77%)]\tTotal Loss: 0.891057\n",
      "Reconstruction: 0.100881, Regularization: 0.790175\n",
      "2019-04-10 00:03:56,561 root         INFO     Train Epoch: 33 [6656/8000 (83%)]\tTotal Loss: 1.035579\n",
      "Reconstruction: 0.100981, Regularization: 0.934598\n",
      "2019-04-10 00:03:56,623 root         INFO     Train Epoch: 33 [7168/8000 (90%)]\tTotal Loss: 0.908151\n",
      "Reconstruction: 0.105289, Regularization: 0.802862\n",
      "2019-04-10 00:03:56,687 root         INFO     Train Epoch: 33 [7680/8000 (96%)]\tTotal Loss: 0.849173\n",
      "Reconstruction: 0.107231, Regularization: 0.741943\n",
      "2019-04-10 00:03:56,740 root         INFO     ====> Epoch: 33 Average loss: 0.9684\n",
      "2019-04-10 00:03:56,764 root         INFO     Train Epoch: 34 [0/8000 (0%)]\tTotal Loss: 0.910686\n",
      "Reconstruction: 0.105658, Regularization: 0.805028\n",
      "2019-04-10 00:03:56,828 root         INFO     Train Epoch: 34 [512/8000 (6%)]\tTotal Loss: 1.009857\n",
      "Reconstruction: 0.101607, Regularization: 0.908250\n",
      "2019-04-10 00:03:56,891 root         INFO     Train Epoch: 34 [1024/8000 (13%)]\tTotal Loss: 0.913838\n",
      "Reconstruction: 0.099960, Regularization: 0.813878\n",
      "2019-04-10 00:03:56,955 root         INFO     Train Epoch: 34 [1536/8000 (19%)]\tTotal Loss: 0.921438\n",
      "Reconstruction: 0.102594, Regularization: 0.818843\n",
      "2019-04-10 00:03:57,018 root         INFO     Train Epoch: 34 [2048/8000 (26%)]\tTotal Loss: 0.810338\n",
      "Reconstruction: 0.104440, Regularization: 0.705897\n",
      "2019-04-10 00:03:57,082 root         INFO     Train Epoch: 34 [2560/8000 (32%)]\tTotal Loss: 0.914069\n",
      "Reconstruction: 0.106064, Regularization: 0.808006\n",
      "2019-04-10 00:03:57,144 root         INFO     Train Epoch: 34 [3072/8000 (38%)]\tTotal Loss: 0.913424\n",
      "Reconstruction: 0.105594, Regularization: 0.807829\n",
      "2019-04-10 00:03:57,206 root         INFO     Train Epoch: 34 [3584/8000 (45%)]\tTotal Loss: 0.805688\n",
      "Reconstruction: 0.103322, Regularization: 0.702367\n",
      "2019-04-10 00:03:57,268 root         INFO     Train Epoch: 34 [4096/8000 (51%)]\tTotal Loss: 0.863036\n",
      "Reconstruction: 0.108503, Regularization: 0.754533\n",
      "2019-04-10 00:03:57,330 root         INFO     Train Epoch: 34 [4608/8000 (58%)]\tTotal Loss: 0.806866\n",
      "Reconstruction: 0.097647, Regularization: 0.709219\n",
      "2019-04-10 00:03:57,393 root         INFO     Train Epoch: 34 [5120/8000 (64%)]\tTotal Loss: 0.908165\n",
      "Reconstruction: 0.094905, Regularization: 0.813260\n",
      "2019-04-10 00:03:57,455 root         INFO     Train Epoch: 34 [5632/8000 (70%)]\tTotal Loss: 0.963981\n",
      "Reconstruction: 0.109023, Regularization: 0.854958\n",
      "2019-04-10 00:03:57,517 root         INFO     Train Epoch: 34 [6144/8000 (77%)]\tTotal Loss: 0.902171\n",
      "Reconstruction: 0.102299, Regularization: 0.799872\n",
      "2019-04-10 00:03:57,579 root         INFO     Train Epoch: 34 [6656/8000 (83%)]\tTotal Loss: 0.847456\n",
      "Reconstruction: 0.100452, Regularization: 0.747004\n",
      "2019-04-10 00:03:57,641 root         INFO     Train Epoch: 34 [7168/8000 (90%)]\tTotal Loss: 0.920074\n",
      "Reconstruction: 0.102003, Regularization: 0.818071\n",
      "2019-04-10 00:03:57,703 root         INFO     Train Epoch: 34 [7680/8000 (96%)]\tTotal Loss: 0.947606\n",
      "Reconstruction: 0.097651, Regularization: 0.849955\n",
      "2019-04-10 00:03:57,756 root         INFO     ====> Epoch: 34 Average loss: 0.9020\n",
      "2019-04-10 00:03:57,780 root         INFO     Train Epoch: 35 [0/8000 (0%)]\tTotal Loss: 0.828857\n",
      "Reconstruction: 0.099419, Regularization: 0.729438\n",
      "2019-04-10 00:03:57,845 root         INFO     Train Epoch: 35 [512/8000 (6%)]\tTotal Loss: 0.789798\n",
      "Reconstruction: 0.098335, Regularization: 0.691463\n",
      "2019-04-10 00:03:57,909 root         INFO     Train Epoch: 35 [1024/8000 (13%)]\tTotal Loss: 1.003849\n",
      "Reconstruction: 0.091907, Regularization: 0.911942\n",
      "2019-04-10 00:03:57,973 root         INFO     Train Epoch: 35 [1536/8000 (19%)]\tTotal Loss: 0.902589\n",
      "Reconstruction: 0.098675, Regularization: 0.803914\n",
      "2019-04-10 00:03:58,038 root         INFO     Train Epoch: 35 [2048/8000 (26%)]\tTotal Loss: 0.850682\n",
      "Reconstruction: 0.093935, Regularization: 0.756747\n",
      "2019-04-10 00:03:58,102 root         INFO     Train Epoch: 35 [2560/8000 (32%)]\tTotal Loss: 0.962192\n",
      "Reconstruction: 0.098257, Regularization: 0.863934\n",
      "2019-04-10 00:03:58,167 root         INFO     Train Epoch: 35 [3072/8000 (38%)]\tTotal Loss: 0.798110\n",
      "Reconstruction: 0.092473, Regularization: 0.705637\n",
      "2019-04-10 00:03:58,232 root         INFO     Train Epoch: 35 [3584/8000 (45%)]\tTotal Loss: 0.823589\n",
      "Reconstruction: 0.098552, Regularization: 0.725037\n",
      "2019-04-10 00:03:58,296 root         INFO     Train Epoch: 35 [4096/8000 (51%)]\tTotal Loss: 0.848107\n",
      "Reconstruction: 0.094217, Regularization: 0.753890\n",
      "2019-04-10 00:03:58,360 root         INFO     Train Epoch: 35 [4608/8000 (58%)]\tTotal Loss: 0.843857\n",
      "Reconstruction: 0.096278, Regularization: 0.747579\n",
      "2019-04-10 00:03:58,424 root         INFO     Train Epoch: 35 [5120/8000 (64%)]\tTotal Loss: 0.832466\n",
      "Reconstruction: 0.092472, Regularization: 0.739994\n",
      "2019-04-10 00:03:58,488 root         INFO     Train Epoch: 35 [5632/8000 (70%)]\tTotal Loss: 0.741872\n",
      "Reconstruction: 0.093512, Regularization: 0.648360\n",
      "2019-04-10 00:03:58,552 root         INFO     Train Epoch: 35 [6144/8000 (77%)]\tTotal Loss: 0.847636\n",
      "Reconstruction: 0.090579, Regularization: 0.757056\n",
      "2019-04-10 00:03:58,616 root         INFO     Train Epoch: 35 [6656/8000 (83%)]\tTotal Loss: 0.819833\n",
      "Reconstruction: 0.096439, Regularization: 0.723394\n",
      "2019-04-10 00:03:58,680 root         INFO     Train Epoch: 35 [7168/8000 (90%)]\tTotal Loss: 0.712809\n",
      "Reconstruction: 0.090867, Regularization: 0.621943\n",
      "2019-04-10 00:03:58,745 root         INFO     Train Epoch: 35 [7680/8000 (96%)]\tTotal Loss: 0.756385\n",
      "Reconstruction: 0.090162, Regularization: 0.666223\n",
      "2019-04-10 00:03:58,800 root         INFO     ====> Epoch: 35 Average loss: 0.8411\n",
      "2019-04-10 00:03:58,824 root         INFO     Train Epoch: 36 [0/8000 (0%)]\tTotal Loss: 0.753879\n",
      "Reconstruction: 0.091173, Regularization: 0.662706\n",
      "2019-04-10 00:03:58,887 root         INFO     Train Epoch: 36 [512/8000 (6%)]\tTotal Loss: 0.859325\n",
      "Reconstruction: 0.090741, Regularization: 0.768584\n",
      "2019-04-10 00:03:58,949 root         INFO     Train Epoch: 36 [1024/8000 (13%)]\tTotal Loss: 0.713971\n",
      "Reconstruction: 0.092456, Regularization: 0.621515\n",
      "2019-04-10 00:03:59,011 root         INFO     Train Epoch: 36 [1536/8000 (19%)]\tTotal Loss: 0.768191\n",
      "Reconstruction: 0.090476, Regularization: 0.677715\n",
      "2019-04-10 00:03:59,073 root         INFO     Train Epoch: 36 [2048/8000 (26%)]\tTotal Loss: 0.830941\n",
      "Reconstruction: 0.093269, Regularization: 0.737671\n",
      "2019-04-10 00:03:59,135 root         INFO     Train Epoch: 36 [2560/8000 (32%)]\tTotal Loss: 0.686183\n",
      "Reconstruction: 0.085129, Regularization: 0.601055\n",
      "2019-04-10 00:03:59,197 root         INFO     Train Epoch: 36 [3072/8000 (38%)]\tTotal Loss: 0.721727\n",
      "Reconstruction: 0.088566, Regularization: 0.633161\n",
      "2019-04-10 00:03:59,259 root         INFO     Train Epoch: 36 [3584/8000 (45%)]\tTotal Loss: 0.783734\n",
      "Reconstruction: 0.089783, Regularization: 0.693951\n",
      "2019-04-10 00:03:59,321 root         INFO     Train Epoch: 36 [4096/8000 (51%)]\tTotal Loss: 0.754489\n",
      "Reconstruction: 0.094318, Regularization: 0.660171\n",
      "2019-04-10 00:03:59,384 root         INFO     Train Epoch: 36 [4608/8000 (58%)]\tTotal Loss: 0.909064\n",
      "Reconstruction: 0.087228, Regularization: 0.821835\n",
      "2019-04-10 00:03:59,446 root         INFO     Train Epoch: 36 [5120/8000 (64%)]\tTotal Loss: 0.774827\n",
      "Reconstruction: 0.088870, Regularization: 0.685957\n",
      "2019-04-10 00:03:59,508 root         INFO     Train Epoch: 36 [5632/8000 (70%)]\tTotal Loss: 0.682178\n",
      "Reconstruction: 0.093102, Regularization: 0.589076\n",
      "2019-04-10 00:03:59,570 root         INFO     Train Epoch: 36 [6144/8000 (77%)]\tTotal Loss: 0.817870\n",
      "Reconstruction: 0.092833, Regularization: 0.725037\n",
      "2019-04-10 00:03:59,632 root         INFO     Train Epoch: 36 [6656/8000 (83%)]\tTotal Loss: 0.746619\n",
      "Reconstruction: 0.093889, Regularization: 0.652731\n",
      "2019-04-10 00:03:59,695 root         INFO     Train Epoch: 36 [7168/8000 (90%)]\tTotal Loss: 0.730798\n",
      "Reconstruction: 0.085700, Regularization: 0.645097\n",
      "2019-04-10 00:03:59,757 root         INFO     Train Epoch: 36 [7680/8000 (96%)]\tTotal Loss: 0.800107\n",
      "Reconstruction: 0.086733, Regularization: 0.713374\n",
      "2019-04-10 00:03:59,810 root         INFO     ====> Epoch: 36 Average loss: 0.7843\n",
      "2019-04-10 00:03:59,834 root         INFO     Train Epoch: 37 [0/8000 (0%)]\tTotal Loss: 0.741858\n",
      "Reconstruction: 0.085814, Regularization: 0.656044\n",
      "2019-04-10 00:03:59,899 root         INFO     Train Epoch: 37 [512/8000 (6%)]\tTotal Loss: 0.725925\n",
      "Reconstruction: 0.085220, Regularization: 0.640705\n",
      "2019-04-10 00:03:59,964 root         INFO     Train Epoch: 37 [1024/8000 (13%)]\tTotal Loss: 0.736613\n",
      "Reconstruction: 0.098787, Regularization: 0.637826\n",
      "2019-04-10 00:04:00,028 root         INFO     Train Epoch: 37 [1536/8000 (19%)]\tTotal Loss: 0.743482\n",
      "Reconstruction: 0.092453, Regularization: 0.651030\n",
      "2019-04-10 00:04:00,091 root         INFO     Train Epoch: 37 [2048/8000 (26%)]\tTotal Loss: 0.719313\n",
      "Reconstruction: 0.084079, Regularization: 0.635234\n",
      "2019-04-10 00:04:00,155 root         INFO     Train Epoch: 37 [2560/8000 (32%)]\tTotal Loss: 0.760945\n",
      "Reconstruction: 0.081161, Regularization: 0.679784\n",
      "2019-04-10 00:04:00,219 root         INFO     Train Epoch: 37 [3072/8000 (38%)]\tTotal Loss: 0.697592\n",
      "Reconstruction: 0.092139, Regularization: 0.605453\n",
      "2019-04-10 00:04:00,283 root         INFO     Train Epoch: 37 [3584/8000 (45%)]\tTotal Loss: 0.816126\n",
      "Reconstruction: 0.085347, Regularization: 0.730779\n",
      "2019-04-10 00:04:00,347 root         INFO     Train Epoch: 37 [4096/8000 (51%)]\tTotal Loss: 0.776794\n",
      "Reconstruction: 0.088002, Regularization: 0.688792\n",
      "2019-04-10 00:04:00,411 root         INFO     Train Epoch: 37 [4608/8000 (58%)]\tTotal Loss: 0.731263\n",
      "Reconstruction: 0.088425, Regularization: 0.642838\n",
      "2019-04-10 00:04:00,476 root         INFO     Train Epoch: 37 [5120/8000 (64%)]\tTotal Loss: 0.768915\n",
      "Reconstruction: 0.090583, Regularization: 0.678332\n",
      "2019-04-10 00:04:00,539 root         INFO     Train Epoch: 37 [5632/8000 (70%)]\tTotal Loss: 0.687774\n",
      "Reconstruction: 0.087733, Regularization: 0.600041\n",
      "2019-04-10 00:04:00,602 root         INFO     Train Epoch: 37 [6144/8000 (77%)]\tTotal Loss: 0.747957\n",
      "Reconstruction: 0.079092, Regularization: 0.668865\n",
      "2019-04-10 00:04:00,666 root         INFO     Train Epoch: 37 [6656/8000 (83%)]\tTotal Loss: 0.686690\n",
      "Reconstruction: 0.079927, Regularization: 0.606763\n",
      "2019-04-10 00:04:00,729 root         INFO     Train Epoch: 37 [7168/8000 (90%)]\tTotal Loss: 0.683264\n",
      "Reconstruction: 0.088886, Regularization: 0.594378\n",
      "2019-04-10 00:04:00,793 root         INFO     Train Epoch: 37 [7680/8000 (96%)]\tTotal Loss: 0.664593\n",
      "Reconstruction: 0.084369, Regularization: 0.580223\n",
      "2019-04-10 00:04:00,846 root         INFO     ====> Epoch: 37 Average loss: 0.7320\n",
      "2019-04-10 00:04:00,870 root         INFO     Train Epoch: 38 [0/8000 (0%)]\tTotal Loss: 0.597221\n",
      "Reconstruction: 0.082930, Regularization: 0.514292\n",
      "2019-04-10 00:04:00,934 root         INFO     Train Epoch: 38 [512/8000 (6%)]\tTotal Loss: 0.598537\n",
      "Reconstruction: 0.092358, Regularization: 0.506179\n",
      "2019-04-10 00:04:00,997 root         INFO     Train Epoch: 38 [1024/8000 (13%)]\tTotal Loss: 0.847197\n",
      "Reconstruction: 0.086053, Regularization: 0.761145\n",
      "2019-04-10 00:04:01,060 root         INFO     Train Epoch: 38 [1536/8000 (19%)]\tTotal Loss: 0.645774\n",
      "Reconstruction: 0.091888, Regularization: 0.553886\n",
      "2019-04-10 00:04:01,123 root         INFO     Train Epoch: 38 [2048/8000 (26%)]\tTotal Loss: 0.756136\n",
      "Reconstruction: 0.087457, Regularization: 0.668679\n",
      "2019-04-10 00:04:01,186 root         INFO     Train Epoch: 38 [2560/8000 (32%)]\tTotal Loss: 0.700555\n",
      "Reconstruction: 0.083395, Regularization: 0.617160\n",
      "2019-04-10 00:04:01,248 root         INFO     Train Epoch: 38 [3072/8000 (38%)]\tTotal Loss: 0.768933\n",
      "Reconstruction: 0.081113, Regularization: 0.687820\n",
      "2019-04-10 00:04:01,310 root         INFO     Train Epoch: 38 [3584/8000 (45%)]\tTotal Loss: 0.738929\n",
      "Reconstruction: 0.087958, Regularization: 0.650971\n",
      "2019-04-10 00:04:01,372 root         INFO     Train Epoch: 38 [4096/8000 (51%)]\tTotal Loss: 0.700581\n",
      "Reconstruction: 0.091174, Regularization: 0.609408\n",
      "2019-04-10 00:04:01,434 root         INFO     Train Epoch: 38 [4608/8000 (58%)]\tTotal Loss: 0.612278\n",
      "Reconstruction: 0.088557, Regularization: 0.523722\n",
      "2019-04-10 00:04:01,496 root         INFO     Train Epoch: 38 [5120/8000 (64%)]\tTotal Loss: 0.650960\n",
      "Reconstruction: 0.089554, Regularization: 0.561406\n",
      "2019-04-10 00:04:01,558 root         INFO     Train Epoch: 38 [5632/8000 (70%)]\tTotal Loss: 0.699519\n",
      "Reconstruction: 0.086930, Regularization: 0.612590\n",
      "2019-04-10 00:04:01,620 root         INFO     Train Epoch: 38 [6144/8000 (77%)]\tTotal Loss: 0.616629\n",
      "Reconstruction: 0.083876, Regularization: 0.532754\n",
      "2019-04-10 00:04:01,681 root         INFO     Train Epoch: 38 [6656/8000 (83%)]\tTotal Loss: 0.709736\n",
      "Reconstruction: 0.086034, Regularization: 0.623702\n",
      "2019-04-10 00:04:01,743 root         INFO     Train Epoch: 38 [7168/8000 (90%)]\tTotal Loss: 0.708438\n",
      "Reconstruction: 0.082019, Regularization: 0.626418\n",
      "2019-04-10 00:04:01,805 root         INFO     Train Epoch: 38 [7680/8000 (96%)]\tTotal Loss: 0.602269\n",
      "Reconstruction: 0.079027, Regularization: 0.523242\n",
      "2019-04-10 00:04:01,858 root         INFO     ====> Epoch: 38 Average loss: 0.6826\n",
      "2019-04-10 00:04:01,881 root         INFO     Train Epoch: 39 [0/8000 (0%)]\tTotal Loss: 0.642212\n",
      "Reconstruction: 0.079014, Regularization: 0.563198\n",
      "2019-04-10 00:04:01,945 root         INFO     Train Epoch: 39 [512/8000 (6%)]\tTotal Loss: 0.620481\n",
      "Reconstruction: 0.083019, Regularization: 0.537462\n",
      "2019-04-10 00:04:02,008 root         INFO     Train Epoch: 39 [1024/8000 (13%)]\tTotal Loss: 0.624714\n",
      "Reconstruction: 0.075010, Regularization: 0.549704\n",
      "2019-04-10 00:04:02,071 root         INFO     Train Epoch: 39 [1536/8000 (19%)]\tTotal Loss: 0.614519\n",
      "Reconstruction: 0.075621, Regularization: 0.538898\n",
      "2019-04-10 00:04:02,135 root         INFO     Train Epoch: 39 [2048/8000 (26%)]\tTotal Loss: 0.641199\n",
      "Reconstruction: 0.076237, Regularization: 0.564962\n",
      "2019-04-10 00:04:02,198 root         INFO     Train Epoch: 39 [2560/8000 (32%)]\tTotal Loss: 0.650800\n",
      "Reconstruction: 0.080355, Regularization: 0.570445\n",
      "2019-04-10 00:04:02,261 root         INFO     Train Epoch: 39 [3072/8000 (38%)]\tTotal Loss: 0.704552\n",
      "Reconstruction: 0.089624, Regularization: 0.614928\n",
      "2019-04-10 00:04:02,324 root         INFO     Train Epoch: 39 [3584/8000 (45%)]\tTotal Loss: 0.659296\n",
      "Reconstruction: 0.083065, Regularization: 0.576231\n",
      "2019-04-10 00:04:02,387 root         INFO     Train Epoch: 39 [4096/8000 (51%)]\tTotal Loss: 0.560180\n",
      "Reconstruction: 0.088424, Regularization: 0.471756\n",
      "2019-04-10 00:04:02,451 root         INFO     Train Epoch: 39 [4608/8000 (58%)]\tTotal Loss: 0.615257\n",
      "Reconstruction: 0.081275, Regularization: 0.533982\n",
      "2019-04-10 00:04:02,514 root         INFO     Train Epoch: 39 [5120/8000 (64%)]\tTotal Loss: 0.548424\n",
      "Reconstruction: 0.080482, Regularization: 0.467942\n",
      "2019-04-10 00:04:02,577 root         INFO     Train Epoch: 39 [5632/8000 (70%)]\tTotal Loss: 0.640697\n",
      "Reconstruction: 0.091331, Regularization: 0.549365\n",
      "2019-04-10 00:04:02,640 root         INFO     Train Epoch: 39 [6144/8000 (77%)]\tTotal Loss: 0.628812\n",
      "Reconstruction: 0.078387, Regularization: 0.550426\n",
      "2019-04-10 00:04:02,702 root         INFO     Train Epoch: 39 [6656/8000 (83%)]\tTotal Loss: 0.569381\n",
      "Reconstruction: 0.081524, Regularization: 0.487857\n",
      "2019-04-10 00:04:02,764 root         INFO     Train Epoch: 39 [7168/8000 (90%)]\tTotal Loss: 0.651714\n",
      "Reconstruction: 0.092590, Regularization: 0.559124\n",
      "2019-04-10 00:04:02,826 root         INFO     Train Epoch: 39 [7680/8000 (96%)]\tTotal Loss: 0.670548\n",
      "Reconstruction: 0.082180, Regularization: 0.588368\n",
      "2019-04-10 00:04:02,880 root         INFO     ====> Epoch: 39 Average loss: 0.6362\n",
      "2019-04-10 00:04:02,904 root         INFO     Train Epoch: 40 [0/8000 (0%)]\tTotal Loss: 0.626943\n",
      "Reconstruction: 0.078078, Regularization: 0.548865\n",
      "2019-04-10 00:04:02,967 root         INFO     Train Epoch: 40 [512/8000 (6%)]\tTotal Loss: 0.632931\n",
      "Reconstruction: 0.084197, Regularization: 0.548733\n",
      "2019-04-10 00:04:03,029 root         INFO     Train Epoch: 40 [1024/8000 (13%)]\tTotal Loss: 0.578767\n",
      "Reconstruction: 0.082357, Regularization: 0.496410\n",
      "2019-04-10 00:04:03,092 root         INFO     Train Epoch: 40 [1536/8000 (19%)]\tTotal Loss: 0.595381\n",
      "Reconstruction: 0.078978, Regularization: 0.516404\n",
      "2019-04-10 00:04:03,154 root         INFO     Train Epoch: 40 [2048/8000 (26%)]\tTotal Loss: 0.575405\n",
      "Reconstruction: 0.085806, Regularization: 0.489599\n",
      "2019-04-10 00:04:03,216 root         INFO     Train Epoch: 40 [2560/8000 (32%)]\tTotal Loss: 0.610382\n",
      "Reconstruction: 0.073819, Regularization: 0.536563\n",
      "2019-04-10 00:04:03,279 root         INFO     Train Epoch: 40 [3072/8000 (38%)]\tTotal Loss: 0.609195\n",
      "Reconstruction: 0.078981, Regularization: 0.530213\n",
      "2019-04-10 00:04:03,341 root         INFO     Train Epoch: 40 [3584/8000 (45%)]\tTotal Loss: 0.676826\n",
      "Reconstruction: 0.081628, Regularization: 0.595198\n",
      "2019-04-10 00:04:03,404 root         INFO     Train Epoch: 40 [4096/8000 (51%)]\tTotal Loss: 0.568515\n",
      "Reconstruction: 0.080203, Regularization: 0.488312\n",
      "2019-04-10 00:04:03,466 root         INFO     Train Epoch: 40 [4608/8000 (58%)]\tTotal Loss: 0.592609\n",
      "Reconstruction: 0.081744, Regularization: 0.510865\n",
      "2019-04-10 00:04:03,529 root         INFO     Train Epoch: 40 [5120/8000 (64%)]\tTotal Loss: 0.590813\n",
      "Reconstruction: 0.082491, Regularization: 0.508322\n",
      "2019-04-10 00:04:03,592 root         INFO     Train Epoch: 40 [5632/8000 (70%)]\tTotal Loss: 0.618663\n",
      "Reconstruction: 0.074181, Regularization: 0.544482\n",
      "2019-04-10 00:04:03,655 root         INFO     Train Epoch: 40 [6144/8000 (77%)]\tTotal Loss: 0.514675\n",
      "Reconstruction: 0.083530, Regularization: 0.431146\n",
      "2019-04-10 00:04:03,718 root         INFO     Train Epoch: 40 [6656/8000 (83%)]\tTotal Loss: 0.512365\n",
      "Reconstruction: 0.078417, Regularization: 0.433947\n",
      "2019-04-10 00:04:03,781 root         INFO     Train Epoch: 40 [7168/8000 (90%)]\tTotal Loss: 0.499089\n",
      "Reconstruction: 0.080685, Regularization: 0.418404\n",
      "2019-04-10 00:04:03,844 root         INFO     Train Epoch: 40 [7680/8000 (96%)]\tTotal Loss: 0.652855\n",
      "Reconstruction: 0.083319, Regularization: 0.569536\n",
      "2019-04-10 00:04:03,897 root         INFO     ====> Epoch: 40 Average loss: 0.5920\n",
      "2019-04-10 00:04:03,921 root         INFO     Train Epoch: 41 [0/8000 (0%)]\tTotal Loss: 0.632488\n",
      "Reconstruction: 0.093154, Regularization: 0.539334\n",
      "2019-04-10 00:04:03,985 root         INFO     Train Epoch: 41 [512/8000 (6%)]\tTotal Loss: 0.518735\n",
      "Reconstruction: 0.084838, Regularization: 0.433897\n",
      "2019-04-10 00:04:04,048 root         INFO     Train Epoch: 41 [1024/8000 (13%)]\tTotal Loss: 0.659973\n",
      "Reconstruction: 0.092036, Regularization: 0.567937\n",
      "2019-04-10 00:04:04,110 root         INFO     Train Epoch: 41 [1536/8000 (19%)]\tTotal Loss: 0.500232\n",
      "Reconstruction: 0.082996, Regularization: 0.417236\n",
      "2019-04-10 00:04:04,174 root         INFO     Train Epoch: 41 [2048/8000 (26%)]\tTotal Loss: 0.535615\n",
      "Reconstruction: 0.079162, Regularization: 0.456453\n",
      "2019-04-10 00:04:04,237 root         INFO     Train Epoch: 41 [2560/8000 (32%)]\tTotal Loss: 0.588595\n",
      "Reconstruction: 0.081848, Regularization: 0.506747\n",
      "2019-04-10 00:04:04,300 root         INFO     Train Epoch: 41 [3072/8000 (38%)]\tTotal Loss: 0.540758\n",
      "Reconstruction: 0.083937, Regularization: 0.456821\n",
      "2019-04-10 00:04:04,362 root         INFO     Train Epoch: 41 [3584/8000 (45%)]\tTotal Loss: 0.604044\n",
      "Reconstruction: 0.092963, Regularization: 0.511081\n",
      "2019-04-10 00:04:04,425 root         INFO     Train Epoch: 41 [4096/8000 (51%)]\tTotal Loss: 0.580669\n",
      "Reconstruction: 0.083135, Regularization: 0.497535\n",
      "2019-04-10 00:04:04,488 root         INFO     Train Epoch: 41 [4608/8000 (58%)]\tTotal Loss: 0.519953\n",
      "Reconstruction: 0.082044, Regularization: 0.437909\n",
      "2019-04-10 00:04:04,551 root         INFO     Train Epoch: 41 [5120/8000 (64%)]\tTotal Loss: 0.575699\n",
      "Reconstruction: 0.083746, Regularization: 0.491952\n",
      "2019-04-10 00:04:04,614 root         INFO     Train Epoch: 41 [5632/8000 (70%)]\tTotal Loss: 0.531300\n",
      "Reconstruction: 0.087401, Regularization: 0.443899\n",
      "2019-04-10 00:04:04,678 root         INFO     Train Epoch: 41 [6144/8000 (77%)]\tTotal Loss: 0.509252\n",
      "Reconstruction: 0.084894, Regularization: 0.424358\n",
      "2019-04-10 00:04:04,741 root         INFO     Train Epoch: 41 [6656/8000 (83%)]\tTotal Loss: 0.501853\n",
      "Reconstruction: 0.089197, Regularization: 0.412656\n",
      "2019-04-10 00:04:04,803 root         INFO     Train Epoch: 41 [7168/8000 (90%)]\tTotal Loss: 0.621260\n",
      "Reconstruction: 0.091481, Regularization: 0.529778\n",
      "2019-04-10 00:04:04,865 root         INFO     Train Epoch: 41 [7680/8000 (96%)]\tTotal Loss: 0.650988\n",
      "Reconstruction: 0.074780, Regularization: 0.576208\n",
      "2019-04-10 00:04:04,918 root         INFO     ====> Epoch: 41 Average loss: 0.5516\n",
      "2019-04-10 00:04:04,942 root         INFO     Train Epoch: 42 [0/8000 (0%)]\tTotal Loss: 0.453838\n",
      "Reconstruction: 0.080604, Regularization: 0.373234\n",
      "2019-04-10 00:04:05,006 root         INFO     Train Epoch: 42 [512/8000 (6%)]\tTotal Loss: 0.528905\n",
      "Reconstruction: 0.085759, Regularization: 0.443146\n",
      "2019-04-10 00:04:05,069 root         INFO     Train Epoch: 42 [1024/8000 (13%)]\tTotal Loss: 0.506584\n",
      "Reconstruction: 0.085643, Regularization: 0.420941\n",
      "2019-04-10 00:04:05,133 root         INFO     Train Epoch: 42 [1536/8000 (19%)]\tTotal Loss: 0.524980\n",
      "Reconstruction: 0.083459, Regularization: 0.441521\n",
      "2019-04-10 00:04:05,196 root         INFO     Train Epoch: 42 [2048/8000 (26%)]\tTotal Loss: 0.535540\n",
      "Reconstruction: 0.080692, Regularization: 0.454849\n",
      "2019-04-10 00:04:05,259 root         INFO     Train Epoch: 42 [2560/8000 (32%)]\tTotal Loss: 0.515354\n",
      "Reconstruction: 0.076137, Regularization: 0.439217\n",
      "2019-04-10 00:04:05,322 root         INFO     Train Epoch: 42 [3072/8000 (38%)]\tTotal Loss: 0.506544\n",
      "Reconstruction: 0.088597, Regularization: 0.417947\n",
      "2019-04-10 00:04:05,385 root         INFO     Train Epoch: 42 [3584/8000 (45%)]\tTotal Loss: 0.537605\n",
      "Reconstruction: 0.095858, Regularization: 0.441747\n",
      "2019-04-10 00:04:05,449 root         INFO     Train Epoch: 42 [4096/8000 (51%)]\tTotal Loss: 0.508425\n",
      "Reconstruction: 0.081412, Regularization: 0.427012\n",
      "2019-04-10 00:04:05,511 root         INFO     Train Epoch: 42 [4608/8000 (58%)]\tTotal Loss: 0.411858\n",
      "Reconstruction: 0.084761, Regularization: 0.327097\n",
      "2019-04-10 00:04:05,574 root         INFO     Train Epoch: 42 [5120/8000 (64%)]\tTotal Loss: 0.523031\n",
      "Reconstruction: 0.081956, Regularization: 0.441075\n",
      "2019-04-10 00:04:05,635 root         INFO     Train Epoch: 42 [5632/8000 (70%)]\tTotal Loss: 0.455575\n",
      "Reconstruction: 0.088938, Regularization: 0.366637\n",
      "2019-04-10 00:04:05,698 root         INFO     Train Epoch: 42 [6144/8000 (77%)]\tTotal Loss: 0.476990\n",
      "Reconstruction: 0.079217, Regularization: 0.397774\n",
      "2019-04-10 00:04:05,760 root         INFO     Train Epoch: 42 [6656/8000 (83%)]\tTotal Loss: 0.509848\n",
      "Reconstruction: 0.077313, Regularization: 0.432535\n",
      "2019-04-10 00:04:05,823 root         INFO     Train Epoch: 42 [7168/8000 (90%)]\tTotal Loss: 0.464075\n",
      "Reconstruction: 0.084960, Regularization: 0.379114\n",
      "2019-04-10 00:04:05,885 root         INFO     Train Epoch: 42 [7680/8000 (96%)]\tTotal Loss: 0.495453\n",
      "Reconstruction: 0.086290, Regularization: 0.409164\n",
      "2019-04-10 00:04:05,938 root         INFO     ====> Epoch: 42 Average loss: 0.5132\n",
      "2019-04-10 00:04:05,962 root         INFO     Train Epoch: 43 [0/8000 (0%)]\tTotal Loss: 0.450573\n",
      "Reconstruction: 0.081902, Regularization: 0.368671\n",
      "2019-04-10 00:04:06,024 root         INFO     Train Epoch: 43 [512/8000 (6%)]\tTotal Loss: 0.473283\n",
      "Reconstruction: 0.085833, Regularization: 0.387450\n",
      "2019-04-10 00:04:06,086 root         INFO     Train Epoch: 43 [1024/8000 (13%)]\tTotal Loss: 0.438036\n",
      "Reconstruction: 0.084495, Regularization: 0.353542\n",
      "2019-04-10 00:04:06,148 root         INFO     Train Epoch: 43 [1536/8000 (19%)]\tTotal Loss: 0.490861\n",
      "Reconstruction: 0.082358, Regularization: 0.408503\n",
      "2019-04-10 00:04:06,209 root         INFO     Train Epoch: 43 [2048/8000 (26%)]\tTotal Loss: 0.526561\n",
      "Reconstruction: 0.080567, Regularization: 0.445994\n",
      "2019-04-10 00:04:06,271 root         INFO     Train Epoch: 43 [2560/8000 (32%)]\tTotal Loss: 0.439167\n",
      "Reconstruction: 0.083190, Regularization: 0.355978\n",
      "2019-04-10 00:04:06,333 root         INFO     Train Epoch: 43 [3072/8000 (38%)]\tTotal Loss: 0.439896\n",
      "Reconstruction: 0.090075, Regularization: 0.349821\n",
      "2019-04-10 00:04:06,395 root         INFO     Train Epoch: 43 [3584/8000 (45%)]\tTotal Loss: 0.448828\n",
      "Reconstruction: 0.083737, Regularization: 0.365091\n",
      "2019-04-10 00:04:06,457 root         INFO     Train Epoch: 43 [4096/8000 (51%)]\tTotal Loss: 0.421145\n",
      "Reconstruction: 0.078116, Regularization: 0.343029\n",
      "2019-04-10 00:04:06,519 root         INFO     Train Epoch: 43 [4608/8000 (58%)]\tTotal Loss: 0.480040\n",
      "Reconstruction: 0.096079, Regularization: 0.383962\n",
      "2019-04-10 00:04:06,581 root         INFO     Train Epoch: 43 [5120/8000 (64%)]\tTotal Loss: 0.501957\n",
      "Reconstruction: 0.087437, Regularization: 0.414520\n",
      "2019-04-10 00:04:06,642 root         INFO     Train Epoch: 43 [5632/8000 (70%)]\tTotal Loss: 0.467764\n",
      "Reconstruction: 0.081096, Regularization: 0.386668\n",
      "2019-04-10 00:04:06,704 root         INFO     Train Epoch: 43 [6144/8000 (77%)]\tTotal Loss: 0.434396\n",
      "Reconstruction: 0.081792, Regularization: 0.352603\n",
      "2019-04-10 00:04:06,766 root         INFO     Train Epoch: 43 [6656/8000 (83%)]\tTotal Loss: 0.461653\n",
      "Reconstruction: 0.075652, Regularization: 0.386001\n",
      "2019-04-10 00:04:06,828 root         INFO     Train Epoch: 43 [7168/8000 (90%)]\tTotal Loss: 0.480996\n",
      "Reconstruction: 0.082490, Regularization: 0.398506\n",
      "2019-04-10 00:04:06,890 root         INFO     Train Epoch: 43 [7680/8000 (96%)]\tTotal Loss: 0.444045\n",
      "Reconstruction: 0.080270, Regularization: 0.363775\n",
      "2019-04-10 00:04:06,944 root         INFO     ====> Epoch: 43 Average loss: 0.4769\n",
      "2019-04-10 00:04:06,968 root         INFO     Train Epoch: 44 [0/8000 (0%)]\tTotal Loss: 0.444785\n",
      "Reconstruction: 0.083247, Regularization: 0.361538\n",
      "2019-04-10 00:04:07,030 root         INFO     Train Epoch: 44 [512/8000 (6%)]\tTotal Loss: 0.546470\n",
      "Reconstruction: 0.075714, Regularization: 0.470756\n",
      "2019-04-10 00:04:07,092 root         INFO     Train Epoch: 44 [1024/8000 (13%)]\tTotal Loss: 0.416826\n",
      "Reconstruction: 0.091094, Regularization: 0.325732\n",
      "2019-04-10 00:04:07,155 root         INFO     Train Epoch: 44 [1536/8000 (19%)]\tTotal Loss: 0.413732\n",
      "Reconstruction: 0.080112, Regularization: 0.333620\n",
      "2019-04-10 00:04:07,217 root         INFO     Train Epoch: 44 [2048/8000 (26%)]\tTotal Loss: 0.449340\n",
      "Reconstruction: 0.086968, Regularization: 0.362372\n",
      "2019-04-10 00:04:07,279 root         INFO     Train Epoch: 44 [2560/8000 (32%)]\tTotal Loss: 0.431829\n",
      "Reconstruction: 0.087610, Regularization: 0.344220\n",
      "2019-04-10 00:04:07,342 root         INFO     Train Epoch: 44 [3072/8000 (38%)]\tTotal Loss: 0.401738\n",
      "Reconstruction: 0.080374, Regularization: 0.321364\n",
      "2019-04-10 00:04:07,403 root         INFO     Train Epoch: 44 [3584/8000 (45%)]\tTotal Loss: 0.462563\n",
      "Reconstruction: 0.082337, Regularization: 0.380225\n",
      "2019-04-10 00:04:07,465 root         INFO     Train Epoch: 44 [4096/8000 (51%)]\tTotal Loss: 0.423597\n",
      "Reconstruction: 0.076746, Regularization: 0.346851\n",
      "2019-04-10 00:04:07,527 root         INFO     Train Epoch: 44 [4608/8000 (58%)]\tTotal Loss: 0.463345\n",
      "Reconstruction: 0.083214, Regularization: 0.380131\n",
      "2019-04-10 00:04:07,588 root         INFO     Train Epoch: 44 [5120/8000 (64%)]\tTotal Loss: 0.471253\n",
      "Reconstruction: 0.082761, Regularization: 0.388493\n",
      "2019-04-10 00:04:07,650 root         INFO     Train Epoch: 44 [5632/8000 (70%)]\tTotal Loss: 0.436600\n",
      "Reconstruction: 0.080968, Regularization: 0.355632\n",
      "2019-04-10 00:04:07,712 root         INFO     Train Epoch: 44 [6144/8000 (77%)]\tTotal Loss: 0.445474\n",
      "Reconstruction: 0.079022, Regularization: 0.366452\n",
      "2019-04-10 00:04:07,773 root         INFO     Train Epoch: 44 [6656/8000 (83%)]\tTotal Loss: 0.424658\n",
      "Reconstruction: 0.090683, Regularization: 0.333975\n",
      "2019-04-10 00:04:07,835 root         INFO     Train Epoch: 44 [7168/8000 (90%)]\tTotal Loss: 0.388107\n",
      "Reconstruction: 0.077520, Regularization: 0.310587\n",
      "2019-04-10 00:04:07,897 root         INFO     Train Epoch: 44 [7680/8000 (96%)]\tTotal Loss: 0.404400\n",
      "Reconstruction: 0.102127, Regularization: 0.302273\n",
      "2019-04-10 00:04:07,951 root         INFO     ====> Epoch: 44 Average loss: 0.4436\n",
      "2019-04-10 00:04:07,974 root         INFO     Train Epoch: 45 [0/8000 (0%)]\tTotal Loss: 0.447587\n",
      "Reconstruction: 0.097855, Regularization: 0.349732\n",
      "2019-04-10 00:04:08,038 root         INFO     Train Epoch: 45 [512/8000 (6%)]\tTotal Loss: 0.443890\n",
      "Reconstruction: 0.090176, Regularization: 0.353715\n",
      "2019-04-10 00:04:08,101 root         INFO     Train Epoch: 45 [1024/8000 (13%)]\tTotal Loss: 0.439818\n",
      "Reconstruction: 0.087937, Regularization: 0.351881\n",
      "2019-04-10 00:04:08,163 root         INFO     Train Epoch: 45 [1536/8000 (19%)]\tTotal Loss: 0.472734\n",
      "Reconstruction: 0.081468, Regularization: 0.391265\n",
      "2019-04-10 00:04:08,225 root         INFO     Train Epoch: 45 [2048/8000 (26%)]\tTotal Loss: 0.424162\n",
      "Reconstruction: 0.084583, Regularization: 0.339579\n",
      "2019-04-10 00:04:08,288 root         INFO     Train Epoch: 45 [2560/8000 (32%)]\tTotal Loss: 0.431103\n",
      "Reconstruction: 0.089066, Regularization: 0.342036\n",
      "2019-04-10 00:04:08,350 root         INFO     Train Epoch: 45 [3072/8000 (38%)]\tTotal Loss: 0.451563\n",
      "Reconstruction: 0.079510, Regularization: 0.372052\n",
      "2019-04-10 00:04:08,412 root         INFO     Train Epoch: 45 [3584/8000 (45%)]\tTotal Loss: 0.432937\n",
      "Reconstruction: 0.087177, Regularization: 0.345760\n",
      "2019-04-10 00:04:08,475 root         INFO     Train Epoch: 45 [4096/8000 (51%)]\tTotal Loss: 0.420848\n",
      "Reconstruction: 0.079552, Regularization: 0.341296\n",
      "2019-04-10 00:04:08,536 root         INFO     Train Epoch: 45 [4608/8000 (58%)]\tTotal Loss: 0.429362\n",
      "Reconstruction: 0.085540, Regularization: 0.343822\n",
      "2019-04-10 00:04:08,598 root         INFO     Train Epoch: 45 [5120/8000 (64%)]\tTotal Loss: 0.419346\n",
      "Reconstruction: 0.087970, Regularization: 0.331376\n",
      "2019-04-10 00:04:08,661 root         INFO     Train Epoch: 45 [5632/8000 (70%)]\tTotal Loss: 0.404919\n",
      "Reconstruction: 0.082346, Regularization: 0.322573\n",
      "2019-04-10 00:04:08,723 root         INFO     Train Epoch: 45 [6144/8000 (77%)]\tTotal Loss: 0.409890\n",
      "Reconstruction: 0.091894, Regularization: 0.317996\n",
      "2019-04-10 00:04:08,785 root         INFO     Train Epoch: 45 [6656/8000 (83%)]\tTotal Loss: 0.438420\n",
      "Reconstruction: 0.087502, Regularization: 0.350918\n",
      "2019-04-10 00:04:08,847 root         INFO     Train Epoch: 45 [7168/8000 (90%)]\tTotal Loss: 0.442597\n",
      "Reconstruction: 0.091628, Regularization: 0.350968\n",
      "2019-04-10 00:04:08,909 root         INFO     Train Epoch: 45 [7680/8000 (96%)]\tTotal Loss: 0.411018\n",
      "Reconstruction: 0.082953, Regularization: 0.328065\n",
      "2019-04-10 00:04:08,962 root         INFO     ====> Epoch: 45 Average loss: 0.4125\n",
      "2019-04-10 00:04:08,986 root         INFO     Train Epoch: 46 [0/8000 (0%)]\tTotal Loss: 0.407595\n",
      "Reconstruction: 0.102576, Regularization: 0.305019\n",
      "2019-04-10 00:04:09,049 root         INFO     Train Epoch: 46 [512/8000 (6%)]\tTotal Loss: 0.387512\n",
      "Reconstruction: 0.085645, Regularization: 0.301867\n",
      "2019-04-10 00:04:09,112 root         INFO     Train Epoch: 46 [1024/8000 (13%)]\tTotal Loss: 0.384965\n",
      "Reconstruction: 0.081471, Regularization: 0.303494\n",
      "2019-04-10 00:04:09,175 root         INFO     Train Epoch: 46 [1536/8000 (19%)]\tTotal Loss: 0.394212\n",
      "Reconstruction: 0.091754, Regularization: 0.302459\n",
      "2019-04-10 00:04:09,237 root         INFO     Train Epoch: 46 [2048/8000 (26%)]\tTotal Loss: 0.372623\n",
      "Reconstruction: 0.086810, Regularization: 0.285814\n",
      "2019-04-10 00:04:09,300 root         INFO     Train Epoch: 46 [2560/8000 (32%)]\tTotal Loss: 0.359024\n",
      "Reconstruction: 0.085803, Regularization: 0.273221\n",
      "2019-04-10 00:04:09,363 root         INFO     Train Epoch: 46 [3072/8000 (38%)]\tTotal Loss: 0.403668\n",
      "Reconstruction: 0.086703, Regularization: 0.316965\n",
      "2019-04-10 00:04:09,427 root         INFO     Train Epoch: 46 [3584/8000 (45%)]\tTotal Loss: 0.387426\n",
      "Reconstruction: 0.085212, Regularization: 0.302213\n",
      "2019-04-10 00:04:09,490 root         INFO     Train Epoch: 46 [4096/8000 (51%)]\tTotal Loss: 0.374850\n",
      "Reconstruction: 0.081969, Regularization: 0.292881\n",
      "2019-04-10 00:04:09,553 root         INFO     Train Epoch: 46 [4608/8000 (58%)]\tTotal Loss: 0.373061\n",
      "Reconstruction: 0.082426, Regularization: 0.290634\n",
      "2019-04-10 00:04:09,616 root         INFO     Train Epoch: 46 [5120/8000 (64%)]\tTotal Loss: 0.455819\n",
      "Reconstruction: 0.100682, Regularization: 0.355138\n",
      "2019-04-10 00:04:09,679 root         INFO     Train Epoch: 46 [5632/8000 (70%)]\tTotal Loss: 0.317858\n",
      "Reconstruction: 0.078169, Regularization: 0.239689\n",
      "2019-04-10 00:04:09,742 root         INFO     Train Epoch: 46 [6144/8000 (77%)]\tTotal Loss: 0.327369\n",
      "Reconstruction: 0.080834, Regularization: 0.246535\n",
      "2019-04-10 00:04:09,805 root         INFO     Train Epoch: 46 [6656/8000 (83%)]\tTotal Loss: 0.372081\n",
      "Reconstruction: 0.089700, Regularization: 0.282381\n",
      "2019-04-10 00:04:09,868 root         INFO     Train Epoch: 46 [7168/8000 (90%)]\tTotal Loss: 0.386732\n",
      "Reconstruction: 0.095235, Regularization: 0.291498\n",
      "2019-04-10 00:04:09,930 root         INFO     Train Epoch: 46 [7680/8000 (96%)]\tTotal Loss: 0.427727\n",
      "Reconstruction: 0.097946, Regularization: 0.329781\n",
      "2019-04-10 00:04:09,985 root         INFO     ====> Epoch: 46 Average loss: 0.3832\n",
      "2019-04-10 00:04:10,009 root         INFO     Train Epoch: 47 [0/8000 (0%)]\tTotal Loss: 0.379493\n",
      "Reconstruction: 0.080774, Regularization: 0.298719\n",
      "2019-04-10 00:04:10,072 root         INFO     Train Epoch: 47 [512/8000 (6%)]\tTotal Loss: 0.403058\n",
      "Reconstruction: 0.081354, Regularization: 0.321704\n",
      "2019-04-10 00:04:10,133 root         INFO     Train Epoch: 47 [1024/8000 (13%)]\tTotal Loss: 0.304521\n",
      "Reconstruction: 0.079103, Regularization: 0.225418\n",
      "2019-04-10 00:04:10,196 root         INFO     Train Epoch: 47 [1536/8000 (19%)]\tTotal Loss: 0.402296\n",
      "Reconstruction: 0.084126, Regularization: 0.318169\n",
      "2019-04-10 00:04:10,258 root         INFO     Train Epoch: 47 [2048/8000 (26%)]\tTotal Loss: 0.378835\n",
      "Reconstruction: 0.096705, Regularization: 0.282130\n",
      "2019-04-10 00:04:10,320 root         INFO     Train Epoch: 47 [2560/8000 (32%)]\tTotal Loss: 0.380346\n",
      "Reconstruction: 0.102649, Regularization: 0.277697\n",
      "2019-04-10 00:04:10,381 root         INFO     Train Epoch: 47 [3072/8000 (38%)]\tTotal Loss: 0.369015\n",
      "Reconstruction: 0.085791, Regularization: 0.283224\n",
      "2019-04-10 00:04:10,442 root         INFO     Train Epoch: 47 [3584/8000 (45%)]\tTotal Loss: 0.348056\n",
      "Reconstruction: 0.092948, Regularization: 0.255107\n",
      "2019-04-10 00:04:10,504 root         INFO     Train Epoch: 47 [4096/8000 (51%)]\tTotal Loss: 0.383195\n",
      "Reconstruction: 0.089840, Regularization: 0.293355\n",
      "2019-04-10 00:04:10,565 root         INFO     Train Epoch: 47 [4608/8000 (58%)]\tTotal Loss: 0.370989\n",
      "Reconstruction: 0.100660, Regularization: 0.270329\n",
      "2019-04-10 00:04:10,626 root         INFO     Train Epoch: 47 [5120/8000 (64%)]\tTotal Loss: 0.347827\n",
      "Reconstruction: 0.089094, Regularization: 0.258733\n",
      "2019-04-10 00:04:10,687 root         INFO     Train Epoch: 47 [5632/8000 (70%)]\tTotal Loss: 0.346091\n",
      "Reconstruction: 0.076970, Regularization: 0.269121\n",
      "2019-04-10 00:04:10,748 root         INFO     Train Epoch: 47 [6144/8000 (77%)]\tTotal Loss: 0.340657\n",
      "Reconstruction: 0.093458, Regularization: 0.247199\n",
      "2019-04-10 00:04:10,809 root         INFO     Train Epoch: 47 [6656/8000 (83%)]\tTotal Loss: 0.331714\n",
      "Reconstruction: 0.081699, Regularization: 0.250014\n",
      "2019-04-10 00:04:10,871 root         INFO     Train Epoch: 47 [7168/8000 (90%)]\tTotal Loss: 0.390944\n",
      "Reconstruction: 0.084459, Regularization: 0.306485\n",
      "2019-04-10 00:04:10,932 root         INFO     Train Epoch: 47 [7680/8000 (96%)]\tTotal Loss: 0.329535\n",
      "Reconstruction: 0.083498, Regularization: 0.246037\n",
      "2019-04-10 00:04:10,985 root         INFO     ====> Epoch: 47 Average loss: 0.3573\n",
      "2019-04-10 00:04:11,009 root         INFO     Train Epoch: 48 [0/8000 (0%)]\tTotal Loss: 0.338082\n",
      "Reconstruction: 0.084386, Regularization: 0.253696\n",
      "2019-04-10 00:04:11,071 root         INFO     Train Epoch: 48 [512/8000 (6%)]\tTotal Loss: 0.354606\n",
      "Reconstruction: 0.084170, Regularization: 0.270436\n",
      "2019-04-10 00:04:11,133 root         INFO     Train Epoch: 48 [1024/8000 (13%)]\tTotal Loss: 0.311663\n",
      "Reconstruction: 0.084066, Regularization: 0.227597\n",
      "2019-04-10 00:04:11,195 root         INFO     Train Epoch: 48 [1536/8000 (19%)]\tTotal Loss: 0.322365\n",
      "Reconstruction: 0.094961, Regularization: 0.227404\n",
      "2019-04-10 00:04:11,257 root         INFO     Train Epoch: 48 [2048/8000 (26%)]\tTotal Loss: 0.322512\n",
      "Reconstruction: 0.086412, Regularization: 0.236099\n",
      "2019-04-10 00:04:11,321 root         INFO     Train Epoch: 48 [2560/8000 (32%)]\tTotal Loss: 0.330588\n",
      "Reconstruction: 0.094245, Regularization: 0.236343\n",
      "2019-04-10 00:04:11,383 root         INFO     Train Epoch: 48 [3072/8000 (38%)]\tTotal Loss: 0.334417\n",
      "Reconstruction: 0.081288, Regularization: 0.253129\n",
      "2019-04-10 00:04:11,445 root         INFO     Train Epoch: 48 [3584/8000 (45%)]\tTotal Loss: 0.334103\n",
      "Reconstruction: 0.094998, Regularization: 0.239105\n",
      "2019-04-10 00:04:11,507 root         INFO     Train Epoch: 48 [4096/8000 (51%)]\tTotal Loss: 0.321809\n",
      "Reconstruction: 0.084694, Regularization: 0.237115\n",
      "2019-04-10 00:04:11,568 root         INFO     Train Epoch: 48 [4608/8000 (58%)]\tTotal Loss: 0.335561\n",
      "Reconstruction: 0.097449, Regularization: 0.238112\n",
      "2019-04-10 00:04:11,630 root         INFO     Train Epoch: 48 [5120/8000 (64%)]\tTotal Loss: 0.348075\n",
      "Reconstruction: 0.087697, Regularization: 0.260377\n",
      "2019-04-10 00:04:11,692 root         INFO     Train Epoch: 48 [5632/8000 (70%)]\tTotal Loss: 0.310311\n",
      "Reconstruction: 0.079562, Regularization: 0.230749\n",
      "2019-04-10 00:04:11,754 root         INFO     Train Epoch: 48 [6144/8000 (77%)]\tTotal Loss: 0.312657\n",
      "Reconstruction: 0.102846, Regularization: 0.209811\n",
      "2019-04-10 00:04:11,815 root         INFO     Train Epoch: 48 [6656/8000 (83%)]\tTotal Loss: 0.317961\n",
      "Reconstruction: 0.100026, Regularization: 0.217935\n",
      "2019-04-10 00:04:11,877 root         INFO     Train Epoch: 48 [7168/8000 (90%)]\tTotal Loss: 0.339091\n",
      "Reconstruction: 0.088811, Regularization: 0.250280\n",
      "2019-04-10 00:04:11,939 root         INFO     Train Epoch: 48 [7680/8000 (96%)]\tTotal Loss: 0.334290\n",
      "Reconstruction: 0.087068, Regularization: 0.247222\n",
      "2019-04-10 00:04:11,992 root         INFO     ====> Epoch: 48 Average loss: 0.3317\n",
      "2019-04-10 00:04:12,016 root         INFO     Train Epoch: 49 [0/8000 (0%)]\tTotal Loss: 0.353988\n",
      "Reconstruction: 0.106701, Regularization: 0.247287\n",
      "2019-04-10 00:04:12,078 root         INFO     Train Epoch: 49 [512/8000 (6%)]\tTotal Loss: 0.334406\n",
      "Reconstruction: 0.087561, Regularization: 0.246845\n",
      "2019-04-10 00:04:12,140 root         INFO     Train Epoch: 49 [1024/8000 (13%)]\tTotal Loss: 0.267354\n",
      "Reconstruction: 0.091461, Regularization: 0.175893\n",
      "2019-04-10 00:04:12,202 root         INFO     Train Epoch: 49 [1536/8000 (19%)]\tTotal Loss: 0.284609\n",
      "Reconstruction: 0.083139, Regularization: 0.201470\n",
      "2019-04-10 00:04:12,263 root         INFO     Train Epoch: 49 [2048/8000 (26%)]\tTotal Loss: 0.334488\n",
      "Reconstruction: 0.100856, Regularization: 0.233632\n",
      "2019-04-10 00:04:12,325 root         INFO     Train Epoch: 49 [2560/8000 (32%)]\tTotal Loss: 0.280832\n",
      "Reconstruction: 0.084116, Regularization: 0.196715\n",
      "2019-04-10 00:04:12,387 root         INFO     Train Epoch: 49 [3072/8000 (38%)]\tTotal Loss: 0.310308\n",
      "Reconstruction: 0.087272, Regularization: 0.223036\n",
      "2019-04-10 00:04:12,449 root         INFO     Train Epoch: 49 [3584/8000 (45%)]\tTotal Loss: 0.331914\n",
      "Reconstruction: 0.077777, Regularization: 0.254138\n",
      "2019-04-10 00:04:12,511 root         INFO     Train Epoch: 49 [4096/8000 (51%)]\tTotal Loss: 0.291202\n",
      "Reconstruction: 0.099407, Regularization: 0.191795\n",
      "2019-04-10 00:04:12,573 root         INFO     Train Epoch: 49 [4608/8000 (58%)]\tTotal Loss: 0.330551\n",
      "Reconstruction: 0.082520, Regularization: 0.248031\n",
      "2019-04-10 00:04:12,635 root         INFO     Train Epoch: 49 [5120/8000 (64%)]\tTotal Loss: 0.329820\n",
      "Reconstruction: 0.097312, Regularization: 0.232508\n",
      "2019-04-10 00:04:12,696 root         INFO     Train Epoch: 49 [5632/8000 (70%)]\tTotal Loss: 0.326134\n",
      "Reconstruction: 0.100241, Regularization: 0.225893\n",
      "2019-04-10 00:04:12,758 root         INFO     Train Epoch: 49 [6144/8000 (77%)]\tTotal Loss: 0.317023\n",
      "Reconstruction: 0.092476, Regularization: 0.224546\n",
      "2019-04-10 00:04:12,818 root         INFO     Train Epoch: 49 [6656/8000 (83%)]\tTotal Loss: 0.281308\n",
      "Reconstruction: 0.109114, Regularization: 0.172193\n",
      "2019-04-10 00:04:12,879 root         INFO     Train Epoch: 49 [7168/8000 (90%)]\tTotal Loss: 0.307568\n",
      "Reconstruction: 0.102998, Regularization: 0.204569\n",
      "2019-04-10 00:04:12,940 root         INFO     Train Epoch: 49 [7680/8000 (96%)]\tTotal Loss: 0.306725\n",
      "Reconstruction: 0.090513, Regularization: 0.216212\n",
      "2019-04-10 00:04:12,993 root         INFO     ====> Epoch: 49 Average loss: 0.3081\n",
      "2019-04-10 00:04:13,017 root         INFO     Train Epoch: 50 [0/8000 (0%)]\tTotal Loss: 0.322264\n",
      "Reconstruction: 0.091003, Regularization: 0.231261\n",
      "2019-04-10 00:04:13,079 root         INFO     Train Epoch: 50 [512/8000 (6%)]\tTotal Loss: 0.260106\n",
      "Reconstruction: 0.094539, Regularization: 0.165567\n",
      "2019-04-10 00:04:13,140 root         INFO     Train Epoch: 50 [1024/8000 (13%)]\tTotal Loss: 0.288350\n",
      "Reconstruction: 0.095222, Regularization: 0.193128\n",
      "2019-04-10 00:04:13,202 root         INFO     Train Epoch: 50 [1536/8000 (19%)]\tTotal Loss: 0.297934\n",
      "Reconstruction: 0.101172, Regularization: 0.196762\n",
      "2019-04-10 00:04:13,263 root         INFO     Train Epoch: 50 [2048/8000 (26%)]\tTotal Loss: 0.297671\n",
      "Reconstruction: 0.092545, Regularization: 0.205126\n",
      "2019-04-10 00:04:13,325 root         INFO     Train Epoch: 50 [2560/8000 (32%)]\tTotal Loss: 0.317721\n",
      "Reconstruction: 0.091730, Regularization: 0.225991\n",
      "2019-04-10 00:04:13,387 root         INFO     Train Epoch: 50 [3072/8000 (38%)]\tTotal Loss: 0.313378\n",
      "Reconstruction: 0.095802, Regularization: 0.217576\n",
      "2019-04-10 00:04:13,449 root         INFO     Train Epoch: 50 [3584/8000 (45%)]\tTotal Loss: 0.250030\n",
      "Reconstruction: 0.099804, Regularization: 0.150226\n",
      "2019-04-10 00:04:13,510 root         INFO     Train Epoch: 50 [4096/8000 (51%)]\tTotal Loss: 0.292746\n",
      "Reconstruction: 0.093514, Regularization: 0.199233\n",
      "2019-04-10 00:04:13,572 root         INFO     Train Epoch: 50 [4608/8000 (58%)]\tTotal Loss: 0.298120\n",
      "Reconstruction: 0.094645, Regularization: 0.203475\n",
      "2019-04-10 00:04:13,633 root         INFO     Train Epoch: 50 [5120/8000 (64%)]\tTotal Loss: 0.296145\n",
      "Reconstruction: 0.093624, Regularization: 0.202522\n",
      "2019-04-10 00:04:13,695 root         INFO     Train Epoch: 50 [5632/8000 (70%)]\tTotal Loss: 0.294761\n",
      "Reconstruction: 0.097073, Regularization: 0.197688\n",
      "2019-04-10 00:04:13,757 root         INFO     Train Epoch: 50 [6144/8000 (77%)]\tTotal Loss: 0.302083\n",
      "Reconstruction: 0.093630, Regularization: 0.208453\n",
      "2019-04-10 00:04:13,819 root         INFO     Train Epoch: 50 [6656/8000 (83%)]\tTotal Loss: 0.255678\n",
      "Reconstruction: 0.091435, Regularization: 0.164243\n",
      "2019-04-10 00:04:13,881 root         INFO     Train Epoch: 50 [7168/8000 (90%)]\tTotal Loss: 0.284939\n",
      "Reconstruction: 0.083518, Regularization: 0.201422\n",
      "2019-04-10 00:04:13,943 root         INFO     Train Epoch: 50 [7680/8000 (96%)]\tTotal Loss: 0.267598\n",
      "Reconstruction: 0.101485, Regularization: 0.166113\n",
      "2019-04-10 00:04:13,996 root         INFO     ====> Epoch: 50 Average loss: 0.2871\n",
      "2019-04-10 00:04:14,020 root         INFO     Train Epoch: 51 [0/8000 (0%)]\tTotal Loss: 0.270892\n",
      "Reconstruction: 0.081749, Regularization: 0.189142\n",
      "2019-04-10 00:04:14,082 root         INFO     Train Epoch: 51 [512/8000 (6%)]\tTotal Loss: 0.264973\n",
      "Reconstruction: 0.095461, Regularization: 0.169513\n",
      "2019-04-10 00:04:14,144 root         INFO     Train Epoch: 51 [1024/8000 (13%)]\tTotal Loss: 0.236131\n",
      "Reconstruction: 0.088035, Regularization: 0.148096\n",
      "2019-04-10 00:04:14,206 root         INFO     Train Epoch: 51 [1536/8000 (19%)]\tTotal Loss: 0.298861\n",
      "Reconstruction: 0.090460, Regularization: 0.208401\n",
      "2019-04-10 00:04:14,268 root         INFO     Train Epoch: 51 [2048/8000 (26%)]\tTotal Loss: 0.251993\n",
      "Reconstruction: 0.091851, Regularization: 0.160142\n",
      "2019-04-10 00:04:14,330 root         INFO     Train Epoch: 51 [2560/8000 (32%)]\tTotal Loss: 0.261947\n",
      "Reconstruction: 0.097364, Regularization: 0.164583\n",
      "2019-04-10 00:04:14,393 root         INFO     Train Epoch: 51 [3072/8000 (38%)]\tTotal Loss: 0.277264\n",
      "Reconstruction: 0.096709, Regularization: 0.180555\n",
      "2019-04-10 00:04:14,455 root         INFO     Train Epoch: 51 [3584/8000 (45%)]\tTotal Loss: 0.262258\n",
      "Reconstruction: 0.089886, Regularization: 0.172372\n",
      "2019-04-10 00:04:14,517 root         INFO     Train Epoch: 51 [4096/8000 (51%)]\tTotal Loss: 0.278385\n",
      "Reconstruction: 0.090667, Regularization: 0.187717\n",
      "2019-04-10 00:04:14,579 root         INFO     Train Epoch: 51 [4608/8000 (58%)]\tTotal Loss: 0.271417\n",
      "Reconstruction: 0.114805, Regularization: 0.156612\n",
      "2019-04-10 00:04:14,641 root         INFO     Train Epoch: 51 [5120/8000 (64%)]\tTotal Loss: 0.296020\n",
      "Reconstruction: 0.101644, Regularization: 0.194375\n",
      "2019-04-10 00:04:14,702 root         INFO     Train Epoch: 51 [5632/8000 (70%)]\tTotal Loss: 0.271426\n",
      "Reconstruction: 0.085561, Regularization: 0.185865\n",
      "2019-04-10 00:04:14,764 root         INFO     Train Epoch: 51 [6144/8000 (77%)]\tTotal Loss: 0.228638\n",
      "Reconstruction: 0.089806, Regularization: 0.138832\n",
      "2019-04-10 00:04:14,826 root         INFO     Train Epoch: 51 [6656/8000 (83%)]\tTotal Loss: 0.267785\n",
      "Reconstruction: 0.089148, Regularization: 0.178637\n",
      "2019-04-10 00:04:14,888 root         INFO     Train Epoch: 51 [7168/8000 (90%)]\tTotal Loss: 0.242539\n",
      "Reconstruction: 0.086151, Regularization: 0.156388\n",
      "2019-04-10 00:04:14,950 root         INFO     Train Epoch: 51 [7680/8000 (96%)]\tTotal Loss: 0.259181\n",
      "Reconstruction: 0.110318, Regularization: 0.148864\n",
      "2019-04-10 00:04:15,004 root         INFO     ====> Epoch: 51 Average loss: 0.2687\n",
      "2019-04-10 00:04:15,028 root         INFO     Train Epoch: 52 [0/8000 (0%)]\tTotal Loss: 0.254119\n",
      "Reconstruction: 0.102893, Regularization: 0.151226\n",
      "2019-04-10 00:04:15,089 root         INFO     Train Epoch: 52 [512/8000 (6%)]\tTotal Loss: 0.260001\n",
      "Reconstruction: 0.098154, Regularization: 0.161847\n",
      "2019-04-10 00:04:15,150 root         INFO     Train Epoch: 52 [1024/8000 (13%)]\tTotal Loss: 0.263679\n",
      "Reconstruction: 0.091522, Regularization: 0.172157\n",
      "2019-04-10 00:04:15,211 root         INFO     Train Epoch: 52 [1536/8000 (19%)]\tTotal Loss: 0.251232\n",
      "Reconstruction: 0.095215, Regularization: 0.156017\n",
      "2019-04-10 00:04:15,273 root         INFO     Train Epoch: 52 [2048/8000 (26%)]\tTotal Loss: 0.250765\n",
      "Reconstruction: 0.101299, Regularization: 0.149465\n",
      "2019-04-10 00:04:15,333 root         INFO     Train Epoch: 52 [2560/8000 (32%)]\tTotal Loss: 0.217121\n",
      "Reconstruction: 0.085955, Regularization: 0.131166\n",
      "2019-04-10 00:04:15,394 root         INFO     Train Epoch: 52 [3072/8000 (38%)]\tTotal Loss: 0.233490\n",
      "Reconstruction: 0.092405, Regularization: 0.141086\n",
      "2019-04-10 00:04:15,455 root         INFO     Train Epoch: 52 [3584/8000 (45%)]\tTotal Loss: 0.268936\n",
      "Reconstruction: 0.099713, Regularization: 0.169223\n",
      "2019-04-10 00:04:15,517 root         INFO     Train Epoch: 52 [4096/8000 (51%)]\tTotal Loss: 0.267471\n",
      "Reconstruction: 0.096603, Regularization: 0.170867\n",
      "2019-04-10 00:04:15,578 root         INFO     Train Epoch: 52 [4608/8000 (58%)]\tTotal Loss: 0.285342\n",
      "Reconstruction: 0.100985, Regularization: 0.184357\n",
      "2019-04-10 00:04:15,639 root         INFO     Train Epoch: 52 [5120/8000 (64%)]\tTotal Loss: 0.242590\n",
      "Reconstruction: 0.093932, Regularization: 0.148658\n",
      "2019-04-10 00:04:15,700 root         INFO     Train Epoch: 52 [5632/8000 (70%)]\tTotal Loss: 0.255344\n",
      "Reconstruction: 0.112662, Regularization: 0.142682\n",
      "2019-04-10 00:04:15,761 root         INFO     Train Epoch: 52 [6144/8000 (77%)]\tTotal Loss: 0.252465\n",
      "Reconstruction: 0.106149, Regularization: 0.146317\n",
      "2019-04-10 00:04:15,822 root         INFO     Train Epoch: 52 [6656/8000 (83%)]\tTotal Loss: 0.258349\n",
      "Reconstruction: 0.118702, Regularization: 0.139647\n",
      "2019-04-10 00:04:15,883 root         INFO     Train Epoch: 52 [7168/8000 (90%)]\tTotal Loss: 0.248443\n",
      "Reconstruction: 0.108087, Regularization: 0.140356\n",
      "2019-04-10 00:04:15,944 root         INFO     Train Epoch: 52 [7680/8000 (96%)]\tTotal Loss: 0.280899\n",
      "Reconstruction: 0.115917, Regularization: 0.164982\n",
      "2019-04-10 00:04:15,997 root         INFO     ====> Epoch: 52 Average loss: 0.2507\n",
      "2019-04-10 00:04:16,021 root         INFO     Train Epoch: 53 [0/8000 (0%)]\tTotal Loss: 0.199835\n",
      "Reconstruction: 0.090820, Regularization: 0.109014\n",
      "2019-04-10 00:04:16,084 root         INFO     Train Epoch: 53 [512/8000 (6%)]\tTotal Loss: 0.247350\n",
      "Reconstruction: 0.101329, Regularization: 0.146021\n",
      "2019-04-10 00:04:16,145 root         INFO     Train Epoch: 53 [1024/8000 (13%)]\tTotal Loss: 0.266011\n",
      "Reconstruction: 0.102201, Regularization: 0.163810\n",
      "2019-04-10 00:04:16,207 root         INFO     Train Epoch: 53 [1536/8000 (19%)]\tTotal Loss: 0.258805\n",
      "Reconstruction: 0.102853, Regularization: 0.155953\n",
      "2019-04-10 00:04:16,269 root         INFO     Train Epoch: 53 [2048/8000 (26%)]\tTotal Loss: 0.209090\n",
      "Reconstruction: 0.090056, Regularization: 0.119034\n",
      "2019-04-10 00:04:16,331 root         INFO     Train Epoch: 53 [2560/8000 (32%)]\tTotal Loss: 0.236495\n",
      "Reconstruction: 0.113451, Regularization: 0.123045\n",
      "2019-04-10 00:04:16,393 root         INFO     Train Epoch: 53 [3072/8000 (38%)]\tTotal Loss: 0.251238\n",
      "Reconstruction: 0.109663, Regularization: 0.141575\n",
      "2019-04-10 00:04:16,455 root         INFO     Train Epoch: 53 [3584/8000 (45%)]\tTotal Loss: 0.261588\n",
      "Reconstruction: 0.121004, Regularization: 0.140584\n",
      "2019-04-10 00:04:16,517 root         INFO     Train Epoch: 53 [4096/8000 (51%)]\tTotal Loss: 0.218175\n",
      "Reconstruction: 0.097658, Regularization: 0.120517\n",
      "2019-04-10 00:04:16,579 root         INFO     Train Epoch: 53 [4608/8000 (58%)]\tTotal Loss: 0.219411\n",
      "Reconstruction: 0.095905, Regularization: 0.123506\n",
      "2019-04-10 00:04:16,641 root         INFO     Train Epoch: 53 [5120/8000 (64%)]\tTotal Loss: 0.236108\n",
      "Reconstruction: 0.097340, Regularization: 0.138767\n",
      "2019-04-10 00:04:16,703 root         INFO     Train Epoch: 53 [5632/8000 (70%)]\tTotal Loss: 0.236467\n",
      "Reconstruction: 0.095742, Regularization: 0.140725\n",
      "2019-04-10 00:04:16,765 root         INFO     Train Epoch: 53 [6144/8000 (77%)]\tTotal Loss: 0.211482\n",
      "Reconstruction: 0.093736, Regularization: 0.117746\n",
      "2019-04-10 00:04:16,826 root         INFO     Train Epoch: 53 [6656/8000 (83%)]\tTotal Loss: 0.256411\n",
      "Reconstruction: 0.108199, Regularization: 0.148212\n",
      "2019-04-10 00:04:16,888 root         INFO     Train Epoch: 53 [7168/8000 (90%)]\tTotal Loss: 0.248708\n",
      "Reconstruction: 0.099056, Regularization: 0.149652\n",
      "2019-04-10 00:04:16,949 root         INFO     Train Epoch: 53 [7680/8000 (96%)]\tTotal Loss: 0.226273\n",
      "Reconstruction: 0.109351, Regularization: 0.116922\n",
      "2019-04-10 00:04:17,002 root         INFO     ====> Epoch: 53 Average loss: 0.2348\n",
      "2019-04-10 00:04:17,026 root         INFO     Train Epoch: 54 [0/8000 (0%)]\tTotal Loss: 0.220061\n",
      "Reconstruction: 0.090363, Regularization: 0.129697\n",
      "2019-04-10 00:04:17,088 root         INFO     Train Epoch: 54 [512/8000 (6%)]\tTotal Loss: 0.214269\n",
      "Reconstruction: 0.105667, Regularization: 0.108602\n",
      "2019-04-10 00:04:17,150 root         INFO     Train Epoch: 54 [1024/8000 (13%)]\tTotal Loss: 0.229027\n",
      "Reconstruction: 0.116000, Regularization: 0.113027\n",
      "2019-04-10 00:04:17,212 root         INFO     Train Epoch: 54 [1536/8000 (19%)]\tTotal Loss: 0.234164\n",
      "Reconstruction: 0.091441, Regularization: 0.142723\n",
      "2019-04-10 00:04:17,274 root         INFO     Train Epoch: 54 [2048/8000 (26%)]\tTotal Loss: 0.212693\n",
      "Reconstruction: 0.097992, Regularization: 0.114702\n",
      "2019-04-10 00:04:17,336 root         INFO     Train Epoch: 54 [2560/8000 (32%)]\tTotal Loss: 0.217748\n",
      "Reconstruction: 0.103402, Regularization: 0.114346\n",
      "2019-04-10 00:04:17,397 root         INFO     Train Epoch: 54 [3072/8000 (38%)]\tTotal Loss: 0.224457\n",
      "Reconstruction: 0.102588, Regularization: 0.121869\n",
      "2019-04-10 00:04:17,459 root         INFO     Train Epoch: 54 [3584/8000 (45%)]\tTotal Loss: 0.209032\n",
      "Reconstruction: 0.094948, Regularization: 0.114083\n",
      "2019-04-10 00:04:17,521 root         INFO     Train Epoch: 54 [4096/8000 (51%)]\tTotal Loss: 0.203962\n",
      "Reconstruction: 0.094996, Regularization: 0.108966\n",
      "2019-04-10 00:04:17,583 root         INFO     Train Epoch: 54 [4608/8000 (58%)]\tTotal Loss: 0.224713\n",
      "Reconstruction: 0.128740, Regularization: 0.095973\n",
      "2019-04-10 00:04:17,644 root         INFO     Train Epoch: 54 [5120/8000 (64%)]\tTotal Loss: 0.227197\n",
      "Reconstruction: 0.107076, Regularization: 0.120120\n",
      "2019-04-10 00:04:17,706 root         INFO     Train Epoch: 54 [5632/8000 (70%)]\tTotal Loss: 0.212285\n",
      "Reconstruction: 0.104255, Regularization: 0.108030\n",
      "2019-04-10 00:04:17,768 root         INFO     Train Epoch: 54 [6144/8000 (77%)]\tTotal Loss: 0.214313\n",
      "Reconstruction: 0.111775, Regularization: 0.102539\n",
      "2019-04-10 00:04:17,830 root         INFO     Train Epoch: 54 [6656/8000 (83%)]\tTotal Loss: 0.195583\n",
      "Reconstruction: 0.095430, Regularization: 0.100154\n",
      "2019-04-10 00:04:17,891 root         INFO     Train Epoch: 54 [7168/8000 (90%)]\tTotal Loss: 0.227647\n",
      "Reconstruction: 0.116685, Regularization: 0.110962\n",
      "2019-04-10 00:04:17,953 root         INFO     Train Epoch: 54 [7680/8000 (96%)]\tTotal Loss: 0.198518\n",
      "Reconstruction: 0.099841, Regularization: 0.098677\n",
      "2019-04-10 00:04:18,005 root         INFO     ====> Epoch: 54 Average loss: 0.2206\n",
      "2019-04-10 00:04:18,029 root         INFO     Train Epoch: 55 [0/8000 (0%)]\tTotal Loss: 0.204078\n",
      "Reconstruction: 0.110879, Regularization: 0.093198\n",
      "2019-04-10 00:04:18,092 root         INFO     Train Epoch: 55 [512/8000 (6%)]\tTotal Loss: 0.216866\n",
      "Reconstruction: 0.103348, Regularization: 0.113518\n",
      "2019-04-10 00:04:18,154 root         INFO     Train Epoch: 55 [1024/8000 (13%)]\tTotal Loss: 0.186444\n",
      "Reconstruction: 0.107318, Regularization: 0.079126\n",
      "2019-04-10 00:04:18,217 root         INFO     Train Epoch: 55 [1536/8000 (19%)]\tTotal Loss: 0.206773\n",
      "Reconstruction: 0.115535, Regularization: 0.091237\n",
      "2019-04-10 00:04:18,278 root         INFO     Train Epoch: 55 [2048/8000 (26%)]\tTotal Loss: 0.201831\n",
      "Reconstruction: 0.096631, Regularization: 0.105200\n",
      "2019-04-10 00:04:18,340 root         INFO     Train Epoch: 55 [2560/8000 (32%)]\tTotal Loss: 0.216925\n",
      "Reconstruction: 0.104203, Regularization: 0.112722\n",
      "2019-04-10 00:04:18,402 root         INFO     Train Epoch: 55 [3072/8000 (38%)]\tTotal Loss: 0.201969\n",
      "Reconstruction: 0.091178, Regularization: 0.110790\n",
      "2019-04-10 00:04:18,464 root         INFO     Train Epoch: 55 [3584/8000 (45%)]\tTotal Loss: 0.202430\n",
      "Reconstruction: 0.101744, Regularization: 0.100686\n",
      "2019-04-10 00:04:18,526 root         INFO     Train Epoch: 55 [4096/8000 (51%)]\tTotal Loss: 0.216525\n",
      "Reconstruction: 0.123670, Regularization: 0.092855\n",
      "2019-04-10 00:04:18,587 root         INFO     Train Epoch: 55 [4608/8000 (58%)]\tTotal Loss: 0.203326\n",
      "Reconstruction: 0.104857, Regularization: 0.098469\n",
      "2019-04-10 00:04:18,649 root         INFO     Train Epoch: 55 [5120/8000 (64%)]\tTotal Loss: 0.201512\n",
      "Reconstruction: 0.094152, Regularization: 0.107360\n",
      "2019-04-10 00:04:18,712 root         INFO     Train Epoch: 55 [5632/8000 (70%)]\tTotal Loss: 0.194885\n",
      "Reconstruction: 0.099639, Regularization: 0.095246\n",
      "2019-04-10 00:04:18,774 root         INFO     Train Epoch: 55 [6144/8000 (77%)]\tTotal Loss: 0.207556\n",
      "Reconstruction: 0.102728, Regularization: 0.104828\n",
      "2019-04-10 00:04:18,836 root         INFO     Train Epoch: 55 [6656/8000 (83%)]\tTotal Loss: 0.198679\n",
      "Reconstruction: 0.108837, Regularization: 0.089842\n",
      "2019-04-10 00:04:18,898 root         INFO     Train Epoch: 55 [7168/8000 (90%)]\tTotal Loss: 0.196396\n",
      "Reconstruction: 0.104200, Regularization: 0.092196\n",
      "2019-04-10 00:04:18,960 root         INFO     Train Epoch: 55 [7680/8000 (96%)]\tTotal Loss: 0.192753\n",
      "Reconstruction: 0.098261, Regularization: 0.094492\n",
      "2019-04-10 00:04:19,013 root         INFO     ====> Epoch: 55 Average loss: 0.2082\n",
      "2019-04-10 00:04:19,037 root         INFO     Train Epoch: 56 [0/8000 (0%)]\tTotal Loss: 0.205074\n",
      "Reconstruction: 0.108809, Regularization: 0.096265\n",
      "2019-04-10 00:04:19,099 root         INFO     Train Epoch: 56 [512/8000 (6%)]\tTotal Loss: 0.211504\n",
      "Reconstruction: 0.111776, Regularization: 0.099728\n",
      "2019-04-10 00:04:19,160 root         INFO     Train Epoch: 56 [1024/8000 (13%)]\tTotal Loss: 0.186537\n",
      "Reconstruction: 0.100946, Regularization: 0.085592\n",
      "2019-04-10 00:04:19,221 root         INFO     Train Epoch: 56 [1536/8000 (19%)]\tTotal Loss: 0.194129\n",
      "Reconstruction: 0.118287, Regularization: 0.075841\n",
      "2019-04-10 00:04:19,282 root         INFO     Train Epoch: 56 [2048/8000 (26%)]\tTotal Loss: 0.201426\n",
      "Reconstruction: 0.101419, Regularization: 0.100007\n",
      "2019-04-10 00:04:19,343 root         INFO     Train Epoch: 56 [2560/8000 (32%)]\tTotal Loss: 0.224059\n",
      "Reconstruction: 0.116598, Regularization: 0.107461\n",
      "2019-04-10 00:04:19,404 root         INFO     Train Epoch: 56 [3072/8000 (38%)]\tTotal Loss: 0.204600\n",
      "Reconstruction: 0.106629, Regularization: 0.097971\n",
      "2019-04-10 00:04:19,466 root         INFO     Train Epoch: 56 [3584/8000 (45%)]\tTotal Loss: 0.169477\n",
      "Reconstruction: 0.090814, Regularization: 0.078663\n",
      "2019-04-10 00:04:19,526 root         INFO     Train Epoch: 56 [4096/8000 (51%)]\tTotal Loss: 0.184626\n",
      "Reconstruction: 0.106013, Regularization: 0.078613\n",
      "2019-04-10 00:04:19,587 root         INFO     Train Epoch: 56 [4608/8000 (58%)]\tTotal Loss: 0.203999\n",
      "Reconstruction: 0.107825, Regularization: 0.096174\n",
      "2019-04-10 00:04:19,649 root         INFO     Train Epoch: 56 [5120/8000 (64%)]\tTotal Loss: 0.201184\n",
      "Reconstruction: 0.098467, Regularization: 0.102718\n",
      "2019-04-10 00:04:19,710 root         INFO     Train Epoch: 56 [5632/8000 (70%)]\tTotal Loss: 0.193877\n",
      "Reconstruction: 0.102208, Regularization: 0.091669\n",
      "2019-04-10 00:04:19,771 root         INFO     Train Epoch: 56 [6144/8000 (77%)]\tTotal Loss: 0.197648\n",
      "Reconstruction: 0.105121, Regularization: 0.092527\n",
      "2019-04-10 00:04:19,833 root         INFO     Train Epoch: 56 [6656/8000 (83%)]\tTotal Loss: 0.212278\n",
      "Reconstruction: 0.125277, Regularization: 0.087001\n",
      "2019-04-10 00:04:19,894 root         INFO     Train Epoch: 56 [7168/8000 (90%)]\tTotal Loss: 0.172364\n",
      "Reconstruction: 0.103318, Regularization: 0.069046\n",
      "2019-04-10 00:04:19,955 root         INFO     Train Epoch: 56 [7680/8000 (96%)]\tTotal Loss: 0.180727\n",
      "Reconstruction: 0.104887, Regularization: 0.075840\n",
      "2019-04-10 00:04:20,008 root         INFO     ====> Epoch: 56 Average loss: 0.1963\n",
      "2019-04-10 00:04:20,032 root         INFO     Train Epoch: 57 [0/8000 (0%)]\tTotal Loss: 0.192823\n",
      "Reconstruction: 0.099115, Regularization: 0.093709\n",
      "2019-04-10 00:04:20,094 root         INFO     Train Epoch: 57 [512/8000 (6%)]\tTotal Loss: 0.201552\n",
      "Reconstruction: 0.107167, Regularization: 0.094385\n",
      "2019-04-10 00:04:20,155 root         INFO     Train Epoch: 57 [1024/8000 (13%)]\tTotal Loss: 0.216329\n",
      "Reconstruction: 0.110838, Regularization: 0.105490\n",
      "2019-04-10 00:04:20,216 root         INFO     Train Epoch: 57 [1536/8000 (19%)]\tTotal Loss: 0.197838\n",
      "Reconstruction: 0.099190, Regularization: 0.098648\n",
      "2019-04-10 00:04:20,278 root         INFO     Train Epoch: 57 [2048/8000 (26%)]\tTotal Loss: 0.210055\n",
      "Reconstruction: 0.117911, Regularization: 0.092144\n",
      "2019-04-10 00:04:20,339 root         INFO     Train Epoch: 57 [2560/8000 (32%)]\tTotal Loss: 0.180801\n",
      "Reconstruction: 0.112544, Regularization: 0.068257\n",
      "2019-04-10 00:04:20,400 root         INFO     Train Epoch: 57 [3072/8000 (38%)]\tTotal Loss: 0.181406\n",
      "Reconstruction: 0.105083, Regularization: 0.076323\n",
      "2019-04-10 00:04:20,462 root         INFO     Train Epoch: 57 [3584/8000 (45%)]\tTotal Loss: 0.224123\n",
      "Reconstruction: 0.120969, Regularization: 0.103154\n",
      "2019-04-10 00:04:20,524 root         INFO     Train Epoch: 57 [4096/8000 (51%)]\tTotal Loss: 0.165456\n",
      "Reconstruction: 0.097137, Regularization: 0.068319\n",
      "2019-04-10 00:04:20,586 root         INFO     Train Epoch: 57 [4608/8000 (58%)]\tTotal Loss: 0.183303\n",
      "Reconstruction: 0.113549, Regularization: 0.069753\n",
      "2019-04-10 00:04:20,648 root         INFO     Train Epoch: 57 [5120/8000 (64%)]\tTotal Loss: 0.180678\n",
      "Reconstruction: 0.096592, Regularization: 0.084086\n",
      "2019-04-10 00:04:20,710 root         INFO     Train Epoch: 57 [5632/8000 (70%)]\tTotal Loss: 0.203517\n",
      "Reconstruction: 0.116338, Regularization: 0.087179\n",
      "2019-04-10 00:04:20,772 root         INFO     Train Epoch: 57 [6144/8000 (77%)]\tTotal Loss: 0.173099\n",
      "Reconstruction: 0.108409, Regularization: 0.064690\n",
      "2019-04-10 00:04:20,834 root         INFO     Train Epoch: 57 [6656/8000 (83%)]\tTotal Loss: 0.178085\n",
      "Reconstruction: 0.105240, Regularization: 0.072845\n",
      "2019-04-10 00:04:20,895 root         INFO     Train Epoch: 57 [7168/8000 (90%)]\tTotal Loss: 0.186929\n",
      "Reconstruction: 0.112640, Regularization: 0.074289\n",
      "2019-04-10 00:04:20,958 root         INFO     Train Epoch: 57 [7680/8000 (96%)]\tTotal Loss: 0.172856\n",
      "Reconstruction: 0.108679, Regularization: 0.064177\n",
      "2019-04-10 00:04:21,010 root         INFO     ====> Epoch: 57 Average loss: 0.1877\n",
      "2019-04-10 00:04:21,034 root         INFO     Train Epoch: 58 [0/8000 (0%)]\tTotal Loss: 0.200433\n",
      "Reconstruction: 0.109931, Regularization: 0.090503\n",
      "2019-04-10 00:04:21,095 root         INFO     Train Epoch: 58 [512/8000 (6%)]\tTotal Loss: 0.189233\n",
      "Reconstruction: 0.115866, Regularization: 0.073367\n",
      "2019-04-10 00:04:21,157 root         INFO     Train Epoch: 58 [1024/8000 (13%)]\tTotal Loss: 0.176178\n",
      "Reconstruction: 0.109640, Regularization: 0.066538\n",
      "2019-04-10 00:04:21,218 root         INFO     Train Epoch: 58 [1536/8000 (19%)]\tTotal Loss: 0.186844\n",
      "Reconstruction: 0.116899, Regularization: 0.069945\n",
      "2019-04-10 00:04:21,279 root         INFO     Train Epoch: 58 [2048/8000 (26%)]\tTotal Loss: 0.185420\n",
      "Reconstruction: 0.116702, Regularization: 0.068719\n",
      "2019-04-10 00:04:21,340 root         INFO     Train Epoch: 58 [2560/8000 (32%)]\tTotal Loss: 0.188554\n",
      "Reconstruction: 0.115738, Regularization: 0.072816\n",
      "2019-04-10 00:04:21,401 root         INFO     Train Epoch: 58 [3072/8000 (38%)]\tTotal Loss: 0.173064\n",
      "Reconstruction: 0.101528, Regularization: 0.071537\n",
      "2019-04-10 00:04:21,463 root         INFO     Train Epoch: 58 [3584/8000 (45%)]\tTotal Loss: 0.171094\n",
      "Reconstruction: 0.110030, Regularization: 0.061064\n",
      "2019-04-10 00:04:21,524 root         INFO     Train Epoch: 58 [4096/8000 (51%)]\tTotal Loss: 0.178480\n",
      "Reconstruction: 0.122430, Regularization: 0.056050\n",
      "2019-04-10 00:04:21,585 root         INFO     Train Epoch: 58 [4608/8000 (58%)]\tTotal Loss: 0.174038\n",
      "Reconstruction: 0.105768, Regularization: 0.068270\n",
      "2019-04-10 00:04:21,647 root         INFO     Train Epoch: 58 [5120/8000 (64%)]\tTotal Loss: 0.168104\n",
      "Reconstruction: 0.097234, Regularization: 0.070869\n",
      "2019-04-10 00:04:21,708 root         INFO     Train Epoch: 58 [5632/8000 (70%)]\tTotal Loss: 0.181577\n",
      "Reconstruction: 0.121719, Regularization: 0.059858\n",
      "2019-04-10 00:04:21,769 root         INFO     Train Epoch: 58 [6144/8000 (77%)]\tTotal Loss: 0.184070\n",
      "Reconstruction: 0.116629, Regularization: 0.067442\n",
      "2019-04-10 00:04:21,830 root         INFO     Train Epoch: 58 [6656/8000 (83%)]\tTotal Loss: 0.166679\n",
      "Reconstruction: 0.109771, Regularization: 0.056908\n",
      "2019-04-10 00:04:21,891 root         INFO     Train Epoch: 58 [7168/8000 (90%)]\tTotal Loss: 0.171569\n",
      "Reconstruction: 0.108574, Regularization: 0.062995\n",
      "2019-04-10 00:04:21,952 root         INFO     Train Epoch: 58 [7680/8000 (96%)]\tTotal Loss: 0.192338\n",
      "Reconstruction: 0.123670, Regularization: 0.068667\n",
      "2019-04-10 00:04:22,005 root         INFO     ====> Epoch: 58 Average loss: 0.1778\n",
      "2019-04-10 00:04:22,029 root         INFO     Train Epoch: 59 [0/8000 (0%)]\tTotal Loss: 0.163019\n",
      "Reconstruction: 0.102636, Regularization: 0.060383\n",
      "2019-04-10 00:04:22,091 root         INFO     Train Epoch: 59 [512/8000 (6%)]\tTotal Loss: 0.168194\n",
      "Reconstruction: 0.106221, Regularization: 0.061973\n",
      "2019-04-10 00:04:22,152 root         INFO     Train Epoch: 59 [1024/8000 (13%)]\tTotal Loss: 0.176703\n",
      "Reconstruction: 0.120040, Regularization: 0.056663\n",
      "2019-04-10 00:04:22,212 root         INFO     Train Epoch: 59 [1536/8000 (19%)]\tTotal Loss: 0.174807\n",
      "Reconstruction: 0.111362, Regularization: 0.063445\n",
      "2019-04-10 00:04:22,274 root         INFO     Train Epoch: 59 [2048/8000 (26%)]\tTotal Loss: 0.169480\n",
      "Reconstruction: 0.117284, Regularization: 0.052196\n",
      "2019-04-10 00:04:22,336 root         INFO     Train Epoch: 59 [2560/8000 (32%)]\tTotal Loss: 0.162134\n",
      "Reconstruction: 0.100486, Regularization: 0.061648\n",
      "2019-04-10 00:04:22,396 root         INFO     Train Epoch: 59 [3072/8000 (38%)]\tTotal Loss: 0.165693\n",
      "Reconstruction: 0.108703, Regularization: 0.056991\n",
      "2019-04-10 00:04:22,457 root         INFO     Train Epoch: 59 [3584/8000 (45%)]\tTotal Loss: 0.170460\n",
      "Reconstruction: 0.105815, Regularization: 0.064645\n",
      "2019-04-10 00:04:22,517 root         INFO     Train Epoch: 59 [4096/8000 (51%)]\tTotal Loss: 0.164526\n",
      "Reconstruction: 0.112723, Regularization: 0.051804\n",
      "2019-04-10 00:04:22,578 root         INFO     Train Epoch: 59 [4608/8000 (58%)]\tTotal Loss: 0.160092\n",
      "Reconstruction: 0.113646, Regularization: 0.046446\n",
      "2019-04-10 00:04:22,639 root         INFO     Train Epoch: 59 [5120/8000 (64%)]\tTotal Loss: 0.171488\n",
      "Reconstruction: 0.113823, Regularization: 0.057665\n",
      "2019-04-10 00:04:22,700 root         INFO     Train Epoch: 59 [5632/8000 (70%)]\tTotal Loss: 0.188136\n",
      "Reconstruction: 0.128649, Regularization: 0.059487\n",
      "2019-04-10 00:04:22,761 root         INFO     Train Epoch: 59 [6144/8000 (77%)]\tTotal Loss: 0.181612\n",
      "Reconstruction: 0.126357, Regularization: 0.055256\n",
      "2019-04-10 00:04:22,823 root         INFO     Train Epoch: 59 [6656/8000 (83%)]\tTotal Loss: 0.151344\n",
      "Reconstruction: 0.105794, Regularization: 0.045549\n",
      "2019-04-10 00:04:22,884 root         INFO     Train Epoch: 59 [7168/8000 (90%)]\tTotal Loss: 0.154391\n",
      "Reconstruction: 0.099088, Regularization: 0.055303\n",
      "2019-04-10 00:04:22,945 root         INFO     Train Epoch: 59 [7680/8000 (96%)]\tTotal Loss: 0.163582\n",
      "Reconstruction: 0.108141, Regularization: 0.055440\n",
      "2019-04-10 00:04:22,998 root         INFO     ====> Epoch: 59 Average loss: 0.1709\n",
      "2019-04-10 00:04:23,022 root         INFO     Train Epoch: 60 [0/8000 (0%)]\tTotal Loss: 0.186455\n",
      "Reconstruction: 0.133017, Regularization: 0.053437\n",
      "2019-04-10 00:04:23,085 root         INFO     Train Epoch: 60 [512/8000 (6%)]\tTotal Loss: 0.149659\n",
      "Reconstruction: 0.105186, Regularization: 0.044474\n",
      "2019-04-10 00:04:23,148 root         INFO     Train Epoch: 60 [1024/8000 (13%)]\tTotal Loss: 0.169298\n",
      "Reconstruction: 0.105898, Regularization: 0.063400\n",
      "2019-04-10 00:04:23,210 root         INFO     Train Epoch: 60 [1536/8000 (19%)]\tTotal Loss: 0.156657\n",
      "Reconstruction: 0.114185, Regularization: 0.042472\n",
      "2019-04-10 00:04:23,273 root         INFO     Train Epoch: 60 [2048/8000 (26%)]\tTotal Loss: 0.170682\n",
      "Reconstruction: 0.113283, Regularization: 0.057399\n",
      "2019-04-10 00:04:23,335 root         INFO     Train Epoch: 60 [2560/8000 (32%)]\tTotal Loss: 0.152703\n",
      "Reconstruction: 0.103815, Regularization: 0.048888\n",
      "2019-04-10 00:04:23,398 root         INFO     Train Epoch: 60 [3072/8000 (38%)]\tTotal Loss: 0.148900\n",
      "Reconstruction: 0.113052, Regularization: 0.035848\n",
      "2019-04-10 00:04:23,460 root         INFO     Train Epoch: 60 [3584/8000 (45%)]\tTotal Loss: 0.164674\n",
      "Reconstruction: 0.108165, Regularization: 0.056509\n",
      "2019-04-10 00:04:23,522 root         INFO     Train Epoch: 60 [4096/8000 (51%)]\tTotal Loss: 0.157863\n",
      "Reconstruction: 0.115293, Regularization: 0.042570\n",
      "2019-04-10 00:04:23,584 root         INFO     Train Epoch: 60 [4608/8000 (58%)]\tTotal Loss: 0.161720\n",
      "Reconstruction: 0.113217, Regularization: 0.048504\n",
      "2019-04-10 00:04:23,646 root         INFO     Train Epoch: 60 [5120/8000 (64%)]\tTotal Loss: 0.151371\n",
      "Reconstruction: 0.108991, Regularization: 0.042380\n",
      "2019-04-10 00:04:23,708 root         INFO     Train Epoch: 60 [5632/8000 (70%)]\tTotal Loss: 0.168593\n",
      "Reconstruction: 0.115782, Regularization: 0.052811\n",
      "2019-04-10 00:04:23,770 root         INFO     Train Epoch: 60 [6144/8000 (77%)]\tTotal Loss: 0.144532\n",
      "Reconstruction: 0.105766, Regularization: 0.038766\n",
      "2019-04-10 00:04:23,832 root         INFO     Train Epoch: 60 [6656/8000 (83%)]\tTotal Loss: 0.160503\n",
      "Reconstruction: 0.120675, Regularization: 0.039827\n",
      "2019-04-10 00:04:23,894 root         INFO     Train Epoch: 60 [7168/8000 (90%)]\tTotal Loss: 0.163962\n",
      "Reconstruction: 0.118582, Regularization: 0.045381\n",
      "2019-04-10 00:04:23,956 root         INFO     Train Epoch: 60 [7680/8000 (96%)]\tTotal Loss: 0.172363\n",
      "Reconstruction: 0.121682, Regularization: 0.050681\n",
      "2019-04-10 00:04:24,009 root         INFO     ====> Epoch: 60 Average loss: 0.1639\n",
      "2019-04-10 00:04:24,033 root         INFO     Train Epoch: 61 [0/8000 (0%)]\tTotal Loss: 0.166821\n",
      "Reconstruction: 0.124870, Regularization: 0.041952\n",
      "2019-04-10 00:04:24,095 root         INFO     Train Epoch: 61 [512/8000 (6%)]\tTotal Loss: 0.176557\n",
      "Reconstruction: 0.120851, Regularization: 0.055706\n",
      "2019-04-10 00:04:24,156 root         INFO     Train Epoch: 61 [1024/8000 (13%)]\tTotal Loss: 0.164953\n",
      "Reconstruction: 0.121314, Regularization: 0.043639\n",
      "2019-04-10 00:04:24,217 root         INFO     Train Epoch: 61 [1536/8000 (19%)]\tTotal Loss: 0.135865\n",
      "Reconstruction: 0.100667, Regularization: 0.035198\n",
      "2019-04-10 00:04:24,278 root         INFO     Train Epoch: 61 [2048/8000 (26%)]\tTotal Loss: 0.159163\n",
      "Reconstruction: 0.113011, Regularization: 0.046153\n",
      "2019-04-10 00:04:24,339 root         INFO     Train Epoch: 61 [2560/8000 (32%)]\tTotal Loss: 0.141290\n",
      "Reconstruction: 0.101255, Regularization: 0.040036\n",
      "2019-04-10 00:04:24,400 root         INFO     Train Epoch: 61 [3072/8000 (38%)]\tTotal Loss: 0.141300\n",
      "Reconstruction: 0.106862, Regularization: 0.034438\n",
      "2019-04-10 00:04:24,461 root         INFO     Train Epoch: 61 [3584/8000 (45%)]\tTotal Loss: 0.157112\n",
      "Reconstruction: 0.113581, Regularization: 0.043531\n",
      "2019-04-10 00:04:24,522 root         INFO     Train Epoch: 61 [4096/8000 (51%)]\tTotal Loss: 0.166080\n",
      "Reconstruction: 0.119723, Regularization: 0.046357\n",
      "2019-04-10 00:04:24,584 root         INFO     Train Epoch: 61 [4608/8000 (58%)]\tTotal Loss: 0.168319\n",
      "Reconstruction: 0.126383, Regularization: 0.041936\n",
      "2019-04-10 00:04:24,645 root         INFO     Train Epoch: 61 [5120/8000 (64%)]\tTotal Loss: 0.152148\n",
      "Reconstruction: 0.110265, Regularization: 0.041883\n",
      "2019-04-10 00:04:24,707 root         INFO     Train Epoch: 61 [5632/8000 (70%)]\tTotal Loss: 0.145412\n",
      "Reconstruction: 0.108344, Regularization: 0.037068\n",
      "2019-04-10 00:04:24,768 root         INFO     Train Epoch: 61 [6144/8000 (77%)]\tTotal Loss: 0.155655\n",
      "Reconstruction: 0.116551, Regularization: 0.039104\n",
      "2019-04-10 00:04:24,829 root         INFO     Train Epoch: 61 [6656/8000 (83%)]\tTotal Loss: 0.157613\n",
      "Reconstruction: 0.117819, Regularization: 0.039794\n",
      "2019-04-10 00:04:24,891 root         INFO     Train Epoch: 61 [7168/8000 (90%)]\tTotal Loss: 0.144429\n",
      "Reconstruction: 0.112380, Regularization: 0.032049\n",
      "2019-04-10 00:04:24,952 root         INFO     Train Epoch: 61 [7680/8000 (96%)]\tTotal Loss: 0.162856\n",
      "Reconstruction: 0.114352, Regularization: 0.048505\n",
      "2019-04-10 00:04:25,005 root         INFO     ====> Epoch: 61 Average loss: 0.1587\n",
      "2019-04-10 00:04:25,028 root         INFO     Train Epoch: 62 [0/8000 (0%)]\tTotal Loss: 0.154191\n",
      "Reconstruction: 0.117012, Regularization: 0.037179\n",
      "2019-04-10 00:04:25,091 root         INFO     Train Epoch: 62 [512/8000 (6%)]\tTotal Loss: 0.149669\n",
      "Reconstruction: 0.105564, Regularization: 0.044105\n",
      "2019-04-10 00:04:25,152 root         INFO     Train Epoch: 62 [1024/8000 (13%)]\tTotal Loss: 0.164171\n",
      "Reconstruction: 0.121769, Regularization: 0.042402\n",
      "2019-04-10 00:04:25,214 root         INFO     Train Epoch: 62 [1536/8000 (19%)]\tTotal Loss: 0.142842\n",
      "Reconstruction: 0.103420, Regularization: 0.039422\n",
      "2019-04-10 00:04:25,276 root         INFO     Train Epoch: 62 [2048/8000 (26%)]\tTotal Loss: 0.142853\n",
      "Reconstruction: 0.106783, Regularization: 0.036070\n",
      "2019-04-10 00:04:25,338 root         INFO     Train Epoch: 62 [2560/8000 (32%)]\tTotal Loss: 0.159350\n",
      "Reconstruction: 0.115445, Regularization: 0.043905\n",
      "2019-04-10 00:04:25,401 root         INFO     Train Epoch: 62 [3072/8000 (38%)]\tTotal Loss: 0.166391\n",
      "Reconstruction: 0.128331, Regularization: 0.038060\n",
      "2019-04-10 00:04:25,462 root         INFO     Train Epoch: 62 [3584/8000 (45%)]\tTotal Loss: 0.161029\n",
      "Reconstruction: 0.111259, Regularization: 0.049770\n",
      "2019-04-10 00:04:25,524 root         INFO     Train Epoch: 62 [4096/8000 (51%)]\tTotal Loss: 0.160233\n",
      "Reconstruction: 0.112686, Regularization: 0.047547\n",
      "2019-04-10 00:04:25,586 root         INFO     Train Epoch: 62 [4608/8000 (58%)]\tTotal Loss: 0.149512\n",
      "Reconstruction: 0.113144, Regularization: 0.036368\n",
      "2019-04-10 00:04:25,647 root         INFO     Train Epoch: 62 [5120/8000 (64%)]\tTotal Loss: 0.163403\n",
      "Reconstruction: 0.117508, Regularization: 0.045895\n",
      "2019-04-10 00:04:25,710 root         INFO     Train Epoch: 62 [5632/8000 (70%)]\tTotal Loss: 0.149861\n",
      "Reconstruction: 0.115402, Regularization: 0.034459\n",
      "2019-04-10 00:04:25,772 root         INFO     Train Epoch: 62 [6144/8000 (77%)]\tTotal Loss: 0.145957\n",
      "Reconstruction: 0.111267, Regularization: 0.034690\n",
      "2019-04-10 00:04:25,834 root         INFO     Train Epoch: 62 [6656/8000 (83%)]\tTotal Loss: 0.174978\n",
      "Reconstruction: 0.134998, Regularization: 0.039979\n",
      "2019-04-10 00:04:25,895 root         INFO     Train Epoch: 62 [7168/8000 (90%)]\tTotal Loss: 0.154931\n",
      "Reconstruction: 0.115910, Regularization: 0.039022\n",
      "2019-04-10 00:04:25,957 root         INFO     Train Epoch: 62 [7680/8000 (96%)]\tTotal Loss: 0.150394\n",
      "Reconstruction: 0.118717, Regularization: 0.031678\n",
      "2019-04-10 00:04:26,010 root         INFO     ====> Epoch: 62 Average loss: 0.1536\n",
      "2019-04-10 00:04:26,035 root         INFO     Train Epoch: 63 [0/8000 (0%)]\tTotal Loss: 0.148099\n",
      "Reconstruction: 0.117159, Regularization: 0.030940\n",
      "2019-04-10 00:04:26,097 root         INFO     Train Epoch: 63 [512/8000 (6%)]\tTotal Loss: 0.170347\n",
      "Reconstruction: 0.127661, Regularization: 0.042686\n",
      "2019-04-10 00:04:26,159 root         INFO     Train Epoch: 63 [1024/8000 (13%)]\tTotal Loss: 0.149936\n",
      "Reconstruction: 0.118237, Regularization: 0.031698\n",
      "2019-04-10 00:04:26,222 root         INFO     Train Epoch: 63 [1536/8000 (19%)]\tTotal Loss: 0.143920\n",
      "Reconstruction: 0.113217, Regularization: 0.030703\n",
      "2019-04-10 00:04:26,284 root         INFO     Train Epoch: 63 [2048/8000 (26%)]\tTotal Loss: 0.175753\n",
      "Reconstruction: 0.130184, Regularization: 0.045569\n",
      "2019-04-10 00:04:26,347 root         INFO     Train Epoch: 63 [2560/8000 (32%)]\tTotal Loss: 0.143705\n",
      "Reconstruction: 0.114573, Regularization: 0.029132\n",
      "2019-04-10 00:04:26,409 root         INFO     Train Epoch: 63 [3072/8000 (38%)]\tTotal Loss: 0.153184\n",
      "Reconstruction: 0.115553, Regularization: 0.037631\n",
      "2019-04-10 00:04:26,471 root         INFO     Train Epoch: 63 [3584/8000 (45%)]\tTotal Loss: 0.151684\n",
      "Reconstruction: 0.116004, Regularization: 0.035680\n",
      "2019-04-10 00:04:26,532 root         INFO     Train Epoch: 63 [4096/8000 (51%)]\tTotal Loss: 0.139022\n",
      "Reconstruction: 0.117905, Regularization: 0.021117\n",
      "2019-04-10 00:04:26,594 root         INFO     Train Epoch: 63 [4608/8000 (58%)]\tTotal Loss: 0.143606\n",
      "Reconstruction: 0.107964, Regularization: 0.035642\n",
      "2019-04-10 00:04:26,655 root         INFO     Train Epoch: 63 [5120/8000 (64%)]\tTotal Loss: 0.150803\n",
      "Reconstruction: 0.112401, Regularization: 0.038402\n",
      "2019-04-10 00:04:26,717 root         INFO     Train Epoch: 63 [5632/8000 (70%)]\tTotal Loss: 0.134122\n",
      "Reconstruction: 0.109859, Regularization: 0.024263\n",
      "2019-04-10 00:04:26,778 root         INFO     Train Epoch: 63 [6144/8000 (77%)]\tTotal Loss: 0.151709\n",
      "Reconstruction: 0.125338, Regularization: 0.026371\n",
      "2019-04-10 00:04:26,840 root         INFO     Train Epoch: 63 [6656/8000 (83%)]\tTotal Loss: 0.161336\n",
      "Reconstruction: 0.123474, Regularization: 0.037862\n",
      "2019-04-10 00:04:26,902 root         INFO     Train Epoch: 63 [7168/8000 (90%)]\tTotal Loss: 0.132684\n",
      "Reconstruction: 0.109303, Regularization: 0.023382\n",
      "2019-04-10 00:04:26,964 root         INFO     Train Epoch: 63 [7680/8000 (96%)]\tTotal Loss: 0.160705\n",
      "Reconstruction: 0.131309, Regularization: 0.029396\n",
      "2019-04-10 00:04:27,018 root         INFO     ====> Epoch: 63 Average loss: 0.1487\n",
      "2019-04-10 00:04:27,042 root         INFO     Train Epoch: 64 [0/8000 (0%)]\tTotal Loss: 0.155909\n",
      "Reconstruction: 0.126681, Regularization: 0.029228\n",
      "2019-04-10 00:04:27,104 root         INFO     Train Epoch: 64 [512/8000 (6%)]\tTotal Loss: 0.150878\n",
      "Reconstruction: 0.115140, Regularization: 0.035738\n",
      "2019-04-10 00:04:27,166 root         INFO     Train Epoch: 64 [1024/8000 (13%)]\tTotal Loss: 0.156739\n",
      "Reconstruction: 0.117553, Regularization: 0.039187\n",
      "2019-04-10 00:04:27,228 root         INFO     Train Epoch: 64 [1536/8000 (19%)]\tTotal Loss: 0.141503\n",
      "Reconstruction: 0.120381, Regularization: 0.021122\n",
      "2019-04-10 00:04:27,290 root         INFO     Train Epoch: 64 [2048/8000 (26%)]\tTotal Loss: 0.148621\n",
      "Reconstruction: 0.125441, Regularization: 0.023180\n",
      "2019-04-10 00:04:27,352 root         INFO     Train Epoch: 64 [2560/8000 (32%)]\tTotal Loss: 0.140774\n",
      "Reconstruction: 0.116080, Regularization: 0.024694\n",
      "2019-04-10 00:04:27,414 root         INFO     Train Epoch: 64 [3072/8000 (38%)]\tTotal Loss: 0.137101\n",
      "Reconstruction: 0.112512, Regularization: 0.024589\n",
      "2019-04-10 00:04:27,476 root         INFO     Train Epoch: 64 [3584/8000 (45%)]\tTotal Loss: 0.158335\n",
      "Reconstruction: 0.122425, Regularization: 0.035910\n",
      "2019-04-10 00:04:27,538 root         INFO     Train Epoch: 64 [4096/8000 (51%)]\tTotal Loss: 0.130302\n",
      "Reconstruction: 0.111428, Regularization: 0.018874\n",
      "2019-04-10 00:04:27,600 root         INFO     Train Epoch: 64 [4608/8000 (58%)]\tTotal Loss: 0.136945\n",
      "Reconstruction: 0.115266, Regularization: 0.021679\n",
      "2019-04-10 00:04:27,662 root         INFO     Train Epoch: 64 [5120/8000 (64%)]\tTotal Loss: 0.148251\n",
      "Reconstruction: 0.114155, Regularization: 0.034096\n",
      "2019-04-10 00:04:27,724 root         INFO     Train Epoch: 64 [5632/8000 (70%)]\tTotal Loss: 0.141875\n",
      "Reconstruction: 0.112145, Regularization: 0.029730\n",
      "2019-04-10 00:04:27,786 root         INFO     Train Epoch: 64 [6144/8000 (77%)]\tTotal Loss: 0.141903\n",
      "Reconstruction: 0.114790, Regularization: 0.027113\n",
      "2019-04-10 00:04:27,848 root         INFO     Train Epoch: 64 [6656/8000 (83%)]\tTotal Loss: 0.154015\n",
      "Reconstruction: 0.131035, Regularization: 0.022979\n",
      "2019-04-10 00:04:27,910 root         INFO     Train Epoch: 64 [7168/8000 (90%)]\tTotal Loss: 0.143213\n",
      "Reconstruction: 0.107877, Regularization: 0.035336\n",
      "2019-04-10 00:04:27,972 root         INFO     Train Epoch: 64 [7680/8000 (96%)]\tTotal Loss: 0.136044\n",
      "Reconstruction: 0.109723, Regularization: 0.026321\n",
      "2019-04-10 00:04:28,025 root         INFO     ====> Epoch: 64 Average loss: 0.1442\n",
      "2019-04-10 00:04:28,048 root         INFO     Train Epoch: 65 [0/8000 (0%)]\tTotal Loss: 0.137789\n",
      "Reconstruction: 0.115948, Regularization: 0.021840\n",
      "2019-04-10 00:04:28,110 root         INFO     Train Epoch: 65 [512/8000 (6%)]\tTotal Loss: 0.141071\n",
      "Reconstruction: 0.114300, Regularization: 0.026771\n",
      "2019-04-10 00:04:28,171 root         INFO     Train Epoch: 65 [1024/8000 (13%)]\tTotal Loss: 0.140252\n",
      "Reconstruction: 0.111571, Regularization: 0.028680\n",
      "2019-04-10 00:04:28,232 root         INFO     Train Epoch: 65 [1536/8000 (19%)]\tTotal Loss: 0.152501\n",
      "Reconstruction: 0.137241, Regularization: 0.015260\n",
      "2019-04-10 00:04:28,292 root         INFO     Train Epoch: 65 [2048/8000 (26%)]\tTotal Loss: 0.136249\n",
      "Reconstruction: 0.111883, Regularization: 0.024366\n",
      "2019-04-10 00:04:28,353 root         INFO     Train Epoch: 65 [2560/8000 (32%)]\tTotal Loss: 0.137050\n",
      "Reconstruction: 0.112414, Regularization: 0.024636\n",
      "2019-04-10 00:04:28,414 root         INFO     Train Epoch: 65 [3072/8000 (38%)]\tTotal Loss: 0.161186\n",
      "Reconstruction: 0.130760, Regularization: 0.030426\n",
      "2019-04-10 00:04:28,475 root         INFO     Train Epoch: 65 [3584/8000 (45%)]\tTotal Loss: 0.136622\n",
      "Reconstruction: 0.111326, Regularization: 0.025296\n",
      "2019-04-10 00:04:28,536 root         INFO     Train Epoch: 65 [4096/8000 (51%)]\tTotal Loss: 0.139892\n",
      "Reconstruction: 0.118542, Regularization: 0.021349\n",
      "2019-04-10 00:04:28,598 root         INFO     Train Epoch: 65 [4608/8000 (58%)]\tTotal Loss: 0.155810\n",
      "Reconstruction: 0.135815, Regularization: 0.019995\n",
      "2019-04-10 00:04:28,658 root         INFO     Train Epoch: 65 [5120/8000 (64%)]\tTotal Loss: 0.140588\n",
      "Reconstruction: 0.119369, Regularization: 0.021219\n",
      "2019-04-10 00:04:28,720 root         INFO     Train Epoch: 65 [5632/8000 (70%)]\tTotal Loss: 0.134578\n",
      "Reconstruction: 0.116282, Regularization: 0.018296\n",
      "2019-04-10 00:04:28,781 root         INFO     Train Epoch: 65 [6144/8000 (77%)]\tTotal Loss: 0.154609\n",
      "Reconstruction: 0.126180, Regularization: 0.028429\n",
      "2019-04-10 00:04:28,843 root         INFO     Train Epoch: 65 [6656/8000 (83%)]\tTotal Loss: 0.147011\n",
      "Reconstruction: 0.121488, Regularization: 0.025523\n",
      "2019-04-10 00:04:28,904 root         INFO     Train Epoch: 65 [7168/8000 (90%)]\tTotal Loss: 0.145599\n",
      "Reconstruction: 0.124352, Regularization: 0.021247\n",
      "2019-04-10 00:04:28,966 root         INFO     Train Epoch: 65 [7680/8000 (96%)]\tTotal Loss: 0.147983\n",
      "Reconstruction: 0.126363, Regularization: 0.021620\n",
      "2019-04-10 00:04:29,018 root         INFO     ====> Epoch: 65 Average loss: 0.1409\n",
      "2019-04-10 00:04:29,043 root         INFO     Train Epoch: 66 [0/8000 (0%)]\tTotal Loss: 0.142146\n",
      "Reconstruction: 0.123920, Regularization: 0.018226\n",
      "2019-04-10 00:04:29,106 root         INFO     Train Epoch: 66 [512/8000 (6%)]\tTotal Loss: 0.124967\n",
      "Reconstruction: 0.105222, Regularization: 0.019745\n",
      "2019-04-10 00:04:29,168 root         INFO     Train Epoch: 66 [1024/8000 (13%)]\tTotal Loss: 0.149179\n",
      "Reconstruction: 0.125192, Regularization: 0.023987\n",
      "2019-04-10 00:04:29,230 root         INFO     Train Epoch: 66 [1536/8000 (19%)]\tTotal Loss: 0.130719\n",
      "Reconstruction: 0.114799, Regularization: 0.015920\n",
      "2019-04-10 00:04:29,292 root         INFO     Train Epoch: 66 [2048/8000 (26%)]\tTotal Loss: 0.142970\n",
      "Reconstruction: 0.115628, Regularization: 0.027342\n",
      "2019-04-10 00:04:29,354 root         INFO     Train Epoch: 66 [2560/8000 (32%)]\tTotal Loss: 0.132396\n",
      "Reconstruction: 0.110091, Regularization: 0.022305\n",
      "2019-04-10 00:04:29,416 root         INFO     Train Epoch: 66 [3072/8000 (38%)]\tTotal Loss: 0.130341\n",
      "Reconstruction: 0.106302, Regularization: 0.024038\n",
      "2019-04-10 00:04:29,478 root         INFO     Train Epoch: 66 [3584/8000 (45%)]\tTotal Loss: 0.127946\n",
      "Reconstruction: 0.106245, Regularization: 0.021700\n",
      "2019-04-10 00:04:29,540 root         INFO     Train Epoch: 66 [4096/8000 (51%)]\tTotal Loss: 0.138737\n",
      "Reconstruction: 0.113445, Regularization: 0.025292\n",
      "2019-04-10 00:04:29,602 root         INFO     Train Epoch: 66 [4608/8000 (58%)]\tTotal Loss: 0.142487\n",
      "Reconstruction: 0.121317, Regularization: 0.021169\n",
      "2019-04-10 00:04:29,663 root         INFO     Train Epoch: 66 [5120/8000 (64%)]\tTotal Loss: 0.127984\n",
      "Reconstruction: 0.108092, Regularization: 0.019891\n",
      "2019-04-10 00:04:29,725 root         INFO     Train Epoch: 66 [5632/8000 (70%)]\tTotal Loss: 0.128562\n",
      "Reconstruction: 0.109853, Regularization: 0.018709\n",
      "2019-04-10 00:04:29,787 root         INFO     Train Epoch: 66 [6144/8000 (77%)]\tTotal Loss: 0.142932\n",
      "Reconstruction: 0.115650, Regularization: 0.027282\n",
      "2019-04-10 00:04:29,849 root         INFO     Train Epoch: 66 [6656/8000 (83%)]\tTotal Loss: 0.152723\n",
      "Reconstruction: 0.127576, Regularization: 0.025147\n",
      "2019-04-10 00:04:29,910 root         INFO     Train Epoch: 66 [7168/8000 (90%)]\tTotal Loss: 0.134337\n",
      "Reconstruction: 0.111828, Regularization: 0.022509\n",
      "2019-04-10 00:04:29,972 root         INFO     Train Epoch: 66 [7680/8000 (96%)]\tTotal Loss: 0.131935\n",
      "Reconstruction: 0.108545, Regularization: 0.023390\n",
      "2019-04-10 00:04:30,026 root         INFO     ====> Epoch: 66 Average loss: 0.1371\n",
      "2019-04-10 00:04:30,050 root         INFO     Train Epoch: 67 [0/8000 (0%)]\tTotal Loss: 0.139055\n",
      "Reconstruction: 0.117734, Regularization: 0.021321\n",
      "2019-04-10 00:04:30,110 root         INFO     Train Epoch: 67 [512/8000 (6%)]\tTotal Loss: 0.148253\n",
      "Reconstruction: 0.124763, Regularization: 0.023490\n",
      "2019-04-10 00:04:30,171 root         INFO     Train Epoch: 67 [1024/8000 (13%)]\tTotal Loss: 0.129919\n",
      "Reconstruction: 0.109339, Regularization: 0.020580\n",
      "2019-04-10 00:04:30,231 root         INFO     Train Epoch: 67 [1536/8000 (19%)]\tTotal Loss: 0.128288\n",
      "Reconstruction: 0.106950, Regularization: 0.021338\n",
      "2019-04-10 00:04:30,292 root         INFO     Train Epoch: 67 [2048/8000 (26%)]\tTotal Loss: 0.124592\n",
      "Reconstruction: 0.110720, Regularization: 0.013872\n",
      "2019-04-10 00:04:30,352 root         INFO     Train Epoch: 67 [2560/8000 (32%)]\tTotal Loss: 0.132760\n",
      "Reconstruction: 0.115549, Regularization: 0.017211\n",
      "2019-04-10 00:04:30,414 root         INFO     Train Epoch: 67 [3072/8000 (38%)]\tTotal Loss: 0.131865\n",
      "Reconstruction: 0.114453, Regularization: 0.017412\n",
      "2019-04-10 00:04:30,475 root         INFO     Train Epoch: 67 [3584/8000 (45%)]\tTotal Loss: 0.138690\n",
      "Reconstruction: 0.123461, Regularization: 0.015229\n",
      "2019-04-10 00:04:30,536 root         INFO     Train Epoch: 67 [4096/8000 (51%)]\tTotal Loss: 0.139316\n",
      "Reconstruction: 0.121069, Regularization: 0.018247\n",
      "2019-04-10 00:04:30,596 root         INFO     Train Epoch: 67 [4608/8000 (58%)]\tTotal Loss: 0.122683\n",
      "Reconstruction: 0.107890, Regularization: 0.014794\n",
      "2019-04-10 00:04:30,657 root         INFO     Train Epoch: 67 [5120/8000 (64%)]\tTotal Loss: 0.124560\n",
      "Reconstruction: 0.111050, Regularization: 0.013510\n",
      "2019-04-10 00:04:30,717 root         INFO     Train Epoch: 67 [5632/8000 (70%)]\tTotal Loss: 0.124880\n",
      "Reconstruction: 0.106477, Regularization: 0.018403\n",
      "2019-04-10 00:04:30,777 root         INFO     Train Epoch: 67 [6144/8000 (77%)]\tTotal Loss: 0.128243\n",
      "Reconstruction: 0.113933, Regularization: 0.014310\n",
      "2019-04-10 00:04:30,837 root         INFO     Train Epoch: 67 [6656/8000 (83%)]\tTotal Loss: 0.124347\n",
      "Reconstruction: 0.108128, Regularization: 0.016220\n",
      "2019-04-10 00:04:30,898 root         INFO     Train Epoch: 67 [7168/8000 (90%)]\tTotal Loss: 0.136240\n",
      "Reconstruction: 0.122452, Regularization: 0.013788\n",
      "2019-04-10 00:04:30,959 root         INFO     Train Epoch: 67 [7680/8000 (96%)]\tTotal Loss: 0.123128\n",
      "Reconstruction: 0.106962, Regularization: 0.016166\n",
      "2019-04-10 00:04:31,011 root         INFO     ====> Epoch: 67 Average loss: 0.1345\n",
      "2019-04-10 00:04:31,035 root         INFO     Train Epoch: 68 [0/8000 (0%)]\tTotal Loss: 0.133288\n",
      "Reconstruction: 0.116712, Regularization: 0.016576\n",
      "2019-04-10 00:04:31,098 root         INFO     Train Epoch: 68 [512/8000 (6%)]\tTotal Loss: 0.122344\n",
      "Reconstruction: 0.108298, Regularization: 0.014046\n",
      "2019-04-10 00:04:31,160 root         INFO     Train Epoch: 68 [1024/8000 (13%)]\tTotal Loss: 0.130886\n",
      "Reconstruction: 0.114203, Regularization: 0.016683\n",
      "2019-04-10 00:04:31,222 root         INFO     Train Epoch: 68 [1536/8000 (19%)]\tTotal Loss: 0.138945\n",
      "Reconstruction: 0.124342, Regularization: 0.014604\n",
      "2019-04-10 00:04:31,284 root         INFO     Train Epoch: 68 [2048/8000 (26%)]\tTotal Loss: 0.128443\n",
      "Reconstruction: 0.112837, Regularization: 0.015606\n",
      "2019-04-10 00:04:31,345 root         INFO     Train Epoch: 68 [2560/8000 (32%)]\tTotal Loss: 0.139919\n",
      "Reconstruction: 0.125780, Regularization: 0.014139\n",
      "2019-04-10 00:04:31,407 root         INFO     Train Epoch: 68 [3072/8000 (38%)]\tTotal Loss: 0.153199\n",
      "Reconstruction: 0.136981, Regularization: 0.016218\n",
      "2019-04-10 00:04:31,469 root         INFO     Train Epoch: 68 [3584/8000 (45%)]\tTotal Loss: 0.127049\n",
      "Reconstruction: 0.113938, Regularization: 0.013111\n",
      "2019-04-10 00:04:31,531 root         INFO     Train Epoch: 68 [4096/8000 (51%)]\tTotal Loss: 0.126913\n",
      "Reconstruction: 0.112496, Regularization: 0.014416\n",
      "2019-04-10 00:04:31,593 root         INFO     Train Epoch: 68 [4608/8000 (58%)]\tTotal Loss: 0.147939\n",
      "Reconstruction: 0.132902, Regularization: 0.015037\n",
      "2019-04-10 00:04:31,655 root         INFO     Train Epoch: 68 [5120/8000 (64%)]\tTotal Loss: 0.120866\n",
      "Reconstruction: 0.111152, Regularization: 0.009715\n",
      "2019-04-10 00:04:31,717 root         INFO     Train Epoch: 68 [5632/8000 (70%)]\tTotal Loss: 0.117363\n",
      "Reconstruction: 0.106170, Regularization: 0.011194\n",
      "2019-04-10 00:04:31,779 root         INFO     Train Epoch: 68 [6144/8000 (77%)]\tTotal Loss: 0.121891\n",
      "Reconstruction: 0.109739, Regularization: 0.012152\n",
      "2019-04-10 00:04:31,841 root         INFO     Train Epoch: 68 [6656/8000 (83%)]\tTotal Loss: 0.123364\n",
      "Reconstruction: 0.111296, Regularization: 0.012067\n",
      "2019-04-10 00:04:31,903 root         INFO     Train Epoch: 68 [7168/8000 (90%)]\tTotal Loss: 0.126610\n",
      "Reconstruction: 0.118978, Regularization: 0.007633\n",
      "2019-04-10 00:04:31,965 root         INFO     Train Epoch: 68 [7680/8000 (96%)]\tTotal Loss: 0.127486\n",
      "Reconstruction: 0.111048, Regularization: 0.016437\n",
      "2019-04-10 00:04:32,018 root         INFO     ====> Epoch: 68 Average loss: 0.1317\n",
      "2019-04-10 00:04:32,042 root         INFO     Train Epoch: 69 [0/8000 (0%)]\tTotal Loss: 0.125180\n",
      "Reconstruction: 0.115108, Regularization: 0.010072\n",
      "2019-04-10 00:04:32,103 root         INFO     Train Epoch: 69 [512/8000 (6%)]\tTotal Loss: 0.114857\n",
      "Reconstruction: 0.103675, Regularization: 0.011182\n",
      "2019-04-10 00:04:32,164 root         INFO     Train Epoch: 69 [1024/8000 (13%)]\tTotal Loss: 0.134737\n",
      "Reconstruction: 0.122898, Regularization: 0.011839\n",
      "2019-04-10 00:04:32,226 root         INFO     Train Epoch: 69 [1536/8000 (19%)]\tTotal Loss: 0.133891\n",
      "Reconstruction: 0.120402, Regularization: 0.013488\n",
      "2019-04-10 00:04:32,288 root         INFO     Train Epoch: 69 [2048/8000 (26%)]\tTotal Loss: 0.125478\n",
      "Reconstruction: 0.108541, Regularization: 0.016937\n",
      "2019-04-10 00:04:32,350 root         INFO     Train Epoch: 69 [2560/8000 (32%)]\tTotal Loss: 0.130081\n",
      "Reconstruction: 0.114743, Regularization: 0.015338\n",
      "2019-04-10 00:04:32,413 root         INFO     Train Epoch: 69 [3072/8000 (38%)]\tTotal Loss: 0.155461\n",
      "Reconstruction: 0.141158, Regularization: 0.014303\n",
      "2019-04-10 00:04:32,474 root         INFO     Train Epoch: 69 [3584/8000 (45%)]\tTotal Loss: 0.131750\n",
      "Reconstruction: 0.121124, Regularization: 0.010625\n",
      "2019-04-10 00:04:32,536 root         INFO     Train Epoch: 69 [4096/8000 (51%)]\tTotal Loss: 0.129405\n",
      "Reconstruction: 0.113513, Regularization: 0.015893\n",
      "2019-04-10 00:04:32,598 root         INFO     Train Epoch: 69 [4608/8000 (58%)]\tTotal Loss: 0.127793\n",
      "Reconstruction: 0.114855, Regularization: 0.012938\n",
      "2019-04-10 00:04:32,659 root         INFO     Train Epoch: 69 [5120/8000 (64%)]\tTotal Loss: 0.117524\n",
      "Reconstruction: 0.103918, Regularization: 0.013606\n",
      "2019-04-10 00:04:32,721 root         INFO     Train Epoch: 69 [5632/8000 (70%)]\tTotal Loss: 0.129098\n",
      "Reconstruction: 0.117705, Regularization: 0.011392\n",
      "2019-04-10 00:04:32,783 root         INFO     Train Epoch: 69 [6144/8000 (77%)]\tTotal Loss: 0.118799\n",
      "Reconstruction: 0.109908, Regularization: 0.008891\n",
      "2019-04-10 00:04:32,845 root         INFO     Train Epoch: 69 [6656/8000 (83%)]\tTotal Loss: 0.134422\n",
      "Reconstruction: 0.122228, Regularization: 0.012194\n",
      "2019-04-10 00:04:32,907 root         INFO     Train Epoch: 69 [7168/8000 (90%)]\tTotal Loss: 0.124878\n",
      "Reconstruction: 0.114684, Regularization: 0.010194\n",
      "2019-04-10 00:04:32,969 root         INFO     Train Epoch: 69 [7680/8000 (96%)]\tTotal Loss: 0.126632\n",
      "Reconstruction: 0.118155, Regularization: 0.008478\n",
      "2019-04-10 00:04:33,022 root         INFO     ====> Epoch: 69 Average loss: 0.1293\n",
      "2019-04-10 00:04:33,046 root         INFO     Train Epoch: 70 [0/8000 (0%)]\tTotal Loss: 0.128192\n",
      "Reconstruction: 0.118093, Regularization: 0.010098\n",
      "2019-04-10 00:04:33,108 root         INFO     Train Epoch: 70 [512/8000 (6%)]\tTotal Loss: 0.124852\n",
      "Reconstruction: 0.113048, Regularization: 0.011804\n",
      "2019-04-10 00:04:33,169 root         INFO     Train Epoch: 70 [1024/8000 (13%)]\tTotal Loss: 0.130196\n",
      "Reconstruction: 0.117063, Regularization: 0.013133\n",
      "2019-04-10 00:04:33,231 root         INFO     Train Epoch: 70 [1536/8000 (19%)]\tTotal Loss: 0.133006\n",
      "Reconstruction: 0.125102, Regularization: 0.007905\n",
      "2019-04-10 00:04:33,292 root         INFO     Train Epoch: 70 [2048/8000 (26%)]\tTotal Loss: 0.115501\n",
      "Reconstruction: 0.107072, Regularization: 0.008429\n",
      "2019-04-10 00:04:33,354 root         INFO     Train Epoch: 70 [2560/8000 (32%)]\tTotal Loss: 0.131331\n",
      "Reconstruction: 0.120937, Regularization: 0.010394\n",
      "2019-04-10 00:04:33,415 root         INFO     Train Epoch: 70 [3072/8000 (38%)]\tTotal Loss: 0.129428\n",
      "Reconstruction: 0.116651, Regularization: 0.012777\n",
      "2019-04-10 00:04:33,477 root         INFO     Train Epoch: 70 [3584/8000 (45%)]\tTotal Loss: 0.135005\n",
      "Reconstruction: 0.121230, Regularization: 0.013775\n",
      "2019-04-10 00:04:33,539 root         INFO     Train Epoch: 70 [4096/8000 (51%)]\tTotal Loss: 0.128317\n",
      "Reconstruction: 0.116393, Regularization: 0.011925\n",
      "2019-04-10 00:04:33,600 root         INFO     Train Epoch: 70 [4608/8000 (58%)]\tTotal Loss: 0.130006\n",
      "Reconstruction: 0.117856, Regularization: 0.012151\n",
      "2019-04-10 00:04:33,662 root         INFO     Train Epoch: 70 [5120/8000 (64%)]\tTotal Loss: 0.117008\n",
      "Reconstruction: 0.107030, Regularization: 0.009978\n",
      "2019-04-10 00:04:33,723 root         INFO     Train Epoch: 70 [5632/8000 (70%)]\tTotal Loss: 0.117905\n",
      "Reconstruction: 0.109749, Regularization: 0.008157\n",
      "2019-04-10 00:04:33,784 root         INFO     Train Epoch: 70 [6144/8000 (77%)]\tTotal Loss: 0.121069\n",
      "Reconstruction: 0.109507, Regularization: 0.011562\n",
      "2019-04-10 00:04:33,845 root         INFO     Train Epoch: 70 [6656/8000 (83%)]\tTotal Loss: 0.111452\n",
      "Reconstruction: 0.098977, Regularization: 0.012475\n",
      "2019-04-10 00:04:33,907 root         INFO     Train Epoch: 70 [7168/8000 (90%)]\tTotal Loss: 0.125969\n",
      "Reconstruction: 0.116710, Regularization: 0.009259\n",
      "2019-04-10 00:04:33,969 root         INFO     Train Epoch: 70 [7680/8000 (96%)]\tTotal Loss: 0.132698\n",
      "Reconstruction: 0.122771, Regularization: 0.009928\n",
      "2019-04-10 00:04:34,021 root         INFO     ====> Epoch: 70 Average loss: 0.1264\n",
      "2019-04-10 00:04:34,045 root         INFO     Train Epoch: 71 [0/8000 (0%)]\tTotal Loss: 0.136910\n",
      "Reconstruction: 0.119792, Regularization: 0.017119\n",
      "2019-04-10 00:04:34,107 root         INFO     Train Epoch: 71 [512/8000 (6%)]\tTotal Loss: 0.126371\n",
      "Reconstruction: 0.116842, Regularization: 0.009529\n",
      "2019-04-10 00:04:34,168 root         INFO     Train Epoch: 71 [1024/8000 (13%)]\tTotal Loss: 0.126609\n",
      "Reconstruction: 0.118233, Regularization: 0.008376\n",
      "2019-04-10 00:04:34,229 root         INFO     Train Epoch: 71 [1536/8000 (19%)]\tTotal Loss: 0.130241\n",
      "Reconstruction: 0.121411, Regularization: 0.008830\n",
      "2019-04-10 00:04:34,290 root         INFO     Train Epoch: 71 [2048/8000 (26%)]\tTotal Loss: 0.130225\n",
      "Reconstruction: 0.120095, Regularization: 0.010130\n",
      "2019-04-10 00:04:34,352 root         INFO     Train Epoch: 71 [2560/8000 (32%)]\tTotal Loss: 0.121792\n",
      "Reconstruction: 0.111708, Regularization: 0.010084\n",
      "2019-04-10 00:04:34,413 root         INFO     Train Epoch: 71 [3072/8000 (38%)]\tTotal Loss: 0.127654\n",
      "Reconstruction: 0.116292, Regularization: 0.011362\n",
      "2019-04-10 00:04:34,474 root         INFO     Train Epoch: 71 [3584/8000 (45%)]\tTotal Loss: 0.114695\n",
      "Reconstruction: 0.106899, Regularization: 0.007797\n",
      "2019-04-10 00:04:34,536 root         INFO     Train Epoch: 71 [4096/8000 (51%)]\tTotal Loss: 0.119934\n",
      "Reconstruction: 0.113710, Regularization: 0.006224\n",
      "2019-04-10 00:04:34,597 root         INFO     Train Epoch: 71 [4608/8000 (58%)]\tTotal Loss: 0.123471\n",
      "Reconstruction: 0.113463, Regularization: 0.010007\n",
      "2019-04-10 00:04:34,658 root         INFO     Train Epoch: 71 [5120/8000 (64%)]\tTotal Loss: 0.131118\n",
      "Reconstruction: 0.121058, Regularization: 0.010060\n",
      "2019-04-10 00:04:34,719 root         INFO     Train Epoch: 71 [5632/8000 (70%)]\tTotal Loss: 0.133536\n",
      "Reconstruction: 0.121609, Regularization: 0.011927\n",
      "2019-04-10 00:04:34,779 root         INFO     Train Epoch: 71 [6144/8000 (77%)]\tTotal Loss: 0.129361\n",
      "Reconstruction: 0.118816, Regularization: 0.010545\n",
      "2019-04-10 00:04:34,840 root         INFO     Train Epoch: 71 [6656/8000 (83%)]\tTotal Loss: 0.119079\n",
      "Reconstruction: 0.110134, Regularization: 0.008945\n",
      "2019-04-10 00:04:34,902 root         INFO     Train Epoch: 71 [7168/8000 (90%)]\tTotal Loss: 0.121782\n",
      "Reconstruction: 0.110936, Regularization: 0.010846\n",
      "2019-04-10 00:04:34,963 root         INFO     Train Epoch: 71 [7680/8000 (96%)]\tTotal Loss: 0.158199\n",
      "Reconstruction: 0.146912, Regularization: 0.011287\n",
      "2019-04-10 00:04:35,016 root         INFO     ====> Epoch: 71 Average loss: 0.1257\n",
      "2019-04-10 00:04:35,040 root         INFO     Train Epoch: 72 [0/8000 (0%)]\tTotal Loss: 0.124037\n",
      "Reconstruction: 0.115933, Regularization: 0.008105\n",
      "2019-04-10 00:04:35,101 root         INFO     Train Epoch: 72 [512/8000 (6%)]\tTotal Loss: 0.120346\n",
      "Reconstruction: 0.111675, Regularization: 0.008671\n",
      "2019-04-10 00:04:35,162 root         INFO     Train Epoch: 72 [1024/8000 (13%)]\tTotal Loss: 0.119904\n",
      "Reconstruction: 0.111712, Regularization: 0.008192\n",
      "2019-04-10 00:04:35,223 root         INFO     Train Epoch: 72 [1536/8000 (19%)]\tTotal Loss: 0.123715\n",
      "Reconstruction: 0.116512, Regularization: 0.007203\n",
      "2019-04-10 00:04:35,285 root         INFO     Train Epoch: 72 [2048/8000 (26%)]\tTotal Loss: 0.127671\n",
      "Reconstruction: 0.120035, Regularization: 0.007636\n",
      "2019-04-10 00:04:35,346 root         INFO     Train Epoch: 72 [2560/8000 (32%)]\tTotal Loss: 0.112053\n",
      "Reconstruction: 0.106469, Regularization: 0.005585\n",
      "2019-04-10 00:04:35,407 root         INFO     Train Epoch: 72 [3072/8000 (38%)]\tTotal Loss: 0.119566\n",
      "Reconstruction: 0.110262, Regularization: 0.009304\n",
      "2019-04-10 00:04:35,469 root         INFO     Train Epoch: 72 [3584/8000 (45%)]\tTotal Loss: 0.108986\n",
      "Reconstruction: 0.101629, Regularization: 0.007357\n",
      "2019-04-10 00:04:35,530 root         INFO     Train Epoch: 72 [4096/8000 (51%)]\tTotal Loss: 0.119494\n",
      "Reconstruction: 0.110838, Regularization: 0.008656\n",
      "2019-04-10 00:04:35,591 root         INFO     Train Epoch: 72 [4608/8000 (58%)]\tTotal Loss: 0.124165\n",
      "Reconstruction: 0.118032, Regularization: 0.006133\n",
      "2019-04-10 00:04:35,652 root         INFO     Train Epoch: 72 [5120/8000 (64%)]\tTotal Loss: 0.113997\n",
      "Reconstruction: 0.106167, Regularization: 0.007830\n",
      "2019-04-10 00:04:35,713 root         INFO     Train Epoch: 72 [5632/8000 (70%)]\tTotal Loss: 0.127495\n",
      "Reconstruction: 0.117140, Regularization: 0.010355\n",
      "2019-04-10 00:04:35,774 root         INFO     Train Epoch: 72 [6144/8000 (77%)]\tTotal Loss: 0.135586\n",
      "Reconstruction: 0.126386, Regularization: 0.009200\n",
      "2019-04-10 00:04:35,835 root         INFO     Train Epoch: 72 [6656/8000 (83%)]\tTotal Loss: 0.128429\n",
      "Reconstruction: 0.121762, Regularization: 0.006667\n",
      "2019-04-10 00:04:35,896 root         INFO     Train Epoch: 72 [7168/8000 (90%)]\tTotal Loss: 0.118012\n",
      "Reconstruction: 0.110468, Regularization: 0.007544\n",
      "2019-04-10 00:04:35,957 root         INFO     Train Epoch: 72 [7680/8000 (96%)]\tTotal Loss: 0.131725\n",
      "Reconstruction: 0.123086, Regularization: 0.008639\n",
      "2019-04-10 00:04:36,010 root         INFO     ====> Epoch: 72 Average loss: 0.1236\n",
      "2019-04-10 00:04:36,034 root         INFO     Train Epoch: 73 [0/8000 (0%)]\tTotal Loss: 0.122499\n",
      "Reconstruction: 0.113024, Regularization: 0.009476\n",
      "2019-04-10 00:04:36,096 root         INFO     Train Epoch: 73 [512/8000 (6%)]\tTotal Loss: 0.129261\n",
      "Reconstruction: 0.122612, Regularization: 0.006649\n",
      "2019-04-10 00:04:36,158 root         INFO     Train Epoch: 73 [1024/8000 (13%)]\tTotal Loss: 0.134988\n",
      "Reconstruction: 0.129083, Regularization: 0.005905\n",
      "2019-04-10 00:04:36,220 root         INFO     Train Epoch: 73 [1536/8000 (19%)]\tTotal Loss: 0.112206\n",
      "Reconstruction: 0.107206, Regularization: 0.005000\n",
      "2019-04-10 00:04:36,282 root         INFO     Train Epoch: 73 [2048/8000 (26%)]\tTotal Loss: 0.124081\n",
      "Reconstruction: 0.117098, Regularization: 0.006983\n",
      "2019-04-10 00:04:36,344 root         INFO     Train Epoch: 73 [2560/8000 (32%)]\tTotal Loss: 0.123580\n",
      "Reconstruction: 0.115797, Regularization: 0.007783\n",
      "2019-04-10 00:04:36,407 root         INFO     Train Epoch: 73 [3072/8000 (38%)]\tTotal Loss: 0.124571\n",
      "Reconstruction: 0.117503, Regularization: 0.007068\n",
      "2019-04-10 00:04:36,469 root         INFO     Train Epoch: 73 [3584/8000 (45%)]\tTotal Loss: 0.119675\n",
      "Reconstruction: 0.112880, Regularization: 0.006795\n",
      "2019-04-10 00:04:36,531 root         INFO     Train Epoch: 73 [4096/8000 (51%)]\tTotal Loss: 0.118850\n",
      "Reconstruction: 0.111607, Regularization: 0.007243\n",
      "2019-04-10 00:04:36,593 root         INFO     Train Epoch: 73 [4608/8000 (58%)]\tTotal Loss: 0.117866\n",
      "Reconstruction: 0.111897, Regularization: 0.005970\n",
      "2019-04-10 00:04:36,655 root         INFO     Train Epoch: 73 [5120/8000 (64%)]\tTotal Loss: 0.117984\n",
      "Reconstruction: 0.107505, Regularization: 0.010479\n",
      "2019-04-10 00:04:36,718 root         INFO     Train Epoch: 73 [5632/8000 (70%)]\tTotal Loss: 0.125028\n",
      "Reconstruction: 0.118191, Regularization: 0.006837\n",
      "2019-04-10 00:04:36,780 root         INFO     Train Epoch: 73 [6144/8000 (77%)]\tTotal Loss: 0.128514\n",
      "Reconstruction: 0.119709, Regularization: 0.008805\n",
      "2019-04-10 00:04:36,843 root         INFO     Train Epoch: 73 [6656/8000 (83%)]\tTotal Loss: 0.128645\n",
      "Reconstruction: 0.121042, Regularization: 0.007603\n",
      "2019-04-10 00:04:36,905 root         INFO     Train Epoch: 73 [7168/8000 (90%)]\tTotal Loss: 0.132055\n",
      "Reconstruction: 0.124267, Regularization: 0.007788\n",
      "2019-04-10 00:04:36,967 root         INFO     Train Epoch: 73 [7680/8000 (96%)]\tTotal Loss: 0.125670\n",
      "Reconstruction: 0.117092, Regularization: 0.008578\n",
      "2019-04-10 00:04:37,020 root         INFO     ====> Epoch: 73 Average loss: 0.1229\n",
      "2019-04-10 00:04:37,044 root         INFO     Train Epoch: 74 [0/8000 (0%)]\tTotal Loss: 0.127370\n",
      "Reconstruction: 0.119352, Regularization: 0.008018\n",
      "2019-04-10 00:04:37,105 root         INFO     Train Epoch: 74 [512/8000 (6%)]\tTotal Loss: 0.123787\n",
      "Reconstruction: 0.117401, Regularization: 0.006386\n",
      "2019-04-10 00:04:37,167 root         INFO     Train Epoch: 74 [1024/8000 (13%)]\tTotal Loss: 0.112833\n",
      "Reconstruction: 0.106511, Regularization: 0.006322\n",
      "2019-04-10 00:04:37,228 root         INFO     Train Epoch: 74 [1536/8000 (19%)]\tTotal Loss: 0.124828\n",
      "Reconstruction: 0.118338, Regularization: 0.006490\n",
      "2019-04-10 00:04:37,289 root         INFO     Train Epoch: 74 [2048/8000 (26%)]\tTotal Loss: 0.119088\n",
      "Reconstruction: 0.111317, Regularization: 0.007771\n",
      "2019-04-10 00:04:37,350 root         INFO     Train Epoch: 74 [2560/8000 (32%)]\tTotal Loss: 0.119325\n",
      "Reconstruction: 0.112193, Regularization: 0.007131\n",
      "2019-04-10 00:04:37,412 root         INFO     Train Epoch: 74 [3072/8000 (38%)]\tTotal Loss: 0.127110\n",
      "Reconstruction: 0.120375, Regularization: 0.006735\n",
      "2019-04-10 00:04:37,473 root         INFO     Train Epoch: 74 [3584/8000 (45%)]\tTotal Loss: 0.111917\n",
      "Reconstruction: 0.107801, Regularization: 0.004115\n",
      "2019-04-10 00:04:37,533 root         INFO     Train Epoch: 74 [4096/8000 (51%)]\tTotal Loss: 0.110659\n",
      "Reconstruction: 0.106144, Regularization: 0.004515\n",
      "2019-04-10 00:04:37,595 root         INFO     Train Epoch: 74 [4608/8000 (58%)]\tTotal Loss: 0.121609\n",
      "Reconstruction: 0.113167, Regularization: 0.008442\n",
      "2019-04-10 00:04:37,656 root         INFO     Train Epoch: 74 [5120/8000 (64%)]\tTotal Loss: 0.128453\n",
      "Reconstruction: 0.118363, Regularization: 0.010090\n",
      "2019-04-10 00:04:37,717 root         INFO     Train Epoch: 74 [5632/8000 (70%)]\tTotal Loss: 0.130452\n",
      "Reconstruction: 0.122649, Regularization: 0.007803\n",
      "2019-04-10 00:04:37,778 root         INFO     Train Epoch: 74 [6144/8000 (77%)]\tTotal Loss: 0.121117\n",
      "Reconstruction: 0.113878, Regularization: 0.007238\n",
      "2019-04-10 00:04:37,838 root         INFO     Train Epoch: 74 [6656/8000 (83%)]\tTotal Loss: 0.127051\n",
      "Reconstruction: 0.122201, Regularization: 0.004850\n",
      "2019-04-10 00:04:37,899 root         INFO     Train Epoch: 74 [7168/8000 (90%)]\tTotal Loss: 0.110287\n",
      "Reconstruction: 0.105401, Regularization: 0.004886\n",
      "2019-04-10 00:04:37,961 root         INFO     Train Epoch: 74 [7680/8000 (96%)]\tTotal Loss: 0.118980\n",
      "Reconstruction: 0.113897, Regularization: 0.005082\n",
      "2019-04-10 00:04:38,013 root         INFO     ====> Epoch: 74 Average loss: 0.1219\n",
      "2019-04-10 00:04:38,037 root         INFO     Train Epoch: 75 [0/8000 (0%)]\tTotal Loss: 0.117224\n",
      "Reconstruction: 0.110944, Regularization: 0.006281\n",
      "2019-04-10 00:04:38,100 root         INFO     Train Epoch: 75 [512/8000 (6%)]\tTotal Loss: 0.122248\n",
      "Reconstruction: 0.116527, Regularization: 0.005721\n",
      "2019-04-10 00:04:38,162 root         INFO     Train Epoch: 75 [1024/8000 (13%)]\tTotal Loss: 0.117417\n",
      "Reconstruction: 0.112622, Regularization: 0.004795\n",
      "2019-04-10 00:04:38,224 root         INFO     Train Epoch: 75 [1536/8000 (19%)]\tTotal Loss: 0.118821\n",
      "Reconstruction: 0.112272, Regularization: 0.006548\n",
      "2019-04-10 00:04:38,286 root         INFO     Train Epoch: 75 [2048/8000 (26%)]\tTotal Loss: 0.117000\n",
      "Reconstruction: 0.110601, Regularization: 0.006399\n",
      "2019-04-10 00:04:38,348 root         INFO     Train Epoch: 75 [2560/8000 (32%)]\tTotal Loss: 0.112498\n",
      "Reconstruction: 0.105790, Regularization: 0.006708\n",
      "2019-04-10 00:04:38,410 root         INFO     Train Epoch: 75 [3072/8000 (38%)]\tTotal Loss: 0.118762\n",
      "Reconstruction: 0.113545, Regularization: 0.005217\n",
      "2019-04-10 00:04:38,473 root         INFO     Train Epoch: 75 [3584/8000 (45%)]\tTotal Loss: 0.130421\n",
      "Reconstruction: 0.123666, Regularization: 0.006755\n",
      "2019-04-10 00:04:38,535 root         INFO     Train Epoch: 75 [4096/8000 (51%)]\tTotal Loss: 0.121791\n",
      "Reconstruction: 0.116223, Regularization: 0.005567\n",
      "2019-04-10 00:04:38,597 root         INFO     Train Epoch: 75 [4608/8000 (58%)]\tTotal Loss: 0.128404\n",
      "Reconstruction: 0.123970, Regularization: 0.004434\n",
      "2019-04-10 00:04:38,659 root         INFO     Train Epoch: 75 [5120/8000 (64%)]\tTotal Loss: 0.127269\n",
      "Reconstruction: 0.121864, Regularization: 0.005405\n",
      "2019-04-10 00:04:38,722 root         INFO     Train Epoch: 75 [5632/8000 (70%)]\tTotal Loss: 0.126493\n",
      "Reconstruction: 0.119659, Regularization: 0.006834\n",
      "2019-04-10 00:04:38,785 root         INFO     Train Epoch: 75 [6144/8000 (77%)]\tTotal Loss: 0.121548\n",
      "Reconstruction: 0.117253, Regularization: 0.004295\n",
      "2019-04-10 00:04:38,847 root         INFO     Train Epoch: 75 [6656/8000 (83%)]\tTotal Loss: 0.117071\n",
      "Reconstruction: 0.112010, Regularization: 0.005061\n",
      "2019-04-10 00:04:38,910 root         INFO     Train Epoch: 75 [7168/8000 (90%)]\tTotal Loss: 0.134068\n",
      "Reconstruction: 0.127019, Regularization: 0.007049\n",
      "2019-04-10 00:04:38,973 root         INFO     Train Epoch: 75 [7680/8000 (96%)]\tTotal Loss: 0.115633\n",
      "Reconstruction: 0.110586, Regularization: 0.005046\n",
      "2019-04-10 00:04:39,027 root         INFO     ====> Epoch: 75 Average loss: 0.1202\n",
      "2019-04-10 00:04:39,051 root         INFO     Train Epoch: 76 [0/8000 (0%)]\tTotal Loss: 0.130316\n",
      "Reconstruction: 0.124534, Regularization: 0.005781\n",
      "2019-04-10 00:04:39,114 root         INFO     Train Epoch: 76 [512/8000 (6%)]\tTotal Loss: 0.127616\n",
      "Reconstruction: 0.121433, Regularization: 0.006183\n",
      "2019-04-10 00:04:39,176 root         INFO     Train Epoch: 76 [1024/8000 (13%)]\tTotal Loss: 0.112170\n",
      "Reconstruction: 0.108638, Regularization: 0.003532\n",
      "2019-04-10 00:04:39,238 root         INFO     Train Epoch: 76 [1536/8000 (19%)]\tTotal Loss: 0.132197\n",
      "Reconstruction: 0.124208, Regularization: 0.007990\n",
      "2019-04-10 00:04:39,301 root         INFO     Train Epoch: 76 [2048/8000 (26%)]\tTotal Loss: 0.123807\n",
      "Reconstruction: 0.117909, Regularization: 0.005898\n",
      "2019-04-10 00:04:39,363 root         INFO     Train Epoch: 76 [2560/8000 (32%)]\tTotal Loss: 0.119071\n",
      "Reconstruction: 0.112325, Regularization: 0.006746\n",
      "2019-04-10 00:04:39,426 root         INFO     Train Epoch: 76 [3072/8000 (38%)]\tTotal Loss: 0.125672\n",
      "Reconstruction: 0.122631, Regularization: 0.003041\n",
      "2019-04-10 00:04:39,489 root         INFO     Train Epoch: 76 [3584/8000 (45%)]\tTotal Loss: 0.120728\n",
      "Reconstruction: 0.117165, Regularization: 0.003563\n",
      "2019-04-10 00:04:39,551 root         INFO     Train Epoch: 76 [4096/8000 (51%)]\tTotal Loss: 0.113436\n",
      "Reconstruction: 0.108508, Regularization: 0.004929\n",
      "2019-04-10 00:04:39,614 root         INFO     Train Epoch: 76 [4608/8000 (58%)]\tTotal Loss: 0.112943\n",
      "Reconstruction: 0.108321, Regularization: 0.004622\n",
      "2019-04-10 00:04:39,677 root         INFO     Train Epoch: 76 [5120/8000 (64%)]\tTotal Loss: 0.144174\n",
      "Reconstruction: 0.137110, Regularization: 0.007064\n",
      "2019-04-10 00:04:39,739 root         INFO     Train Epoch: 76 [5632/8000 (70%)]\tTotal Loss: 0.124686\n",
      "Reconstruction: 0.116632, Regularization: 0.008054\n",
      "2019-04-10 00:04:39,801 root         INFO     Train Epoch: 76 [6144/8000 (77%)]\tTotal Loss: 0.122575\n",
      "Reconstruction: 0.115618, Regularization: 0.006957\n",
      "2019-04-10 00:04:39,863 root         INFO     Train Epoch: 76 [6656/8000 (83%)]\tTotal Loss: 0.119469\n",
      "Reconstruction: 0.112349, Regularization: 0.007120\n",
      "2019-04-10 00:04:39,925 root         INFO     Train Epoch: 76 [7168/8000 (90%)]\tTotal Loss: 0.111782\n",
      "Reconstruction: 0.108121, Regularization: 0.003661\n",
      "2019-04-10 00:04:39,988 root         INFO     Train Epoch: 76 [7680/8000 (96%)]\tTotal Loss: 0.106799\n",
      "Reconstruction: 0.103222, Regularization: 0.003578\n",
      "2019-04-10 00:04:40,041 root         INFO     ====> Epoch: 76 Average loss: 0.1199\n",
      "2019-04-10 00:04:40,065 root         INFO     Train Epoch: 77 [0/8000 (0%)]\tTotal Loss: 0.112643\n",
      "Reconstruction: 0.108674, Regularization: 0.003969\n",
      "2019-04-10 00:04:40,128 root         INFO     Train Epoch: 77 [512/8000 (6%)]\tTotal Loss: 0.116499\n",
      "Reconstruction: 0.112162, Regularization: 0.004337\n",
      "2019-04-10 00:04:40,190 root         INFO     Train Epoch: 77 [1024/8000 (13%)]\tTotal Loss: 0.121849\n",
      "Reconstruction: 0.116638, Regularization: 0.005211\n",
      "2019-04-10 00:04:40,252 root         INFO     Train Epoch: 77 [1536/8000 (19%)]\tTotal Loss: 0.104302\n",
      "Reconstruction: 0.101870, Regularization: 0.002433\n",
      "2019-04-10 00:04:40,313 root         INFO     Train Epoch: 77 [2048/8000 (26%)]\tTotal Loss: 0.109827\n",
      "Reconstruction: 0.106239, Regularization: 0.003588\n",
      "2019-04-10 00:04:40,375 root         INFO     Train Epoch: 77 [2560/8000 (32%)]\tTotal Loss: 0.115629\n",
      "Reconstruction: 0.110881, Regularization: 0.004748\n",
      "2019-04-10 00:04:40,437 root         INFO     Train Epoch: 77 [3072/8000 (38%)]\tTotal Loss: 0.122888\n",
      "Reconstruction: 0.117701, Regularization: 0.005186\n",
      "2019-04-10 00:04:40,502 root         INFO     Train Epoch: 77 [3584/8000 (45%)]\tTotal Loss: 0.114949\n",
      "Reconstruction: 0.111196, Regularization: 0.003753\n",
      "2019-04-10 00:04:40,565 root         INFO     Train Epoch: 77 [4096/8000 (51%)]\tTotal Loss: 0.114480\n",
      "Reconstruction: 0.108925, Regularization: 0.005555\n",
      "2019-04-10 00:04:40,629 root         INFO     Train Epoch: 77 [4608/8000 (58%)]\tTotal Loss: 0.117565\n",
      "Reconstruction: 0.112494, Regularization: 0.005071\n",
      "2019-04-10 00:04:40,694 root         INFO     Train Epoch: 77 [5120/8000 (64%)]\tTotal Loss: 0.116729\n",
      "Reconstruction: 0.111247, Regularization: 0.005482\n",
      "2019-04-10 00:04:40,758 root         INFO     Train Epoch: 77 [5632/8000 (70%)]\tTotal Loss: 0.126243\n",
      "Reconstruction: 0.119625, Regularization: 0.006618\n",
      "2019-04-10 00:04:40,821 root         INFO     Train Epoch: 77 [6144/8000 (77%)]\tTotal Loss: 0.109832\n",
      "Reconstruction: 0.106415, Regularization: 0.003417\n",
      "2019-04-10 00:04:40,885 root         INFO     Train Epoch: 77 [6656/8000 (83%)]\tTotal Loss: 0.115829\n",
      "Reconstruction: 0.112100, Regularization: 0.003729\n",
      "2019-04-10 00:04:40,949 root         INFO     Train Epoch: 77 [7168/8000 (90%)]\tTotal Loss: 0.105822\n",
      "Reconstruction: 0.102439, Regularization: 0.003383\n",
      "2019-04-10 00:04:41,013 root         INFO     Train Epoch: 77 [7680/8000 (96%)]\tTotal Loss: 0.121653\n",
      "Reconstruction: 0.117524, Regularization: 0.004129\n",
      "2019-04-10 00:04:41,068 root         INFO     ====> Epoch: 77 Average loss: 0.1194\n",
      "2019-04-10 00:04:41,092 root         INFO     Train Epoch: 78 [0/8000 (0%)]\tTotal Loss: 0.111004\n",
      "Reconstruction: 0.107985, Regularization: 0.003019\n",
      "2019-04-10 00:04:41,155 root         INFO     Train Epoch: 78 [512/8000 (6%)]\tTotal Loss: 0.111330\n",
      "Reconstruction: 0.107899, Regularization: 0.003431\n",
      "2019-04-10 00:04:41,218 root         INFO     Train Epoch: 78 [1024/8000 (13%)]\tTotal Loss: 0.128811\n",
      "Reconstruction: 0.124058, Regularization: 0.004753\n",
      "2019-04-10 00:04:41,281 root         INFO     Train Epoch: 78 [1536/8000 (19%)]\tTotal Loss: 0.113399\n",
      "Reconstruction: 0.108386, Regularization: 0.005013\n",
      "2019-04-10 00:04:41,346 root         INFO     Train Epoch: 78 [2048/8000 (26%)]\tTotal Loss: 0.118282\n",
      "Reconstruction: 0.114592, Regularization: 0.003691\n",
      "2019-04-10 00:04:41,408 root         INFO     Train Epoch: 78 [2560/8000 (32%)]\tTotal Loss: 0.116804\n",
      "Reconstruction: 0.111217, Regularization: 0.005587\n",
      "2019-04-10 00:04:41,470 root         INFO     Train Epoch: 78 [3072/8000 (38%)]\tTotal Loss: 0.117654\n",
      "Reconstruction: 0.113065, Regularization: 0.004589\n",
      "2019-04-10 00:04:41,532 root         INFO     Train Epoch: 78 [3584/8000 (45%)]\tTotal Loss: 0.114328\n",
      "Reconstruction: 0.108183, Regularization: 0.006146\n",
      "2019-04-10 00:04:41,593 root         INFO     Train Epoch: 78 [4096/8000 (51%)]\tTotal Loss: 0.117800\n",
      "Reconstruction: 0.114345, Regularization: 0.003455\n",
      "2019-04-10 00:04:41,655 root         INFO     Train Epoch: 78 [4608/8000 (58%)]\tTotal Loss: 0.112176\n",
      "Reconstruction: 0.108280, Regularization: 0.003896\n",
      "2019-04-10 00:04:41,718 root         INFO     Train Epoch: 78 [5120/8000 (64%)]\tTotal Loss: 0.119033\n",
      "Reconstruction: 0.115562, Regularization: 0.003471\n",
      "2019-04-10 00:04:41,780 root         INFO     Train Epoch: 78 [5632/8000 (70%)]\tTotal Loss: 0.120276\n",
      "Reconstruction: 0.115252, Regularization: 0.005024\n",
      "2019-04-10 00:04:41,842 root         INFO     Train Epoch: 78 [6144/8000 (77%)]\tTotal Loss: 0.116791\n",
      "Reconstruction: 0.112188, Regularization: 0.004603\n",
      "2019-04-10 00:04:41,905 root         INFO     Train Epoch: 78 [6656/8000 (83%)]\tTotal Loss: 0.114675\n",
      "Reconstruction: 0.109710, Regularization: 0.004965\n",
      "2019-04-10 00:04:41,967 root         INFO     Train Epoch: 78 [7168/8000 (90%)]\tTotal Loss: 0.124531\n",
      "Reconstruction: 0.119739, Regularization: 0.004792\n",
      "2019-04-10 00:04:42,030 root         INFO     Train Epoch: 78 [7680/8000 (96%)]\tTotal Loss: 0.118418\n",
      "Reconstruction: 0.114931, Regularization: 0.003486\n",
      "2019-04-10 00:04:42,083 root         INFO     ====> Epoch: 78 Average loss: 0.1185\n",
      "2019-04-10 00:04:42,108 root         INFO     Train Epoch: 79 [0/8000 (0%)]\tTotal Loss: 0.118287\n",
      "Reconstruction: 0.114686, Regularization: 0.003602\n",
      "2019-04-10 00:04:42,171 root         INFO     Train Epoch: 79 [512/8000 (6%)]\tTotal Loss: 0.115658\n",
      "Reconstruction: 0.111116, Regularization: 0.004542\n",
      "2019-04-10 00:04:42,233 root         INFO     Train Epoch: 79 [1024/8000 (13%)]\tTotal Loss: 0.118807\n",
      "Reconstruction: 0.113986, Regularization: 0.004821\n",
      "2019-04-10 00:04:42,295 root         INFO     Train Epoch: 79 [1536/8000 (19%)]\tTotal Loss: 0.116716\n",
      "Reconstruction: 0.113221, Regularization: 0.003495\n",
      "2019-04-10 00:04:42,358 root         INFO     Train Epoch: 79 [2048/8000 (26%)]\tTotal Loss: 0.119674\n",
      "Reconstruction: 0.115435, Regularization: 0.004239\n",
      "2019-04-10 00:04:42,421 root         INFO     Train Epoch: 79 [2560/8000 (32%)]\tTotal Loss: 0.114250\n",
      "Reconstruction: 0.110567, Regularization: 0.003683\n",
      "2019-04-10 00:04:42,483 root         INFO     Train Epoch: 79 [3072/8000 (38%)]\tTotal Loss: 0.120376\n",
      "Reconstruction: 0.116453, Regularization: 0.003923\n",
      "2019-04-10 00:04:42,546 root         INFO     Train Epoch: 79 [3584/8000 (45%)]\tTotal Loss: 0.118784\n",
      "Reconstruction: 0.115820, Regularization: 0.002964\n",
      "2019-04-10 00:04:42,608 root         INFO     Train Epoch: 79 [4096/8000 (51%)]\tTotal Loss: 0.114544\n",
      "Reconstruction: 0.111192, Regularization: 0.003352\n",
      "2019-04-10 00:04:42,670 root         INFO     Train Epoch: 79 [4608/8000 (58%)]\tTotal Loss: 0.114302\n",
      "Reconstruction: 0.110763, Regularization: 0.003539\n",
      "2019-04-10 00:04:42,733 root         INFO     Train Epoch: 79 [5120/8000 (64%)]\tTotal Loss: 0.117500\n",
      "Reconstruction: 0.113659, Regularization: 0.003841\n",
      "2019-04-10 00:04:42,796 root         INFO     Train Epoch: 79 [5632/8000 (70%)]\tTotal Loss: 0.117781\n",
      "Reconstruction: 0.112818, Regularization: 0.004963\n",
      "2019-04-10 00:04:42,858 root         INFO     Train Epoch: 79 [6144/8000 (77%)]\tTotal Loss: 0.131034\n",
      "Reconstruction: 0.126491, Regularization: 0.004544\n",
      "2019-04-10 00:04:42,921 root         INFO     Train Epoch: 79 [6656/8000 (83%)]\tTotal Loss: 0.114839\n",
      "Reconstruction: 0.111157, Regularization: 0.003681\n",
      "2019-04-10 00:04:42,983 root         INFO     Train Epoch: 79 [7168/8000 (90%)]\tTotal Loss: 0.115980\n",
      "Reconstruction: 0.109803, Regularization: 0.006177\n",
      "2019-04-10 00:04:43,046 root         INFO     Train Epoch: 79 [7680/8000 (96%)]\tTotal Loss: 0.115557\n",
      "Reconstruction: 0.110543, Regularization: 0.005014\n",
      "2019-04-10 00:04:43,099 root         INFO     ====> Epoch: 79 Average loss: 0.1180\n",
      "2019-04-10 00:04:43,123 root         INFO     Train Epoch: 80 [0/8000 (0%)]\tTotal Loss: 0.119812\n",
      "Reconstruction: 0.116941, Regularization: 0.002871\n",
      "2019-04-10 00:04:43,186 root         INFO     Train Epoch: 80 [512/8000 (6%)]\tTotal Loss: 0.121951\n",
      "Reconstruction: 0.118242, Regularization: 0.003709\n",
      "2019-04-10 00:04:43,249 root         INFO     Train Epoch: 80 [1024/8000 (13%)]\tTotal Loss: 0.120311\n",
      "Reconstruction: 0.115737, Regularization: 0.004574\n",
      "2019-04-10 00:04:43,311 root         INFO     Train Epoch: 80 [1536/8000 (19%)]\tTotal Loss: 0.110831\n",
      "Reconstruction: 0.106321, Regularization: 0.004511\n",
      "2019-04-10 00:04:43,374 root         INFO     Train Epoch: 80 [2048/8000 (26%)]\tTotal Loss: 0.120012\n",
      "Reconstruction: 0.116859, Regularization: 0.003153\n",
      "2019-04-10 00:04:43,437 root         INFO     Train Epoch: 80 [2560/8000 (32%)]\tTotal Loss: 0.121949\n",
      "Reconstruction: 0.118509, Regularization: 0.003440\n",
      "2019-04-10 00:04:43,501 root         INFO     Train Epoch: 80 [3072/8000 (38%)]\tTotal Loss: 0.108375\n",
      "Reconstruction: 0.104952, Regularization: 0.003423\n",
      "2019-04-10 00:04:43,565 root         INFO     Train Epoch: 80 [3584/8000 (45%)]\tTotal Loss: 0.118548\n",
      "Reconstruction: 0.113800, Regularization: 0.004747\n",
      "2019-04-10 00:04:43,628 root         INFO     Train Epoch: 80 [4096/8000 (51%)]\tTotal Loss: 0.117531\n",
      "Reconstruction: 0.111774, Regularization: 0.005757\n",
      "2019-04-10 00:04:43,691 root         INFO     Train Epoch: 80 [4608/8000 (58%)]\tTotal Loss: 0.126476\n",
      "Reconstruction: 0.120080, Regularization: 0.006396\n",
      "2019-04-10 00:04:43,754 root         INFO     Train Epoch: 80 [5120/8000 (64%)]\tTotal Loss: 0.121837\n",
      "Reconstruction: 0.118827, Regularization: 0.003010\n",
      "2019-04-10 00:04:43,817 root         INFO     Train Epoch: 80 [5632/8000 (70%)]\tTotal Loss: 0.115342\n",
      "Reconstruction: 0.112797, Regularization: 0.002545\n",
      "2019-04-10 00:04:43,880 root         INFO     Train Epoch: 80 [6144/8000 (77%)]\tTotal Loss: 0.114171\n",
      "Reconstruction: 0.110672, Regularization: 0.003498\n",
      "2019-04-10 00:04:43,943 root         INFO     Train Epoch: 80 [6656/8000 (83%)]\tTotal Loss: 0.125761\n",
      "Reconstruction: 0.120262, Regularization: 0.005498\n",
      "2019-04-10 00:04:44,006 root         INFO     Train Epoch: 80 [7168/8000 (90%)]\tTotal Loss: 0.125586\n",
      "Reconstruction: 0.120967, Regularization: 0.004619\n",
      "2019-04-10 00:04:44,069 root         INFO     Train Epoch: 80 [7680/8000 (96%)]\tTotal Loss: 0.120868\n",
      "Reconstruction: 0.116858, Regularization: 0.004010\n",
      "2019-04-10 00:04:44,122 root         INFO     ====> Epoch: 80 Average loss: 0.1179\n",
      "2019-04-10 00:04:44,147 root         INFO     Train Epoch: 81 [0/8000 (0%)]\tTotal Loss: 0.106294\n",
      "Reconstruction: 0.102576, Regularization: 0.003717\n",
      "2019-04-10 00:04:44,210 root         INFO     Train Epoch: 81 [512/8000 (6%)]\tTotal Loss: 0.110738\n",
      "Reconstruction: 0.107277, Regularization: 0.003461\n",
      "2019-04-10 00:04:44,272 root         INFO     Train Epoch: 81 [1024/8000 (13%)]\tTotal Loss: 0.113498\n",
      "Reconstruction: 0.110414, Regularization: 0.003084\n",
      "2019-04-10 00:04:44,335 root         INFO     Train Epoch: 81 [1536/8000 (19%)]\tTotal Loss: 0.116932\n",
      "Reconstruction: 0.113584, Regularization: 0.003349\n",
      "2019-04-10 00:04:44,398 root         INFO     Train Epoch: 81 [2048/8000 (26%)]\tTotal Loss: 0.119382\n",
      "Reconstruction: 0.115689, Regularization: 0.003693\n",
      "2019-04-10 00:04:44,460 root         INFO     Train Epoch: 81 [2560/8000 (32%)]\tTotal Loss: 0.117988\n",
      "Reconstruction: 0.113879, Regularization: 0.004109\n",
      "2019-04-10 00:04:44,523 root         INFO     Train Epoch: 81 [3072/8000 (38%)]\tTotal Loss: 0.116089\n",
      "Reconstruction: 0.111486, Regularization: 0.004603\n",
      "2019-04-10 00:04:44,586 root         INFO     Train Epoch: 81 [3584/8000 (45%)]\tTotal Loss: 0.101830\n",
      "Reconstruction: 0.097954, Regularization: 0.003876\n",
      "2019-04-10 00:04:44,648 root         INFO     Train Epoch: 81 [4096/8000 (51%)]\tTotal Loss: 0.122691\n",
      "Reconstruction: 0.118940, Regularization: 0.003751\n",
      "2019-04-10 00:04:44,711 root         INFO     Train Epoch: 81 [4608/8000 (58%)]\tTotal Loss: 0.112931\n",
      "Reconstruction: 0.107148, Regularization: 0.005784\n",
      "2019-04-10 00:04:44,773 root         INFO     Train Epoch: 81 [5120/8000 (64%)]\tTotal Loss: 0.119672\n",
      "Reconstruction: 0.116575, Regularization: 0.003097\n",
      "2019-04-10 00:04:44,834 root         INFO     Train Epoch: 81 [5632/8000 (70%)]\tTotal Loss: 0.122839\n",
      "Reconstruction: 0.118718, Regularization: 0.004121\n",
      "2019-04-10 00:04:44,896 root         INFO     Train Epoch: 81 [6144/8000 (77%)]\tTotal Loss: 0.110992\n",
      "Reconstruction: 0.106807, Regularization: 0.004185\n",
      "2019-04-10 00:04:44,958 root         INFO     Train Epoch: 81 [6656/8000 (83%)]\tTotal Loss: 0.130778\n",
      "Reconstruction: 0.128001, Regularization: 0.002776\n",
      "2019-04-10 00:04:45,020 root         INFO     Train Epoch: 81 [7168/8000 (90%)]\tTotal Loss: 0.113565\n",
      "Reconstruction: 0.110452, Regularization: 0.003113\n",
      "2019-04-10 00:04:45,082 root         INFO     Train Epoch: 81 [7680/8000 (96%)]\tTotal Loss: 0.111769\n",
      "Reconstruction: 0.108195, Regularization: 0.003573\n",
      "2019-04-10 00:04:45,135 root         INFO     ====> Epoch: 81 Average loss: 0.1177\n",
      "2019-04-10 00:04:45,160 root         INFO     Train Epoch: 82 [0/8000 (0%)]\tTotal Loss: 0.108751\n",
      "Reconstruction: 0.105901, Regularization: 0.002851\n",
      "2019-04-10 00:04:45,223 root         INFO     Train Epoch: 82 [512/8000 (6%)]\tTotal Loss: 0.129173\n",
      "Reconstruction: 0.125694, Regularization: 0.003479\n",
      "2019-04-10 00:04:45,285 root         INFO     Train Epoch: 82 [1024/8000 (13%)]\tTotal Loss: 0.118197\n",
      "Reconstruction: 0.114699, Regularization: 0.003498\n",
      "2019-04-10 00:04:45,348 root         INFO     Train Epoch: 82 [1536/8000 (19%)]\tTotal Loss: 0.115943\n",
      "Reconstruction: 0.111929, Regularization: 0.004014\n",
      "2019-04-10 00:04:45,411 root         INFO     Train Epoch: 82 [2048/8000 (26%)]\tTotal Loss: 0.115462\n",
      "Reconstruction: 0.112097, Regularization: 0.003365\n",
      "2019-04-10 00:04:45,474 root         INFO     Train Epoch: 82 [2560/8000 (32%)]\tTotal Loss: 0.121735\n",
      "Reconstruction: 0.118604, Regularization: 0.003131\n",
      "2019-04-10 00:04:45,537 root         INFO     Train Epoch: 82 [3072/8000 (38%)]\tTotal Loss: 0.115491\n",
      "Reconstruction: 0.111816, Regularization: 0.003676\n",
      "2019-04-10 00:04:45,600 root         INFO     Train Epoch: 82 [3584/8000 (45%)]\tTotal Loss: 0.122634\n",
      "Reconstruction: 0.119877, Regularization: 0.002757\n",
      "2019-04-10 00:04:45,663 root         INFO     Train Epoch: 82 [4096/8000 (51%)]\tTotal Loss: 0.109248\n",
      "Reconstruction: 0.107106, Regularization: 0.002143\n",
      "2019-04-10 00:04:45,726 root         INFO     Train Epoch: 82 [4608/8000 (58%)]\tTotal Loss: 0.123944\n",
      "Reconstruction: 0.121492, Regularization: 0.002453\n",
      "2019-04-10 00:04:45,789 root         INFO     Train Epoch: 82 [5120/8000 (64%)]\tTotal Loss: 0.118106\n",
      "Reconstruction: 0.114024, Regularization: 0.004082\n",
      "2019-04-10 00:04:45,852 root         INFO     Train Epoch: 82 [5632/8000 (70%)]\tTotal Loss: 0.120056\n",
      "Reconstruction: 0.115407, Regularization: 0.004650\n",
      "2019-04-10 00:04:45,915 root         INFO     Train Epoch: 82 [6144/8000 (77%)]\tTotal Loss: 0.121741\n",
      "Reconstruction: 0.117318, Regularization: 0.004423\n",
      "2019-04-10 00:04:45,978 root         INFO     Train Epoch: 82 [6656/8000 (83%)]\tTotal Loss: 0.115338\n",
      "Reconstruction: 0.112082, Regularization: 0.003256\n",
      "2019-04-10 00:04:46,041 root         INFO     Train Epoch: 82 [7168/8000 (90%)]\tTotal Loss: 0.122506\n",
      "Reconstruction: 0.119600, Regularization: 0.002907\n",
      "2019-04-10 00:04:46,104 root         INFO     Train Epoch: 82 [7680/8000 (96%)]\tTotal Loss: 0.113029\n",
      "Reconstruction: 0.110382, Regularization: 0.002647\n",
      "2019-04-10 00:04:46,158 root         INFO     ====> Epoch: 82 Average loss: 0.1174\n",
      "2019-04-10 00:04:46,182 root         INFO     Train Epoch: 83 [0/8000 (0%)]\tTotal Loss: 0.120874\n",
      "Reconstruction: 0.117660, Regularization: 0.003214\n",
      "2019-04-10 00:04:46,245 root         INFO     Train Epoch: 83 [512/8000 (6%)]\tTotal Loss: 0.119494\n",
      "Reconstruction: 0.114941, Regularization: 0.004553\n",
      "2019-04-10 00:04:46,307 root         INFO     Train Epoch: 83 [1024/8000 (13%)]\tTotal Loss: 0.123144\n",
      "Reconstruction: 0.119484, Regularization: 0.003660\n",
      "2019-04-10 00:04:46,370 root         INFO     Train Epoch: 83 [1536/8000 (19%)]\tTotal Loss: 0.109746\n",
      "Reconstruction: 0.104877, Regularization: 0.004868\n",
      "2019-04-10 00:04:46,432 root         INFO     Train Epoch: 83 [2048/8000 (26%)]\tTotal Loss: 0.115635\n",
      "Reconstruction: 0.111138, Regularization: 0.004497\n",
      "2019-04-10 00:04:46,494 root         INFO     Train Epoch: 83 [2560/8000 (32%)]\tTotal Loss: 0.124628\n",
      "Reconstruction: 0.120493, Regularization: 0.004136\n",
      "2019-04-10 00:04:46,556 root         INFO     Train Epoch: 83 [3072/8000 (38%)]\tTotal Loss: 0.126456\n",
      "Reconstruction: 0.121325, Regularization: 0.005131\n",
      "2019-04-10 00:04:46,618 root         INFO     Train Epoch: 83 [3584/8000 (45%)]\tTotal Loss: 0.132791\n",
      "Reconstruction: 0.128803, Regularization: 0.003988\n",
      "2019-04-10 00:04:46,680 root         INFO     Train Epoch: 83 [4096/8000 (51%)]\tTotal Loss: 0.117475\n",
      "Reconstruction: 0.113852, Regularization: 0.003623\n",
      "2019-04-10 00:04:46,743 root         INFO     Train Epoch: 83 [4608/8000 (58%)]\tTotal Loss: 0.116752\n",
      "Reconstruction: 0.112942, Regularization: 0.003809\n",
      "2019-04-10 00:04:46,804 root         INFO     Train Epoch: 83 [5120/8000 (64%)]\tTotal Loss: 0.119827\n",
      "Reconstruction: 0.116507, Regularization: 0.003320\n",
      "2019-04-10 00:04:46,866 root         INFO     Train Epoch: 83 [5632/8000 (70%)]\tTotal Loss: 0.106370\n",
      "Reconstruction: 0.102596, Regularization: 0.003775\n",
      "2019-04-10 00:04:46,928 root         INFO     Train Epoch: 83 [6144/8000 (77%)]\tTotal Loss: 0.113116\n",
      "Reconstruction: 0.110173, Regularization: 0.002943\n",
      "2019-04-10 00:04:46,991 root         INFO     Train Epoch: 83 [6656/8000 (83%)]\tTotal Loss: 0.123535\n",
      "Reconstruction: 0.118730, Regularization: 0.004805\n",
      "2019-04-10 00:04:47,053 root         INFO     Train Epoch: 83 [7168/8000 (90%)]\tTotal Loss: 0.112057\n",
      "Reconstruction: 0.108229, Regularization: 0.003828\n",
      "2019-04-10 00:04:47,116 root         INFO     Train Epoch: 83 [7680/8000 (96%)]\tTotal Loss: 0.121820\n",
      "Reconstruction: 0.118570, Regularization: 0.003250\n",
      "2019-04-10 00:04:47,169 root         INFO     ====> Epoch: 83 Average loss: 0.1171\n",
      "2019-04-10 00:04:47,194 root         INFO     Train Epoch: 84 [0/8000 (0%)]\tTotal Loss: 0.108672\n",
      "Reconstruction: 0.105315, Regularization: 0.003357\n",
      "2019-04-10 00:04:47,257 root         INFO     Train Epoch: 84 [512/8000 (6%)]\tTotal Loss: 0.107438\n",
      "Reconstruction: 0.104985, Regularization: 0.002454\n",
      "2019-04-10 00:04:47,318 root         INFO     Train Epoch: 84 [1024/8000 (13%)]\tTotal Loss: 0.111928\n",
      "Reconstruction: 0.108592, Regularization: 0.003336\n",
      "2019-04-10 00:04:47,379 root         INFO     Train Epoch: 84 [1536/8000 (19%)]\tTotal Loss: 0.106652\n",
      "Reconstruction: 0.103245, Regularization: 0.003407\n",
      "2019-04-10 00:04:47,440 root         INFO     Train Epoch: 84 [2048/8000 (26%)]\tTotal Loss: 0.122467\n",
      "Reconstruction: 0.118649, Regularization: 0.003818\n",
      "2019-04-10 00:04:47,503 root         INFO     Train Epoch: 84 [2560/8000 (32%)]\tTotal Loss: 0.115950\n",
      "Reconstruction: 0.112005, Regularization: 0.003945\n",
      "2019-04-10 00:04:47,565 root         INFO     Train Epoch: 84 [3072/8000 (38%)]\tTotal Loss: 0.111863\n",
      "Reconstruction: 0.108185, Regularization: 0.003678\n",
      "2019-04-10 00:04:47,629 root         INFO     Train Epoch: 84 [3584/8000 (45%)]\tTotal Loss: 0.108699\n",
      "Reconstruction: 0.105740, Regularization: 0.002960\n",
      "2019-04-10 00:04:47,692 root         INFO     Train Epoch: 84 [4096/8000 (51%)]\tTotal Loss: 0.112391\n",
      "Reconstruction: 0.109153, Regularization: 0.003238\n",
      "2019-04-10 00:04:47,754 root         INFO     Train Epoch: 84 [4608/8000 (58%)]\tTotal Loss: 0.120960\n",
      "Reconstruction: 0.117322, Regularization: 0.003639\n",
      "2019-04-10 00:04:47,817 root         INFO     Train Epoch: 84 [5120/8000 (64%)]\tTotal Loss: 0.123796\n",
      "Reconstruction: 0.120616, Regularization: 0.003180\n",
      "2019-04-10 00:04:47,880 root         INFO     Train Epoch: 84 [5632/8000 (70%)]\tTotal Loss: 0.121447\n",
      "Reconstruction: 0.117812, Regularization: 0.003636\n",
      "2019-04-10 00:04:47,943 root         INFO     Train Epoch: 84 [6144/8000 (77%)]\tTotal Loss: 0.112333\n",
      "Reconstruction: 0.109514, Regularization: 0.002818\n",
      "2019-04-10 00:04:48,006 root         INFO     Train Epoch: 84 [6656/8000 (83%)]\tTotal Loss: 0.127079\n",
      "Reconstruction: 0.122870, Regularization: 0.004209\n",
      "2019-04-10 00:04:48,069 root         INFO     Train Epoch: 84 [7168/8000 (90%)]\tTotal Loss: 0.118345\n",
      "Reconstruction: 0.115971, Regularization: 0.002375\n",
      "2019-04-10 00:04:48,132 root         INFO     Train Epoch: 84 [7680/8000 (96%)]\tTotal Loss: 0.113162\n",
      "Reconstruction: 0.109587, Regularization: 0.003575\n",
      "2019-04-10 00:04:48,185 root         INFO     ====> Epoch: 84 Average loss: 0.1170\n",
      "2019-04-10 00:04:48,210 root         INFO     Train Epoch: 85 [0/8000 (0%)]\tTotal Loss: 0.115767\n",
      "Reconstruction: 0.112108, Regularization: 0.003658\n",
      "2019-04-10 00:04:48,274 root         INFO     Train Epoch: 85 [512/8000 (6%)]\tTotal Loss: 0.122198\n",
      "Reconstruction: 0.119323, Regularization: 0.002875\n",
      "2019-04-10 00:04:48,337 root         INFO     Train Epoch: 85 [1024/8000 (13%)]\tTotal Loss: 0.116069\n",
      "Reconstruction: 0.111998, Regularization: 0.004071\n",
      "2019-04-10 00:04:48,401 root         INFO     Train Epoch: 85 [1536/8000 (19%)]\tTotal Loss: 0.122571\n",
      "Reconstruction: 0.119491, Regularization: 0.003080\n",
      "2019-04-10 00:04:48,465 root         INFO     Train Epoch: 85 [2048/8000 (26%)]\tTotal Loss: 0.115619\n",
      "Reconstruction: 0.113107, Regularization: 0.002512\n",
      "2019-04-10 00:04:48,528 root         INFO     Train Epoch: 85 [2560/8000 (32%)]\tTotal Loss: 0.115358\n",
      "Reconstruction: 0.112719, Regularization: 0.002640\n",
      "2019-04-10 00:04:48,591 root         INFO     Train Epoch: 85 [3072/8000 (38%)]\tTotal Loss: 0.116549\n",
      "Reconstruction: 0.112421, Regularization: 0.004128\n",
      "2019-04-10 00:04:48,654 root         INFO     Train Epoch: 85 [3584/8000 (45%)]\tTotal Loss: 0.112835\n",
      "Reconstruction: 0.109411, Regularization: 0.003424\n",
      "2019-04-10 00:04:48,717 root         INFO     Train Epoch: 85 [4096/8000 (51%)]\tTotal Loss: 0.121298\n",
      "Reconstruction: 0.118188, Regularization: 0.003109\n",
      "2019-04-10 00:04:48,779 root         INFO     Train Epoch: 85 [4608/8000 (58%)]\tTotal Loss: 0.116249\n",
      "Reconstruction: 0.111511, Regularization: 0.004738\n",
      "2019-04-10 00:04:48,841 root         INFO     Train Epoch: 85 [5120/8000 (64%)]\tTotal Loss: 0.129255\n",
      "Reconstruction: 0.125068, Regularization: 0.004187\n",
      "2019-04-10 00:04:48,903 root         INFO     Train Epoch: 85 [5632/8000 (70%)]\tTotal Loss: 0.118148\n",
      "Reconstruction: 0.114891, Regularization: 0.003257\n",
      "2019-04-10 00:04:48,966 root         INFO     Train Epoch: 85 [6144/8000 (77%)]\tTotal Loss: 0.122996\n",
      "Reconstruction: 0.119585, Regularization: 0.003410\n",
      "2019-04-10 00:04:49,028 root         INFO     Train Epoch: 85 [6656/8000 (83%)]\tTotal Loss: 0.117432\n",
      "Reconstruction: 0.114480, Regularization: 0.002952\n",
      "2019-04-10 00:04:49,090 root         INFO     Train Epoch: 85 [7168/8000 (90%)]\tTotal Loss: 0.110804\n",
      "Reconstruction: 0.108034, Regularization: 0.002770\n",
      "2019-04-10 00:04:49,153 root         INFO     Train Epoch: 85 [7680/8000 (96%)]\tTotal Loss: 0.114809\n",
      "Reconstruction: 0.111221, Regularization: 0.003588\n",
      "2019-04-10 00:04:49,206 root         INFO     ====> Epoch: 85 Average loss: 0.1167\n",
      "2019-04-10 00:04:49,230 root         INFO     Train Epoch: 86 [0/8000 (0%)]\tTotal Loss: 0.124334\n",
      "Reconstruction: 0.120094, Regularization: 0.004240\n",
      "2019-04-10 00:04:49,293 root         INFO     Train Epoch: 86 [512/8000 (6%)]\tTotal Loss: 0.111406\n",
      "Reconstruction: 0.107828, Regularization: 0.003578\n",
      "2019-04-10 00:04:49,356 root         INFO     Train Epoch: 86 [1024/8000 (13%)]\tTotal Loss: 0.114098\n",
      "Reconstruction: 0.111642, Regularization: 0.002456\n",
      "2019-04-10 00:04:49,418 root         INFO     Train Epoch: 86 [1536/8000 (19%)]\tTotal Loss: 0.109759\n",
      "Reconstruction: 0.106725, Regularization: 0.003034\n",
      "2019-04-10 00:04:49,482 root         INFO     Train Epoch: 86 [2048/8000 (26%)]\tTotal Loss: 0.113873\n",
      "Reconstruction: 0.111043, Regularization: 0.002830\n",
      "2019-04-10 00:04:49,546 root         INFO     Train Epoch: 86 [2560/8000 (32%)]\tTotal Loss: 0.121582\n",
      "Reconstruction: 0.117461, Regularization: 0.004120\n",
      "2019-04-10 00:04:49,609 root         INFO     Train Epoch: 86 [3072/8000 (38%)]\tTotal Loss: 0.122261\n",
      "Reconstruction: 0.118625, Regularization: 0.003635\n",
      "2019-04-10 00:04:49,671 root         INFO     Train Epoch: 86 [3584/8000 (45%)]\tTotal Loss: 0.113459\n",
      "Reconstruction: 0.110114, Regularization: 0.003345\n",
      "2019-04-10 00:04:49,732 root         INFO     Train Epoch: 86 [4096/8000 (51%)]\tTotal Loss: 0.126389\n",
      "Reconstruction: 0.122124, Regularization: 0.004265\n",
      "2019-04-10 00:04:49,794 root         INFO     Train Epoch: 86 [4608/8000 (58%)]\tTotal Loss: 0.108364\n",
      "Reconstruction: 0.105414, Regularization: 0.002950\n",
      "2019-04-10 00:04:49,855 root         INFO     Train Epoch: 86 [5120/8000 (64%)]\tTotal Loss: 0.115502\n",
      "Reconstruction: 0.112287, Regularization: 0.003215\n",
      "2019-04-10 00:04:49,916 root         INFO     Train Epoch: 86 [5632/8000 (70%)]\tTotal Loss: 0.119964\n",
      "Reconstruction: 0.116506, Regularization: 0.003458\n",
      "2019-04-10 00:04:49,977 root         INFO     Train Epoch: 86 [6144/8000 (77%)]\tTotal Loss: 0.113163\n",
      "Reconstruction: 0.110473, Regularization: 0.002690\n",
      "2019-04-10 00:04:50,038 root         INFO     Train Epoch: 86 [6656/8000 (83%)]\tTotal Loss: 0.109907\n",
      "Reconstruction: 0.106908, Regularization: 0.002999\n",
      "2019-04-10 00:04:50,100 root         INFO     Train Epoch: 86 [7168/8000 (90%)]\tTotal Loss: 0.114445\n",
      "Reconstruction: 0.111108, Regularization: 0.003337\n",
      "2019-04-10 00:04:50,161 root         INFO     Train Epoch: 86 [7680/8000 (96%)]\tTotal Loss: 0.119231\n",
      "Reconstruction: 0.116354, Regularization: 0.002877\n",
      "2019-04-10 00:04:50,214 root         INFO     ====> Epoch: 86 Average loss: 0.1164\n",
      "2019-04-10 00:04:50,238 root         INFO     Train Epoch: 87 [0/8000 (0%)]\tTotal Loss: 0.111360\n",
      "Reconstruction: 0.109375, Regularization: 0.001985\n",
      "2019-04-10 00:04:50,301 root         INFO     Train Epoch: 87 [512/8000 (6%)]\tTotal Loss: 0.108110\n",
      "Reconstruction: 0.105258, Regularization: 0.002852\n",
      "2019-04-10 00:04:50,365 root         INFO     Train Epoch: 87 [1024/8000 (13%)]\tTotal Loss: 0.109519\n",
      "Reconstruction: 0.106926, Regularization: 0.002593\n",
      "2019-04-10 00:04:50,429 root         INFO     Train Epoch: 87 [1536/8000 (19%)]\tTotal Loss: 0.128702\n",
      "Reconstruction: 0.125490, Regularization: 0.003213\n",
      "2019-04-10 00:04:50,491 root         INFO     Train Epoch: 87 [2048/8000 (26%)]\tTotal Loss: 0.117611\n",
      "Reconstruction: 0.114828, Regularization: 0.002783\n",
      "2019-04-10 00:04:50,554 root         INFO     Train Epoch: 87 [2560/8000 (32%)]\tTotal Loss: 0.122936\n",
      "Reconstruction: 0.118658, Regularization: 0.004278\n",
      "2019-04-10 00:04:50,617 root         INFO     Train Epoch: 87 [3072/8000 (38%)]\tTotal Loss: 0.112031\n",
      "Reconstruction: 0.109861, Regularization: 0.002170\n",
      "2019-04-10 00:04:50,680 root         INFO     Train Epoch: 87 [3584/8000 (45%)]\tTotal Loss: 0.111376\n",
      "Reconstruction: 0.107508, Regularization: 0.003868\n",
      "2019-04-10 00:04:50,743 root         INFO     Train Epoch: 87 [4096/8000 (51%)]\tTotal Loss: 0.103173\n",
      "Reconstruction: 0.100825, Regularization: 0.002349\n",
      "2019-04-10 00:04:50,806 root         INFO     Train Epoch: 87 [4608/8000 (58%)]\tTotal Loss: 0.111757\n",
      "Reconstruction: 0.109338, Regularization: 0.002419\n",
      "2019-04-10 00:04:50,868 root         INFO     Train Epoch: 87 [5120/8000 (64%)]\tTotal Loss: 0.115248\n",
      "Reconstruction: 0.112154, Regularization: 0.003093\n",
      "2019-04-10 00:04:50,931 root         INFO     Train Epoch: 87 [5632/8000 (70%)]\tTotal Loss: 0.117745\n",
      "Reconstruction: 0.113954, Regularization: 0.003791\n",
      "2019-04-10 00:04:50,993 root         INFO     Train Epoch: 87 [6144/8000 (77%)]\tTotal Loss: 0.125741\n",
      "Reconstruction: 0.121965, Regularization: 0.003776\n",
      "2019-04-10 00:04:51,054 root         INFO     Train Epoch: 87 [6656/8000 (83%)]\tTotal Loss: 0.117646\n",
      "Reconstruction: 0.114549, Regularization: 0.003097\n",
      "2019-04-10 00:04:51,116 root         INFO     Train Epoch: 87 [7168/8000 (90%)]\tTotal Loss: 0.120174\n",
      "Reconstruction: 0.116242, Regularization: 0.003932\n",
      "2019-04-10 00:04:51,179 root         INFO     Train Epoch: 87 [7680/8000 (96%)]\tTotal Loss: 0.104617\n",
      "Reconstruction: 0.101238, Regularization: 0.003379\n",
      "2019-04-10 00:04:51,232 root         INFO     ====> Epoch: 87 Average loss: 0.1164\n",
      "2019-04-10 00:04:51,256 root         INFO     Train Epoch: 88 [0/8000 (0%)]\tTotal Loss: 0.113830\n",
      "Reconstruction: 0.111015, Regularization: 0.002815\n",
      "2019-04-10 00:04:51,320 root         INFO     Train Epoch: 88 [512/8000 (6%)]\tTotal Loss: 0.105172\n",
      "Reconstruction: 0.102355, Regularization: 0.002817\n",
      "2019-04-10 00:04:51,385 root         INFO     Train Epoch: 88 [1024/8000 (13%)]\tTotal Loss: 0.109642\n",
      "Reconstruction: 0.107393, Regularization: 0.002249\n",
      "2019-04-10 00:04:51,448 root         INFO     Train Epoch: 88 [1536/8000 (19%)]\tTotal Loss: 0.112292\n",
      "Reconstruction: 0.109421, Regularization: 0.002871\n",
      "2019-04-10 00:04:51,511 root         INFO     Train Epoch: 88 [2048/8000 (26%)]\tTotal Loss: 0.114496\n",
      "Reconstruction: 0.110830, Regularization: 0.003666\n",
      "2019-04-10 00:04:51,574 root         INFO     Train Epoch: 88 [2560/8000 (32%)]\tTotal Loss: 0.115746\n",
      "Reconstruction: 0.113156, Regularization: 0.002590\n",
      "2019-04-10 00:04:51,635 root         INFO     Train Epoch: 88 [3072/8000 (38%)]\tTotal Loss: 0.115762\n",
      "Reconstruction: 0.113234, Regularization: 0.002529\n",
      "2019-04-10 00:04:51,697 root         INFO     Train Epoch: 88 [3584/8000 (45%)]\tTotal Loss: 0.132493\n",
      "Reconstruction: 0.127930, Regularization: 0.004563\n",
      "2019-04-10 00:04:51,759 root         INFO     Train Epoch: 88 [4096/8000 (51%)]\tTotal Loss: 0.109040\n",
      "Reconstruction: 0.106232, Regularization: 0.002809\n",
      "2019-04-10 00:04:51,821 root         INFO     Train Epoch: 88 [4608/8000 (58%)]\tTotal Loss: 0.123823\n",
      "Reconstruction: 0.120898, Regularization: 0.002925\n",
      "2019-04-10 00:04:51,882 root         INFO     Train Epoch: 88 [5120/8000 (64%)]\tTotal Loss: 0.126013\n",
      "Reconstruction: 0.120863, Regularization: 0.005150\n",
      "2019-04-10 00:04:51,944 root         INFO     Train Epoch: 88 [5632/8000 (70%)]\tTotal Loss: 0.118628\n",
      "Reconstruction: 0.114850, Regularization: 0.003778\n",
      "2019-04-10 00:04:52,005 root         INFO     Train Epoch: 88 [6144/8000 (77%)]\tTotal Loss: 0.131993\n",
      "Reconstruction: 0.127229, Regularization: 0.004764\n",
      "2019-04-10 00:04:52,067 root         INFO     Train Epoch: 88 [6656/8000 (83%)]\tTotal Loss: 0.112164\n",
      "Reconstruction: 0.108670, Regularization: 0.003495\n",
      "2019-04-10 00:04:52,129 root         INFO     Train Epoch: 88 [7168/8000 (90%)]\tTotal Loss: 0.116350\n",
      "Reconstruction: 0.113227, Regularization: 0.003122\n",
      "2019-04-10 00:04:52,191 root         INFO     Train Epoch: 88 [7680/8000 (96%)]\tTotal Loss: 0.119417\n",
      "Reconstruction: 0.116912, Regularization: 0.002505\n",
      "2019-04-10 00:04:52,245 root         INFO     ====> Epoch: 88 Average loss: 0.1162\n",
      "2019-04-10 00:04:52,269 root         INFO     Train Epoch: 89 [0/8000 (0%)]\tTotal Loss: 0.114576\n",
      "Reconstruction: 0.110669, Regularization: 0.003908\n",
      "2019-04-10 00:04:52,333 root         INFO     Train Epoch: 89 [512/8000 (6%)]\tTotal Loss: 0.114564\n",
      "Reconstruction: 0.112554, Regularization: 0.002010\n",
      "2019-04-10 00:04:52,396 root         INFO     Train Epoch: 89 [1024/8000 (13%)]\tTotal Loss: 0.114961\n",
      "Reconstruction: 0.112862, Regularization: 0.002099\n",
      "2019-04-10 00:04:52,460 root         INFO     Train Epoch: 89 [1536/8000 (19%)]\tTotal Loss: 0.112098\n",
      "Reconstruction: 0.108600, Regularization: 0.003498\n",
      "2019-04-10 00:04:52,523 root         INFO     Train Epoch: 89 [2048/8000 (26%)]\tTotal Loss: 0.111362\n",
      "Reconstruction: 0.107515, Regularization: 0.003847\n",
      "2019-04-10 00:04:52,587 root         INFO     Train Epoch: 89 [2560/8000 (32%)]\tTotal Loss: 0.120183\n",
      "Reconstruction: 0.117217, Regularization: 0.002966\n",
      "2019-04-10 00:04:52,649 root         INFO     Train Epoch: 89 [3072/8000 (38%)]\tTotal Loss: 0.114039\n",
      "Reconstruction: 0.110634, Regularization: 0.003405\n",
      "2019-04-10 00:04:52,711 root         INFO     Train Epoch: 89 [3584/8000 (45%)]\tTotal Loss: 0.113684\n",
      "Reconstruction: 0.110964, Regularization: 0.002720\n",
      "2019-04-10 00:04:52,773 root         INFO     Train Epoch: 89 [4096/8000 (51%)]\tTotal Loss: 0.110720\n",
      "Reconstruction: 0.107870, Regularization: 0.002850\n",
      "2019-04-10 00:04:52,834 root         INFO     Train Epoch: 89 [4608/8000 (58%)]\tTotal Loss: 0.104755\n",
      "Reconstruction: 0.102119, Regularization: 0.002636\n",
      "2019-04-10 00:04:52,896 root         INFO     Train Epoch: 89 [5120/8000 (64%)]\tTotal Loss: 0.106880\n",
      "Reconstruction: 0.103937, Regularization: 0.002943\n",
      "2019-04-10 00:04:52,958 root         INFO     Train Epoch: 89 [5632/8000 (70%)]\tTotal Loss: 0.114058\n",
      "Reconstruction: 0.111281, Regularization: 0.002777\n",
      "2019-04-10 00:04:53,020 root         INFO     Train Epoch: 89 [6144/8000 (77%)]\tTotal Loss: 0.123021\n",
      "Reconstruction: 0.120429, Regularization: 0.002592\n",
      "2019-04-10 00:04:53,082 root         INFO     Train Epoch: 89 [6656/8000 (83%)]\tTotal Loss: 0.124496\n",
      "Reconstruction: 0.121311, Regularization: 0.003185\n",
      "2019-04-10 00:04:53,144 root         INFO     Train Epoch: 89 [7168/8000 (90%)]\tTotal Loss: 0.121269\n",
      "Reconstruction: 0.119039, Regularization: 0.002231\n",
      "2019-04-10 00:04:53,207 root         INFO     Train Epoch: 89 [7680/8000 (96%)]\tTotal Loss: 0.114708\n",
      "Reconstruction: 0.110994, Regularization: 0.003714\n",
      "2019-04-10 00:04:53,260 root         INFO     ====> Epoch: 89 Average loss: 0.1161\n",
      "2019-04-10 00:04:53,284 root         INFO     Train Epoch: 90 [0/8000 (0%)]\tTotal Loss: 0.115375\n",
      "Reconstruction: 0.111565, Regularization: 0.003810\n",
      "2019-04-10 00:04:53,349 root         INFO     Train Epoch: 90 [512/8000 (6%)]\tTotal Loss: 0.121012\n",
      "Reconstruction: 0.117862, Regularization: 0.003150\n",
      "2019-04-10 00:04:53,413 root         INFO     Train Epoch: 90 [1024/8000 (13%)]\tTotal Loss: 0.114518\n",
      "Reconstruction: 0.112269, Regularization: 0.002249\n",
      "2019-04-10 00:04:53,477 root         INFO     Train Epoch: 90 [1536/8000 (19%)]\tTotal Loss: 0.131877\n",
      "Reconstruction: 0.126244, Regularization: 0.005633\n",
      "2019-04-10 00:04:53,541 root         INFO     Train Epoch: 90 [2048/8000 (26%)]\tTotal Loss: 0.121772\n",
      "Reconstruction: 0.119204, Regularization: 0.002568\n",
      "2019-04-10 00:04:53,605 root         INFO     Train Epoch: 90 [2560/8000 (32%)]\tTotal Loss: 0.121147\n",
      "Reconstruction: 0.118837, Regularization: 0.002310\n",
      "2019-04-10 00:04:53,667 root         INFO     Train Epoch: 90 [3072/8000 (38%)]\tTotal Loss: 0.122878\n",
      "Reconstruction: 0.120449, Regularization: 0.002429\n",
      "2019-04-10 00:04:53,729 root         INFO     Train Epoch: 90 [3584/8000 (45%)]\tTotal Loss: 0.124082\n",
      "Reconstruction: 0.120138, Regularization: 0.003944\n",
      "2019-04-10 00:04:53,791 root         INFO     Train Epoch: 90 [4096/8000 (51%)]\tTotal Loss: 0.112880\n",
      "Reconstruction: 0.110022, Regularization: 0.002858\n",
      "2019-04-10 00:04:53,854 root         INFO     Train Epoch: 90 [4608/8000 (58%)]\tTotal Loss: 0.112299\n",
      "Reconstruction: 0.109483, Regularization: 0.002817\n",
      "2019-04-10 00:04:53,916 root         INFO     Train Epoch: 90 [5120/8000 (64%)]\tTotal Loss: 0.114445\n",
      "Reconstruction: 0.111754, Regularization: 0.002690\n",
      "2019-04-10 00:04:53,978 root         INFO     Train Epoch: 90 [5632/8000 (70%)]\tTotal Loss: 0.112458\n",
      "Reconstruction: 0.109340, Regularization: 0.003118\n",
      "2019-04-10 00:04:54,040 root         INFO     Train Epoch: 90 [6144/8000 (77%)]\tTotal Loss: 0.118208\n",
      "Reconstruction: 0.116031, Regularization: 0.002177\n",
      "2019-04-10 00:04:54,102 root         INFO     Train Epoch: 90 [6656/8000 (83%)]\tTotal Loss: 0.121956\n",
      "Reconstruction: 0.117932, Regularization: 0.004024\n",
      "2019-04-10 00:04:54,165 root         INFO     Train Epoch: 90 [7168/8000 (90%)]\tTotal Loss: 0.112923\n",
      "Reconstruction: 0.109850, Regularization: 0.003074\n",
      "2019-04-10 00:04:54,228 root         INFO     Train Epoch: 90 [7680/8000 (96%)]\tTotal Loss: 0.127140\n",
      "Reconstruction: 0.122647, Regularization: 0.004493\n",
      "2019-04-10 00:04:54,282 root         INFO     ====> Epoch: 90 Average loss: 0.1159\n",
      "2019-04-10 00:04:54,306 root         INFO     Train Epoch: 91 [0/8000 (0%)]\tTotal Loss: 0.112495\n",
      "Reconstruction: 0.109843, Regularization: 0.002652\n",
      "2019-04-10 00:04:54,368 root         INFO     Train Epoch: 91 [512/8000 (6%)]\tTotal Loss: 0.112190\n",
      "Reconstruction: 0.109359, Regularization: 0.002831\n",
      "2019-04-10 00:04:54,432 root         INFO     Train Epoch: 91 [1024/8000 (13%)]\tTotal Loss: 0.120781\n",
      "Reconstruction: 0.118438, Regularization: 0.002342\n",
      "2019-04-10 00:04:54,494 root         INFO     Train Epoch: 91 [1536/8000 (19%)]\tTotal Loss: 0.112694\n",
      "Reconstruction: 0.109317, Regularization: 0.003376\n",
      "2019-04-10 00:04:54,557 root         INFO     Train Epoch: 91 [2048/8000 (26%)]\tTotal Loss: 0.123197\n",
      "Reconstruction: 0.120281, Regularization: 0.002916\n",
      "2019-04-10 00:04:54,618 root         INFO     Train Epoch: 91 [2560/8000 (32%)]\tTotal Loss: 0.112333\n",
      "Reconstruction: 0.109927, Regularization: 0.002406\n",
      "2019-04-10 00:04:54,678 root         INFO     Train Epoch: 91 [3072/8000 (38%)]\tTotal Loss: 0.126230\n",
      "Reconstruction: 0.123573, Regularization: 0.002657\n",
      "2019-04-10 00:04:54,739 root         INFO     Train Epoch: 91 [3584/8000 (45%)]\tTotal Loss: 0.110171\n",
      "Reconstruction: 0.107673, Regularization: 0.002498\n",
      "2019-04-10 00:04:54,799 root         INFO     Train Epoch: 91 [4096/8000 (51%)]\tTotal Loss: 0.107171\n",
      "Reconstruction: 0.105170, Regularization: 0.002001\n",
      "2019-04-10 00:04:54,859 root         INFO     Train Epoch: 91 [4608/8000 (58%)]\tTotal Loss: 0.126924\n",
      "Reconstruction: 0.122571, Regularization: 0.004352\n",
      "2019-04-10 00:04:54,921 root         INFO     Train Epoch: 91 [5120/8000 (64%)]\tTotal Loss: 0.114356\n",
      "Reconstruction: 0.112657, Regularization: 0.001699\n",
      "2019-04-10 00:04:54,982 root         INFO     Train Epoch: 91 [5632/8000 (70%)]\tTotal Loss: 0.118274\n",
      "Reconstruction: 0.115714, Regularization: 0.002560\n",
      "2019-04-10 00:04:55,043 root         INFO     Train Epoch: 91 [6144/8000 (77%)]\tTotal Loss: 0.105218\n",
      "Reconstruction: 0.102919, Regularization: 0.002299\n",
      "2019-04-10 00:04:55,103 root         INFO     Train Epoch: 91 [6656/8000 (83%)]\tTotal Loss: 0.120591\n",
      "Reconstruction: 0.117445, Regularization: 0.003146\n",
      "2019-04-10 00:04:55,162 root         INFO     Train Epoch: 91 [7168/8000 (90%)]\tTotal Loss: 0.116789\n",
      "Reconstruction: 0.113883, Regularization: 0.002906\n",
      "2019-04-10 00:04:55,222 root         INFO     Train Epoch: 91 [7680/8000 (96%)]\tTotal Loss: 0.118116\n",
      "Reconstruction: 0.115051, Regularization: 0.003065\n",
      "2019-04-10 00:04:55,275 root         INFO     ====> Epoch: 91 Average loss: 0.1154\n",
      "2019-04-10 00:04:55,298 root         INFO     Train Epoch: 92 [0/8000 (0%)]\tTotal Loss: 0.113101\n",
      "Reconstruction: 0.110696, Regularization: 0.002405\n",
      "2019-04-10 00:04:55,362 root         INFO     Train Epoch: 92 [512/8000 (6%)]\tTotal Loss: 0.104734\n",
      "Reconstruction: 0.103134, Regularization: 0.001599\n",
      "2019-04-10 00:04:55,425 root         INFO     Train Epoch: 92 [1024/8000 (13%)]\tTotal Loss: 0.114158\n",
      "Reconstruction: 0.111911, Regularization: 0.002247\n",
      "2019-04-10 00:04:55,488 root         INFO     Train Epoch: 92 [1536/8000 (19%)]\tTotal Loss: 0.115649\n",
      "Reconstruction: 0.113355, Regularization: 0.002293\n",
      "2019-04-10 00:04:55,551 root         INFO     Train Epoch: 92 [2048/8000 (26%)]\tTotal Loss: 0.109002\n",
      "Reconstruction: 0.107134, Regularization: 0.001868\n",
      "2019-04-10 00:04:55,613 root         INFO     Train Epoch: 92 [2560/8000 (32%)]\tTotal Loss: 0.120966\n",
      "Reconstruction: 0.117955, Regularization: 0.003011\n",
      "2019-04-10 00:04:55,676 root         INFO     Train Epoch: 92 [3072/8000 (38%)]\tTotal Loss: 0.119649\n",
      "Reconstruction: 0.115898, Regularization: 0.003751\n",
      "2019-04-10 00:04:55,739 root         INFO     Train Epoch: 92 [3584/8000 (45%)]\tTotal Loss: 0.107660\n",
      "Reconstruction: 0.105395, Regularization: 0.002265\n",
      "2019-04-10 00:04:55,801 root         INFO     Train Epoch: 92 [4096/8000 (51%)]\tTotal Loss: 0.120662\n",
      "Reconstruction: 0.117665, Regularization: 0.002997\n",
      "2019-04-10 00:04:55,863 root         INFO     Train Epoch: 92 [4608/8000 (58%)]\tTotal Loss: 0.118563\n",
      "Reconstruction: 0.115480, Regularization: 0.003083\n",
      "2019-04-10 00:04:55,924 root         INFO     Train Epoch: 92 [5120/8000 (64%)]\tTotal Loss: 0.113976\n",
      "Reconstruction: 0.111262, Regularization: 0.002714\n",
      "2019-04-10 00:04:55,986 root         INFO     Train Epoch: 92 [5632/8000 (70%)]\tTotal Loss: 0.113705\n",
      "Reconstruction: 0.110382, Regularization: 0.003323\n",
      "2019-04-10 00:04:56,048 root         INFO     Train Epoch: 92 [6144/8000 (77%)]\tTotal Loss: 0.118489\n",
      "Reconstruction: 0.115878, Regularization: 0.002611\n",
      "2019-04-10 00:04:56,109 root         INFO     Train Epoch: 92 [6656/8000 (83%)]\tTotal Loss: 0.115145\n",
      "Reconstruction: 0.112249, Regularization: 0.002896\n",
      "2019-04-10 00:04:56,171 root         INFO     Train Epoch: 92 [7168/8000 (90%)]\tTotal Loss: 0.114601\n",
      "Reconstruction: 0.111487, Regularization: 0.003114\n",
      "2019-04-10 00:04:56,232 root         INFO     Train Epoch: 92 [7680/8000 (96%)]\tTotal Loss: 0.115321\n",
      "Reconstruction: 0.112660, Regularization: 0.002661\n",
      "2019-04-10 00:04:56,284 root         INFO     ====> Epoch: 92 Average loss: 0.1153\n",
      "2019-04-10 00:04:56,309 root         INFO     Train Epoch: 93 [0/8000 (0%)]\tTotal Loss: 0.121410\n",
      "Reconstruction: 0.117444, Regularization: 0.003966\n",
      "2019-04-10 00:04:56,372 root         INFO     Train Epoch: 93 [512/8000 (6%)]\tTotal Loss: 0.120516\n",
      "Reconstruction: 0.117676, Regularization: 0.002840\n",
      "2019-04-10 00:04:56,435 root         INFO     Train Epoch: 93 [1024/8000 (13%)]\tTotal Loss: 0.122315\n",
      "Reconstruction: 0.118444, Regularization: 0.003871\n",
      "2019-04-10 00:04:56,499 root         INFO     Train Epoch: 93 [1536/8000 (19%)]\tTotal Loss: 0.112019\n",
      "Reconstruction: 0.109002, Regularization: 0.003018\n",
      "2019-04-10 00:04:56,562 root         INFO     Train Epoch: 93 [2048/8000 (26%)]\tTotal Loss: 0.118954\n",
      "Reconstruction: 0.115377, Regularization: 0.003577\n",
      "2019-04-10 00:04:56,625 root         INFO     Train Epoch: 93 [2560/8000 (32%)]\tTotal Loss: 0.118407\n",
      "Reconstruction: 0.115311, Regularization: 0.003096\n",
      "2019-04-10 00:04:56,689 root         INFO     Train Epoch: 93 [3072/8000 (38%)]\tTotal Loss: 0.098085\n",
      "Reconstruction: 0.095626, Regularization: 0.002459\n",
      "2019-04-10 00:04:56,752 root         INFO     Train Epoch: 93 [3584/8000 (45%)]\tTotal Loss: 0.113760\n",
      "Reconstruction: 0.111309, Regularization: 0.002451\n",
      "2019-04-10 00:04:56,815 root         INFO     Train Epoch: 93 [4096/8000 (51%)]\tTotal Loss: 0.110146\n",
      "Reconstruction: 0.106509, Regularization: 0.003636\n",
      "2019-04-10 00:04:56,877 root         INFO     Train Epoch: 93 [4608/8000 (58%)]\tTotal Loss: 0.118848\n",
      "Reconstruction: 0.115898, Regularization: 0.002950\n",
      "2019-04-10 00:04:56,940 root         INFO     Train Epoch: 93 [5120/8000 (64%)]\tTotal Loss: 0.101384\n",
      "Reconstruction: 0.099778, Regularization: 0.001606\n",
      "2019-04-10 00:04:57,002 root         INFO     Train Epoch: 93 [5632/8000 (70%)]\tTotal Loss: 0.117695\n",
      "Reconstruction: 0.113805, Regularization: 0.003889\n",
      "2019-04-10 00:04:57,065 root         INFO     Train Epoch: 93 [6144/8000 (77%)]\tTotal Loss: 0.111519\n",
      "Reconstruction: 0.108981, Regularization: 0.002538\n",
      "2019-04-10 00:04:57,127 root         INFO     Train Epoch: 93 [6656/8000 (83%)]\tTotal Loss: 0.107787\n",
      "Reconstruction: 0.105757, Regularization: 0.002030\n",
      "2019-04-10 00:04:57,190 root         INFO     Train Epoch: 93 [7168/8000 (90%)]\tTotal Loss: 0.132303\n",
      "Reconstruction: 0.129011, Regularization: 0.003292\n",
      "2019-04-10 00:04:57,253 root         INFO     Train Epoch: 93 [7680/8000 (96%)]\tTotal Loss: 0.116299\n",
      "Reconstruction: 0.113263, Regularization: 0.003036\n",
      "2019-04-10 00:04:57,306 root         INFO     ====> Epoch: 93 Average loss: 0.1156\n",
      "2019-04-10 00:04:57,329 root         INFO     Train Epoch: 94 [0/8000 (0%)]\tTotal Loss: 0.120905\n",
      "Reconstruction: 0.116466, Regularization: 0.004439\n",
      "2019-04-10 00:04:57,393 root         INFO     Train Epoch: 94 [512/8000 (6%)]\tTotal Loss: 0.117422\n",
      "Reconstruction: 0.114108, Regularization: 0.003315\n",
      "2019-04-10 00:04:57,456 root         INFO     Train Epoch: 94 [1024/8000 (13%)]\tTotal Loss: 0.106845\n",
      "Reconstruction: 0.104877, Regularization: 0.001969\n",
      "2019-04-10 00:04:57,518 root         INFO     Train Epoch: 94 [1536/8000 (19%)]\tTotal Loss: 0.109488\n",
      "Reconstruction: 0.107194, Regularization: 0.002294\n",
      "2019-04-10 00:04:57,581 root         INFO     Train Epoch: 94 [2048/8000 (26%)]\tTotal Loss: 0.119791\n",
      "Reconstruction: 0.117262, Regularization: 0.002529\n",
      "2019-04-10 00:04:57,643 root         INFO     Train Epoch: 94 [2560/8000 (32%)]\tTotal Loss: 0.122827\n",
      "Reconstruction: 0.120463, Regularization: 0.002363\n",
      "2019-04-10 00:04:57,705 root         INFO     Train Epoch: 94 [3072/8000 (38%)]\tTotal Loss: 0.114030\n",
      "Reconstruction: 0.111132, Regularization: 0.002897\n",
      "2019-04-10 00:04:57,768 root         INFO     Train Epoch: 94 [3584/8000 (45%)]\tTotal Loss: 0.113024\n",
      "Reconstruction: 0.109966, Regularization: 0.003057\n",
      "2019-04-10 00:04:57,830 root         INFO     Train Epoch: 94 [4096/8000 (51%)]\tTotal Loss: 0.113558\n",
      "Reconstruction: 0.111006, Regularization: 0.002553\n",
      "2019-04-10 00:04:57,892 root         INFO     Train Epoch: 94 [4608/8000 (58%)]\tTotal Loss: 0.117823\n",
      "Reconstruction: 0.114361, Regularization: 0.003462\n",
      "2019-04-10 00:04:57,954 root         INFO     Train Epoch: 94 [5120/8000 (64%)]\tTotal Loss: 0.116918\n",
      "Reconstruction: 0.113672, Regularization: 0.003246\n",
      "2019-04-10 00:04:58,017 root         INFO     Train Epoch: 94 [5632/8000 (70%)]\tTotal Loss: 0.111572\n",
      "Reconstruction: 0.108650, Regularization: 0.002922\n",
      "2019-04-10 00:04:58,079 root         INFO     Train Epoch: 94 [6144/8000 (77%)]\tTotal Loss: 0.103292\n",
      "Reconstruction: 0.100552, Regularization: 0.002740\n",
      "2019-04-10 00:04:58,141 root         INFO     Train Epoch: 94 [6656/8000 (83%)]\tTotal Loss: 0.114888\n",
      "Reconstruction: 0.112284, Regularization: 0.002605\n",
      "2019-04-10 00:04:58,202 root         INFO     Train Epoch: 94 [7168/8000 (90%)]\tTotal Loss: 0.116455\n",
      "Reconstruction: 0.113988, Regularization: 0.002467\n",
      "2019-04-10 00:04:58,264 root         INFO     Train Epoch: 94 [7680/8000 (96%)]\tTotal Loss: 0.110310\n",
      "Reconstruction: 0.108061, Regularization: 0.002249\n",
      "2019-04-10 00:04:58,318 root         INFO     ====> Epoch: 94 Average loss: 0.1152\n",
      "2019-04-10 00:04:58,342 root         INFO     Train Epoch: 95 [0/8000 (0%)]\tTotal Loss: 0.116751\n",
      "Reconstruction: 0.114429, Regularization: 0.002323\n",
      "2019-04-10 00:04:58,405 root         INFO     Train Epoch: 95 [512/8000 (6%)]\tTotal Loss: 0.118688\n",
      "Reconstruction: 0.116275, Regularization: 0.002412\n",
      "2019-04-10 00:04:58,468 root         INFO     Train Epoch: 95 [1024/8000 (13%)]\tTotal Loss: 0.109382\n",
      "Reconstruction: 0.106880, Regularization: 0.002502\n",
      "2019-04-10 00:04:58,532 root         INFO     Train Epoch: 95 [1536/8000 (19%)]\tTotal Loss: 0.120432\n",
      "Reconstruction: 0.117402, Regularization: 0.003030\n",
      "2019-04-10 00:04:58,595 root         INFO     Train Epoch: 95 [2048/8000 (26%)]\tTotal Loss: 0.112908\n",
      "Reconstruction: 0.109222, Regularization: 0.003686\n",
      "2019-04-10 00:04:58,658 root         INFO     Train Epoch: 95 [2560/8000 (32%)]\tTotal Loss: 0.116038\n",
      "Reconstruction: 0.113278, Regularization: 0.002760\n",
      "2019-04-10 00:04:58,721 root         INFO     Train Epoch: 95 [3072/8000 (38%)]\tTotal Loss: 0.117475\n",
      "Reconstruction: 0.114593, Regularization: 0.002882\n",
      "2019-04-10 00:04:58,784 root         INFO     Train Epoch: 95 [3584/8000 (45%)]\tTotal Loss: 0.109145\n",
      "Reconstruction: 0.107685, Regularization: 0.001460\n",
      "2019-04-10 00:04:58,847 root         INFO     Train Epoch: 95 [4096/8000 (51%)]\tTotal Loss: 0.129384\n",
      "Reconstruction: 0.126376, Regularization: 0.003008\n",
      "2019-04-10 00:04:58,909 root         INFO     Train Epoch: 95 [4608/8000 (58%)]\tTotal Loss: 0.113654\n",
      "Reconstruction: 0.111427, Regularization: 0.002227\n",
      "2019-04-10 00:04:58,971 root         INFO     Train Epoch: 95 [5120/8000 (64%)]\tTotal Loss: 0.120315\n",
      "Reconstruction: 0.116249, Regularization: 0.004066\n",
      "2019-04-10 00:04:59,034 root         INFO     Train Epoch: 95 [5632/8000 (70%)]\tTotal Loss: 0.118516\n",
      "Reconstruction: 0.115303, Regularization: 0.003213\n",
      "2019-04-10 00:04:59,096 root         INFO     Train Epoch: 95 [6144/8000 (77%)]\tTotal Loss: 0.106841\n",
      "Reconstruction: 0.103967, Regularization: 0.002874\n",
      "2019-04-10 00:04:59,159 root         INFO     Train Epoch: 95 [6656/8000 (83%)]\tTotal Loss: 0.117483\n",
      "Reconstruction: 0.113175, Regularization: 0.004308\n",
      "2019-04-10 00:04:59,222 root         INFO     Train Epoch: 95 [7168/8000 (90%)]\tTotal Loss: 0.112673\n",
      "Reconstruction: 0.110003, Regularization: 0.002670\n",
      "2019-04-10 00:04:59,285 root         INFO     Train Epoch: 95 [7680/8000 (96%)]\tTotal Loss: 0.120041\n",
      "Reconstruction: 0.117001, Regularization: 0.003041\n",
      "2019-04-10 00:04:59,338 root         INFO     ====> Epoch: 95 Average loss: 0.1149\n",
      "2019-04-10 00:04:59,362 root         INFO     Train Epoch: 96 [0/8000 (0%)]\tTotal Loss: 0.118730\n",
      "Reconstruction: 0.115935, Regularization: 0.002794\n",
      "2019-04-10 00:04:59,425 root         INFO     Train Epoch: 96 [512/8000 (6%)]\tTotal Loss: 0.110506\n",
      "Reconstruction: 0.108223, Regularization: 0.002283\n",
      "2019-04-10 00:04:59,489 root         INFO     Train Epoch: 96 [1024/8000 (13%)]\tTotal Loss: 0.121353\n",
      "Reconstruction: 0.116871, Regularization: 0.004482\n",
      "2019-04-10 00:04:59,552 root         INFO     Train Epoch: 96 [1536/8000 (19%)]\tTotal Loss: 0.106974\n",
      "Reconstruction: 0.104143, Regularization: 0.002830\n",
      "2019-04-10 00:04:59,614 root         INFO     Train Epoch: 96 [2048/8000 (26%)]\tTotal Loss: 0.115540\n",
      "Reconstruction: 0.112265, Regularization: 0.003275\n",
      "2019-04-10 00:04:59,677 root         INFO     Train Epoch: 96 [2560/8000 (32%)]\tTotal Loss: 0.123088\n",
      "Reconstruction: 0.120195, Regularization: 0.002893\n",
      "2019-04-10 00:04:59,739 root         INFO     Train Epoch: 96 [3072/8000 (38%)]\tTotal Loss: 0.115435\n",
      "Reconstruction: 0.112270, Regularization: 0.003165\n",
      "2019-04-10 00:04:59,801 root         INFO     Train Epoch: 96 [3584/8000 (45%)]\tTotal Loss: 0.117527\n",
      "Reconstruction: 0.114844, Regularization: 0.002683\n",
      "2019-04-10 00:04:59,864 root         INFO     Train Epoch: 96 [4096/8000 (51%)]\tTotal Loss: 0.131996\n",
      "Reconstruction: 0.128291, Regularization: 0.003705\n",
      "2019-04-10 00:04:59,927 root         INFO     Train Epoch: 96 [4608/8000 (58%)]\tTotal Loss: 0.120585\n",
      "Reconstruction: 0.116110, Regularization: 0.004475\n",
      "2019-04-10 00:04:59,990 root         INFO     Train Epoch: 96 [5120/8000 (64%)]\tTotal Loss: 0.118782\n",
      "Reconstruction: 0.115729, Regularization: 0.003053\n",
      "2019-04-10 00:05:00,053 root         INFO     Train Epoch: 96 [5632/8000 (70%)]\tTotal Loss: 0.122929\n",
      "Reconstruction: 0.119281, Regularization: 0.003648\n",
      "2019-04-10 00:05:00,116 root         INFO     Train Epoch: 96 [6144/8000 (77%)]\tTotal Loss: 0.121319\n",
      "Reconstruction: 0.117734, Regularization: 0.003585\n",
      "2019-04-10 00:05:00,179 root         INFO     Train Epoch: 96 [6656/8000 (83%)]\tTotal Loss: 0.117010\n",
      "Reconstruction: 0.114488, Regularization: 0.002521\n",
      "2019-04-10 00:05:00,243 root         INFO     Train Epoch: 96 [7168/8000 (90%)]\tTotal Loss: 0.118733\n",
      "Reconstruction: 0.115890, Regularization: 0.002843\n",
      "2019-04-10 00:05:00,306 root         INFO     Train Epoch: 96 [7680/8000 (96%)]\tTotal Loss: 0.121707\n",
      "Reconstruction: 0.117736, Regularization: 0.003972\n",
      "2019-04-10 00:05:00,360 root         INFO     ====> Epoch: 96 Average loss: 0.1148\n",
      "2019-04-10 00:05:00,384 root         INFO     Train Epoch: 97 [0/8000 (0%)]\tTotal Loss: 0.120067\n",
      "Reconstruction: 0.116501, Regularization: 0.003566\n",
      "2019-04-10 00:05:00,448 root         INFO     Train Epoch: 97 [512/8000 (6%)]\tTotal Loss: 0.117444\n",
      "Reconstruction: 0.115146, Regularization: 0.002298\n",
      "2019-04-10 00:05:00,510 root         INFO     Train Epoch: 97 [1024/8000 (13%)]\tTotal Loss: 0.110820\n",
      "Reconstruction: 0.108109, Regularization: 0.002711\n",
      "2019-04-10 00:05:00,573 root         INFO     Train Epoch: 97 [1536/8000 (19%)]\tTotal Loss: 0.118995\n",
      "Reconstruction: 0.116987, Regularization: 0.002008\n",
      "2019-04-10 00:05:00,635 root         INFO     Train Epoch: 97 [2048/8000 (26%)]\tTotal Loss: 0.107113\n",
      "Reconstruction: 0.104526, Regularization: 0.002587\n",
      "2019-04-10 00:05:00,698 root         INFO     Train Epoch: 97 [2560/8000 (32%)]\tTotal Loss: 0.109658\n",
      "Reconstruction: 0.106923, Regularization: 0.002735\n",
      "2019-04-10 00:05:00,761 root         INFO     Train Epoch: 97 [3072/8000 (38%)]\tTotal Loss: 0.121978\n",
      "Reconstruction: 0.118927, Regularization: 0.003051\n",
      "2019-04-10 00:05:00,823 root         INFO     Train Epoch: 97 [3584/8000 (45%)]\tTotal Loss: 0.121258\n",
      "Reconstruction: 0.116774, Regularization: 0.004484\n",
      "2019-04-10 00:05:00,885 root         INFO     Train Epoch: 97 [4096/8000 (51%)]\tTotal Loss: 0.107252\n",
      "Reconstruction: 0.105351, Regularization: 0.001901\n",
      "2019-04-10 00:05:00,947 root         INFO     Train Epoch: 97 [4608/8000 (58%)]\tTotal Loss: 0.105824\n",
      "Reconstruction: 0.103054, Regularization: 0.002770\n",
      "2019-04-10 00:05:01,010 root         INFO     Train Epoch: 97 [5120/8000 (64%)]\tTotal Loss: 0.114018\n",
      "Reconstruction: 0.111052, Regularization: 0.002967\n",
      "2019-04-10 00:05:01,074 root         INFO     Train Epoch: 97 [5632/8000 (70%)]\tTotal Loss: 0.106073\n",
      "Reconstruction: 0.103764, Regularization: 0.002309\n",
      "2019-04-10 00:05:01,138 root         INFO     Train Epoch: 97 [6144/8000 (77%)]\tTotal Loss: 0.125165\n",
      "Reconstruction: 0.122592, Regularization: 0.002573\n",
      "2019-04-10 00:05:01,201 root         INFO     Train Epoch: 97 [6656/8000 (83%)]\tTotal Loss: 0.114532\n",
      "Reconstruction: 0.110756, Regularization: 0.003776\n",
      "2019-04-10 00:05:01,264 root         INFO     Train Epoch: 97 [7168/8000 (90%)]\tTotal Loss: 0.108161\n",
      "Reconstruction: 0.105753, Regularization: 0.002408\n",
      "2019-04-10 00:05:01,328 root         INFO     Train Epoch: 97 [7680/8000 (96%)]\tTotal Loss: 0.114418\n",
      "Reconstruction: 0.111977, Regularization: 0.002441\n",
      "2019-04-10 00:05:01,382 root         INFO     ====> Epoch: 97 Average loss: 0.1147\n",
      "2019-04-10 00:05:01,407 root         INFO     Train Epoch: 98 [0/8000 (0%)]\tTotal Loss: 0.114747\n",
      "Reconstruction: 0.110600, Regularization: 0.004147\n",
      "2019-04-10 00:05:01,472 root         INFO     Train Epoch: 98 [512/8000 (6%)]\tTotal Loss: 0.117874\n",
      "Reconstruction: 0.115553, Regularization: 0.002321\n",
      "2019-04-10 00:05:01,535 root         INFO     Train Epoch: 98 [1024/8000 (13%)]\tTotal Loss: 0.102278\n",
      "Reconstruction: 0.100264, Regularization: 0.002015\n",
      "2019-04-10 00:05:01,599 root         INFO     Train Epoch: 98 [1536/8000 (19%)]\tTotal Loss: 0.133092\n",
      "Reconstruction: 0.128931, Regularization: 0.004161\n",
      "2019-04-10 00:05:01,663 root         INFO     Train Epoch: 98 [2048/8000 (26%)]\tTotal Loss: 0.108586\n",
      "Reconstruction: 0.106140, Regularization: 0.002446\n",
      "2019-04-10 00:05:01,727 root         INFO     Train Epoch: 98 [2560/8000 (32%)]\tTotal Loss: 0.119492\n",
      "Reconstruction: 0.116280, Regularization: 0.003212\n",
      "2019-04-10 00:05:01,791 root         INFO     Train Epoch: 98 [3072/8000 (38%)]\tTotal Loss: 0.107205\n",
      "Reconstruction: 0.105016, Regularization: 0.002189\n",
      "2019-04-10 00:05:01,855 root         INFO     Train Epoch: 98 [3584/8000 (45%)]\tTotal Loss: 0.109210\n",
      "Reconstruction: 0.105788, Regularization: 0.003422\n",
      "2019-04-10 00:05:01,919 root         INFO     Train Epoch: 98 [4096/8000 (51%)]\tTotal Loss: 0.113916\n",
      "Reconstruction: 0.110507, Regularization: 0.003409\n",
      "2019-04-10 00:05:01,983 root         INFO     Train Epoch: 98 [4608/8000 (58%)]\tTotal Loss: 0.117715\n",
      "Reconstruction: 0.113673, Regularization: 0.004041\n",
      "2019-04-10 00:05:02,047 root         INFO     Train Epoch: 98 [5120/8000 (64%)]\tTotal Loss: 0.110149\n",
      "Reconstruction: 0.107341, Regularization: 0.002809\n",
      "2019-04-10 00:05:02,110 root         INFO     Train Epoch: 98 [5632/8000 (70%)]\tTotal Loss: 0.118802\n",
      "Reconstruction: 0.115906, Regularization: 0.002896\n",
      "2019-04-10 00:05:02,174 root         INFO     Train Epoch: 98 [6144/8000 (77%)]\tTotal Loss: 0.113517\n",
      "Reconstruction: 0.110266, Regularization: 0.003252\n",
      "2019-04-10 00:05:02,239 root         INFO     Train Epoch: 98 [6656/8000 (83%)]\tTotal Loss: 0.114635\n",
      "Reconstruction: 0.111559, Regularization: 0.003077\n",
      "2019-04-10 00:05:02,303 root         INFO     Train Epoch: 98 [7168/8000 (90%)]\tTotal Loss: 0.107384\n",
      "Reconstruction: 0.104873, Regularization: 0.002511\n",
      "2019-04-10 00:05:02,366 root         INFO     Train Epoch: 98 [7680/8000 (96%)]\tTotal Loss: 0.114355\n",
      "Reconstruction: 0.111166, Regularization: 0.003189\n",
      "2019-04-10 00:05:02,421 root         INFO     ====> Epoch: 98 Average loss: 0.1144\n",
      "2019-04-10 00:05:02,445 root         INFO     Train Epoch: 99 [0/8000 (0%)]\tTotal Loss: 0.118821\n",
      "Reconstruction: 0.116079, Regularization: 0.002742\n",
      "2019-04-10 00:05:02,510 root         INFO     Train Epoch: 99 [512/8000 (6%)]\tTotal Loss: 0.109667\n",
      "Reconstruction: 0.106054, Regularization: 0.003613\n",
      "2019-04-10 00:05:02,574 root         INFO     Train Epoch: 99 [1024/8000 (13%)]\tTotal Loss: 0.122565\n",
      "Reconstruction: 0.117962, Regularization: 0.004602\n",
      "2019-04-10 00:05:02,639 root         INFO     Train Epoch: 99 [1536/8000 (19%)]\tTotal Loss: 0.113431\n",
      "Reconstruction: 0.110305, Regularization: 0.003125\n",
      "2019-04-10 00:05:02,703 root         INFO     Train Epoch: 99 [2048/8000 (26%)]\tTotal Loss: 0.114221\n",
      "Reconstruction: 0.110625, Regularization: 0.003596\n",
      "2019-04-10 00:05:02,767 root         INFO     Train Epoch: 99 [2560/8000 (32%)]\tTotal Loss: 0.118805\n",
      "Reconstruction: 0.116002, Regularization: 0.002803\n",
      "2019-04-10 00:05:02,831 root         INFO     Train Epoch: 99 [3072/8000 (38%)]\tTotal Loss: 0.116256\n",
      "Reconstruction: 0.113335, Regularization: 0.002921\n",
      "2019-04-10 00:05:02,894 root         INFO     Train Epoch: 99 [3584/8000 (45%)]\tTotal Loss: 0.115615\n",
      "Reconstruction: 0.112961, Regularization: 0.002654\n",
      "2019-04-10 00:05:02,959 root         INFO     Train Epoch: 99 [4096/8000 (51%)]\tTotal Loss: 0.122426\n",
      "Reconstruction: 0.118116, Regularization: 0.004310\n",
      "2019-04-10 00:05:03,023 root         INFO     Train Epoch: 99 [4608/8000 (58%)]\tTotal Loss: 0.126794\n",
      "Reconstruction: 0.123034, Regularization: 0.003760\n",
      "2019-04-10 00:05:03,085 root         INFO     Train Epoch: 99 [5120/8000 (64%)]\tTotal Loss: 0.115589\n",
      "Reconstruction: 0.112671, Regularization: 0.002918\n",
      "2019-04-10 00:05:03,149 root         INFO     Train Epoch: 99 [5632/8000 (70%)]\tTotal Loss: 0.108676\n",
      "Reconstruction: 0.106143, Regularization: 0.002533\n",
      "2019-04-10 00:05:03,212 root         INFO     Train Epoch: 99 [6144/8000 (77%)]\tTotal Loss: 0.116110\n",
      "Reconstruction: 0.112223, Regularization: 0.003887\n",
      "2019-04-10 00:05:03,275 root         INFO     Train Epoch: 99 [6656/8000 (83%)]\tTotal Loss: 0.118011\n",
      "Reconstruction: 0.114779, Regularization: 0.003232\n",
      "2019-04-10 00:05:03,337 root         INFO     Train Epoch: 99 [7168/8000 (90%)]\tTotal Loss: 0.119631\n",
      "Reconstruction: 0.116541, Regularization: 0.003089\n",
      "2019-04-10 00:05:03,400 root         INFO     Train Epoch: 99 [7680/8000 (96%)]\tTotal Loss: 0.102519\n",
      "Reconstruction: 0.099508, Regularization: 0.003011\n",
      "2019-04-10 00:05:03,456 root         INFO     ====> Epoch: 99 Average loss: 0.1144\n",
      "2019-04-10 00:05:03,480 root         INFO     Train Epoch: 100 [0/8000 (0%)]\tTotal Loss: 0.118598\n",
      "Reconstruction: 0.115347, Regularization: 0.003250\n",
      "2019-04-10 00:05:03,546 root         INFO     Train Epoch: 100 [512/8000 (6%)]\tTotal Loss: 0.119510\n",
      "Reconstruction: 0.115732, Regularization: 0.003778\n",
      "2019-04-10 00:05:03,612 root         INFO     Train Epoch: 100 [1024/8000 (13%)]\tTotal Loss: 0.107091\n",
      "Reconstruction: 0.105054, Regularization: 0.002037\n",
      "2019-04-10 00:05:03,679 root         INFO     Train Epoch: 100 [1536/8000 (19%)]\tTotal Loss: 0.117059\n",
      "Reconstruction: 0.113499, Regularization: 0.003560\n",
      "2019-04-10 00:05:03,744 root         INFO     Train Epoch: 100 [2048/8000 (26%)]\tTotal Loss: 0.113379\n",
      "Reconstruction: 0.109725, Regularization: 0.003653\n",
      "2019-04-10 00:05:03,807 root         INFO     Train Epoch: 100 [2560/8000 (32%)]\tTotal Loss: 0.121209\n",
      "Reconstruction: 0.115886, Regularization: 0.005323\n",
      "2019-04-10 00:05:03,873 root         INFO     Train Epoch: 100 [3072/8000 (38%)]\tTotal Loss: 0.113028\n",
      "Reconstruction: 0.109814, Regularization: 0.003215\n",
      "2019-04-10 00:05:03,935 root         INFO     Train Epoch: 100 [3584/8000 (45%)]\tTotal Loss: 0.113607\n",
      "Reconstruction: 0.110005, Regularization: 0.003602\n",
      "2019-04-10 00:05:03,999 root         INFO     Train Epoch: 100 [4096/8000 (51%)]\tTotal Loss: 0.122232\n",
      "Reconstruction: 0.118404, Regularization: 0.003829\n",
      "2019-04-10 00:05:04,062 root         INFO     Train Epoch: 100 [4608/8000 (58%)]\tTotal Loss: 0.110476\n",
      "Reconstruction: 0.107323, Regularization: 0.003153\n",
      "2019-04-10 00:05:04,126 root         INFO     Train Epoch: 100 [5120/8000 (64%)]\tTotal Loss: 0.113869\n",
      "Reconstruction: 0.110504, Regularization: 0.003365\n",
      "2019-04-10 00:05:04,190 root         INFO     Train Epoch: 100 [5632/8000 (70%)]\tTotal Loss: 0.113642\n",
      "Reconstruction: 0.110755, Regularization: 0.002887\n",
      "2019-04-10 00:05:04,253 root         INFO     Train Epoch: 100 [6144/8000 (77%)]\tTotal Loss: 0.121965\n",
      "Reconstruction: 0.117250, Regularization: 0.004715\n",
      "2019-04-10 00:05:04,319 root         INFO     Train Epoch: 100 [6656/8000 (83%)]\tTotal Loss: 0.117770\n",
      "Reconstruction: 0.114173, Regularization: 0.003597\n",
      "2019-04-10 00:05:04,383 root         INFO     Train Epoch: 100 [7168/8000 (90%)]\tTotal Loss: 0.107457\n",
      "Reconstruction: 0.104699, Regularization: 0.002758\n",
      "2019-04-10 00:05:04,447 root         INFO     Train Epoch: 100 [7680/8000 (96%)]\tTotal Loss: 0.109150\n",
      "Reconstruction: 0.106045, Regularization: 0.003105\n",
      "2019-04-10 00:05:04,502 root         INFO     ====> Epoch: 100 Average loss: 0.1143\n",
      "2019-04-10 00:05:04,526 root         INFO     Train Epoch: 101 [0/8000 (0%)]\tTotal Loss: 0.111605\n",
      "Reconstruction: 0.108701, Regularization: 0.002904\n",
      "2019-04-10 00:05:04,591 root         INFO     Train Epoch: 101 [512/8000 (6%)]\tTotal Loss: 0.112872\n",
      "Reconstruction: 0.109173, Regularization: 0.003698\n",
      "2019-04-10 00:05:04,658 root         INFO     Train Epoch: 101 [1024/8000 (13%)]\tTotal Loss: 0.118253\n",
      "Reconstruction: 0.115100, Regularization: 0.003153\n",
      "2019-04-10 00:05:04,726 root         INFO     Train Epoch: 101 [1536/8000 (19%)]\tTotal Loss: 0.100152\n",
      "Reconstruction: 0.097753, Regularization: 0.002399\n",
      "2019-04-10 00:05:04,794 root         INFO     Train Epoch: 101 [2048/8000 (26%)]\tTotal Loss: 0.107318\n",
      "Reconstruction: 0.104078, Regularization: 0.003240\n",
      "2019-04-10 00:05:04,861 root         INFO     Train Epoch: 101 [2560/8000 (32%)]\tTotal Loss: 0.112426\n",
      "Reconstruction: 0.109499, Regularization: 0.002926\n",
      "2019-04-10 00:05:04,928 root         INFO     Train Epoch: 101 [3072/8000 (38%)]\tTotal Loss: 0.107471\n",
      "Reconstruction: 0.104844, Regularization: 0.002627\n",
      "2019-04-10 00:05:05,003 root         INFO     Train Epoch: 101 [3584/8000 (45%)]\tTotal Loss: 0.123707\n",
      "Reconstruction: 0.118283, Regularization: 0.005425\n",
      "2019-04-10 00:05:05,073 root         INFO     Train Epoch: 101 [4096/8000 (51%)]\tTotal Loss: 0.116573\n",
      "Reconstruction: 0.112654, Regularization: 0.003918\n",
      "2019-04-10 00:05:05,140 root         INFO     Train Epoch: 101 [4608/8000 (58%)]\tTotal Loss: 0.111834\n",
      "Reconstruction: 0.108487, Regularization: 0.003348\n",
      "2019-04-10 00:05:05,205 root         INFO     Train Epoch: 101 [5120/8000 (64%)]\tTotal Loss: 0.110891\n",
      "Reconstruction: 0.107568, Regularization: 0.003323\n",
      "2019-04-10 00:05:05,271 root         INFO     Train Epoch: 101 [5632/8000 (70%)]\tTotal Loss: 0.111190\n",
      "Reconstruction: 0.107972, Regularization: 0.003218\n",
      "2019-04-10 00:05:05,336 root         INFO     Train Epoch: 101 [6144/8000 (77%)]\tTotal Loss: 0.114273\n",
      "Reconstruction: 0.111690, Regularization: 0.002583\n",
      "2019-04-10 00:05:05,400 root         INFO     Train Epoch: 101 [6656/8000 (83%)]\tTotal Loss: 0.119787\n",
      "Reconstruction: 0.115640, Regularization: 0.004146\n",
      "2019-04-10 00:05:05,464 root         INFO     Train Epoch: 101 [7168/8000 (90%)]\tTotal Loss: 0.113739\n",
      "Reconstruction: 0.110060, Regularization: 0.003679\n",
      "2019-04-10 00:05:05,528 root         INFO     Train Epoch: 101 [7680/8000 (96%)]\tTotal Loss: 0.109214\n",
      "Reconstruction: 0.107012, Regularization: 0.002202\n",
      "2019-04-10 00:05:05,583 root         INFO     ====> Epoch: 101 Average loss: 0.1143\n",
      "2019-04-10 00:05:05,607 root         INFO     Train Epoch: 102 [0/8000 (0%)]\tTotal Loss: 0.131805\n",
      "Reconstruction: 0.126417, Regularization: 0.005389\n",
      "2019-04-10 00:05:05,672 root         INFO     Train Epoch: 102 [512/8000 (6%)]\tTotal Loss: 0.127773\n",
      "Reconstruction: 0.122958, Regularization: 0.004815\n",
      "2019-04-10 00:05:05,737 root         INFO     Train Epoch: 102 [1024/8000 (13%)]\tTotal Loss: 0.114332\n",
      "Reconstruction: 0.110912, Regularization: 0.003420\n",
      "2019-04-10 00:05:05,802 root         INFO     Train Epoch: 102 [1536/8000 (19%)]\tTotal Loss: 0.122902\n",
      "Reconstruction: 0.118500, Regularization: 0.004403\n",
      "2019-04-10 00:05:05,864 root         INFO     Train Epoch: 102 [2048/8000 (26%)]\tTotal Loss: 0.109782\n",
      "Reconstruction: 0.106742, Regularization: 0.003039\n",
      "2019-04-10 00:05:05,925 root         INFO     Train Epoch: 102 [2560/8000 (32%)]\tTotal Loss: 0.111836\n",
      "Reconstruction: 0.108416, Regularization: 0.003421\n",
      "2019-04-10 00:05:05,987 root         INFO     Train Epoch: 102 [3072/8000 (38%)]\tTotal Loss: 0.106505\n",
      "Reconstruction: 0.104250, Regularization: 0.002255\n",
      "2019-04-10 00:05:06,048 root         INFO     Train Epoch: 102 [3584/8000 (45%)]\tTotal Loss: 0.106229\n",
      "Reconstruction: 0.103655, Regularization: 0.002575\n",
      "2019-04-10 00:05:06,109 root         INFO     Train Epoch: 102 [4096/8000 (51%)]\tTotal Loss: 0.110875\n",
      "Reconstruction: 0.108173, Regularization: 0.002703\n",
      "2019-04-10 00:05:06,170 root         INFO     Train Epoch: 102 [4608/8000 (58%)]\tTotal Loss: 0.111310\n",
      "Reconstruction: 0.108042, Regularization: 0.003268\n",
      "2019-04-10 00:05:06,231 root         INFO     Train Epoch: 102 [5120/8000 (64%)]\tTotal Loss: 0.116108\n",
      "Reconstruction: 0.112869, Regularization: 0.003240\n",
      "2019-04-10 00:05:06,292 root         INFO     Train Epoch: 102 [5632/8000 (70%)]\tTotal Loss: 0.125468\n",
      "Reconstruction: 0.120849, Regularization: 0.004619\n",
      "2019-04-10 00:05:06,353 root         INFO     Train Epoch: 102 [6144/8000 (77%)]\tTotal Loss: 0.112155\n",
      "Reconstruction: 0.108327, Regularization: 0.003829\n",
      "2019-04-10 00:05:06,414 root         INFO     Train Epoch: 102 [6656/8000 (83%)]\tTotal Loss: 0.113275\n",
      "Reconstruction: 0.110019, Regularization: 0.003256\n",
      "2019-04-10 00:05:06,475 root         INFO     Train Epoch: 102 [7168/8000 (90%)]\tTotal Loss: 0.111037\n",
      "Reconstruction: 0.107858, Regularization: 0.003179\n",
      "2019-04-10 00:05:06,536 root         INFO     Train Epoch: 102 [7680/8000 (96%)]\tTotal Loss: 0.111030\n",
      "Reconstruction: 0.108614, Regularization: 0.002416\n",
      "2019-04-10 00:05:06,588 root         INFO     ====> Epoch: 102 Average loss: 0.1142\n",
      "2019-04-10 00:05:06,612 root         INFO     Train Epoch: 103 [0/8000 (0%)]\tTotal Loss: 0.112783\n",
      "Reconstruction: 0.110183, Regularization: 0.002600\n",
      "2019-04-10 00:05:06,674 root         INFO     Train Epoch: 103 [512/8000 (6%)]\tTotal Loss: 0.115088\n",
      "Reconstruction: 0.111763, Regularization: 0.003325\n",
      "2019-04-10 00:05:06,737 root         INFO     Train Epoch: 103 [1024/8000 (13%)]\tTotal Loss: 0.110046\n",
      "Reconstruction: 0.106623, Regularization: 0.003423\n",
      "2019-04-10 00:05:06,798 root         INFO     Train Epoch: 103 [1536/8000 (19%)]\tTotal Loss: 0.113526\n",
      "Reconstruction: 0.109877, Regularization: 0.003649\n",
      "2019-04-10 00:05:06,860 root         INFO     Train Epoch: 103 [2048/8000 (26%)]\tTotal Loss: 0.121478\n",
      "Reconstruction: 0.117375, Regularization: 0.004103\n",
      "2019-04-10 00:05:06,921 root         INFO     Train Epoch: 103 [2560/8000 (32%)]\tTotal Loss: 0.123477\n",
      "Reconstruction: 0.118268, Regularization: 0.005209\n",
      "2019-04-10 00:05:06,983 root         INFO     Train Epoch: 103 [3072/8000 (38%)]\tTotal Loss: 0.115739\n",
      "Reconstruction: 0.111794, Regularization: 0.003945\n",
      "2019-04-10 00:05:07,044 root         INFO     Train Epoch: 103 [3584/8000 (45%)]\tTotal Loss: 0.125219\n",
      "Reconstruction: 0.121240, Regularization: 0.003979\n",
      "2019-04-10 00:05:07,106 root         INFO     Train Epoch: 103 [4096/8000 (51%)]\tTotal Loss: 0.122053\n",
      "Reconstruction: 0.117164, Regularization: 0.004889\n",
      "2019-04-10 00:05:07,168 root         INFO     Train Epoch: 103 [4608/8000 (58%)]\tTotal Loss: 0.116142\n",
      "Reconstruction: 0.112272, Regularization: 0.003869\n",
      "2019-04-10 00:05:07,229 root         INFO     Train Epoch: 103 [5120/8000 (64%)]\tTotal Loss: 0.111476\n",
      "Reconstruction: 0.108841, Regularization: 0.002635\n",
      "2019-04-10 00:05:07,292 root         INFO     Train Epoch: 103 [5632/8000 (70%)]\tTotal Loss: 0.096146\n",
      "Reconstruction: 0.093358, Regularization: 0.002788\n",
      "2019-04-10 00:05:07,354 root         INFO     Train Epoch: 103 [6144/8000 (77%)]\tTotal Loss: 0.112554\n",
      "Reconstruction: 0.109033, Regularization: 0.003521\n",
      "2019-04-10 00:05:07,416 root         INFO     Train Epoch: 103 [6656/8000 (83%)]\tTotal Loss: 0.112775\n",
      "Reconstruction: 0.109839, Regularization: 0.002935\n",
      "2019-04-10 00:05:07,478 root         INFO     Train Epoch: 103 [7168/8000 (90%)]\tTotal Loss: 0.110344\n",
      "Reconstruction: 0.105994, Regularization: 0.004350\n",
      "2019-04-10 00:05:07,541 root         INFO     Train Epoch: 103 [7680/8000 (96%)]\tTotal Loss: 0.109585\n",
      "Reconstruction: 0.106486, Regularization: 0.003099\n",
      "2019-04-10 00:05:07,594 root         INFO     ====> Epoch: 103 Average loss: 0.1142\n",
      "2019-04-10 00:05:07,618 root         INFO     Train Epoch: 104 [0/8000 (0%)]\tTotal Loss: 0.115773\n",
      "Reconstruction: 0.112492, Regularization: 0.003281\n",
      "2019-04-10 00:05:07,680 root         INFO     Train Epoch: 104 [512/8000 (6%)]\tTotal Loss: 0.108681\n",
      "Reconstruction: 0.105534, Regularization: 0.003147\n",
      "2019-04-10 00:05:07,743 root         INFO     Train Epoch: 104 [1024/8000 (13%)]\tTotal Loss: 0.107369\n",
      "Reconstruction: 0.104371, Regularization: 0.002998\n",
      "2019-04-10 00:05:07,805 root         INFO     Train Epoch: 104 [1536/8000 (19%)]\tTotal Loss: 0.110084\n",
      "Reconstruction: 0.106175, Regularization: 0.003909\n",
      "2019-04-10 00:05:07,867 root         INFO     Train Epoch: 104 [2048/8000 (26%)]\tTotal Loss: 0.121555\n",
      "Reconstruction: 0.116912, Regularization: 0.004643\n",
      "2019-04-10 00:05:07,930 root         INFO     Train Epoch: 104 [2560/8000 (32%)]\tTotal Loss: 0.120983\n",
      "Reconstruction: 0.116254, Regularization: 0.004729\n",
      "2019-04-10 00:05:07,993 root         INFO     Train Epoch: 104 [3072/8000 (38%)]\tTotal Loss: 0.117182\n",
      "Reconstruction: 0.113129, Regularization: 0.004053\n",
      "2019-04-10 00:05:08,055 root         INFO     Train Epoch: 104 [3584/8000 (45%)]\tTotal Loss: 0.113019\n",
      "Reconstruction: 0.108580, Regularization: 0.004439\n",
      "2019-04-10 00:05:08,118 root         INFO     Train Epoch: 104 [4096/8000 (51%)]\tTotal Loss: 0.120018\n",
      "Reconstruction: 0.114918, Regularization: 0.005101\n",
      "2019-04-10 00:05:08,180 root         INFO     Train Epoch: 104 [4608/8000 (58%)]\tTotal Loss: 0.120080\n",
      "Reconstruction: 0.115114, Regularization: 0.004966\n",
      "2019-04-10 00:05:08,242 root         INFO     Train Epoch: 104 [5120/8000 (64%)]\tTotal Loss: 0.109778\n",
      "Reconstruction: 0.106091, Regularization: 0.003687\n",
      "2019-04-10 00:05:08,305 root         INFO     Train Epoch: 104 [5632/8000 (70%)]\tTotal Loss: 0.114148\n",
      "Reconstruction: 0.110557, Regularization: 0.003591\n",
      "2019-04-10 00:05:08,367 root         INFO     Train Epoch: 104 [6144/8000 (77%)]\tTotal Loss: 0.107058\n",
      "Reconstruction: 0.103646, Regularization: 0.003412\n",
      "2019-04-10 00:05:08,430 root         INFO     Train Epoch: 104 [6656/8000 (83%)]\tTotal Loss: 0.116154\n",
      "Reconstruction: 0.113183, Regularization: 0.002971\n",
      "2019-04-10 00:05:08,492 root         INFO     Train Epoch: 104 [7168/8000 (90%)]\tTotal Loss: 0.113507\n",
      "Reconstruction: 0.109651, Regularization: 0.003856\n",
      "2019-04-10 00:05:08,555 root         INFO     Train Epoch: 104 [7680/8000 (96%)]\tTotal Loss: 0.124750\n",
      "Reconstruction: 0.120225, Regularization: 0.004525\n",
      "2019-04-10 00:05:08,609 root         INFO     ====> Epoch: 104 Average loss: 0.1138\n",
      "2019-04-10 00:05:08,633 root         INFO     Train Epoch: 105 [0/8000 (0%)]\tTotal Loss: 0.103274\n",
      "Reconstruction: 0.100749, Regularization: 0.002526\n",
      "2019-04-10 00:05:08,695 root         INFO     Train Epoch: 105 [512/8000 (6%)]\tTotal Loss: 0.107061\n",
      "Reconstruction: 0.104486, Regularization: 0.002575\n",
      "2019-04-10 00:05:08,758 root         INFO     Train Epoch: 105 [1024/8000 (13%)]\tTotal Loss: 0.106795\n",
      "Reconstruction: 0.103354, Regularization: 0.003440\n",
      "2019-04-10 00:05:08,820 root         INFO     Train Epoch: 105 [1536/8000 (19%)]\tTotal Loss: 0.107055\n",
      "Reconstruction: 0.103494, Regularization: 0.003561\n",
      "2019-04-10 00:05:08,881 root         INFO     Train Epoch: 105 [2048/8000 (26%)]\tTotal Loss: 0.110607\n",
      "Reconstruction: 0.107082, Regularization: 0.003525\n",
      "2019-04-10 00:05:08,943 root         INFO     Train Epoch: 105 [2560/8000 (32%)]\tTotal Loss: 0.105735\n",
      "Reconstruction: 0.102972, Regularization: 0.002764\n",
      "2019-04-10 00:05:09,005 root         INFO     Train Epoch: 105 [3072/8000 (38%)]\tTotal Loss: 0.113439\n",
      "Reconstruction: 0.109135, Regularization: 0.004304\n",
      "2019-04-10 00:05:09,067 root         INFO     Train Epoch: 105 [3584/8000 (45%)]\tTotal Loss: 0.108919\n",
      "Reconstruction: 0.105798, Regularization: 0.003121\n",
      "2019-04-10 00:05:09,128 root         INFO     Train Epoch: 105 [4096/8000 (51%)]\tTotal Loss: 0.129147\n",
      "Reconstruction: 0.124615, Regularization: 0.004532\n",
      "2019-04-10 00:05:09,190 root         INFO     Train Epoch: 105 [4608/8000 (58%)]\tTotal Loss: 0.110646\n",
      "Reconstruction: 0.106919, Regularization: 0.003727\n",
      "2019-04-10 00:05:09,251 root         INFO     Train Epoch: 105 [5120/8000 (64%)]\tTotal Loss: 0.111314\n",
      "Reconstruction: 0.107024, Regularization: 0.004290\n",
      "2019-04-10 00:05:09,313 root         INFO     Train Epoch: 105 [5632/8000 (70%)]\tTotal Loss: 0.108628\n",
      "Reconstruction: 0.105487, Regularization: 0.003141\n",
      "2019-04-10 00:05:09,374 root         INFO     Train Epoch: 105 [6144/8000 (77%)]\tTotal Loss: 0.122761\n",
      "Reconstruction: 0.117936, Regularization: 0.004825\n",
      "2019-04-10 00:05:09,436 root         INFO     Train Epoch: 105 [6656/8000 (83%)]\tTotal Loss: 0.117841\n",
      "Reconstruction: 0.113253, Regularization: 0.004588\n",
      "2019-04-10 00:05:09,498 root         INFO     Train Epoch: 105 [7168/8000 (90%)]\tTotal Loss: 0.115792\n",
      "Reconstruction: 0.111026, Regularization: 0.004766\n",
      "2019-04-10 00:05:09,560 root         INFO     Train Epoch: 105 [7680/8000 (96%)]\tTotal Loss: 0.096807\n",
      "Reconstruction: 0.094370, Regularization: 0.002438\n",
      "2019-04-10 00:05:09,613 root         INFO     ====> Epoch: 105 Average loss: 0.1142\n",
      "2019-04-10 00:05:09,637 root         INFO     Train Epoch: 106 [0/8000 (0%)]\tTotal Loss: 0.124380\n",
      "Reconstruction: 0.118514, Regularization: 0.005866\n",
      "2019-04-10 00:05:09,701 root         INFO     Train Epoch: 106 [512/8000 (6%)]\tTotal Loss: 0.110610\n",
      "Reconstruction: 0.107527, Regularization: 0.003083\n",
      "2019-04-10 00:05:09,765 root         INFO     Train Epoch: 106 [1024/8000 (13%)]\tTotal Loss: 0.100871\n",
      "Reconstruction: 0.098083, Regularization: 0.002788\n",
      "2019-04-10 00:05:09,829 root         INFO     Train Epoch: 106 [1536/8000 (19%)]\tTotal Loss: 0.106917\n",
      "Reconstruction: 0.103696, Regularization: 0.003221\n",
      "2019-04-10 00:05:09,893 root         INFO     Train Epoch: 106 [2048/8000 (26%)]\tTotal Loss: 0.105371\n",
      "Reconstruction: 0.101932, Regularization: 0.003439\n",
      "2019-04-10 00:05:09,957 root         INFO     Train Epoch: 106 [2560/8000 (32%)]\tTotal Loss: 0.102756\n",
      "Reconstruction: 0.099491, Regularization: 0.003265\n",
      "2019-04-10 00:05:10,021 root         INFO     Train Epoch: 106 [3072/8000 (38%)]\tTotal Loss: 0.125015\n",
      "Reconstruction: 0.119699, Regularization: 0.005316\n",
      "2019-04-10 00:05:10,086 root         INFO     Train Epoch: 106 [3584/8000 (45%)]\tTotal Loss: 0.112117\n",
      "Reconstruction: 0.107468, Regularization: 0.004649\n",
      "2019-04-10 00:05:10,150 root         INFO     Train Epoch: 106 [4096/8000 (51%)]\tTotal Loss: 0.107960\n",
      "Reconstruction: 0.104086, Regularization: 0.003874\n",
      "2019-04-10 00:05:10,214 root         INFO     Train Epoch: 106 [4608/8000 (58%)]\tTotal Loss: 0.121602\n",
      "Reconstruction: 0.117259, Regularization: 0.004343\n",
      "2019-04-10 00:05:10,278 root         INFO     Train Epoch: 106 [5120/8000 (64%)]\tTotal Loss: 0.108639\n",
      "Reconstruction: 0.105194, Regularization: 0.003445\n",
      "2019-04-10 00:05:10,342 root         INFO     Train Epoch: 106 [5632/8000 (70%)]\tTotal Loss: 0.104492\n",
      "Reconstruction: 0.101127, Regularization: 0.003365\n",
      "2019-04-10 00:05:10,407 root         INFO     Train Epoch: 106 [6144/8000 (77%)]\tTotal Loss: 0.112608\n",
      "Reconstruction: 0.108186, Regularization: 0.004422\n",
      "2019-04-10 00:05:10,471 root         INFO     Train Epoch: 106 [6656/8000 (83%)]\tTotal Loss: 0.114139\n",
      "Reconstruction: 0.110438, Regularization: 0.003700\n",
      "2019-04-10 00:05:10,534 root         INFO     Train Epoch: 106 [7168/8000 (90%)]\tTotal Loss: 0.117168\n",
      "Reconstruction: 0.112835, Regularization: 0.004333\n",
      "2019-04-10 00:05:10,597 root         INFO     Train Epoch: 106 [7680/8000 (96%)]\tTotal Loss: 0.130027\n",
      "Reconstruction: 0.123776, Regularization: 0.006251\n",
      "2019-04-10 00:05:10,651 root         INFO     ====> Epoch: 106 Average loss: 0.1137\n",
      "2019-04-10 00:05:10,675 root         INFO     Train Epoch: 107 [0/8000 (0%)]\tTotal Loss: 0.109804\n",
      "Reconstruction: 0.105924, Regularization: 0.003880\n",
      "2019-04-10 00:05:10,740 root         INFO     Train Epoch: 107 [512/8000 (6%)]\tTotal Loss: 0.122659\n",
      "Reconstruction: 0.118189, Regularization: 0.004470\n",
      "2019-04-10 00:05:10,804 root         INFO     Train Epoch: 107 [1024/8000 (13%)]\tTotal Loss: 0.107714\n",
      "Reconstruction: 0.104129, Regularization: 0.003585\n",
      "2019-04-10 00:05:10,867 root         INFO     Train Epoch: 107 [1536/8000 (19%)]\tTotal Loss: 0.114859\n",
      "Reconstruction: 0.110247, Regularization: 0.004612\n",
      "2019-04-10 00:05:10,931 root         INFO     Train Epoch: 107 [2048/8000 (26%)]\tTotal Loss: 0.105714\n",
      "Reconstruction: 0.101864, Regularization: 0.003850\n",
      "2019-04-10 00:05:10,995 root         INFO     Train Epoch: 107 [2560/8000 (32%)]\tTotal Loss: 0.104556\n",
      "Reconstruction: 0.101383, Regularization: 0.003173\n",
      "2019-04-10 00:05:11,059 root         INFO     Train Epoch: 107 [3072/8000 (38%)]\tTotal Loss: 0.109468\n",
      "Reconstruction: 0.106205, Regularization: 0.003263\n",
      "2019-04-10 00:05:11,123 root         INFO     Train Epoch: 107 [3584/8000 (45%)]\tTotal Loss: 0.110792\n",
      "Reconstruction: 0.107555, Regularization: 0.003238\n",
      "2019-04-10 00:05:11,187 root         INFO     Train Epoch: 107 [4096/8000 (51%)]\tTotal Loss: 0.116300\n",
      "Reconstruction: 0.111061, Regularization: 0.005239\n",
      "2019-04-10 00:05:11,251 root         INFO     Train Epoch: 107 [4608/8000 (58%)]\tTotal Loss: 0.110678\n",
      "Reconstruction: 0.107143, Regularization: 0.003535\n",
      "2019-04-10 00:05:11,316 root         INFO     Train Epoch: 107 [5120/8000 (64%)]\tTotal Loss: 0.122016\n",
      "Reconstruction: 0.116544, Regularization: 0.005472\n",
      "2019-04-10 00:05:11,379 root         INFO     Train Epoch: 107 [5632/8000 (70%)]\tTotal Loss: 0.118168\n",
      "Reconstruction: 0.114461, Regularization: 0.003707\n",
      "2019-04-10 00:05:11,443 root         INFO     Train Epoch: 107 [6144/8000 (77%)]\tTotal Loss: 0.121800\n",
      "Reconstruction: 0.115818, Regularization: 0.005982\n",
      "2019-04-10 00:05:11,507 root         INFO     Train Epoch: 107 [6656/8000 (83%)]\tTotal Loss: 0.106848\n",
      "Reconstruction: 0.103692, Regularization: 0.003156\n",
      "2019-04-10 00:05:11,571 root         INFO     Train Epoch: 107 [7168/8000 (90%)]\tTotal Loss: 0.109979\n",
      "Reconstruction: 0.106713, Regularization: 0.003266\n",
      "2019-04-10 00:05:11,635 root         INFO     Train Epoch: 107 [7680/8000 (96%)]\tTotal Loss: 0.112946\n",
      "Reconstruction: 0.107715, Regularization: 0.005231\n",
      "2019-04-10 00:05:11,689 root         INFO     ====> Epoch: 107 Average loss: 0.1137\n",
      "2019-04-10 00:05:11,713 root         INFO     Train Epoch: 108 [0/8000 (0%)]\tTotal Loss: 0.111103\n",
      "Reconstruction: 0.106742, Regularization: 0.004360\n",
      "2019-04-10 00:05:11,778 root         INFO     Train Epoch: 108 [512/8000 (6%)]\tTotal Loss: 0.107864\n",
      "Reconstruction: 0.104289, Regularization: 0.003575\n",
      "2019-04-10 00:05:11,841 root         INFO     Train Epoch: 108 [1024/8000 (13%)]\tTotal Loss: 0.108655\n",
      "Reconstruction: 0.104008, Regularization: 0.004647\n",
      "2019-04-10 00:05:11,904 root         INFO     Train Epoch: 108 [1536/8000 (19%)]\tTotal Loss: 0.112817\n",
      "Reconstruction: 0.107783, Regularization: 0.005034\n",
      "2019-04-10 00:05:11,967 root         INFO     Train Epoch: 108 [2048/8000 (26%)]\tTotal Loss: 0.110901\n",
      "Reconstruction: 0.106438, Regularization: 0.004463\n",
      "2019-04-10 00:05:12,030 root         INFO     Train Epoch: 108 [2560/8000 (32%)]\tTotal Loss: 0.117010\n",
      "Reconstruction: 0.111906, Regularization: 0.005103\n",
      "2019-04-10 00:05:12,093 root         INFO     Train Epoch: 108 [3072/8000 (38%)]\tTotal Loss: 0.110281\n",
      "Reconstruction: 0.106843, Regularization: 0.003438\n",
      "2019-04-10 00:05:12,156 root         INFO     Train Epoch: 108 [3584/8000 (45%)]\tTotal Loss: 0.115214\n",
      "Reconstruction: 0.110497, Regularization: 0.004717\n",
      "2019-04-10 00:05:12,218 root         INFO     Train Epoch: 108 [4096/8000 (51%)]\tTotal Loss: 0.112971\n",
      "Reconstruction: 0.108632, Regularization: 0.004340\n",
      "2019-04-10 00:05:12,281 root         INFO     Train Epoch: 108 [4608/8000 (58%)]\tTotal Loss: 0.110928\n",
      "Reconstruction: 0.105987, Regularization: 0.004941\n",
      "2019-04-10 00:05:12,345 root         INFO     Train Epoch: 108 [5120/8000 (64%)]\tTotal Loss: 0.115018\n",
      "Reconstruction: 0.110639, Regularization: 0.004379\n",
      "2019-04-10 00:05:12,408 root         INFO     Train Epoch: 108 [5632/8000 (70%)]\tTotal Loss: 0.128952\n",
      "Reconstruction: 0.121633, Regularization: 0.007319\n",
      "2019-04-10 00:05:12,470 root         INFO     Train Epoch: 108 [6144/8000 (77%)]\tTotal Loss: 0.097897\n",
      "Reconstruction: 0.094822, Regularization: 0.003075\n",
      "2019-04-10 00:05:12,532 root         INFO     Train Epoch: 108 [6656/8000 (83%)]\tTotal Loss: 0.113736\n",
      "Reconstruction: 0.109281, Regularization: 0.004456\n",
      "2019-04-10 00:05:12,594 root         INFO     Train Epoch: 108 [7168/8000 (90%)]\tTotal Loss: 0.109606\n",
      "Reconstruction: 0.105375, Regularization: 0.004230\n",
      "2019-04-10 00:05:12,656 root         INFO     Train Epoch: 108 [7680/8000 (96%)]\tTotal Loss: 0.115655\n",
      "Reconstruction: 0.109945, Regularization: 0.005709\n",
      "2019-04-10 00:05:12,709 root         INFO     ====> Epoch: 108 Average loss: 0.1139\n",
      "2019-04-10 00:05:12,733 root         INFO     Train Epoch: 109 [0/8000 (0%)]\tTotal Loss: 0.125997\n",
      "Reconstruction: 0.119608, Regularization: 0.006389\n",
      "2019-04-10 00:05:12,798 root         INFO     Train Epoch: 109 [512/8000 (6%)]\tTotal Loss: 0.130096\n",
      "Reconstruction: 0.123510, Regularization: 0.006586\n",
      "2019-04-10 00:05:12,862 root         INFO     Train Epoch: 109 [1024/8000 (13%)]\tTotal Loss: 0.109889\n",
      "Reconstruction: 0.105940, Regularization: 0.003949\n",
      "2019-04-10 00:05:12,926 root         INFO     Train Epoch: 109 [1536/8000 (19%)]\tTotal Loss: 0.126465\n",
      "Reconstruction: 0.121252, Regularization: 0.005213\n",
      "2019-04-10 00:05:12,990 root         INFO     Train Epoch: 109 [2048/8000 (26%)]\tTotal Loss: 0.116467\n",
      "Reconstruction: 0.111037, Regularization: 0.005430\n",
      "2019-04-10 00:05:13,054 root         INFO     Train Epoch: 109 [2560/8000 (32%)]\tTotal Loss: 0.116183\n",
      "Reconstruction: 0.111016, Regularization: 0.005167\n",
      "2019-04-10 00:05:13,118 root         INFO     Train Epoch: 109 [3072/8000 (38%)]\tTotal Loss: 0.125407\n",
      "Reconstruction: 0.120410, Regularization: 0.004997\n",
      "2019-04-10 00:05:13,182 root         INFO     Train Epoch: 109 [3584/8000 (45%)]\tTotal Loss: 0.116996\n",
      "Reconstruction: 0.112937, Regularization: 0.004060\n",
      "2019-04-10 00:05:13,246 root         INFO     Train Epoch: 109 [4096/8000 (51%)]\tTotal Loss: 0.118275\n",
      "Reconstruction: 0.113022, Regularization: 0.005254\n",
      "2019-04-10 00:05:13,310 root         INFO     Train Epoch: 109 [4608/8000 (58%)]\tTotal Loss: 0.111442\n",
      "Reconstruction: 0.106674, Regularization: 0.004768\n",
      "2019-04-10 00:05:13,374 root         INFO     Train Epoch: 109 [5120/8000 (64%)]\tTotal Loss: 0.115962\n",
      "Reconstruction: 0.111156, Regularization: 0.004806\n",
      "2019-04-10 00:05:13,438 root         INFO     Train Epoch: 109 [5632/8000 (70%)]\tTotal Loss: 0.114879\n",
      "Reconstruction: 0.111394, Regularization: 0.003486\n",
      "2019-04-10 00:05:13,502 root         INFO     Train Epoch: 109 [6144/8000 (77%)]\tTotal Loss: 0.104541\n",
      "Reconstruction: 0.100426, Regularization: 0.004115\n",
      "2019-04-10 00:05:13,566 root         INFO     Train Epoch: 109 [6656/8000 (83%)]\tTotal Loss: 0.110804\n",
      "Reconstruction: 0.106907, Regularization: 0.003897\n",
      "2019-04-10 00:05:13,630 root         INFO     Train Epoch: 109 [7168/8000 (90%)]\tTotal Loss: 0.107183\n",
      "Reconstruction: 0.102811, Regularization: 0.004372\n",
      "2019-04-10 00:05:13,694 root         INFO     Train Epoch: 109 [7680/8000 (96%)]\tTotal Loss: 0.107918\n",
      "Reconstruction: 0.104232, Regularization: 0.003686\n",
      "2019-04-10 00:05:13,749 root         INFO     ====> Epoch: 109 Average loss: 0.1136\n",
      "2019-04-10 00:05:13,772 root         INFO     Train Epoch: 110 [0/8000 (0%)]\tTotal Loss: 0.114743\n",
      "Reconstruction: 0.110098, Regularization: 0.004645\n",
      "2019-04-10 00:05:13,836 root         INFO     Train Epoch: 110 [512/8000 (6%)]\tTotal Loss: 0.118107\n",
      "Reconstruction: 0.113247, Regularization: 0.004859\n",
      "2019-04-10 00:05:13,899 root         INFO     Train Epoch: 110 [1024/8000 (13%)]\tTotal Loss: 0.113909\n",
      "Reconstruction: 0.109316, Regularization: 0.004593\n",
      "2019-04-10 00:05:13,963 root         INFO     Train Epoch: 110 [1536/8000 (19%)]\tTotal Loss: 0.113673\n",
      "Reconstruction: 0.109242, Regularization: 0.004430\n",
      "2019-04-10 00:05:14,027 root         INFO     Train Epoch: 110 [2048/8000 (26%)]\tTotal Loss: 0.116177\n",
      "Reconstruction: 0.112085, Regularization: 0.004092\n",
      "2019-04-10 00:05:14,091 root         INFO     Train Epoch: 110 [2560/8000 (32%)]\tTotal Loss: 0.124945\n",
      "Reconstruction: 0.116868, Regularization: 0.008077\n",
      "2019-04-10 00:05:14,155 root         INFO     Train Epoch: 110 [3072/8000 (38%)]\tTotal Loss: 0.114938\n",
      "Reconstruction: 0.111133, Regularization: 0.003804\n",
      "2019-04-10 00:05:14,218 root         INFO     Train Epoch: 110 [3584/8000 (45%)]\tTotal Loss: 0.125581\n",
      "Reconstruction: 0.118936, Regularization: 0.006645\n",
      "2019-04-10 00:05:14,281 root         INFO     Train Epoch: 110 [4096/8000 (51%)]\tTotal Loss: 0.099007\n",
      "Reconstruction: 0.095434, Regularization: 0.003572\n",
      "2019-04-10 00:05:14,345 root         INFO     Train Epoch: 110 [4608/8000 (58%)]\tTotal Loss: 0.112191\n",
      "Reconstruction: 0.107764, Regularization: 0.004427\n",
      "2019-04-10 00:05:14,409 root         INFO     Train Epoch: 110 [5120/8000 (64%)]\tTotal Loss: 0.106381\n",
      "Reconstruction: 0.102828, Regularization: 0.003553\n",
      "2019-04-10 00:05:14,473 root         INFO     Train Epoch: 110 [5632/8000 (70%)]\tTotal Loss: 0.118289\n",
      "Reconstruction: 0.112943, Regularization: 0.005346\n",
      "2019-04-10 00:05:14,536 root         INFO     Train Epoch: 110 [6144/8000 (77%)]\tTotal Loss: 0.123372\n",
      "Reconstruction: 0.117889, Regularization: 0.005483\n",
      "2019-04-10 00:05:14,597 root         INFO     Train Epoch: 110 [6656/8000 (83%)]\tTotal Loss: 0.110773\n",
      "Reconstruction: 0.105945, Regularization: 0.004828\n",
      "2019-04-10 00:05:14,659 root         INFO     Train Epoch: 110 [7168/8000 (90%)]\tTotal Loss: 0.105663\n",
      "Reconstruction: 0.102133, Regularization: 0.003530\n",
      "2019-04-10 00:05:14,721 root         INFO     Train Epoch: 110 [7680/8000 (96%)]\tTotal Loss: 0.108588\n",
      "Reconstruction: 0.104437, Regularization: 0.004151\n",
      "2019-04-10 00:05:14,776 root         INFO     ====> Epoch: 110 Average loss: 0.1135\n",
      "2019-04-10 00:05:14,800 root         INFO     Train Epoch: 111 [0/8000 (0%)]\tTotal Loss: 0.122815\n",
      "Reconstruction: 0.116673, Regularization: 0.006142\n",
      "2019-04-10 00:05:14,864 root         INFO     Train Epoch: 111 [512/8000 (6%)]\tTotal Loss: 0.123792\n",
      "Reconstruction: 0.118201, Regularization: 0.005591\n",
      "2019-04-10 00:05:14,927 root         INFO     Train Epoch: 111 [1024/8000 (13%)]\tTotal Loss: 0.106600\n",
      "Reconstruction: 0.103910, Regularization: 0.002690\n",
      "2019-04-10 00:05:14,990 root         INFO     Train Epoch: 111 [1536/8000 (19%)]\tTotal Loss: 0.104598\n",
      "Reconstruction: 0.100769, Regularization: 0.003829\n",
      "2019-04-10 00:05:15,053 root         INFO     Train Epoch: 111 [2048/8000 (26%)]\tTotal Loss: 0.126139\n",
      "Reconstruction: 0.119051, Regularization: 0.007088\n",
      "2019-04-10 00:05:15,116 root         INFO     Train Epoch: 111 [2560/8000 (32%)]\tTotal Loss: 0.119791\n",
      "Reconstruction: 0.113369, Regularization: 0.006423\n",
      "2019-04-10 00:05:15,179 root         INFO     Train Epoch: 111 [3072/8000 (38%)]\tTotal Loss: 0.120018\n",
      "Reconstruction: 0.115159, Regularization: 0.004858\n",
      "2019-04-10 00:05:15,241 root         INFO     Train Epoch: 111 [3584/8000 (45%)]\tTotal Loss: 0.111480\n",
      "Reconstruction: 0.106458, Regularization: 0.005022\n",
      "2019-04-10 00:05:15,305 root         INFO     Train Epoch: 111 [4096/8000 (51%)]\tTotal Loss: 0.124474\n",
      "Reconstruction: 0.119469, Regularization: 0.005005\n",
      "2019-04-10 00:05:15,367 root         INFO     Train Epoch: 111 [4608/8000 (58%)]\tTotal Loss: 0.121240\n",
      "Reconstruction: 0.116543, Regularization: 0.004697\n",
      "2019-04-10 00:05:15,430 root         INFO     Train Epoch: 111 [5120/8000 (64%)]\tTotal Loss: 0.104316\n",
      "Reconstruction: 0.101080, Regularization: 0.003236\n",
      "2019-04-10 00:05:15,492 root         INFO     Train Epoch: 111 [5632/8000 (70%)]\tTotal Loss: 0.125230\n",
      "Reconstruction: 0.117927, Regularization: 0.007303\n",
      "2019-04-10 00:05:15,554 root         INFO     Train Epoch: 111 [6144/8000 (77%)]\tTotal Loss: 0.128899\n",
      "Reconstruction: 0.121677, Regularization: 0.007222\n",
      "2019-04-10 00:05:15,617 root         INFO     Train Epoch: 111 [6656/8000 (83%)]\tTotal Loss: 0.101895\n",
      "Reconstruction: 0.097227, Regularization: 0.004668\n",
      "2019-04-10 00:05:15,680 root         INFO     Train Epoch: 111 [7168/8000 (90%)]\tTotal Loss: 0.108992\n",
      "Reconstruction: 0.103972, Regularization: 0.005020\n",
      "2019-04-10 00:05:15,742 root         INFO     Train Epoch: 111 [7680/8000 (96%)]\tTotal Loss: 0.117824\n",
      "Reconstruction: 0.111530, Regularization: 0.006294\n",
      "2019-04-10 00:05:15,796 root         INFO     ====> Epoch: 111 Average loss: 0.1139\n",
      "2019-04-10 00:05:15,820 root         INFO     Train Epoch: 112 [0/8000 (0%)]\tTotal Loss: 0.123446\n",
      "Reconstruction: 0.117008, Regularization: 0.006438\n",
      "2019-04-10 00:05:15,884 root         INFO     Train Epoch: 112 [512/8000 (6%)]\tTotal Loss: 0.112499\n",
      "Reconstruction: 0.108943, Regularization: 0.003556\n",
      "2019-04-10 00:05:15,947 root         INFO     Train Epoch: 112 [1024/8000 (13%)]\tTotal Loss: 0.106539\n",
      "Reconstruction: 0.101356, Regularization: 0.005182\n",
      "2019-04-10 00:05:16,010 root         INFO     Train Epoch: 112 [1536/8000 (19%)]\tTotal Loss: 0.114613\n",
      "Reconstruction: 0.109515, Regularization: 0.005098\n",
      "2019-04-10 00:05:16,072 root         INFO     Train Epoch: 112 [2048/8000 (26%)]\tTotal Loss: 0.131716\n",
      "Reconstruction: 0.124429, Regularization: 0.007287\n",
      "2019-04-10 00:05:16,135 root         INFO     Train Epoch: 112 [2560/8000 (32%)]\tTotal Loss: 0.110449\n",
      "Reconstruction: 0.106063, Regularization: 0.004386\n",
      "2019-04-10 00:05:16,198 root         INFO     Train Epoch: 112 [3072/8000 (38%)]\tTotal Loss: 0.118062\n",
      "Reconstruction: 0.111860, Regularization: 0.006202\n",
      "2019-04-10 00:05:16,261 root         INFO     Train Epoch: 112 [3584/8000 (45%)]\tTotal Loss: 0.106560\n",
      "Reconstruction: 0.102422, Regularization: 0.004137\n",
      "2019-04-10 00:05:16,324 root         INFO     Train Epoch: 112 [4096/8000 (51%)]\tTotal Loss: 0.111012\n",
      "Reconstruction: 0.105494, Regularization: 0.005518\n",
      "2019-04-10 00:05:16,387 root         INFO     Train Epoch: 112 [4608/8000 (58%)]\tTotal Loss: 0.113253\n",
      "Reconstruction: 0.108586, Regularization: 0.004667\n",
      "2019-04-10 00:05:16,451 root         INFO     Train Epoch: 112 [5120/8000 (64%)]\tTotal Loss: 0.120417\n",
      "Reconstruction: 0.115217, Regularization: 0.005200\n",
      "2019-04-10 00:05:16,512 root         INFO     Train Epoch: 112 [5632/8000 (70%)]\tTotal Loss: 0.121106\n",
      "Reconstruction: 0.116241, Regularization: 0.004865\n",
      "2019-04-10 00:05:16,573 root         INFO     Train Epoch: 112 [6144/8000 (77%)]\tTotal Loss: 0.121616\n",
      "Reconstruction: 0.115039, Regularization: 0.006577\n",
      "2019-04-10 00:05:16,635 root         INFO     Train Epoch: 112 [6656/8000 (83%)]\tTotal Loss: 0.119884\n",
      "Reconstruction: 0.114074, Regularization: 0.005810\n",
      "2019-04-10 00:05:16,699 root         INFO     Train Epoch: 112 [7168/8000 (90%)]\tTotal Loss: 0.106993\n",
      "Reconstruction: 0.102763, Regularization: 0.004229\n",
      "2019-04-10 00:05:16,761 root         INFO     Train Epoch: 112 [7680/8000 (96%)]\tTotal Loss: 0.110033\n",
      "Reconstruction: 0.106067, Regularization: 0.003966\n",
      "2019-04-10 00:05:16,814 root         INFO     ====> Epoch: 112 Average loss: 0.1139\n",
      "2019-04-10 00:05:16,839 root         INFO     Train Epoch: 113 [0/8000 (0%)]\tTotal Loss: 0.109555\n",
      "Reconstruction: 0.106054, Regularization: 0.003501\n",
      "2019-04-10 00:05:16,903 root         INFO     Train Epoch: 113 [512/8000 (6%)]\tTotal Loss: 0.109009\n",
      "Reconstruction: 0.102853, Regularization: 0.006156\n",
      "2019-04-10 00:05:16,966 root         INFO     Train Epoch: 113 [1024/8000 (13%)]\tTotal Loss: 0.111889\n",
      "Reconstruction: 0.105810, Regularization: 0.006079\n",
      "2019-04-10 00:05:17,029 root         INFO     Train Epoch: 113 [1536/8000 (19%)]\tTotal Loss: 0.111509\n",
      "Reconstruction: 0.106136, Regularization: 0.005373\n",
      "2019-04-10 00:05:17,093 root         INFO     Train Epoch: 113 [2048/8000 (26%)]\tTotal Loss: 0.095795\n",
      "Reconstruction: 0.091990, Regularization: 0.003805\n",
      "2019-04-10 00:05:17,157 root         INFO     Train Epoch: 113 [2560/8000 (32%)]\tTotal Loss: 0.106064\n",
      "Reconstruction: 0.101854, Regularization: 0.004209\n",
      "2019-04-10 00:05:17,220 root         INFO     Train Epoch: 113 [3072/8000 (38%)]\tTotal Loss: 0.122006\n",
      "Reconstruction: 0.116306, Regularization: 0.005700\n",
      "2019-04-10 00:05:17,284 root         INFO     Train Epoch: 113 [3584/8000 (45%)]\tTotal Loss: 0.120193\n",
      "Reconstruction: 0.113716, Regularization: 0.006477\n",
      "2019-04-10 00:05:17,347 root         INFO     Train Epoch: 113 [4096/8000 (51%)]\tTotal Loss: 0.116637\n",
      "Reconstruction: 0.110476, Regularization: 0.006161\n",
      "2019-04-10 00:05:17,411 root         INFO     Train Epoch: 113 [4608/8000 (58%)]\tTotal Loss: 0.111978\n",
      "Reconstruction: 0.107435, Regularization: 0.004543\n",
      "2019-04-10 00:05:17,474 root         INFO     Train Epoch: 113 [5120/8000 (64%)]\tTotal Loss: 0.114180\n",
      "Reconstruction: 0.109351, Regularization: 0.004830\n",
      "2019-04-10 00:05:17,538 root         INFO     Train Epoch: 113 [5632/8000 (70%)]\tTotal Loss: 0.119459\n",
      "Reconstruction: 0.113218, Regularization: 0.006242\n",
      "2019-04-10 00:05:17,601 root         INFO     Train Epoch: 113 [6144/8000 (77%)]\tTotal Loss: 0.098003\n",
      "Reconstruction: 0.094802, Regularization: 0.003201\n",
      "2019-04-10 00:05:17,665 root         INFO     Train Epoch: 113 [6656/8000 (83%)]\tTotal Loss: 0.121696\n",
      "Reconstruction: 0.116478, Regularization: 0.005218\n",
      "2019-04-10 00:05:17,727 root         INFO     Train Epoch: 113 [7168/8000 (90%)]\tTotal Loss: 0.117833\n",
      "Reconstruction: 0.112353, Regularization: 0.005480\n",
      "2019-04-10 00:05:17,790 root         INFO     Train Epoch: 113 [7680/8000 (96%)]\tTotal Loss: 0.119688\n",
      "Reconstruction: 0.112766, Regularization: 0.006922\n",
      "2019-04-10 00:05:17,844 root         INFO     ====> Epoch: 113 Average loss: 0.1136\n",
      "2019-04-10 00:05:17,868 root         INFO     Train Epoch: 114 [0/8000 (0%)]\tTotal Loss: 0.124192\n",
      "Reconstruction: 0.117744, Regularization: 0.006448\n",
      "2019-04-10 00:05:17,933 root         INFO     Train Epoch: 114 [512/8000 (6%)]\tTotal Loss: 0.116204\n",
      "Reconstruction: 0.110865, Regularization: 0.005338\n",
      "2019-04-10 00:05:17,997 root         INFO     Train Epoch: 114 [1024/8000 (13%)]\tTotal Loss: 0.112380\n",
      "Reconstruction: 0.107256, Regularization: 0.005125\n",
      "2019-04-10 00:05:18,061 root         INFO     Train Epoch: 114 [1536/8000 (19%)]\tTotal Loss: 0.113638\n",
      "Reconstruction: 0.108164, Regularization: 0.005474\n",
      "2019-04-10 00:05:18,124 root         INFO     Train Epoch: 114 [2048/8000 (26%)]\tTotal Loss: 0.119226\n",
      "Reconstruction: 0.113006, Regularization: 0.006219\n",
      "2019-04-10 00:05:18,187 root         INFO     Train Epoch: 114 [2560/8000 (32%)]\tTotal Loss: 0.125948\n",
      "Reconstruction: 0.120336, Regularization: 0.005612\n",
      "2019-04-10 00:05:18,249 root         INFO     Train Epoch: 114 [3072/8000 (38%)]\tTotal Loss: 0.123645\n",
      "Reconstruction: 0.117776, Regularization: 0.005869\n",
      "2019-04-10 00:05:18,311 root         INFO     Train Epoch: 114 [3584/8000 (45%)]\tTotal Loss: 0.125808\n",
      "Reconstruction: 0.119753, Regularization: 0.006056\n",
      "2019-04-10 00:05:18,374 root         INFO     Train Epoch: 114 [4096/8000 (51%)]\tTotal Loss: 0.119254\n",
      "Reconstruction: 0.112731, Regularization: 0.006523\n",
      "2019-04-10 00:05:18,437 root         INFO     Train Epoch: 114 [4608/8000 (58%)]\tTotal Loss: 0.111827\n",
      "Reconstruction: 0.107617, Regularization: 0.004209\n",
      "2019-04-10 00:05:18,500 root         INFO     Train Epoch: 114 [5120/8000 (64%)]\tTotal Loss: 0.116577\n",
      "Reconstruction: 0.111035, Regularization: 0.005542\n",
      "2019-04-10 00:05:18,562 root         INFO     Train Epoch: 114 [5632/8000 (70%)]\tTotal Loss: 0.097313\n",
      "Reconstruction: 0.093646, Regularization: 0.003667\n",
      "2019-04-10 00:05:18,625 root         INFO     Train Epoch: 114 [6144/8000 (77%)]\tTotal Loss: 0.108359\n",
      "Reconstruction: 0.104313, Regularization: 0.004046\n",
      "2019-04-10 00:05:18,688 root         INFO     Train Epoch: 114 [6656/8000 (83%)]\tTotal Loss: 0.115029\n",
      "Reconstruction: 0.109594, Regularization: 0.005435\n",
      "2019-04-10 00:05:18,750 root         INFO     Train Epoch: 114 [7168/8000 (90%)]\tTotal Loss: 0.115719\n",
      "Reconstruction: 0.108813, Regularization: 0.006906\n",
      "2019-04-10 00:05:18,813 root         INFO     Train Epoch: 114 [7680/8000 (96%)]\tTotal Loss: 0.110868\n",
      "Reconstruction: 0.106282, Regularization: 0.004586\n",
      "2019-04-10 00:05:18,867 root         INFO     ====> Epoch: 114 Average loss: 0.1135\n",
      "2019-04-10 00:05:18,890 root         INFO     Train Epoch: 115 [0/8000 (0%)]\tTotal Loss: 0.106514\n",
      "Reconstruction: 0.102169, Regularization: 0.004345\n",
      "2019-04-10 00:05:18,954 root         INFO     Train Epoch: 115 [512/8000 (6%)]\tTotal Loss: 0.117146\n",
      "Reconstruction: 0.111763, Regularization: 0.005384\n",
      "2019-04-10 00:05:19,018 root         INFO     Train Epoch: 115 [1024/8000 (13%)]\tTotal Loss: 0.114423\n",
      "Reconstruction: 0.107934, Regularization: 0.006489\n",
      "2019-04-10 00:05:19,081 root         INFO     Train Epoch: 115 [1536/8000 (19%)]\tTotal Loss: 0.108288\n",
      "Reconstruction: 0.103698, Regularization: 0.004590\n",
      "2019-04-10 00:05:19,144 root         INFO     Train Epoch: 115 [2048/8000 (26%)]\tTotal Loss: 0.120156\n",
      "Reconstruction: 0.114813, Regularization: 0.005342\n",
      "2019-04-10 00:05:19,208 root         INFO     Train Epoch: 115 [2560/8000 (32%)]\tTotal Loss: 0.106697\n",
      "Reconstruction: 0.102631, Regularization: 0.004066\n",
      "2019-04-10 00:05:19,271 root         INFO     Train Epoch: 115 [3072/8000 (38%)]\tTotal Loss: 0.117715\n",
      "Reconstruction: 0.112337, Regularization: 0.005378\n",
      "2019-04-10 00:05:19,333 root         INFO     Train Epoch: 115 [3584/8000 (45%)]\tTotal Loss: 0.114260\n",
      "Reconstruction: 0.107859, Regularization: 0.006401\n",
      "2019-04-10 00:05:19,396 root         INFO     Train Epoch: 115 [4096/8000 (51%)]\tTotal Loss: 0.107208\n",
      "Reconstruction: 0.103148, Regularization: 0.004060\n",
      "2019-04-10 00:05:19,459 root         INFO     Train Epoch: 115 [4608/8000 (58%)]\tTotal Loss: 0.121865\n",
      "Reconstruction: 0.115351, Regularization: 0.006514\n",
      "2019-04-10 00:05:19,521 root         INFO     Train Epoch: 115 [5120/8000 (64%)]\tTotal Loss: 0.114162\n",
      "Reconstruction: 0.108595, Regularization: 0.005567\n",
      "2019-04-10 00:05:19,584 root         INFO     Train Epoch: 115 [5632/8000 (70%)]\tTotal Loss: 0.115263\n",
      "Reconstruction: 0.109284, Regularization: 0.005979\n",
      "2019-04-10 00:05:19,648 root         INFO     Train Epoch: 115 [6144/8000 (77%)]\tTotal Loss: 0.105862\n",
      "Reconstruction: 0.101513, Regularization: 0.004349\n",
      "2019-04-10 00:05:19,712 root         INFO     Train Epoch: 115 [6656/8000 (83%)]\tTotal Loss: 0.116149\n",
      "Reconstruction: 0.110814, Regularization: 0.005334\n",
      "2019-04-10 00:05:19,776 root         INFO     Train Epoch: 115 [7168/8000 (90%)]\tTotal Loss: 0.121224\n",
      "Reconstruction: 0.114255, Regularization: 0.006969\n",
      "2019-04-10 00:05:19,839 root         INFO     Train Epoch: 115 [7680/8000 (96%)]\tTotal Loss: 0.116490\n",
      "Reconstruction: 0.111210, Regularization: 0.005280\n",
      "2019-04-10 00:05:19,893 root         INFO     ====> Epoch: 115 Average loss: 0.1136\n",
      "2019-04-10 00:05:19,917 root         INFO     Train Epoch: 116 [0/8000 (0%)]\tTotal Loss: 0.111146\n",
      "Reconstruction: 0.106046, Regularization: 0.005100\n",
      "2019-04-10 00:05:19,982 root         INFO     Train Epoch: 116 [512/8000 (6%)]\tTotal Loss: 0.113376\n",
      "Reconstruction: 0.108258, Regularization: 0.005118\n",
      "2019-04-10 00:05:20,046 root         INFO     Train Epoch: 116 [1024/8000 (13%)]\tTotal Loss: 0.101119\n",
      "Reconstruction: 0.097840, Regularization: 0.003279\n",
      "2019-04-10 00:05:20,110 root         INFO     Train Epoch: 116 [1536/8000 (19%)]\tTotal Loss: 0.115900\n",
      "Reconstruction: 0.111048, Regularization: 0.004852\n",
      "2019-04-10 00:05:20,173 root         INFO     Train Epoch: 116 [2048/8000 (26%)]\tTotal Loss: 0.105158\n",
      "Reconstruction: 0.100848, Regularization: 0.004309\n",
      "2019-04-10 00:05:20,237 root         INFO     Train Epoch: 116 [2560/8000 (32%)]\tTotal Loss: 0.116825\n",
      "Reconstruction: 0.109733, Regularization: 0.007092\n",
      "2019-04-10 00:05:20,301 root         INFO     Train Epoch: 116 [3072/8000 (38%)]\tTotal Loss: 0.117981\n",
      "Reconstruction: 0.110214, Regularization: 0.007767\n",
      "2019-04-10 00:05:20,364 root         INFO     Train Epoch: 116 [3584/8000 (45%)]\tTotal Loss: 0.109858\n",
      "Reconstruction: 0.104078, Regularization: 0.005780\n",
      "2019-04-10 00:05:20,427 root         INFO     Train Epoch: 116 [4096/8000 (51%)]\tTotal Loss: 0.106264\n",
      "Reconstruction: 0.102929, Regularization: 0.003335\n",
      "2019-04-10 00:05:20,489 root         INFO     Train Epoch: 116 [4608/8000 (58%)]\tTotal Loss: 0.111810\n",
      "Reconstruction: 0.105732, Regularization: 0.006078\n",
      "2019-04-10 00:05:20,550 root         INFO     Train Epoch: 116 [5120/8000 (64%)]\tTotal Loss: 0.118674\n",
      "Reconstruction: 0.112068, Regularization: 0.006606\n",
      "2019-04-10 00:05:20,611 root         INFO     Train Epoch: 116 [5632/8000 (70%)]\tTotal Loss: 0.102933\n",
      "Reconstruction: 0.098660, Regularization: 0.004273\n",
      "2019-04-10 00:05:20,671 root         INFO     Train Epoch: 116 [6144/8000 (77%)]\tTotal Loss: 0.118353\n",
      "Reconstruction: 0.111454, Regularization: 0.006899\n",
      "2019-04-10 00:05:20,732 root         INFO     Train Epoch: 116 [6656/8000 (83%)]\tTotal Loss: 0.110070\n",
      "Reconstruction: 0.104638, Regularization: 0.005433\n",
      "2019-04-10 00:05:20,792 root         INFO     Train Epoch: 116 [7168/8000 (90%)]\tTotal Loss: 0.105687\n",
      "Reconstruction: 0.101086, Regularization: 0.004601\n",
      "2019-04-10 00:05:20,853 root         INFO     Train Epoch: 116 [7680/8000 (96%)]\tTotal Loss: 0.117508\n",
      "Reconstruction: 0.110768, Regularization: 0.006740\n",
      "2019-04-10 00:05:20,906 root         INFO     ====> Epoch: 116 Average loss: 0.1134\n",
      "2019-04-10 00:05:20,931 root         INFO     Train Epoch: 117 [0/8000 (0%)]\tTotal Loss: 0.116100\n",
      "Reconstruction: 0.109943, Regularization: 0.006157\n",
      "2019-04-10 00:05:20,993 root         INFO     Train Epoch: 117 [512/8000 (6%)]\tTotal Loss: 0.122663\n",
      "Reconstruction: 0.116031, Regularization: 0.006632\n",
      "2019-04-10 00:05:21,055 root         INFO     Train Epoch: 117 [1024/8000 (13%)]\tTotal Loss: 0.106790\n",
      "Reconstruction: 0.102648, Regularization: 0.004142\n",
      "2019-04-10 00:05:21,118 root         INFO     Train Epoch: 117 [1536/8000 (19%)]\tTotal Loss: 0.112606\n",
      "Reconstruction: 0.106294, Regularization: 0.006312\n",
      "2019-04-10 00:05:21,180 root         INFO     Train Epoch: 117 [2048/8000 (26%)]\tTotal Loss: 0.119212\n",
      "Reconstruction: 0.111547, Regularization: 0.007665\n",
      "2019-04-10 00:05:21,241 root         INFO     Train Epoch: 117 [2560/8000 (32%)]\tTotal Loss: 0.120283\n",
      "Reconstruction: 0.113149, Regularization: 0.007133\n",
      "2019-04-10 00:05:21,303 root         INFO     Train Epoch: 117 [3072/8000 (38%)]\tTotal Loss: 0.104936\n",
      "Reconstruction: 0.101131, Regularization: 0.003806\n",
      "2019-04-10 00:05:21,365 root         INFO     Train Epoch: 117 [3584/8000 (45%)]\tTotal Loss: 0.109204\n",
      "Reconstruction: 0.105143, Regularization: 0.004061\n",
      "2019-04-10 00:05:21,427 root         INFO     Train Epoch: 117 [4096/8000 (51%)]\tTotal Loss: 0.100007\n",
      "Reconstruction: 0.095918, Regularization: 0.004088\n",
      "2019-04-10 00:05:21,489 root         INFO     Train Epoch: 117 [4608/8000 (58%)]\tTotal Loss: 0.112361\n",
      "Reconstruction: 0.107432, Regularization: 0.004929\n",
      "2019-04-10 00:05:21,551 root         INFO     Train Epoch: 117 [5120/8000 (64%)]\tTotal Loss: 0.123497\n",
      "Reconstruction: 0.115458, Regularization: 0.008040\n",
      "2019-04-10 00:05:21,613 root         INFO     Train Epoch: 117 [5632/8000 (70%)]\tTotal Loss: 0.104531\n",
      "Reconstruction: 0.100561, Regularization: 0.003969\n",
      "2019-04-10 00:05:21,675 root         INFO     Train Epoch: 117 [6144/8000 (77%)]\tTotal Loss: 0.113556\n",
      "Reconstruction: 0.107673, Regularization: 0.005883\n",
      "2019-04-10 00:05:21,738 root         INFO     Train Epoch: 117 [6656/8000 (83%)]\tTotal Loss: 0.114528\n",
      "Reconstruction: 0.109274, Regularization: 0.005254\n",
      "2019-04-10 00:05:21,801 root         INFO     Train Epoch: 117 [7168/8000 (90%)]\tTotal Loss: 0.107746\n",
      "Reconstruction: 0.102410, Regularization: 0.005336\n",
      "2019-04-10 00:05:21,863 root         INFO     Train Epoch: 117 [7680/8000 (96%)]\tTotal Loss: 0.106605\n",
      "Reconstruction: 0.102391, Regularization: 0.004215\n",
      "2019-04-10 00:05:21,916 root         INFO     ====> Epoch: 117 Average loss: 0.1133\n",
      "2019-04-10 00:05:21,940 root         INFO     Train Epoch: 118 [0/8000 (0%)]\tTotal Loss: 0.124607\n",
      "Reconstruction: 0.118712, Regularization: 0.005895\n",
      "2019-04-10 00:05:22,005 root         INFO     Train Epoch: 118 [512/8000 (6%)]\tTotal Loss: 0.106226\n",
      "Reconstruction: 0.101195, Regularization: 0.005032\n",
      "2019-04-10 00:05:22,070 root         INFO     Train Epoch: 118 [1024/8000 (13%)]\tTotal Loss: 0.118106\n",
      "Reconstruction: 0.111635, Regularization: 0.006471\n",
      "2019-04-10 00:05:22,134 root         INFO     Train Epoch: 118 [1536/8000 (19%)]\tTotal Loss: 0.100397\n",
      "Reconstruction: 0.096290, Regularization: 0.004107\n",
      "2019-04-10 00:05:22,198 root         INFO     Train Epoch: 118 [2048/8000 (26%)]\tTotal Loss: 0.105141\n",
      "Reconstruction: 0.100348, Regularization: 0.004793\n",
      "2019-04-10 00:05:22,262 root         INFO     Train Epoch: 118 [2560/8000 (32%)]\tTotal Loss: 0.113090\n",
      "Reconstruction: 0.106326, Regularization: 0.006763\n",
      "2019-04-10 00:05:22,326 root         INFO     Train Epoch: 118 [3072/8000 (38%)]\tTotal Loss: 0.098482\n",
      "Reconstruction: 0.094979, Regularization: 0.003504\n",
      "2019-04-10 00:05:22,390 root         INFO     Train Epoch: 118 [3584/8000 (45%)]\tTotal Loss: 0.105838\n",
      "Reconstruction: 0.101704, Regularization: 0.004135\n",
      "2019-04-10 00:05:22,454 root         INFO     Train Epoch: 118 [4096/8000 (51%)]\tTotal Loss: 0.104607\n",
      "Reconstruction: 0.101363, Regularization: 0.003244\n",
      "2019-04-10 00:05:22,517 root         INFO     Train Epoch: 118 [4608/8000 (58%)]\tTotal Loss: 0.118121\n",
      "Reconstruction: 0.113118, Regularization: 0.005003\n",
      "2019-04-10 00:05:22,580 root         INFO     Train Epoch: 118 [5120/8000 (64%)]\tTotal Loss: 0.107970\n",
      "Reconstruction: 0.103669, Regularization: 0.004301\n",
      "2019-04-10 00:05:22,643 root         INFO     Train Epoch: 118 [5632/8000 (70%)]\tTotal Loss: 0.124559\n",
      "Reconstruction: 0.116478, Regularization: 0.008081\n",
      "2019-04-10 00:05:22,705 root         INFO     Train Epoch: 118 [6144/8000 (77%)]\tTotal Loss: 0.113212\n",
      "Reconstruction: 0.106611, Regularization: 0.006601\n",
      "2019-04-10 00:05:22,767 root         INFO     Train Epoch: 118 [6656/8000 (83%)]\tTotal Loss: 0.104155\n",
      "Reconstruction: 0.097833, Regularization: 0.006322\n",
      "2019-04-10 00:05:22,829 root         INFO     Train Epoch: 118 [7168/8000 (90%)]\tTotal Loss: 0.113411\n",
      "Reconstruction: 0.107806, Regularization: 0.005605\n",
      "2019-04-10 00:05:22,892 root         INFO     Train Epoch: 118 [7680/8000 (96%)]\tTotal Loss: 0.116112\n",
      "Reconstruction: 0.110192, Regularization: 0.005919\n",
      "2019-04-10 00:05:22,945 root         INFO     ====> Epoch: 118 Average loss: 0.1136\n",
      "2019-04-10 00:05:22,969 root         INFO     Train Epoch: 119 [0/8000 (0%)]\tTotal Loss: 0.101522\n",
      "Reconstruction: 0.096915, Regularization: 0.004607\n",
      "2019-04-10 00:05:23,032 root         INFO     Train Epoch: 119 [512/8000 (6%)]\tTotal Loss: 0.119455\n",
      "Reconstruction: 0.112439, Regularization: 0.007016\n",
      "2019-04-10 00:05:23,094 root         INFO     Train Epoch: 119 [1024/8000 (13%)]\tTotal Loss: 0.118202\n",
      "Reconstruction: 0.113857, Regularization: 0.004345\n",
      "2019-04-10 00:05:23,156 root         INFO     Train Epoch: 119 [1536/8000 (19%)]\tTotal Loss: 0.118607\n",
      "Reconstruction: 0.112605, Regularization: 0.006001\n",
      "2019-04-10 00:05:23,219 root         INFO     Train Epoch: 119 [2048/8000 (26%)]\tTotal Loss: 0.105377\n",
      "Reconstruction: 0.100275, Regularization: 0.005103\n",
      "2019-04-10 00:05:23,281 root         INFO     Train Epoch: 119 [2560/8000 (32%)]\tTotal Loss: 0.131310\n",
      "Reconstruction: 0.123652, Regularization: 0.007657\n",
      "2019-04-10 00:05:23,344 root         INFO     Train Epoch: 119 [3072/8000 (38%)]\tTotal Loss: 0.132360\n",
      "Reconstruction: 0.124921, Regularization: 0.007439\n",
      "2019-04-10 00:05:23,407 root         INFO     Train Epoch: 119 [3584/8000 (45%)]\tTotal Loss: 0.121995\n",
      "Reconstruction: 0.113175, Regularization: 0.008820\n",
      "2019-04-10 00:05:23,469 root         INFO     Train Epoch: 119 [4096/8000 (51%)]\tTotal Loss: 0.109288\n",
      "Reconstruction: 0.103513, Regularization: 0.005775\n",
      "2019-04-10 00:05:23,531 root         INFO     Train Epoch: 119 [4608/8000 (58%)]\tTotal Loss: 0.106977\n",
      "Reconstruction: 0.100749, Regularization: 0.006228\n",
      "2019-04-10 00:05:23,592 root         INFO     Train Epoch: 119 [5120/8000 (64%)]\tTotal Loss: 0.116479\n",
      "Reconstruction: 0.109703, Regularization: 0.006776\n",
      "2019-04-10 00:05:23,654 root         INFO     Train Epoch: 119 [5632/8000 (70%)]\tTotal Loss: 0.113108\n",
      "Reconstruction: 0.107851, Regularization: 0.005258\n",
      "2019-04-10 00:05:23,716 root         INFO     Train Epoch: 119 [6144/8000 (77%)]\tTotal Loss: 0.120856\n",
      "Reconstruction: 0.116132, Regularization: 0.004724\n",
      "2019-04-10 00:05:23,778 root         INFO     Train Epoch: 119 [6656/8000 (83%)]\tTotal Loss: 0.116590\n",
      "Reconstruction: 0.111213, Regularization: 0.005377\n",
      "2019-04-10 00:05:23,840 root         INFO     Train Epoch: 119 [7168/8000 (90%)]\tTotal Loss: 0.112848\n",
      "Reconstruction: 0.106411, Regularization: 0.006437\n",
      "2019-04-10 00:05:23,901 root         INFO     Train Epoch: 119 [7680/8000 (96%)]\tTotal Loss: 0.104458\n",
      "Reconstruction: 0.099205, Regularization: 0.005253\n",
      "2019-04-10 00:05:23,955 root         INFO     ====> Epoch: 119 Average loss: 0.1137\n",
      "2019-04-10 00:05:23,979 root         INFO     Train Epoch: 120 [0/8000 (0%)]\tTotal Loss: 0.109289\n",
      "Reconstruction: 0.103733, Regularization: 0.005556\n",
      "2019-04-10 00:05:24,040 root         INFO     Train Epoch: 120 [512/8000 (6%)]\tTotal Loss: 0.119217\n",
      "Reconstruction: 0.114191, Regularization: 0.005027\n",
      "2019-04-10 00:05:24,100 root         INFO     Train Epoch: 120 [1024/8000 (13%)]\tTotal Loss: 0.129314\n",
      "Reconstruction: 0.120503, Regularization: 0.008811\n",
      "2019-04-10 00:05:24,161 root         INFO     Train Epoch: 120 [1536/8000 (19%)]\tTotal Loss: 0.118644\n",
      "Reconstruction: 0.111902, Regularization: 0.006741\n",
      "2019-04-10 00:05:24,222 root         INFO     Train Epoch: 120 [2048/8000 (26%)]\tTotal Loss: 0.109907\n",
      "Reconstruction: 0.105071, Regularization: 0.004836\n",
      "2019-04-10 00:05:24,283 root         INFO     Train Epoch: 120 [2560/8000 (32%)]\tTotal Loss: 0.112013\n",
      "Reconstruction: 0.107746, Regularization: 0.004267\n",
      "2019-04-10 00:05:24,344 root         INFO     Train Epoch: 120 [3072/8000 (38%)]\tTotal Loss: 0.101047\n",
      "Reconstruction: 0.096946, Regularization: 0.004101\n",
      "2019-04-10 00:05:24,405 root         INFO     Train Epoch: 120 [3584/8000 (45%)]\tTotal Loss: 0.114463\n",
      "Reconstruction: 0.108331, Regularization: 0.006132\n",
      "2019-04-10 00:05:24,465 root         INFO     Train Epoch: 120 [4096/8000 (51%)]\tTotal Loss: 0.099289\n",
      "Reconstruction: 0.094396, Regularization: 0.004893\n",
      "2019-04-10 00:05:24,526 root         INFO     Train Epoch: 120 [4608/8000 (58%)]\tTotal Loss: 0.116076\n",
      "Reconstruction: 0.111641, Regularization: 0.004436\n",
      "2019-04-10 00:05:24,588 root         INFO     Train Epoch: 120 [5120/8000 (64%)]\tTotal Loss: 0.122103\n",
      "Reconstruction: 0.115701, Regularization: 0.006402\n",
      "2019-04-10 00:05:24,651 root         INFO     Train Epoch: 120 [5632/8000 (70%)]\tTotal Loss: 0.108887\n",
      "Reconstruction: 0.102631, Regularization: 0.006256\n",
      "2019-04-10 00:05:24,713 root         INFO     Train Epoch: 120 [6144/8000 (77%)]\tTotal Loss: 0.113001\n",
      "Reconstruction: 0.106285, Regularization: 0.006716\n",
      "2019-04-10 00:05:24,775 root         INFO     Train Epoch: 120 [6656/8000 (83%)]\tTotal Loss: 0.123843\n",
      "Reconstruction: 0.114215, Regularization: 0.009627\n",
      "2019-04-10 00:05:24,837 root         INFO     Train Epoch: 120 [7168/8000 (90%)]\tTotal Loss: 0.118613\n",
      "Reconstruction: 0.112256, Regularization: 0.006358\n",
      "2019-04-10 00:05:24,898 root         INFO     Train Epoch: 120 [7680/8000 (96%)]\tTotal Loss: 0.120520\n",
      "Reconstruction: 0.113819, Regularization: 0.006700\n",
      "2019-04-10 00:05:24,950 root         INFO     ====> Epoch: 120 Average loss: 0.1136\n",
      "2019-04-10 00:05:24,974 root         INFO     Train Epoch: 121 [0/8000 (0%)]\tTotal Loss: 0.112056\n",
      "Reconstruction: 0.104856, Regularization: 0.007200\n",
      "2019-04-10 00:05:25,037 root         INFO     Train Epoch: 121 [512/8000 (6%)]\tTotal Loss: 0.128190\n",
      "Reconstruction: 0.119630, Regularization: 0.008560\n",
      "2019-04-10 00:05:25,098 root         INFO     Train Epoch: 121 [1024/8000 (13%)]\tTotal Loss: 0.105363\n",
      "Reconstruction: 0.100835, Regularization: 0.004528\n",
      "2019-04-10 00:05:25,159 root         INFO     Train Epoch: 121 [1536/8000 (19%)]\tTotal Loss: 0.117695\n",
      "Reconstruction: 0.110186, Regularization: 0.007508\n",
      "2019-04-10 00:05:25,220 root         INFO     Train Epoch: 121 [2048/8000 (26%)]\tTotal Loss: 0.107536\n",
      "Reconstruction: 0.102382, Regularization: 0.005154\n",
      "2019-04-10 00:05:25,282 root         INFO     Train Epoch: 121 [2560/8000 (32%)]\tTotal Loss: 0.112598\n",
      "Reconstruction: 0.107176, Regularization: 0.005423\n",
      "2019-04-10 00:05:25,343 root         INFO     Train Epoch: 121 [3072/8000 (38%)]\tTotal Loss: 0.104708\n",
      "Reconstruction: 0.100519, Regularization: 0.004189\n",
      "2019-04-10 00:05:25,404 root         INFO     Train Epoch: 121 [3584/8000 (45%)]\tTotal Loss: 0.114801\n",
      "Reconstruction: 0.108308, Regularization: 0.006493\n",
      "2019-04-10 00:05:25,467 root         INFO     Train Epoch: 121 [4096/8000 (51%)]\tTotal Loss: 0.110176\n",
      "Reconstruction: 0.103866, Regularization: 0.006310\n",
      "2019-04-10 00:05:25,528 root         INFO     Train Epoch: 121 [4608/8000 (58%)]\tTotal Loss: 0.120787\n",
      "Reconstruction: 0.113703, Regularization: 0.007084\n",
      "2019-04-10 00:05:25,590 root         INFO     Train Epoch: 121 [5120/8000 (64%)]\tTotal Loss: 0.108893\n",
      "Reconstruction: 0.102948, Regularization: 0.005945\n",
      "2019-04-10 00:05:25,652 root         INFO     Train Epoch: 121 [5632/8000 (70%)]\tTotal Loss: 0.117135\n",
      "Reconstruction: 0.111429, Regularization: 0.005706\n",
      "2019-04-10 00:05:25,714 root         INFO     Train Epoch: 121 [6144/8000 (77%)]\tTotal Loss: 0.106875\n",
      "Reconstruction: 0.101603, Regularization: 0.005272\n",
      "2019-04-10 00:05:25,776 root         INFO     Train Epoch: 121 [6656/8000 (83%)]\tTotal Loss: 0.112614\n",
      "Reconstruction: 0.105084, Regularization: 0.007531\n",
      "2019-04-10 00:05:25,839 root         INFO     Train Epoch: 121 [7168/8000 (90%)]\tTotal Loss: 0.113428\n",
      "Reconstruction: 0.106987, Regularization: 0.006441\n",
      "2019-04-10 00:05:25,901 root         INFO     Train Epoch: 121 [7680/8000 (96%)]\tTotal Loss: 0.117598\n",
      "Reconstruction: 0.111806, Regularization: 0.005793\n",
      "2019-04-10 00:05:25,954 root         INFO     ====> Epoch: 121 Average loss: 0.1135\n",
      "2019-04-10 00:05:25,979 root         INFO     Train Epoch: 122 [0/8000 (0%)]\tTotal Loss: 0.120856\n",
      "Reconstruction: 0.112879, Regularization: 0.007977\n",
      "2019-04-10 00:05:26,041 root         INFO     Train Epoch: 122 [512/8000 (6%)]\tTotal Loss: 0.117019\n",
      "Reconstruction: 0.110310, Regularization: 0.006709\n",
      "2019-04-10 00:05:26,103 root         INFO     Train Epoch: 122 [1024/8000 (13%)]\tTotal Loss: 0.118574\n",
      "Reconstruction: 0.111630, Regularization: 0.006944\n",
      "2019-04-10 00:05:26,165 root         INFO     Train Epoch: 122 [1536/8000 (19%)]\tTotal Loss: 0.126126\n",
      "Reconstruction: 0.117152, Regularization: 0.008974\n",
      "2019-04-10 00:05:26,226 root         INFO     Train Epoch: 122 [2048/8000 (26%)]\tTotal Loss: 0.112031\n",
      "Reconstruction: 0.104653, Regularization: 0.007378\n",
      "2019-04-10 00:05:26,287 root         INFO     Train Epoch: 122 [2560/8000 (32%)]\tTotal Loss: 0.111515\n",
      "Reconstruction: 0.106958, Regularization: 0.004558\n",
      "2019-04-10 00:05:26,349 root         INFO     Train Epoch: 122 [3072/8000 (38%)]\tTotal Loss: 0.105334\n",
      "Reconstruction: 0.100763, Regularization: 0.004571\n",
      "2019-04-10 00:05:26,411 root         INFO     Train Epoch: 122 [3584/8000 (45%)]\tTotal Loss: 0.114363\n",
      "Reconstruction: 0.108615, Regularization: 0.005749\n",
      "2019-04-10 00:05:26,474 root         INFO     Train Epoch: 122 [4096/8000 (51%)]\tTotal Loss: 0.113342\n",
      "Reconstruction: 0.107419, Regularization: 0.005923\n",
      "2019-04-10 00:05:26,536 root         INFO     Train Epoch: 122 [4608/8000 (58%)]\tTotal Loss: 0.112374\n",
      "Reconstruction: 0.106654, Regularization: 0.005720\n",
      "2019-04-10 00:05:26,598 root         INFO     Train Epoch: 122 [5120/8000 (64%)]\tTotal Loss: 0.122601\n",
      "Reconstruction: 0.114035, Regularization: 0.008566\n",
      "2019-04-10 00:05:26,661 root         INFO     Train Epoch: 122 [5632/8000 (70%)]\tTotal Loss: 0.108425\n",
      "Reconstruction: 0.102806, Regularization: 0.005619\n",
      "2019-04-10 00:05:26,723 root         INFO     Train Epoch: 122 [6144/8000 (77%)]\tTotal Loss: 0.105675\n",
      "Reconstruction: 0.100447, Regularization: 0.005228\n",
      "2019-04-10 00:05:26,786 root         INFO     Train Epoch: 122 [6656/8000 (83%)]\tTotal Loss: 0.118269\n",
      "Reconstruction: 0.111971, Regularization: 0.006298\n",
      "2019-04-10 00:05:26,848 root         INFO     Train Epoch: 122 [7168/8000 (90%)]\tTotal Loss: 0.109067\n",
      "Reconstruction: 0.103222, Regularization: 0.005845\n",
      "2019-04-10 00:05:26,910 root         INFO     Train Epoch: 122 [7680/8000 (96%)]\tTotal Loss: 0.099912\n",
      "Reconstruction: 0.096162, Regularization: 0.003750\n",
      "2019-04-10 00:05:26,964 root         INFO     ====> Epoch: 122 Average loss: 0.1128\n",
      "2019-04-10 00:05:26,989 root         INFO     Train Epoch: 123 [0/8000 (0%)]\tTotal Loss: 0.125931\n",
      "Reconstruction: 0.118523, Regularization: 0.007408\n",
      "2019-04-10 00:05:27,054 root         INFO     Train Epoch: 123 [512/8000 (6%)]\tTotal Loss: 0.105509\n",
      "Reconstruction: 0.100272, Regularization: 0.005236\n",
      "2019-04-10 00:05:27,118 root         INFO     Train Epoch: 123 [1024/8000 (13%)]\tTotal Loss: 0.120900\n",
      "Reconstruction: 0.114179, Regularization: 0.006721\n",
      "2019-04-10 00:05:27,182 root         INFO     Train Epoch: 123 [1536/8000 (19%)]\tTotal Loss: 0.113540\n",
      "Reconstruction: 0.106911, Regularization: 0.006629\n",
      "2019-04-10 00:05:27,246 root         INFO     Train Epoch: 123 [2048/8000 (26%)]\tTotal Loss: 0.106559\n",
      "Reconstruction: 0.101545, Regularization: 0.005013\n",
      "2019-04-10 00:05:27,310 root         INFO     Train Epoch: 123 [2560/8000 (32%)]\tTotal Loss: 0.113493\n",
      "Reconstruction: 0.107673, Regularization: 0.005820\n",
      "2019-04-10 00:05:27,373 root         INFO     Train Epoch: 123 [3072/8000 (38%)]\tTotal Loss: 0.119652\n",
      "Reconstruction: 0.112065, Regularization: 0.007587\n",
      "2019-04-10 00:05:27,437 root         INFO     Train Epoch: 123 [3584/8000 (45%)]\tTotal Loss: 0.103203\n",
      "Reconstruction: 0.098950, Regularization: 0.004253\n",
      "2019-04-10 00:05:27,499 root         INFO     Train Epoch: 123 [4096/8000 (51%)]\tTotal Loss: 0.108922\n",
      "Reconstruction: 0.103512, Regularization: 0.005410\n",
      "2019-04-10 00:05:27,562 root         INFO     Train Epoch: 123 [4608/8000 (58%)]\tTotal Loss: 0.100453\n",
      "Reconstruction: 0.096298, Regularization: 0.004156\n",
      "2019-04-10 00:05:27,626 root         INFO     Train Epoch: 123 [5120/8000 (64%)]\tTotal Loss: 0.113462\n",
      "Reconstruction: 0.106459, Regularization: 0.007003\n",
      "2019-04-10 00:05:27,689 root         INFO     Train Epoch: 123 [5632/8000 (70%)]\tTotal Loss: 0.114261\n",
      "Reconstruction: 0.107596, Regularization: 0.006664\n",
      "2019-04-10 00:05:27,751 root         INFO     Train Epoch: 123 [6144/8000 (77%)]\tTotal Loss: 0.111027\n",
      "Reconstruction: 0.104567, Regularization: 0.006460\n",
      "2019-04-10 00:05:27,814 root         INFO     Train Epoch: 123 [6656/8000 (83%)]\tTotal Loss: 0.111398\n",
      "Reconstruction: 0.106220, Regularization: 0.005178\n",
      "2019-04-10 00:05:27,878 root         INFO     Train Epoch: 123 [7168/8000 (90%)]\tTotal Loss: 0.105321\n",
      "Reconstruction: 0.100319, Regularization: 0.005002\n",
      "2019-04-10 00:05:27,941 root         INFO     Train Epoch: 123 [7680/8000 (96%)]\tTotal Loss: 0.112856\n",
      "Reconstruction: 0.106099, Regularization: 0.006757\n",
      "2019-04-10 00:05:27,995 root         INFO     ====> Epoch: 123 Average loss: 0.1136\n",
      "2019-04-10 00:05:28,019 root         INFO     Train Epoch: 124 [0/8000 (0%)]\tTotal Loss: 0.131923\n",
      "Reconstruction: 0.122242, Regularization: 0.009681\n",
      "2019-04-10 00:05:28,082 root         INFO     Train Epoch: 124 [512/8000 (6%)]\tTotal Loss: 0.111201\n",
      "Reconstruction: 0.104028, Regularization: 0.007173\n",
      "2019-04-10 00:05:28,146 root         INFO     Train Epoch: 124 [1024/8000 (13%)]\tTotal Loss: 0.105963\n",
      "Reconstruction: 0.100677, Regularization: 0.005287\n",
      "2019-04-10 00:05:28,210 root         INFO     Train Epoch: 124 [1536/8000 (19%)]\tTotal Loss: 0.131623\n",
      "Reconstruction: 0.122226, Regularization: 0.009397\n",
      "2019-04-10 00:05:28,274 root         INFO     Train Epoch: 124 [2048/8000 (26%)]\tTotal Loss: 0.120633\n",
      "Reconstruction: 0.112570, Regularization: 0.008063\n",
      "2019-04-10 00:05:28,337 root         INFO     Train Epoch: 124 [2560/8000 (32%)]\tTotal Loss: 0.116007\n",
      "Reconstruction: 0.109402, Regularization: 0.006605\n",
      "2019-04-10 00:05:28,401 root         INFO     Train Epoch: 124 [3072/8000 (38%)]\tTotal Loss: 0.110574\n",
      "Reconstruction: 0.104297, Regularization: 0.006277\n",
      "2019-04-10 00:05:28,465 root         INFO     Train Epoch: 124 [3584/8000 (45%)]\tTotal Loss: 0.119104\n",
      "Reconstruction: 0.111717, Regularization: 0.007388\n",
      "2019-04-10 00:05:28,528 root         INFO     Train Epoch: 124 [4096/8000 (51%)]\tTotal Loss: 0.121158\n",
      "Reconstruction: 0.114077, Regularization: 0.007080\n",
      "2019-04-10 00:05:28,591 root         INFO     Train Epoch: 124 [4608/8000 (58%)]\tTotal Loss: 0.109034\n",
      "Reconstruction: 0.104341, Regularization: 0.004693\n",
      "2019-04-10 00:05:28,655 root         INFO     Train Epoch: 124 [5120/8000 (64%)]\tTotal Loss: 0.124479\n",
      "Reconstruction: 0.117358, Regularization: 0.007121\n",
      "2019-04-10 00:05:28,718 root         INFO     Train Epoch: 124 [5632/8000 (70%)]\tTotal Loss: 0.108901\n",
      "Reconstruction: 0.101567, Regularization: 0.007334\n",
      "2019-04-10 00:05:28,781 root         INFO     Train Epoch: 124 [6144/8000 (77%)]\tTotal Loss: 0.113895\n",
      "Reconstruction: 0.107110, Regularization: 0.006785\n",
      "2019-04-10 00:05:28,844 root         INFO     Train Epoch: 124 [6656/8000 (83%)]\tTotal Loss: 0.117198\n",
      "Reconstruction: 0.109393, Regularization: 0.007804\n",
      "2019-04-10 00:05:28,907 root         INFO     Train Epoch: 124 [7168/8000 (90%)]\tTotal Loss: 0.123471\n",
      "Reconstruction: 0.117545, Regularization: 0.005926\n",
      "2019-04-10 00:05:28,970 root         INFO     Train Epoch: 124 [7680/8000 (96%)]\tTotal Loss: 0.109059\n",
      "Reconstruction: 0.102802, Regularization: 0.006257\n",
      "2019-04-10 00:05:29,023 root         INFO     ====> Epoch: 124 Average loss: 0.1134\n",
      "2019-04-10 00:05:29,047 root         INFO     Train Epoch: 125 [0/8000 (0%)]\tTotal Loss: 0.105024\n",
      "Reconstruction: 0.098825, Regularization: 0.006198\n",
      "2019-04-10 00:05:29,111 root         INFO     Train Epoch: 125 [512/8000 (6%)]\tTotal Loss: 0.104239\n",
      "Reconstruction: 0.098464, Regularization: 0.005776\n",
      "2019-04-10 00:05:29,173 root         INFO     Train Epoch: 125 [1024/8000 (13%)]\tTotal Loss: 0.118988\n",
      "Reconstruction: 0.111824, Regularization: 0.007164\n",
      "2019-04-10 00:05:29,235 root         INFO     Train Epoch: 125 [1536/8000 (19%)]\tTotal Loss: 0.108771\n",
      "Reconstruction: 0.102494, Regularization: 0.006278\n",
      "2019-04-10 00:05:29,296 root         INFO     Train Epoch: 125 [2048/8000 (26%)]\tTotal Loss: 0.117253\n",
      "Reconstruction: 0.109868, Regularization: 0.007385\n",
      "2019-04-10 00:05:29,357 root         INFO     Train Epoch: 125 [2560/8000 (32%)]\tTotal Loss: 0.116308\n",
      "Reconstruction: 0.108560, Regularization: 0.007749\n",
      "2019-04-10 00:05:29,419 root         INFO     Train Epoch: 125 [3072/8000 (38%)]\tTotal Loss: 0.114829\n",
      "Reconstruction: 0.109154, Regularization: 0.005675\n",
      "2019-04-10 00:05:29,481 root         INFO     Train Epoch: 125 [3584/8000 (45%)]\tTotal Loss: 0.117731\n",
      "Reconstruction: 0.111928, Regularization: 0.005803\n",
      "2019-04-10 00:05:29,542 root         INFO     Train Epoch: 125 [4096/8000 (51%)]\tTotal Loss: 0.115107\n",
      "Reconstruction: 0.109942, Regularization: 0.005165\n",
      "2019-04-10 00:05:29,605 root         INFO     Train Epoch: 125 [4608/8000 (58%)]\tTotal Loss: 0.110122\n",
      "Reconstruction: 0.104076, Regularization: 0.006046\n",
      "2019-04-10 00:05:29,668 root         INFO     Train Epoch: 125 [5120/8000 (64%)]\tTotal Loss: 0.110194\n",
      "Reconstruction: 0.103698, Regularization: 0.006496\n",
      "2019-04-10 00:05:29,731 root         INFO     Train Epoch: 125 [5632/8000 (70%)]\tTotal Loss: 0.116026\n",
      "Reconstruction: 0.108006, Regularization: 0.008020\n",
      "2019-04-10 00:05:29,793 root         INFO     Train Epoch: 125 [6144/8000 (77%)]\tTotal Loss: 0.114614\n",
      "Reconstruction: 0.108305, Regularization: 0.006309\n",
      "2019-04-10 00:05:29,855 root         INFO     Train Epoch: 125 [6656/8000 (83%)]\tTotal Loss: 0.116888\n",
      "Reconstruction: 0.109852, Regularization: 0.007036\n",
      "2019-04-10 00:05:29,919 root         INFO     Train Epoch: 125 [7168/8000 (90%)]\tTotal Loss: 0.120996\n",
      "Reconstruction: 0.112627, Regularization: 0.008369\n",
      "2019-04-10 00:05:29,982 root         INFO     Train Epoch: 125 [7680/8000 (96%)]\tTotal Loss: 0.114193\n",
      "Reconstruction: 0.107875, Regularization: 0.006318\n",
      "2019-04-10 00:05:30,035 root         INFO     ====> Epoch: 125 Average loss: 0.1132\n",
      "2019-04-10 00:05:30,059 root         INFO     Train Epoch: 126 [0/8000 (0%)]\tTotal Loss: 0.120742\n",
      "Reconstruction: 0.113178, Regularization: 0.007563\n",
      "2019-04-10 00:05:30,121 root         INFO     Train Epoch: 126 [512/8000 (6%)]\tTotal Loss: 0.102748\n",
      "Reconstruction: 0.097629, Regularization: 0.005119\n",
      "2019-04-10 00:05:30,184 root         INFO     Train Epoch: 126 [1024/8000 (13%)]\tTotal Loss: 0.108564\n",
      "Reconstruction: 0.103156, Regularization: 0.005408\n",
      "2019-04-10 00:05:30,246 root         INFO     Train Epoch: 126 [1536/8000 (19%)]\tTotal Loss: 0.110524\n",
      "Reconstruction: 0.104688, Regularization: 0.005836\n",
      "2019-04-10 00:05:30,308 root         INFO     Train Epoch: 126 [2048/8000 (26%)]\tTotal Loss: 0.113201\n",
      "Reconstruction: 0.107318, Regularization: 0.005883\n",
      "2019-04-10 00:05:30,370 root         INFO     Train Epoch: 126 [2560/8000 (32%)]\tTotal Loss: 0.111651\n",
      "Reconstruction: 0.105510, Regularization: 0.006141\n",
      "2019-04-10 00:05:30,433 root         INFO     Train Epoch: 126 [3072/8000 (38%)]\tTotal Loss: 0.100743\n",
      "Reconstruction: 0.095484, Regularization: 0.005259\n",
      "2019-04-10 00:05:30,495 root         INFO     Train Epoch: 126 [3584/8000 (45%)]\tTotal Loss: 0.102478\n",
      "Reconstruction: 0.097624, Regularization: 0.004854\n",
      "2019-04-10 00:05:30,557 root         INFO     Train Epoch: 126 [4096/8000 (51%)]\tTotal Loss: 0.115053\n",
      "Reconstruction: 0.107706, Regularization: 0.007347\n",
      "2019-04-10 00:05:30,619 root         INFO     Train Epoch: 126 [4608/8000 (58%)]\tTotal Loss: 0.102602\n",
      "Reconstruction: 0.098097, Regularization: 0.004506\n",
      "2019-04-10 00:05:30,681 root         INFO     Train Epoch: 126 [5120/8000 (64%)]\tTotal Loss: 0.107299\n",
      "Reconstruction: 0.101573, Regularization: 0.005726\n",
      "2019-04-10 00:05:30,742 root         INFO     Train Epoch: 126 [5632/8000 (70%)]\tTotal Loss: 0.114142\n",
      "Reconstruction: 0.107282, Regularization: 0.006861\n",
      "2019-04-10 00:05:30,803 root         INFO     Train Epoch: 126 [6144/8000 (77%)]\tTotal Loss: 0.113900\n",
      "Reconstruction: 0.108294, Regularization: 0.005606\n",
      "2019-04-10 00:05:30,864 root         INFO     Train Epoch: 126 [6656/8000 (83%)]\tTotal Loss: 0.120958\n",
      "Reconstruction: 0.114919, Regularization: 0.006039\n",
      "2019-04-10 00:05:30,925 root         INFO     Train Epoch: 126 [7168/8000 (90%)]\tTotal Loss: 0.118154\n",
      "Reconstruction: 0.110947, Regularization: 0.007207\n",
      "2019-04-10 00:05:30,986 root         INFO     Train Epoch: 126 [7680/8000 (96%)]\tTotal Loss: 0.116227\n",
      "Reconstruction: 0.108463, Regularization: 0.007764\n",
      "2019-04-10 00:05:31,039 root         INFO     ====> Epoch: 126 Average loss: 0.1135\n",
      "2019-04-10 00:05:31,063 root         INFO     Train Epoch: 127 [0/8000 (0%)]\tTotal Loss: 0.128953\n",
      "Reconstruction: 0.117151, Regularization: 0.011802\n",
      "2019-04-10 00:05:31,126 root         INFO     Train Epoch: 127 [512/8000 (6%)]\tTotal Loss: 0.125748\n",
      "Reconstruction: 0.116816, Regularization: 0.008933\n",
      "2019-04-10 00:05:31,189 root         INFO     Train Epoch: 127 [1024/8000 (13%)]\tTotal Loss: 0.104363\n",
      "Reconstruction: 0.098986, Regularization: 0.005377\n",
      "2019-04-10 00:05:31,251 root         INFO     Train Epoch: 127 [1536/8000 (19%)]\tTotal Loss: 0.116534\n",
      "Reconstruction: 0.110098, Regularization: 0.006436\n",
      "2019-04-10 00:05:31,313 root         INFO     Train Epoch: 127 [2048/8000 (26%)]\tTotal Loss: 0.105425\n",
      "Reconstruction: 0.099730, Regularization: 0.005694\n",
      "2019-04-10 00:05:31,376 root         INFO     Train Epoch: 127 [2560/8000 (32%)]\tTotal Loss: 0.106816\n",
      "Reconstruction: 0.101149, Regularization: 0.005667\n",
      "2019-04-10 00:05:31,438 root         INFO     Train Epoch: 127 [3072/8000 (38%)]\tTotal Loss: 0.103871\n",
      "Reconstruction: 0.098700, Regularization: 0.005172\n",
      "2019-04-10 00:05:31,501 root         INFO     Train Epoch: 127 [3584/8000 (45%)]\tTotal Loss: 0.118841\n",
      "Reconstruction: 0.111671, Regularization: 0.007170\n",
      "2019-04-10 00:05:31,563 root         INFO     Train Epoch: 127 [4096/8000 (51%)]\tTotal Loss: 0.123590\n",
      "Reconstruction: 0.115487, Regularization: 0.008104\n",
      "2019-04-10 00:05:31,626 root         INFO     Train Epoch: 127 [4608/8000 (58%)]\tTotal Loss: 0.109004\n",
      "Reconstruction: 0.102010, Regularization: 0.006994\n",
      "2019-04-10 00:05:31,689 root         INFO     Train Epoch: 127 [5120/8000 (64%)]\tTotal Loss: 0.110030\n",
      "Reconstruction: 0.104348, Regularization: 0.005682\n",
      "2019-04-10 00:05:31,753 root         INFO     Train Epoch: 127 [5632/8000 (70%)]\tTotal Loss: 0.108401\n",
      "Reconstruction: 0.103256, Regularization: 0.005145\n",
      "2019-04-10 00:05:31,816 root         INFO     Train Epoch: 127 [6144/8000 (77%)]\tTotal Loss: 0.112595\n",
      "Reconstruction: 0.106628, Regularization: 0.005967\n",
      "2019-04-10 00:05:31,879 root         INFO     Train Epoch: 127 [6656/8000 (83%)]\tTotal Loss: 0.116065\n",
      "Reconstruction: 0.109924, Regularization: 0.006141\n",
      "2019-04-10 00:05:31,942 root         INFO     Train Epoch: 127 [7168/8000 (90%)]\tTotal Loss: 0.111016\n",
      "Reconstruction: 0.103661, Regularization: 0.007356\n",
      "2019-04-10 00:05:32,004 root         INFO     Train Epoch: 127 [7680/8000 (96%)]\tTotal Loss: 0.111509\n",
      "Reconstruction: 0.103551, Regularization: 0.007958\n",
      "2019-04-10 00:05:32,058 root         INFO     ====> Epoch: 127 Average loss: 0.1131\n",
      "2019-04-10 00:05:32,082 root         INFO     Train Epoch: 128 [0/8000 (0%)]\tTotal Loss: 0.107725\n",
      "Reconstruction: 0.100497, Regularization: 0.007229\n",
      "2019-04-10 00:05:32,146 root         INFO     Train Epoch: 128 [512/8000 (6%)]\tTotal Loss: 0.111378\n",
      "Reconstruction: 0.105435, Regularization: 0.005943\n",
      "2019-04-10 00:05:32,210 root         INFO     Train Epoch: 128 [1024/8000 (13%)]\tTotal Loss: 0.111972\n",
      "Reconstruction: 0.106160, Regularization: 0.005812\n",
      "2019-04-10 00:05:32,275 root         INFO     Train Epoch: 128 [1536/8000 (19%)]\tTotal Loss: 0.113667\n",
      "Reconstruction: 0.106705, Regularization: 0.006962\n",
      "2019-04-10 00:05:32,339 root         INFO     Train Epoch: 128 [2048/8000 (26%)]\tTotal Loss: 0.114515\n",
      "Reconstruction: 0.108003, Regularization: 0.006512\n",
      "2019-04-10 00:05:32,403 root         INFO     Train Epoch: 128 [2560/8000 (32%)]\tTotal Loss: 0.111689\n",
      "Reconstruction: 0.105112, Regularization: 0.006577\n",
      "2019-04-10 00:05:32,466 root         INFO     Train Epoch: 128 [3072/8000 (38%)]\tTotal Loss: 0.111240\n",
      "Reconstruction: 0.104833, Regularization: 0.006407\n",
      "2019-04-10 00:05:32,529 root         INFO     Train Epoch: 128 [3584/8000 (45%)]\tTotal Loss: 0.103313\n",
      "Reconstruction: 0.098640, Regularization: 0.004674\n",
      "2019-04-10 00:05:32,592 root         INFO     Train Epoch: 128 [4096/8000 (51%)]\tTotal Loss: 0.124384\n",
      "Reconstruction: 0.113849, Regularization: 0.010535\n",
      "2019-04-10 00:05:32,655 root         INFO     Train Epoch: 128 [4608/8000 (58%)]\tTotal Loss: 0.112679\n",
      "Reconstruction: 0.107381, Regularization: 0.005298\n",
      "2019-04-10 00:05:32,718 root         INFO     Train Epoch: 128 [5120/8000 (64%)]\tTotal Loss: 0.112217\n",
      "Reconstruction: 0.105218, Regularization: 0.006999\n",
      "2019-04-10 00:05:32,781 root         INFO     Train Epoch: 128 [5632/8000 (70%)]\tTotal Loss: 0.110238\n",
      "Reconstruction: 0.102557, Regularization: 0.007681\n",
      "2019-04-10 00:05:32,844 root         INFO     Train Epoch: 128 [6144/8000 (77%)]\tTotal Loss: 0.119080\n",
      "Reconstruction: 0.112197, Regularization: 0.006883\n",
      "2019-04-10 00:05:32,906 root         INFO     Train Epoch: 128 [6656/8000 (83%)]\tTotal Loss: 0.111776\n",
      "Reconstruction: 0.104691, Regularization: 0.007085\n",
      "2019-04-10 00:05:32,967 root         INFO     Train Epoch: 128 [7168/8000 (90%)]\tTotal Loss: 0.119004\n",
      "Reconstruction: 0.111206, Regularization: 0.007798\n",
      "2019-04-10 00:05:33,028 root         INFO     Train Epoch: 128 [7680/8000 (96%)]\tTotal Loss: 0.105161\n",
      "Reconstruction: 0.100315, Regularization: 0.004846\n",
      "2019-04-10 00:05:33,082 root         INFO     ====> Epoch: 128 Average loss: 0.1130\n",
      "2019-04-10 00:05:33,106 root         INFO     Train Epoch: 129 [0/8000 (0%)]\tTotal Loss: 0.111524\n",
      "Reconstruction: 0.104927, Regularization: 0.006596\n",
      "2019-04-10 00:05:33,169 root         INFO     Train Epoch: 129 [512/8000 (6%)]\tTotal Loss: 0.108720\n",
      "Reconstruction: 0.101977, Regularization: 0.006743\n",
      "2019-04-10 00:05:33,233 root         INFO     Train Epoch: 129 [1024/8000 (13%)]\tTotal Loss: 0.107351\n",
      "Reconstruction: 0.099987, Regularization: 0.007364\n",
      "2019-04-10 00:05:33,296 root         INFO     Train Epoch: 129 [1536/8000 (19%)]\tTotal Loss: 0.105530\n",
      "Reconstruction: 0.100089, Regularization: 0.005441\n",
      "2019-04-10 00:05:33,359 root         INFO     Train Epoch: 129 [2048/8000 (26%)]\tTotal Loss: 0.103043\n",
      "Reconstruction: 0.097422, Regularization: 0.005621\n",
      "2019-04-10 00:05:33,422 root         INFO     Train Epoch: 129 [2560/8000 (32%)]\tTotal Loss: 0.110051\n",
      "Reconstruction: 0.104154, Regularization: 0.005897\n",
      "2019-04-10 00:05:33,485 root         INFO     Train Epoch: 129 [3072/8000 (38%)]\tTotal Loss: 0.121764\n",
      "Reconstruction: 0.113939, Regularization: 0.007825\n",
      "2019-04-10 00:05:33,548 root         INFO     Train Epoch: 129 [3584/8000 (45%)]\tTotal Loss: 0.123799\n",
      "Reconstruction: 0.116067, Regularization: 0.007731\n",
      "2019-04-10 00:05:33,610 root         INFO     Train Epoch: 129 [4096/8000 (51%)]\tTotal Loss: 0.116836\n",
      "Reconstruction: 0.110366, Regularization: 0.006469\n",
      "2019-04-10 00:05:33,673 root         INFO     Train Epoch: 129 [4608/8000 (58%)]\tTotal Loss: 0.114485\n",
      "Reconstruction: 0.107992, Regularization: 0.006493\n",
      "2019-04-10 00:05:33,735 root         INFO     Train Epoch: 129 [5120/8000 (64%)]\tTotal Loss: 0.109987\n",
      "Reconstruction: 0.102720, Regularization: 0.007266\n",
      "2019-04-10 00:05:33,798 root         INFO     Train Epoch: 129 [5632/8000 (70%)]\tTotal Loss: 0.109447\n",
      "Reconstruction: 0.103237, Regularization: 0.006210\n",
      "2019-04-10 00:05:33,860 root         INFO     Train Epoch: 129 [6144/8000 (77%)]\tTotal Loss: 0.113252\n",
      "Reconstruction: 0.107344, Regularization: 0.005908\n",
      "2019-04-10 00:05:33,923 root         INFO     Train Epoch: 129 [6656/8000 (83%)]\tTotal Loss: 0.119779\n",
      "Reconstruction: 0.113547, Regularization: 0.006231\n",
      "2019-04-10 00:05:33,985 root         INFO     Train Epoch: 129 [7168/8000 (90%)]\tTotal Loss: 0.114261\n",
      "Reconstruction: 0.106044, Regularization: 0.008218\n",
      "2019-04-10 00:05:34,049 root         INFO     Train Epoch: 129 [7680/8000 (96%)]\tTotal Loss: 0.112915\n",
      "Reconstruction: 0.106824, Regularization: 0.006091\n",
      "2019-04-10 00:05:34,103 root         INFO     ====> Epoch: 129 Average loss: 0.1134\n",
      "2019-04-10 00:05:34,127 root         INFO     Train Epoch: 130 [0/8000 (0%)]\tTotal Loss: 0.111098\n",
      "Reconstruction: 0.106270, Regularization: 0.004828\n",
      "2019-04-10 00:05:34,190 root         INFO     Train Epoch: 130 [512/8000 (6%)]\tTotal Loss: 0.101610\n",
      "Reconstruction: 0.096074, Regularization: 0.005537\n",
      "2019-04-10 00:05:34,253 root         INFO     Train Epoch: 130 [1024/8000 (13%)]\tTotal Loss: 0.119612\n",
      "Reconstruction: 0.111454, Regularization: 0.008158\n",
      "2019-04-10 00:05:34,317 root         INFO     Train Epoch: 130 [1536/8000 (19%)]\tTotal Loss: 0.111734\n",
      "Reconstruction: 0.105563, Regularization: 0.006171\n",
      "2019-04-10 00:05:34,380 root         INFO     Train Epoch: 130 [2048/8000 (26%)]\tTotal Loss: 0.125374\n",
      "Reconstruction: 0.116844, Regularization: 0.008530\n",
      "2019-04-10 00:05:34,443 root         INFO     Train Epoch: 130 [2560/8000 (32%)]\tTotal Loss: 0.108855\n",
      "Reconstruction: 0.103458, Regularization: 0.005397\n",
      "2019-04-10 00:05:34,506 root         INFO     Train Epoch: 130 [3072/8000 (38%)]\tTotal Loss: 0.114378\n",
      "Reconstruction: 0.107973, Regularization: 0.006405\n",
      "2019-04-10 00:05:34,569 root         INFO     Train Epoch: 130 [3584/8000 (45%)]\tTotal Loss: 0.114587\n",
      "Reconstruction: 0.106130, Regularization: 0.008457\n",
      "2019-04-10 00:05:34,631 root         INFO     Train Epoch: 130 [4096/8000 (51%)]\tTotal Loss: 0.109744\n",
      "Reconstruction: 0.104072, Regularization: 0.005672\n",
      "2019-04-10 00:05:34,694 root         INFO     Train Epoch: 130 [4608/8000 (58%)]\tTotal Loss: 0.109547\n",
      "Reconstruction: 0.102696, Regularization: 0.006851\n",
      "2019-04-10 00:05:34,758 root         INFO     Train Epoch: 130 [5120/8000 (64%)]\tTotal Loss: 0.121338\n",
      "Reconstruction: 0.112913, Regularization: 0.008425\n",
      "2019-04-10 00:05:34,821 root         INFO     Train Epoch: 130 [5632/8000 (70%)]\tTotal Loss: 0.124208\n",
      "Reconstruction: 0.115134, Regularization: 0.009074\n",
      "2019-04-10 00:05:34,883 root         INFO     Train Epoch: 130 [6144/8000 (77%)]\tTotal Loss: 0.114868\n",
      "Reconstruction: 0.107205, Regularization: 0.007663\n",
      "2019-04-10 00:05:34,945 root         INFO     Train Epoch: 130 [6656/8000 (83%)]\tTotal Loss: 0.112021\n",
      "Reconstruction: 0.104799, Regularization: 0.007223\n",
      "2019-04-10 00:05:35,007 root         INFO     Train Epoch: 130 [7168/8000 (90%)]\tTotal Loss: 0.117083\n",
      "Reconstruction: 0.108035, Regularization: 0.009048\n",
      "2019-04-10 00:05:35,069 root         INFO     Train Epoch: 130 [7680/8000 (96%)]\tTotal Loss: 0.114937\n",
      "Reconstruction: 0.105691, Regularization: 0.009246\n",
      "2019-04-10 00:05:35,122 root         INFO     ====> Epoch: 130 Average loss: 0.1133\n",
      "2019-04-10 00:05:35,146 root         INFO     Train Epoch: 131 [0/8000 (0%)]\tTotal Loss: 0.112467\n",
      "Reconstruction: 0.106501, Regularization: 0.005965\n",
      "2019-04-10 00:05:35,210 root         INFO     Train Epoch: 131 [512/8000 (6%)]\tTotal Loss: 0.117844\n",
      "Reconstruction: 0.110584, Regularization: 0.007260\n",
      "2019-04-10 00:05:35,274 root         INFO     Train Epoch: 131 [1024/8000 (13%)]\tTotal Loss: 0.107816\n",
      "Reconstruction: 0.101877, Regularization: 0.005939\n",
      "2019-04-10 00:05:35,339 root         INFO     Train Epoch: 131 [1536/8000 (19%)]\tTotal Loss: 0.117251\n",
      "Reconstruction: 0.109413, Regularization: 0.007838\n",
      "2019-04-10 00:05:35,401 root         INFO     Train Epoch: 131 [2048/8000 (26%)]\tTotal Loss: 0.119137\n",
      "Reconstruction: 0.110404, Regularization: 0.008733\n",
      "2019-04-10 00:05:35,464 root         INFO     Train Epoch: 131 [2560/8000 (32%)]\tTotal Loss: 0.113684\n",
      "Reconstruction: 0.106163, Regularization: 0.007521\n",
      "2019-04-10 00:05:35,527 root         INFO     Train Epoch: 131 [3072/8000 (38%)]\tTotal Loss: 0.111244\n",
      "Reconstruction: 0.104111, Regularization: 0.007133\n",
      "2019-04-10 00:05:35,589 root         INFO     Train Epoch: 131 [3584/8000 (45%)]\tTotal Loss: 0.126638\n",
      "Reconstruction: 0.118830, Regularization: 0.007808\n",
      "2019-04-10 00:05:35,651 root         INFO     Train Epoch: 131 [4096/8000 (51%)]\tTotal Loss: 0.107129\n",
      "Reconstruction: 0.101694, Regularization: 0.005435\n",
      "2019-04-10 00:05:35,714 root         INFO     Train Epoch: 131 [4608/8000 (58%)]\tTotal Loss: 0.120503\n",
      "Reconstruction: 0.114051, Regularization: 0.006453\n",
      "2019-04-10 00:05:35,774 root         INFO     Train Epoch: 131 [5120/8000 (64%)]\tTotal Loss: 0.109732\n",
      "Reconstruction: 0.103012, Regularization: 0.006720\n",
      "2019-04-10 00:05:35,835 root         INFO     Train Epoch: 131 [5632/8000 (70%)]\tTotal Loss: 0.118478\n",
      "Reconstruction: 0.108949, Regularization: 0.009529\n",
      "2019-04-10 00:05:35,896 root         INFO     Train Epoch: 131 [6144/8000 (77%)]\tTotal Loss: 0.108374\n",
      "Reconstruction: 0.100930, Regularization: 0.007444\n",
      "2019-04-10 00:05:35,959 root         INFO     Train Epoch: 131 [6656/8000 (83%)]\tTotal Loss: 0.118189\n",
      "Reconstruction: 0.110318, Regularization: 0.007872\n",
      "2019-04-10 00:05:36,022 root         INFO     Train Epoch: 131 [7168/8000 (90%)]\tTotal Loss: 0.110745\n",
      "Reconstruction: 0.103861, Regularization: 0.006884\n",
      "2019-04-10 00:05:36,083 root         INFO     Train Epoch: 131 [7680/8000 (96%)]\tTotal Loss: 0.107885\n",
      "Reconstruction: 0.102190, Regularization: 0.005696\n",
      "2019-04-10 00:05:36,137 root         INFO     ====> Epoch: 131 Average loss: 0.1133\n",
      "2019-04-10 00:05:36,160 root         INFO     Train Epoch: 132 [0/8000 (0%)]\tTotal Loss: 0.102200\n",
      "Reconstruction: 0.096516, Regularization: 0.005685\n",
      "2019-04-10 00:05:36,223 root         INFO     Train Epoch: 132 [512/8000 (6%)]\tTotal Loss: 0.118835\n",
      "Reconstruction: 0.109572, Regularization: 0.009263\n",
      "2019-04-10 00:05:36,286 root         INFO     Train Epoch: 132 [1024/8000 (13%)]\tTotal Loss: 0.111344\n",
      "Reconstruction: 0.103539, Regularization: 0.007805\n",
      "2019-04-10 00:05:36,349 root         INFO     Train Epoch: 132 [1536/8000 (19%)]\tTotal Loss: 0.106875\n",
      "Reconstruction: 0.101024, Regularization: 0.005851\n",
      "2019-04-10 00:05:36,412 root         INFO     Train Epoch: 132 [2048/8000 (26%)]\tTotal Loss: 0.121719\n",
      "Reconstruction: 0.114356, Regularization: 0.007363\n",
      "2019-04-10 00:05:36,474 root         INFO     Train Epoch: 132 [2560/8000 (32%)]\tTotal Loss: 0.114414\n",
      "Reconstruction: 0.106724, Regularization: 0.007690\n",
      "2019-04-10 00:05:36,537 root         INFO     Train Epoch: 132 [3072/8000 (38%)]\tTotal Loss: 0.109126\n",
      "Reconstruction: 0.101383, Regularization: 0.007742\n",
      "2019-04-10 00:05:36,599 root         INFO     Train Epoch: 132 [3584/8000 (45%)]\tTotal Loss: 0.114940\n",
      "Reconstruction: 0.107944, Regularization: 0.006996\n",
      "2019-04-10 00:05:36,662 root         INFO     Train Epoch: 132 [4096/8000 (51%)]\tTotal Loss: 0.107845\n",
      "Reconstruction: 0.101433, Regularization: 0.006412\n",
      "2019-04-10 00:05:36,724 root         INFO     Train Epoch: 132 [4608/8000 (58%)]\tTotal Loss: 0.112757\n",
      "Reconstruction: 0.106515, Regularization: 0.006242\n",
      "2019-04-10 00:05:36,786 root         INFO     Train Epoch: 132 [5120/8000 (64%)]\tTotal Loss: 0.105476\n",
      "Reconstruction: 0.099089, Regularization: 0.006387\n",
      "2019-04-10 00:05:36,848 root         INFO     Train Epoch: 132 [5632/8000 (70%)]\tTotal Loss: 0.115559\n",
      "Reconstruction: 0.107868, Regularization: 0.007690\n",
      "2019-04-10 00:05:36,910 root         INFO     Train Epoch: 132 [6144/8000 (77%)]\tTotal Loss: 0.119171\n",
      "Reconstruction: 0.111441, Regularization: 0.007730\n",
      "2019-04-10 00:05:36,972 root         INFO     Train Epoch: 132 [6656/8000 (83%)]\tTotal Loss: 0.118315\n",
      "Reconstruction: 0.111594, Regularization: 0.006720\n",
      "2019-04-10 00:05:37,034 root         INFO     Train Epoch: 132 [7168/8000 (90%)]\tTotal Loss: 0.106334\n",
      "Reconstruction: 0.101605, Regularization: 0.004729\n",
      "2019-04-10 00:05:37,096 root         INFO     Train Epoch: 132 [7680/8000 (96%)]\tTotal Loss: 0.117206\n",
      "Reconstruction: 0.108492, Regularization: 0.008714\n",
      "2019-04-10 00:05:37,149 root         INFO     ====> Epoch: 132 Average loss: 0.1132\n",
      "2019-04-10 00:05:37,173 root         INFO     Train Epoch: 133 [0/8000 (0%)]\tTotal Loss: 0.114918\n",
      "Reconstruction: 0.107683, Regularization: 0.007235\n",
      "2019-04-10 00:05:37,236 root         INFO     Train Epoch: 133 [512/8000 (6%)]\tTotal Loss: 0.107868\n",
      "Reconstruction: 0.099989, Regularization: 0.007879\n",
      "2019-04-10 00:05:37,298 root         INFO     Train Epoch: 133 [1024/8000 (13%)]\tTotal Loss: 0.120068\n",
      "Reconstruction: 0.110221, Regularization: 0.009846\n",
      "2019-04-10 00:05:37,360 root         INFO     Train Epoch: 133 [1536/8000 (19%)]\tTotal Loss: 0.111173\n",
      "Reconstruction: 0.104473, Regularization: 0.006700\n",
      "2019-04-10 00:05:37,423 root         INFO     Train Epoch: 133 [2048/8000 (26%)]\tTotal Loss: 0.108654\n",
      "Reconstruction: 0.102540, Regularization: 0.006114\n",
      "2019-04-10 00:05:37,484 root         INFO     Train Epoch: 133 [2560/8000 (32%)]\tTotal Loss: 0.119348\n",
      "Reconstruction: 0.110900, Regularization: 0.008449\n",
      "2019-04-10 00:05:37,545 root         INFO     Train Epoch: 133 [3072/8000 (38%)]\tTotal Loss: 0.120800\n",
      "Reconstruction: 0.112709, Regularization: 0.008091\n",
      "2019-04-10 00:05:37,607 root         INFO     Train Epoch: 133 [3584/8000 (45%)]\tTotal Loss: 0.120444\n",
      "Reconstruction: 0.113518, Regularization: 0.006926\n",
      "2019-04-10 00:05:37,669 root         INFO     Train Epoch: 133 [4096/8000 (51%)]\tTotal Loss: 0.106813\n",
      "Reconstruction: 0.100564, Regularization: 0.006249\n",
      "2019-04-10 00:05:37,731 root         INFO     Train Epoch: 133 [4608/8000 (58%)]\tTotal Loss: 0.106623\n",
      "Reconstruction: 0.101157, Regularization: 0.005467\n",
      "2019-04-10 00:05:37,793 root         INFO     Train Epoch: 133 [5120/8000 (64%)]\tTotal Loss: 0.110401\n",
      "Reconstruction: 0.102428, Regularization: 0.007973\n",
      "2019-04-10 00:05:37,854 root         INFO     Train Epoch: 133 [5632/8000 (70%)]\tTotal Loss: 0.110519\n",
      "Reconstruction: 0.103940, Regularization: 0.006579\n",
      "2019-04-10 00:05:37,916 root         INFO     Train Epoch: 133 [6144/8000 (77%)]\tTotal Loss: 0.109714\n",
      "Reconstruction: 0.103063, Regularization: 0.006651\n",
      "2019-04-10 00:05:37,978 root         INFO     Train Epoch: 133 [6656/8000 (83%)]\tTotal Loss: 0.118000\n",
      "Reconstruction: 0.109147, Regularization: 0.008853\n",
      "2019-04-10 00:05:38,039 root         INFO     Train Epoch: 133 [7168/8000 (90%)]\tTotal Loss: 0.116220\n",
      "Reconstruction: 0.110042, Regularization: 0.006179\n",
      "2019-04-10 00:05:38,101 root         INFO     Train Epoch: 133 [7680/8000 (96%)]\tTotal Loss: 0.114932\n",
      "Reconstruction: 0.107149, Regularization: 0.007783\n",
      "2019-04-10 00:05:38,154 root         INFO     ====> Epoch: 133 Average loss: 0.1129\n",
      "2019-04-10 00:05:38,178 root         INFO     Train Epoch: 134 [0/8000 (0%)]\tTotal Loss: 0.102009\n",
      "Reconstruction: 0.097505, Regularization: 0.004505\n",
      "2019-04-10 00:05:38,241 root         INFO     Train Epoch: 134 [512/8000 (6%)]\tTotal Loss: 0.123465\n",
      "Reconstruction: 0.114390, Regularization: 0.009075\n",
      "2019-04-10 00:05:38,304 root         INFO     Train Epoch: 134 [1024/8000 (13%)]\tTotal Loss: 0.106833\n",
      "Reconstruction: 0.099718, Regularization: 0.007115\n",
      "2019-04-10 00:05:38,367 root         INFO     Train Epoch: 134 [1536/8000 (19%)]\tTotal Loss: 0.114944\n",
      "Reconstruction: 0.106317, Regularization: 0.008627\n",
      "2019-04-10 00:05:38,430 root         INFO     Train Epoch: 134 [2048/8000 (26%)]\tTotal Loss: 0.119218\n",
      "Reconstruction: 0.110835, Regularization: 0.008383\n",
      "2019-04-10 00:05:38,493 root         INFO     Train Epoch: 134 [2560/8000 (32%)]\tTotal Loss: 0.129232\n",
      "Reconstruction: 0.120342, Regularization: 0.008890\n",
      "2019-04-10 00:05:38,555 root         INFO     Train Epoch: 134 [3072/8000 (38%)]\tTotal Loss: 0.121092\n",
      "Reconstruction: 0.111844, Regularization: 0.009248\n",
      "2019-04-10 00:05:38,616 root         INFO     Train Epoch: 134 [3584/8000 (45%)]\tTotal Loss: 0.111496\n",
      "Reconstruction: 0.102533, Regularization: 0.008963\n",
      "2019-04-10 00:05:38,679 root         INFO     Train Epoch: 134 [4096/8000 (51%)]\tTotal Loss: 0.102631\n",
      "Reconstruction: 0.096935, Regularization: 0.005697\n",
      "2019-04-10 00:05:38,741 root         INFO     Train Epoch: 134 [4608/8000 (58%)]\tTotal Loss: 0.107818\n",
      "Reconstruction: 0.100369, Regularization: 0.007449\n",
      "2019-04-10 00:05:38,803 root         INFO     Train Epoch: 134 [5120/8000 (64%)]\tTotal Loss: 0.112665\n",
      "Reconstruction: 0.104912, Regularization: 0.007754\n",
      "2019-04-10 00:05:38,865 root         INFO     Train Epoch: 134 [5632/8000 (70%)]\tTotal Loss: 0.114997\n",
      "Reconstruction: 0.106438, Regularization: 0.008559\n",
      "2019-04-10 00:05:38,927 root         INFO     Train Epoch: 134 [6144/8000 (77%)]\tTotal Loss: 0.109095\n",
      "Reconstruction: 0.103229, Regularization: 0.005866\n",
      "2019-04-10 00:05:38,989 root         INFO     Train Epoch: 134 [6656/8000 (83%)]\tTotal Loss: 0.120730\n",
      "Reconstruction: 0.112716, Regularization: 0.008014\n",
      "2019-04-10 00:05:39,052 root         INFO     Train Epoch: 134 [7168/8000 (90%)]\tTotal Loss: 0.110140\n",
      "Reconstruction: 0.103166, Regularization: 0.006974\n",
      "2019-04-10 00:05:39,116 root         INFO     Train Epoch: 134 [7680/8000 (96%)]\tTotal Loss: 0.114611\n",
      "Reconstruction: 0.105320, Regularization: 0.009291\n",
      "2019-04-10 00:05:39,169 root         INFO     ====> Epoch: 134 Average loss: 0.1131\n",
      "2019-04-10 00:05:39,193 root         INFO     Train Epoch: 135 [0/8000 (0%)]\tTotal Loss: 0.117043\n",
      "Reconstruction: 0.109464, Regularization: 0.007579\n",
      "2019-04-10 00:05:39,257 root         INFO     Train Epoch: 135 [512/8000 (6%)]\tTotal Loss: 0.111028\n",
      "Reconstruction: 0.103923, Regularization: 0.007105\n",
      "2019-04-10 00:05:39,319 root         INFO     Train Epoch: 135 [1024/8000 (13%)]\tTotal Loss: 0.101063\n",
      "Reconstruction: 0.096631, Regularization: 0.004432\n",
      "2019-04-10 00:05:39,382 root         INFO     Train Epoch: 135 [1536/8000 (19%)]\tTotal Loss: 0.107694\n",
      "Reconstruction: 0.103175, Regularization: 0.004519\n",
      "2019-04-10 00:05:39,445 root         INFO     Train Epoch: 135 [2048/8000 (26%)]\tTotal Loss: 0.109183\n",
      "Reconstruction: 0.102746, Regularization: 0.006437\n",
      "2019-04-10 00:05:39,508 root         INFO     Train Epoch: 135 [2560/8000 (32%)]\tTotal Loss: 0.110428\n",
      "Reconstruction: 0.104492, Regularization: 0.005936\n",
      "2019-04-10 00:05:39,570 root         INFO     Train Epoch: 135 [3072/8000 (38%)]\tTotal Loss: 0.120078\n",
      "Reconstruction: 0.111844, Regularization: 0.008233\n",
      "2019-04-10 00:05:39,633 root         INFO     Train Epoch: 135 [3584/8000 (45%)]\tTotal Loss: 0.126947\n",
      "Reconstruction: 0.119726, Regularization: 0.007221\n",
      "2019-04-10 00:05:39,696 root         INFO     Train Epoch: 135 [4096/8000 (51%)]\tTotal Loss: 0.107052\n",
      "Reconstruction: 0.100081, Regularization: 0.006972\n",
      "2019-04-10 00:05:39,759 root         INFO     Train Epoch: 135 [4608/8000 (58%)]\tTotal Loss: 0.110751\n",
      "Reconstruction: 0.103795, Regularization: 0.006956\n",
      "2019-04-10 00:05:39,822 root         INFO     Train Epoch: 135 [5120/8000 (64%)]\tTotal Loss: 0.106744\n",
      "Reconstruction: 0.101023, Regularization: 0.005720\n",
      "2019-04-10 00:05:39,885 root         INFO     Train Epoch: 135 [5632/8000 (70%)]\tTotal Loss: 0.107707\n",
      "Reconstruction: 0.102047, Regularization: 0.005660\n",
      "2019-04-10 00:05:39,948 root         INFO     Train Epoch: 135 [6144/8000 (77%)]\tTotal Loss: 0.106826\n",
      "Reconstruction: 0.100465, Regularization: 0.006361\n",
      "2019-04-10 00:05:40,010 root         INFO     Train Epoch: 135 [6656/8000 (83%)]\tTotal Loss: 0.113080\n",
      "Reconstruction: 0.104962, Regularization: 0.008118\n",
      "2019-04-10 00:05:40,073 root         INFO     Train Epoch: 135 [7168/8000 (90%)]\tTotal Loss: 0.118176\n",
      "Reconstruction: 0.108995, Regularization: 0.009181\n",
      "2019-04-10 00:05:40,136 root         INFO     Train Epoch: 135 [7680/8000 (96%)]\tTotal Loss: 0.110730\n",
      "Reconstruction: 0.102878, Regularization: 0.007852\n",
      "2019-04-10 00:05:40,189 root         INFO     ====> Epoch: 135 Average loss: 0.1130\n",
      "2019-04-10 00:05:40,213 root         INFO     Train Epoch: 136 [0/8000 (0%)]\tTotal Loss: 0.116064\n",
      "Reconstruction: 0.109501, Regularization: 0.006563\n",
      "2019-04-10 00:05:40,277 root         INFO     Train Epoch: 136 [512/8000 (6%)]\tTotal Loss: 0.110030\n",
      "Reconstruction: 0.103796, Regularization: 0.006234\n",
      "2019-04-10 00:05:40,341 root         INFO     Train Epoch: 136 [1024/8000 (13%)]\tTotal Loss: 0.106842\n",
      "Reconstruction: 0.101034, Regularization: 0.005808\n",
      "2019-04-10 00:05:40,404 root         INFO     Train Epoch: 136 [1536/8000 (19%)]\tTotal Loss: 0.114946\n",
      "Reconstruction: 0.107455, Regularization: 0.007491\n",
      "2019-04-10 00:05:40,468 root         INFO     Train Epoch: 136 [2048/8000 (26%)]\tTotal Loss: 0.115446\n",
      "Reconstruction: 0.109464, Regularization: 0.005982\n",
      "2019-04-10 00:05:40,532 root         INFO     Train Epoch: 136 [2560/8000 (32%)]\tTotal Loss: 0.108046\n",
      "Reconstruction: 0.101276, Regularization: 0.006770\n",
      "2019-04-10 00:05:40,595 root         INFO     Train Epoch: 136 [3072/8000 (38%)]\tTotal Loss: 0.107317\n",
      "Reconstruction: 0.099232, Regularization: 0.008085\n",
      "2019-04-10 00:05:40,658 root         INFO     Train Epoch: 136 [3584/8000 (45%)]\tTotal Loss: 0.119382\n",
      "Reconstruction: 0.111470, Regularization: 0.007912\n",
      "2019-04-10 00:05:40,722 root         INFO     Train Epoch: 136 [4096/8000 (51%)]\tTotal Loss: 0.112734\n",
      "Reconstruction: 0.106809, Regularization: 0.005926\n",
      "2019-04-10 00:05:40,786 root         INFO     Train Epoch: 136 [4608/8000 (58%)]\tTotal Loss: 0.102220\n",
      "Reconstruction: 0.096410, Regularization: 0.005810\n",
      "2019-04-10 00:05:40,848 root         INFO     Train Epoch: 136 [5120/8000 (64%)]\tTotal Loss: 0.098444\n",
      "Reconstruction: 0.093753, Regularization: 0.004691\n",
      "2019-04-10 00:05:40,909 root         INFO     Train Epoch: 136 [5632/8000 (70%)]\tTotal Loss: 0.112678\n",
      "Reconstruction: 0.105550, Regularization: 0.007129\n",
      "2019-04-10 00:05:40,971 root         INFO     Train Epoch: 136 [6144/8000 (77%)]\tTotal Loss: 0.118363\n",
      "Reconstruction: 0.110335, Regularization: 0.008028\n",
      "2019-04-10 00:05:41,033 root         INFO     Train Epoch: 136 [6656/8000 (83%)]\tTotal Loss: 0.116586\n",
      "Reconstruction: 0.107764, Regularization: 0.008822\n",
      "2019-04-10 00:05:41,095 root         INFO     Train Epoch: 136 [7168/8000 (90%)]\tTotal Loss: 0.110686\n",
      "Reconstruction: 0.104826, Regularization: 0.005860\n",
      "2019-04-10 00:05:41,157 root         INFO     Train Epoch: 136 [7680/8000 (96%)]\tTotal Loss: 0.103206\n",
      "Reconstruction: 0.098651, Regularization: 0.004555\n",
      "2019-04-10 00:05:41,211 root         INFO     ====> Epoch: 136 Average loss: 0.1127\n",
      "2019-04-10 00:05:41,235 root         INFO     Train Epoch: 137 [0/8000 (0%)]\tTotal Loss: 0.112765\n",
      "Reconstruction: 0.104411, Regularization: 0.008354\n",
      "2019-04-10 00:05:41,298 root         INFO     Train Epoch: 137 [512/8000 (6%)]\tTotal Loss: 0.106857\n",
      "Reconstruction: 0.100563, Regularization: 0.006294\n",
      "2019-04-10 00:05:41,361 root         INFO     Train Epoch: 137 [1024/8000 (13%)]\tTotal Loss: 0.106700\n",
      "Reconstruction: 0.100023, Regularization: 0.006677\n",
      "2019-04-10 00:05:41,423 root         INFO     Train Epoch: 137 [1536/8000 (19%)]\tTotal Loss: 0.124342\n",
      "Reconstruction: 0.114050, Regularization: 0.010292\n",
      "2019-04-10 00:05:41,485 root         INFO     Train Epoch: 137 [2048/8000 (26%)]\tTotal Loss: 0.117530\n",
      "Reconstruction: 0.110889, Regularization: 0.006641\n",
      "2019-04-10 00:05:41,546 root         INFO     Train Epoch: 137 [2560/8000 (32%)]\tTotal Loss: 0.125245\n",
      "Reconstruction: 0.115835, Regularization: 0.009410\n",
      "2019-04-10 00:05:41,608 root         INFO     Train Epoch: 137 [3072/8000 (38%)]\tTotal Loss: 0.113443\n",
      "Reconstruction: 0.106266, Regularization: 0.007178\n",
      "2019-04-10 00:05:41,669 root         INFO     Train Epoch: 137 [3584/8000 (45%)]\tTotal Loss: 0.113493\n",
      "Reconstruction: 0.105958, Regularization: 0.007535\n",
      "2019-04-10 00:05:41,731 root         INFO     Train Epoch: 137 [4096/8000 (51%)]\tTotal Loss: 0.111146\n",
      "Reconstruction: 0.104978, Regularization: 0.006169\n",
      "2019-04-10 00:05:41,794 root         INFO     Train Epoch: 137 [4608/8000 (58%)]\tTotal Loss: 0.105808\n",
      "Reconstruction: 0.098417, Regularization: 0.007391\n",
      "2019-04-10 00:05:41,857 root         INFO     Train Epoch: 137 [5120/8000 (64%)]\tTotal Loss: 0.106400\n",
      "Reconstruction: 0.102169, Regularization: 0.004230\n",
      "2019-04-10 00:05:41,919 root         INFO     Train Epoch: 137 [5632/8000 (70%)]\tTotal Loss: 0.120550\n",
      "Reconstruction: 0.111477, Regularization: 0.009073\n",
      "2019-04-10 00:05:41,981 root         INFO     Train Epoch: 137 [6144/8000 (77%)]\tTotal Loss: 0.108626\n",
      "Reconstruction: 0.101131, Regularization: 0.007494\n",
      "2019-04-10 00:05:42,044 root         INFO     Train Epoch: 137 [6656/8000 (83%)]\tTotal Loss: 0.112652\n",
      "Reconstruction: 0.105599, Regularization: 0.007053\n",
      "2019-04-10 00:05:42,106 root         INFO     Train Epoch: 137 [7168/8000 (90%)]\tTotal Loss: 0.128306\n",
      "Reconstruction: 0.117859, Regularization: 0.010447\n",
      "2019-04-10 00:05:42,168 root         INFO     Train Epoch: 137 [7680/8000 (96%)]\tTotal Loss: 0.102952\n",
      "Reconstruction: 0.097244, Regularization: 0.005708\n",
      "2019-04-10 00:05:42,221 root         INFO     ====> Epoch: 137 Average loss: 0.1127\n",
      "2019-04-10 00:05:42,246 root         INFO     Train Epoch: 138 [0/8000 (0%)]\tTotal Loss: 0.106686\n",
      "Reconstruction: 0.100116, Regularization: 0.006570\n",
      "2019-04-10 00:05:42,307 root         INFO     Train Epoch: 138 [512/8000 (6%)]\tTotal Loss: 0.104740\n",
      "Reconstruction: 0.097699, Regularization: 0.007041\n",
      "2019-04-10 00:05:42,369 root         INFO     Train Epoch: 138 [1024/8000 (13%)]\tTotal Loss: 0.103917\n",
      "Reconstruction: 0.097602, Regularization: 0.006315\n",
      "2019-04-10 00:05:42,431 root         INFO     Train Epoch: 138 [1536/8000 (19%)]\tTotal Loss: 0.116787\n",
      "Reconstruction: 0.108988, Regularization: 0.007799\n",
      "2019-04-10 00:05:42,493 root         INFO     Train Epoch: 138 [2048/8000 (26%)]\tTotal Loss: 0.110122\n",
      "Reconstruction: 0.101582, Regularization: 0.008540\n",
      "2019-04-10 00:05:42,555 root         INFO     Train Epoch: 138 [2560/8000 (32%)]\tTotal Loss: 0.103756\n",
      "Reconstruction: 0.098559, Regularization: 0.005197\n",
      "2019-04-10 00:05:42,617 root         INFO     Train Epoch: 138 [3072/8000 (38%)]\tTotal Loss: 0.119153\n",
      "Reconstruction: 0.110908, Regularization: 0.008245\n",
      "2019-04-10 00:05:42,679 root         INFO     Train Epoch: 138 [3584/8000 (45%)]\tTotal Loss: 0.111597\n",
      "Reconstruction: 0.105150, Regularization: 0.006447\n",
      "2019-04-10 00:05:42,741 root         INFO     Train Epoch: 138 [4096/8000 (51%)]\tTotal Loss: 0.113096\n",
      "Reconstruction: 0.105420, Regularization: 0.007676\n",
      "2019-04-10 00:05:42,803 root         INFO     Train Epoch: 138 [4608/8000 (58%)]\tTotal Loss: 0.112063\n",
      "Reconstruction: 0.102825, Regularization: 0.009238\n",
      "2019-04-10 00:05:42,865 root         INFO     Train Epoch: 138 [5120/8000 (64%)]\tTotal Loss: 0.118107\n",
      "Reconstruction: 0.108974, Regularization: 0.009133\n",
      "2019-04-10 00:05:42,927 root         INFO     Train Epoch: 138 [5632/8000 (70%)]\tTotal Loss: 0.107495\n",
      "Reconstruction: 0.099740, Regularization: 0.007755\n",
      "2019-04-10 00:05:42,989 root         INFO     Train Epoch: 138 [6144/8000 (77%)]\tTotal Loss: 0.111963\n",
      "Reconstruction: 0.103873, Regularization: 0.008090\n",
      "2019-04-10 00:05:43,052 root         INFO     Train Epoch: 138 [6656/8000 (83%)]\tTotal Loss: 0.107472\n",
      "Reconstruction: 0.100035, Regularization: 0.007436\n",
      "2019-04-10 00:05:43,113 root         INFO     Train Epoch: 138 [7168/8000 (90%)]\tTotal Loss: 0.129059\n",
      "Reconstruction: 0.117734, Regularization: 0.011325\n",
      "2019-04-10 00:05:43,175 root         INFO     Train Epoch: 138 [7680/8000 (96%)]\tTotal Loss: 0.124043\n",
      "Reconstruction: 0.115818, Regularization: 0.008224\n",
      "2019-04-10 00:05:43,228 root         INFO     ====> Epoch: 138 Average loss: 0.1131\n",
      "2019-04-10 00:05:43,251 root         INFO     Train Epoch: 139 [0/8000 (0%)]\tTotal Loss: 0.115164\n",
      "Reconstruction: 0.106715, Regularization: 0.008449\n",
      "2019-04-10 00:05:43,314 root         INFO     Train Epoch: 139 [512/8000 (6%)]\tTotal Loss: 0.110490\n",
      "Reconstruction: 0.102918, Regularization: 0.007573\n",
      "2019-04-10 00:05:43,375 root         INFO     Train Epoch: 139 [1024/8000 (13%)]\tTotal Loss: 0.108655\n",
      "Reconstruction: 0.101324, Regularization: 0.007331\n",
      "2019-04-10 00:05:43,436 root         INFO     Train Epoch: 139 [1536/8000 (19%)]\tTotal Loss: 0.112359\n",
      "Reconstruction: 0.103728, Regularization: 0.008631\n",
      "2019-04-10 00:05:43,498 root         INFO     Train Epoch: 139 [2048/8000 (26%)]\tTotal Loss: 0.114532\n",
      "Reconstruction: 0.106329, Regularization: 0.008202\n",
      "2019-04-10 00:05:43,560 root         INFO     Train Epoch: 139 [2560/8000 (32%)]\tTotal Loss: 0.106500\n",
      "Reconstruction: 0.098796, Regularization: 0.007704\n",
      "2019-04-10 00:05:43,621 root         INFO     Train Epoch: 139 [3072/8000 (38%)]\tTotal Loss: 0.100745\n",
      "Reconstruction: 0.094450, Regularization: 0.006295\n",
      "2019-04-10 00:05:43,682 root         INFO     Train Epoch: 139 [3584/8000 (45%)]\tTotal Loss: 0.106829\n",
      "Reconstruction: 0.100425, Regularization: 0.006405\n",
      "2019-04-10 00:05:43,744 root         INFO     Train Epoch: 139 [4096/8000 (51%)]\tTotal Loss: 0.113166\n",
      "Reconstruction: 0.106162, Regularization: 0.007004\n",
      "2019-04-10 00:05:43,806 root         INFO     Train Epoch: 139 [4608/8000 (58%)]\tTotal Loss: 0.122661\n",
      "Reconstruction: 0.113795, Regularization: 0.008866\n",
      "2019-04-10 00:05:43,867 root         INFO     Train Epoch: 139 [5120/8000 (64%)]\tTotal Loss: 0.106287\n",
      "Reconstruction: 0.099218, Regularization: 0.007070\n",
      "2019-04-10 00:05:43,929 root         INFO     Train Epoch: 139 [5632/8000 (70%)]\tTotal Loss: 0.129030\n",
      "Reconstruction: 0.119665, Regularization: 0.009365\n",
      "2019-04-10 00:05:43,990 root         INFO     Train Epoch: 139 [6144/8000 (77%)]\tTotal Loss: 0.117001\n",
      "Reconstruction: 0.107857, Regularization: 0.009144\n",
      "2019-04-10 00:05:44,051 root         INFO     Train Epoch: 139 [6656/8000 (83%)]\tTotal Loss: 0.122652\n",
      "Reconstruction: 0.112182, Regularization: 0.010470\n",
      "2019-04-10 00:05:44,113 root         INFO     Train Epoch: 139 [7168/8000 (90%)]\tTotal Loss: 0.111903\n",
      "Reconstruction: 0.105397, Regularization: 0.006507\n",
      "2019-04-10 00:05:44,174 root         INFO     Train Epoch: 139 [7680/8000 (96%)]\tTotal Loss: 0.111272\n",
      "Reconstruction: 0.101541, Regularization: 0.009732\n",
      "2019-04-10 00:05:44,227 root         INFO     ====> Epoch: 139 Average loss: 0.1125\n",
      "2019-04-10 00:05:44,251 root         INFO     Train Epoch: 140 [0/8000 (0%)]\tTotal Loss: 0.108194\n",
      "Reconstruction: 0.101821, Regularization: 0.006373\n",
      "2019-04-10 00:05:44,313 root         INFO     Train Epoch: 140 [512/8000 (6%)]\tTotal Loss: 0.105819\n",
      "Reconstruction: 0.098991, Regularization: 0.006828\n",
      "2019-04-10 00:05:44,374 root         INFO     Train Epoch: 140 [1024/8000 (13%)]\tTotal Loss: 0.106945\n",
      "Reconstruction: 0.099285, Regularization: 0.007660\n",
      "2019-04-10 00:05:44,436 root         INFO     Train Epoch: 140 [1536/8000 (19%)]\tTotal Loss: 0.106992\n",
      "Reconstruction: 0.099941, Regularization: 0.007051\n",
      "2019-04-10 00:05:44,498 root         INFO     Train Epoch: 140 [2048/8000 (26%)]\tTotal Loss: 0.100948\n",
      "Reconstruction: 0.094915, Regularization: 0.006033\n",
      "2019-04-10 00:05:44,560 root         INFO     Train Epoch: 140 [2560/8000 (32%)]\tTotal Loss: 0.107694\n",
      "Reconstruction: 0.101179, Regularization: 0.006516\n",
      "2019-04-10 00:05:44,623 root         INFO     Train Epoch: 140 [3072/8000 (38%)]\tTotal Loss: 0.096010\n",
      "Reconstruction: 0.088703, Regularization: 0.007307\n",
      "2019-04-10 00:05:44,685 root         INFO     Train Epoch: 140 [3584/8000 (45%)]\tTotal Loss: 0.114210\n",
      "Reconstruction: 0.105576, Regularization: 0.008634\n",
      "2019-04-10 00:05:44,750 root         INFO     Train Epoch: 140 [4096/8000 (51%)]\tTotal Loss: 0.132369\n",
      "Reconstruction: 0.122136, Regularization: 0.010233\n",
      "2019-04-10 00:05:44,815 root         INFO     Train Epoch: 140 [4608/8000 (58%)]\tTotal Loss: 0.107831\n",
      "Reconstruction: 0.100345, Regularization: 0.007486\n",
      "2019-04-10 00:05:44,879 root         INFO     Train Epoch: 140 [5120/8000 (64%)]\tTotal Loss: 0.106158\n",
      "Reconstruction: 0.100158, Regularization: 0.006000\n",
      "2019-04-10 00:05:44,943 root         INFO     Train Epoch: 140 [5632/8000 (70%)]\tTotal Loss: 0.113333\n",
      "Reconstruction: 0.105042, Regularization: 0.008291\n",
      "2019-04-10 00:05:45,005 root         INFO     Train Epoch: 140 [6144/8000 (77%)]\tTotal Loss: 0.113073\n",
      "Reconstruction: 0.105769, Regularization: 0.007304\n",
      "2019-04-10 00:05:45,066 root         INFO     Train Epoch: 140 [6656/8000 (83%)]\tTotal Loss: 0.099782\n",
      "Reconstruction: 0.092606, Regularization: 0.007176\n",
      "2019-04-10 00:05:45,128 root         INFO     Train Epoch: 140 [7168/8000 (90%)]\tTotal Loss: 0.117531\n",
      "Reconstruction: 0.109903, Regularization: 0.007628\n",
      "2019-04-10 00:05:45,189 root         INFO     Train Epoch: 140 [7680/8000 (96%)]\tTotal Loss: 0.110617\n",
      "Reconstruction: 0.104108, Regularization: 0.006509\n",
      "2019-04-10 00:05:45,243 root         INFO     ====> Epoch: 140 Average loss: 0.1124\n",
      "2019-04-10 00:05:45,267 root         INFO     Train Epoch: 141 [0/8000 (0%)]\tTotal Loss: 0.107763\n",
      "Reconstruction: 0.099779, Regularization: 0.007984\n",
      "2019-04-10 00:05:45,330 root         INFO     Train Epoch: 141 [512/8000 (6%)]\tTotal Loss: 0.109068\n",
      "Reconstruction: 0.102418, Regularization: 0.006650\n",
      "2019-04-10 00:05:45,393 root         INFO     Train Epoch: 141 [1024/8000 (13%)]\tTotal Loss: 0.107971\n",
      "Reconstruction: 0.101313, Regularization: 0.006658\n",
      "2019-04-10 00:05:45,455 root         INFO     Train Epoch: 141 [1536/8000 (19%)]\tTotal Loss: 0.113855\n",
      "Reconstruction: 0.105094, Regularization: 0.008762\n",
      "2019-04-10 00:05:45,517 root         INFO     Train Epoch: 141 [2048/8000 (26%)]\tTotal Loss: 0.125489\n",
      "Reconstruction: 0.115832, Regularization: 0.009657\n",
      "2019-04-10 00:05:45,579 root         INFO     Train Epoch: 141 [2560/8000 (32%)]\tTotal Loss: 0.115629\n",
      "Reconstruction: 0.107448, Regularization: 0.008182\n",
      "2019-04-10 00:05:45,640 root         INFO     Train Epoch: 141 [3072/8000 (38%)]\tTotal Loss: 0.101767\n",
      "Reconstruction: 0.096124, Regularization: 0.005643\n",
      "2019-04-10 00:05:45,703 root         INFO     Train Epoch: 141 [3584/8000 (45%)]\tTotal Loss: 0.103887\n",
      "Reconstruction: 0.097157, Regularization: 0.006730\n",
      "2019-04-10 00:05:45,766 root         INFO     Train Epoch: 141 [4096/8000 (51%)]\tTotal Loss: 0.116480\n",
      "Reconstruction: 0.109248, Regularization: 0.007232\n",
      "2019-04-10 00:05:45,828 root         INFO     Train Epoch: 141 [4608/8000 (58%)]\tTotal Loss: 0.121987\n",
      "Reconstruction: 0.112548, Regularization: 0.009439\n",
      "2019-04-10 00:05:45,890 root         INFO     Train Epoch: 141 [5120/8000 (64%)]\tTotal Loss: 0.113840\n",
      "Reconstruction: 0.107501, Regularization: 0.006340\n",
      "2019-04-10 00:05:45,952 root         INFO     Train Epoch: 141 [5632/8000 (70%)]\tTotal Loss: 0.108434\n",
      "Reconstruction: 0.101177, Regularization: 0.007257\n",
      "2019-04-10 00:05:46,014 root         INFO     Train Epoch: 141 [6144/8000 (77%)]\tTotal Loss: 0.109906\n",
      "Reconstruction: 0.104046, Regularization: 0.005860\n",
      "2019-04-10 00:05:46,077 root         INFO     Train Epoch: 141 [6656/8000 (83%)]\tTotal Loss: 0.112218\n",
      "Reconstruction: 0.106569, Regularization: 0.005649\n",
      "2019-04-10 00:05:46,139 root         INFO     Train Epoch: 141 [7168/8000 (90%)]\tTotal Loss: 0.102834\n",
      "Reconstruction: 0.095985, Regularization: 0.006850\n",
      "2019-04-10 00:05:46,201 root         INFO     Train Epoch: 141 [7680/8000 (96%)]\tTotal Loss: 0.122372\n",
      "Reconstruction: 0.112299, Regularization: 0.010073\n",
      "2019-04-10 00:05:46,255 root         INFO     ====> Epoch: 141 Average loss: 0.1129\n",
      "2019-04-10 00:05:46,279 root         INFO     Train Epoch: 142 [0/8000 (0%)]\tTotal Loss: 0.106662\n",
      "Reconstruction: 0.101275, Regularization: 0.005387\n",
      "2019-04-10 00:05:46,342 root         INFO     Train Epoch: 142 [512/8000 (6%)]\tTotal Loss: 0.112053\n",
      "Reconstruction: 0.104936, Regularization: 0.007117\n",
      "2019-04-10 00:05:46,405 root         INFO     Train Epoch: 142 [1024/8000 (13%)]\tTotal Loss: 0.104825\n",
      "Reconstruction: 0.098180, Regularization: 0.006645\n",
      "2019-04-10 00:05:46,468 root         INFO     Train Epoch: 142 [1536/8000 (19%)]\tTotal Loss: 0.116693\n",
      "Reconstruction: 0.108906, Regularization: 0.007787\n",
      "2019-04-10 00:05:46,531 root         INFO     Train Epoch: 142 [2048/8000 (26%)]\tTotal Loss: 0.120021\n",
      "Reconstruction: 0.110563, Regularization: 0.009458\n",
      "2019-04-10 00:05:46,594 root         INFO     Train Epoch: 142 [2560/8000 (32%)]\tTotal Loss: 0.108667\n",
      "Reconstruction: 0.101836, Regularization: 0.006831\n",
      "2019-04-10 00:05:46,658 root         INFO     Train Epoch: 142 [3072/8000 (38%)]\tTotal Loss: 0.118682\n",
      "Reconstruction: 0.109117, Regularization: 0.009564\n",
      "2019-04-10 00:05:46,721 root         INFO     Train Epoch: 142 [3584/8000 (45%)]\tTotal Loss: 0.110536\n",
      "Reconstruction: 0.102700, Regularization: 0.007836\n",
      "2019-04-10 00:05:46,785 root         INFO     Train Epoch: 142 [4096/8000 (51%)]\tTotal Loss: 0.107272\n",
      "Reconstruction: 0.099413, Regularization: 0.007859\n",
      "2019-04-10 00:05:46,848 root         INFO     Train Epoch: 142 [4608/8000 (58%)]\tTotal Loss: 0.118742\n",
      "Reconstruction: 0.109222, Regularization: 0.009520\n",
      "2019-04-10 00:05:46,911 root         INFO     Train Epoch: 142 [5120/8000 (64%)]\tTotal Loss: 0.110513\n",
      "Reconstruction: 0.102655, Regularization: 0.007858\n",
      "2019-04-10 00:05:46,975 root         INFO     Train Epoch: 142 [5632/8000 (70%)]\tTotal Loss: 0.104614\n",
      "Reconstruction: 0.097733, Regularization: 0.006881\n",
      "2019-04-10 00:05:47,038 root         INFO     Train Epoch: 142 [6144/8000 (77%)]\tTotal Loss: 0.110890\n",
      "Reconstruction: 0.103414, Regularization: 0.007476\n",
      "2019-04-10 00:05:47,101 root         INFO     Train Epoch: 142 [6656/8000 (83%)]\tTotal Loss: 0.106084\n",
      "Reconstruction: 0.099040, Regularization: 0.007044\n",
      "2019-04-10 00:05:47,162 root         INFO     Train Epoch: 142 [7168/8000 (90%)]\tTotal Loss: 0.111610\n",
      "Reconstruction: 0.104051, Regularization: 0.007559\n",
      "2019-04-10 00:05:47,224 root         INFO     Train Epoch: 142 [7680/8000 (96%)]\tTotal Loss: 0.110026\n",
      "Reconstruction: 0.102891, Regularization: 0.007135\n",
      "2019-04-10 00:05:47,277 root         INFO     ====> Epoch: 142 Average loss: 0.1124\n",
      "2019-04-10 00:05:47,301 root         INFO     Train Epoch: 143 [0/8000 (0%)]\tTotal Loss: 0.108808\n",
      "Reconstruction: 0.102896, Regularization: 0.005912\n",
      "2019-04-10 00:05:47,366 root         INFO     Train Epoch: 143 [512/8000 (6%)]\tTotal Loss: 0.119168\n",
      "Reconstruction: 0.110205, Regularization: 0.008963\n",
      "2019-04-10 00:05:47,430 root         INFO     Train Epoch: 143 [1024/8000 (13%)]\tTotal Loss: 0.110601\n",
      "Reconstruction: 0.104531, Regularization: 0.006070\n",
      "2019-04-10 00:05:47,494 root         INFO     Train Epoch: 143 [1536/8000 (19%)]\tTotal Loss: 0.121301\n",
      "Reconstruction: 0.113073, Regularization: 0.008228\n",
      "2019-04-10 00:05:47,558 root         INFO     Train Epoch: 143 [2048/8000 (26%)]\tTotal Loss: 0.102968\n",
      "Reconstruction: 0.095788, Regularization: 0.007180\n",
      "2019-04-10 00:05:47,622 root         INFO     Train Epoch: 143 [2560/8000 (32%)]\tTotal Loss: 0.116932\n",
      "Reconstruction: 0.107377, Regularization: 0.009555\n",
      "2019-04-10 00:05:47,686 root         INFO     Train Epoch: 143 [3072/8000 (38%)]\tTotal Loss: 0.103653\n",
      "Reconstruction: 0.095020, Regularization: 0.008633\n",
      "2019-04-10 00:05:47,749 root         INFO     Train Epoch: 143 [3584/8000 (45%)]\tTotal Loss: 0.111896\n",
      "Reconstruction: 0.104740, Regularization: 0.007156\n",
      "2019-04-10 00:05:47,812 root         INFO     Train Epoch: 143 [4096/8000 (51%)]\tTotal Loss: 0.104872\n",
      "Reconstruction: 0.098086, Regularization: 0.006786\n",
      "2019-04-10 00:05:47,875 root         INFO     Train Epoch: 143 [4608/8000 (58%)]\tTotal Loss: 0.115353\n",
      "Reconstruction: 0.106275, Regularization: 0.009078\n",
      "2019-04-10 00:05:47,939 root         INFO     Train Epoch: 143 [5120/8000 (64%)]\tTotal Loss: 0.112681\n",
      "Reconstruction: 0.103675, Regularization: 0.009007\n",
      "2019-04-10 00:05:48,002 root         INFO     Train Epoch: 143 [5632/8000 (70%)]\tTotal Loss: 0.115520\n",
      "Reconstruction: 0.107410, Regularization: 0.008110\n",
      "2019-04-10 00:05:48,066 root         INFO     Train Epoch: 143 [6144/8000 (77%)]\tTotal Loss: 0.110737\n",
      "Reconstruction: 0.101349, Regularization: 0.009388\n",
      "2019-04-10 00:05:48,129 root         INFO     Train Epoch: 143 [6656/8000 (83%)]\tTotal Loss: 0.110398\n",
      "Reconstruction: 0.102538, Regularization: 0.007859\n",
      "2019-04-10 00:05:48,192 root         INFO     Train Epoch: 143 [7168/8000 (90%)]\tTotal Loss: 0.106959\n",
      "Reconstruction: 0.100174, Regularization: 0.006785\n",
      "2019-04-10 00:05:48,256 root         INFO     Train Epoch: 143 [7680/8000 (96%)]\tTotal Loss: 0.112532\n",
      "Reconstruction: 0.105128, Regularization: 0.007405\n",
      "2019-04-10 00:05:48,310 root         INFO     ====> Epoch: 143 Average loss: 0.1130\n",
      "2019-04-10 00:05:48,334 root         INFO     Train Epoch: 144 [0/8000 (0%)]\tTotal Loss: 0.111464\n",
      "Reconstruction: 0.104574, Regularization: 0.006890\n",
      "2019-04-10 00:05:48,397 root         INFO     Train Epoch: 144 [512/8000 (6%)]\tTotal Loss: 0.115781\n",
      "Reconstruction: 0.105968, Regularization: 0.009813\n",
      "2019-04-10 00:05:48,459 root         INFO     Train Epoch: 144 [1024/8000 (13%)]\tTotal Loss: 0.117162\n",
      "Reconstruction: 0.108549, Regularization: 0.008613\n",
      "2019-04-10 00:05:48,521 root         INFO     Train Epoch: 144 [1536/8000 (19%)]\tTotal Loss: 0.124513\n",
      "Reconstruction: 0.114049, Regularization: 0.010464\n",
      "2019-04-10 00:05:48,584 root         INFO     Train Epoch: 144 [2048/8000 (26%)]\tTotal Loss: 0.102152\n",
      "Reconstruction: 0.096628, Regularization: 0.005525\n",
      "2019-04-10 00:05:48,648 root         INFO     Train Epoch: 144 [2560/8000 (32%)]\tTotal Loss: 0.107673\n",
      "Reconstruction: 0.100147, Regularization: 0.007526\n",
      "2019-04-10 00:05:48,710 root         INFO     Train Epoch: 144 [3072/8000 (38%)]\tTotal Loss: 0.116667\n",
      "Reconstruction: 0.106661, Regularization: 0.010006\n",
      "2019-04-10 00:05:48,773 root         INFO     Train Epoch: 144 [3584/8000 (45%)]\tTotal Loss: 0.118133\n",
      "Reconstruction: 0.109330, Regularization: 0.008803\n",
      "2019-04-10 00:05:48,835 root         INFO     Train Epoch: 144 [4096/8000 (51%)]\tTotal Loss: 0.115375\n",
      "Reconstruction: 0.107618, Regularization: 0.007757\n",
      "2019-04-10 00:05:48,897 root         INFO     Train Epoch: 144 [4608/8000 (58%)]\tTotal Loss: 0.109945\n",
      "Reconstruction: 0.101302, Regularization: 0.008642\n",
      "2019-04-10 00:05:48,959 root         INFO     Train Epoch: 144 [5120/8000 (64%)]\tTotal Loss: 0.114986\n",
      "Reconstruction: 0.106260, Regularization: 0.008726\n",
      "2019-04-10 00:05:49,021 root         INFO     Train Epoch: 144 [5632/8000 (70%)]\tTotal Loss: 0.111476\n",
      "Reconstruction: 0.103050, Regularization: 0.008426\n",
      "2019-04-10 00:05:49,083 root         INFO     Train Epoch: 144 [6144/8000 (77%)]\tTotal Loss: 0.111663\n",
      "Reconstruction: 0.103161, Regularization: 0.008502\n",
      "2019-04-10 00:05:49,145 root         INFO     Train Epoch: 144 [6656/8000 (83%)]\tTotal Loss: 0.104174\n",
      "Reconstruction: 0.095450, Regularization: 0.008724\n",
      "2019-04-10 00:05:49,208 root         INFO     Train Epoch: 144 [7168/8000 (90%)]\tTotal Loss: 0.123252\n",
      "Reconstruction: 0.114773, Regularization: 0.008479\n",
      "2019-04-10 00:05:49,270 root         INFO     Train Epoch: 144 [7680/8000 (96%)]\tTotal Loss: 0.110380\n",
      "Reconstruction: 0.102776, Regularization: 0.007604\n",
      "2019-04-10 00:05:49,324 root         INFO     ====> Epoch: 144 Average loss: 0.1124\n",
      "2019-04-10 00:05:49,348 root         INFO     Train Epoch: 145 [0/8000 (0%)]\tTotal Loss: 0.109469\n",
      "Reconstruction: 0.101371, Regularization: 0.008098\n",
      "2019-04-10 00:05:49,411 root         INFO     Train Epoch: 145 [512/8000 (6%)]\tTotal Loss: 0.114458\n",
      "Reconstruction: 0.106300, Regularization: 0.008158\n",
      "2019-04-10 00:05:49,474 root         INFO     Train Epoch: 145 [1024/8000 (13%)]\tTotal Loss: 0.111856\n",
      "Reconstruction: 0.102744, Regularization: 0.009112\n",
      "2019-04-10 00:05:49,537 root         INFO     Train Epoch: 145 [1536/8000 (19%)]\tTotal Loss: 0.108646\n",
      "Reconstruction: 0.100268, Regularization: 0.008378\n",
      "2019-04-10 00:05:49,600 root         INFO     Train Epoch: 145 [2048/8000 (26%)]\tTotal Loss: 0.109395\n",
      "Reconstruction: 0.102055, Regularization: 0.007340\n",
      "2019-04-10 00:05:49,662 root         INFO     Train Epoch: 145 [2560/8000 (32%)]\tTotal Loss: 0.113997\n",
      "Reconstruction: 0.105229, Regularization: 0.008768\n",
      "2019-04-10 00:05:49,724 root         INFO     Train Epoch: 145 [3072/8000 (38%)]\tTotal Loss: 0.103628\n",
      "Reconstruction: 0.096390, Regularization: 0.007238\n",
      "2019-04-10 00:05:49,786 root         INFO     Train Epoch: 145 [3584/8000 (45%)]\tTotal Loss: 0.106710\n",
      "Reconstruction: 0.099207, Regularization: 0.007503\n",
      "2019-04-10 00:05:49,849 root         INFO     Train Epoch: 145 [4096/8000 (51%)]\tTotal Loss: 0.115894\n",
      "Reconstruction: 0.106048, Regularization: 0.009846\n",
      "2019-04-10 00:05:49,911 root         INFO     Train Epoch: 145 [4608/8000 (58%)]\tTotal Loss: 0.105542\n",
      "Reconstruction: 0.098597, Regularization: 0.006945\n",
      "2019-04-10 00:05:49,973 root         INFO     Train Epoch: 145 [5120/8000 (64%)]\tTotal Loss: 0.113672\n",
      "Reconstruction: 0.104471, Regularization: 0.009202\n",
      "2019-04-10 00:05:50,036 root         INFO     Train Epoch: 145 [5632/8000 (70%)]\tTotal Loss: 0.113670\n",
      "Reconstruction: 0.105825, Regularization: 0.007845\n",
      "2019-04-10 00:05:50,098 root         INFO     Train Epoch: 145 [6144/8000 (77%)]\tTotal Loss: 0.108132\n",
      "Reconstruction: 0.099744, Regularization: 0.008388\n",
      "2019-04-10 00:05:50,159 root         INFO     Train Epoch: 145 [6656/8000 (83%)]\tTotal Loss: 0.113677\n",
      "Reconstruction: 0.106219, Regularization: 0.007457\n",
      "2019-04-10 00:05:50,221 root         INFO     Train Epoch: 145 [7168/8000 (90%)]\tTotal Loss: 0.114483\n",
      "Reconstruction: 0.105538, Regularization: 0.008946\n",
      "2019-04-10 00:05:50,284 root         INFO     Train Epoch: 145 [7680/8000 (96%)]\tTotal Loss: 0.098778\n",
      "Reconstruction: 0.092758, Regularization: 0.006020\n",
      "2019-04-10 00:05:50,337 root         INFO     ====> Epoch: 145 Average loss: 0.1124\n",
      "2019-04-10 00:05:50,361 root         INFO     Train Epoch: 146 [0/8000 (0%)]\tTotal Loss: 0.106767\n",
      "Reconstruction: 0.099735, Regularization: 0.007033\n",
      "2019-04-10 00:05:50,424 root         INFO     Train Epoch: 146 [512/8000 (6%)]\tTotal Loss: 0.107036\n",
      "Reconstruction: 0.099221, Regularization: 0.007815\n",
      "2019-04-10 00:05:50,488 root         INFO     Train Epoch: 146 [1024/8000 (13%)]\tTotal Loss: 0.112145\n",
      "Reconstruction: 0.103716, Regularization: 0.008428\n",
      "2019-04-10 00:05:50,552 root         INFO     Train Epoch: 146 [1536/8000 (19%)]\tTotal Loss: 0.098909\n",
      "Reconstruction: 0.093028, Regularization: 0.005881\n",
      "2019-04-10 00:05:50,615 root         INFO     Train Epoch: 146 [2048/8000 (26%)]\tTotal Loss: 0.112466\n",
      "Reconstruction: 0.105359, Regularization: 0.007108\n",
      "2019-04-10 00:05:50,677 root         INFO     Train Epoch: 146 [2560/8000 (32%)]\tTotal Loss: 0.116383\n",
      "Reconstruction: 0.106855, Regularization: 0.009529\n",
      "2019-04-10 00:05:50,738 root         INFO     Train Epoch: 146 [3072/8000 (38%)]\tTotal Loss: 0.109177\n",
      "Reconstruction: 0.101255, Regularization: 0.007923\n",
      "2019-04-10 00:05:50,801 root         INFO     Train Epoch: 146 [3584/8000 (45%)]\tTotal Loss: 0.107445\n",
      "Reconstruction: 0.099309, Regularization: 0.008136\n",
      "2019-04-10 00:05:50,863 root         INFO     Train Epoch: 146 [4096/8000 (51%)]\tTotal Loss: 0.112256\n",
      "Reconstruction: 0.104963, Regularization: 0.007293\n",
      "2019-04-10 00:05:50,925 root         INFO     Train Epoch: 146 [4608/8000 (58%)]\tTotal Loss: 0.111797\n",
      "Reconstruction: 0.102065, Regularization: 0.009732\n",
      "2019-04-10 00:05:50,988 root         INFO     Train Epoch: 146 [5120/8000 (64%)]\tTotal Loss: 0.115193\n",
      "Reconstruction: 0.107754, Regularization: 0.007439\n",
      "2019-04-10 00:05:51,050 root         INFO     Train Epoch: 146 [5632/8000 (70%)]\tTotal Loss: 0.124315\n",
      "Reconstruction: 0.113897, Regularization: 0.010418\n",
      "2019-04-10 00:05:51,112 root         INFO     Train Epoch: 146 [6144/8000 (77%)]\tTotal Loss: 0.115551\n",
      "Reconstruction: 0.106674, Regularization: 0.008877\n",
      "2019-04-10 00:05:51,174 root         INFO     Train Epoch: 146 [6656/8000 (83%)]\tTotal Loss: 0.115349\n",
      "Reconstruction: 0.107504, Regularization: 0.007846\n",
      "2019-04-10 00:05:51,236 root         INFO     Train Epoch: 146 [7168/8000 (90%)]\tTotal Loss: 0.122543\n",
      "Reconstruction: 0.112085, Regularization: 0.010458\n",
      "2019-04-10 00:05:51,299 root         INFO     Train Epoch: 146 [7680/8000 (96%)]\tTotal Loss: 0.120402\n",
      "Reconstruction: 0.111012, Regularization: 0.009390\n",
      "2019-04-10 00:05:51,352 root         INFO     ====> Epoch: 146 Average loss: 0.1128\n",
      "2019-04-10 00:05:51,377 root         INFO     Train Epoch: 147 [0/8000 (0%)]\tTotal Loss: 0.101517\n",
      "Reconstruction: 0.095688, Regularization: 0.005828\n",
      "2019-04-10 00:05:51,442 root         INFO     Train Epoch: 147 [512/8000 (6%)]\tTotal Loss: 0.122992\n",
      "Reconstruction: 0.111653, Regularization: 0.011339\n",
      "2019-04-10 00:05:51,505 root         INFO     Train Epoch: 147 [1024/8000 (13%)]\tTotal Loss: 0.111408\n",
      "Reconstruction: 0.102953, Regularization: 0.008455\n",
      "2019-04-10 00:05:51,569 root         INFO     Train Epoch: 147 [1536/8000 (19%)]\tTotal Loss: 0.119050\n",
      "Reconstruction: 0.107498, Regularization: 0.011552\n",
      "2019-04-10 00:05:51,633 root         INFO     Train Epoch: 147 [2048/8000 (26%)]\tTotal Loss: 0.128402\n",
      "Reconstruction: 0.116419, Regularization: 0.011983\n",
      "2019-04-10 00:05:51,697 root         INFO     Train Epoch: 147 [2560/8000 (32%)]\tTotal Loss: 0.104814\n",
      "Reconstruction: 0.097853, Regularization: 0.006961\n",
      "2019-04-10 00:05:51,761 root         INFO     Train Epoch: 147 [3072/8000 (38%)]\tTotal Loss: 0.112660\n",
      "Reconstruction: 0.104107, Regularization: 0.008553\n",
      "2019-04-10 00:05:51,825 root         INFO     Train Epoch: 147 [3584/8000 (45%)]\tTotal Loss: 0.106528\n",
      "Reconstruction: 0.098791, Regularization: 0.007737\n",
      "2019-04-10 00:05:51,888 root         INFO     Train Epoch: 147 [4096/8000 (51%)]\tTotal Loss: 0.108473\n",
      "Reconstruction: 0.100711, Regularization: 0.007763\n",
      "2019-04-10 00:05:51,952 root         INFO     Train Epoch: 147 [4608/8000 (58%)]\tTotal Loss: 0.110974\n",
      "Reconstruction: 0.102644, Regularization: 0.008330\n",
      "2019-04-10 00:05:52,015 root         INFO     Train Epoch: 147 [5120/8000 (64%)]\tTotal Loss: 0.126315\n",
      "Reconstruction: 0.117286, Regularization: 0.009029\n",
      "2019-04-10 00:05:52,078 root         INFO     Train Epoch: 147 [5632/8000 (70%)]\tTotal Loss: 0.112354\n",
      "Reconstruction: 0.103072, Regularization: 0.009282\n",
      "2019-04-10 00:05:52,142 root         INFO     Train Epoch: 147 [6144/8000 (77%)]\tTotal Loss: 0.110518\n",
      "Reconstruction: 0.101181, Regularization: 0.009338\n",
      "2019-04-10 00:05:52,205 root         INFO     Train Epoch: 147 [6656/8000 (83%)]\tTotal Loss: 0.106102\n",
      "Reconstruction: 0.097552, Regularization: 0.008550\n",
      "2019-04-10 00:05:52,269 root         INFO     Train Epoch: 147 [7168/8000 (90%)]\tTotal Loss: 0.107623\n",
      "Reconstruction: 0.101830, Regularization: 0.005793\n",
      "2019-04-10 00:05:52,333 root         INFO     Train Epoch: 147 [7680/8000 (96%)]\tTotal Loss: 0.117922\n",
      "Reconstruction: 0.108443, Regularization: 0.009479\n",
      "2019-04-10 00:05:52,388 root         INFO     ====> Epoch: 147 Average loss: 0.1124\n",
      "2019-04-10 00:05:52,412 root         INFO     Train Epoch: 148 [0/8000 (0%)]\tTotal Loss: 0.102970\n",
      "Reconstruction: 0.095620, Regularization: 0.007350\n",
      "2019-04-10 00:05:52,475 root         INFO     Train Epoch: 148 [512/8000 (6%)]\tTotal Loss: 0.107447\n",
      "Reconstruction: 0.099298, Regularization: 0.008149\n",
      "2019-04-10 00:05:52,538 root         INFO     Train Epoch: 148 [1024/8000 (13%)]\tTotal Loss: 0.109859\n",
      "Reconstruction: 0.103481, Regularization: 0.006378\n",
      "2019-04-10 00:05:52,602 root         INFO     Train Epoch: 148 [1536/8000 (19%)]\tTotal Loss: 0.102492\n",
      "Reconstruction: 0.095636, Regularization: 0.006856\n",
      "2019-04-10 00:05:52,666 root         INFO     Train Epoch: 148 [2048/8000 (26%)]\tTotal Loss: 0.123143\n",
      "Reconstruction: 0.113093, Regularization: 0.010051\n",
      "2019-04-10 00:05:52,730 root         INFO     Train Epoch: 148 [2560/8000 (32%)]\tTotal Loss: 0.106818\n",
      "Reconstruction: 0.098577, Regularization: 0.008241\n",
      "2019-04-10 00:05:52,794 root         INFO     Train Epoch: 148 [3072/8000 (38%)]\tTotal Loss: 0.119640\n",
      "Reconstruction: 0.108788, Regularization: 0.010853\n",
      "2019-04-10 00:05:52,859 root         INFO     Train Epoch: 148 [3584/8000 (45%)]\tTotal Loss: 0.100247\n",
      "Reconstruction: 0.092286, Regularization: 0.007961\n",
      "2019-04-10 00:05:52,923 root         INFO     Train Epoch: 148 [4096/8000 (51%)]\tTotal Loss: 0.116863\n",
      "Reconstruction: 0.107281, Regularization: 0.009582\n",
      "2019-04-10 00:05:52,987 root         INFO     Train Epoch: 148 [4608/8000 (58%)]\tTotal Loss: 0.125587\n",
      "Reconstruction: 0.115524, Regularization: 0.010064\n",
      "2019-04-10 00:05:53,051 root         INFO     Train Epoch: 148 [5120/8000 (64%)]\tTotal Loss: 0.115282\n",
      "Reconstruction: 0.107527, Regularization: 0.007754\n",
      "2019-04-10 00:05:53,115 root         INFO     Train Epoch: 148 [5632/8000 (70%)]\tTotal Loss: 0.111219\n",
      "Reconstruction: 0.104615, Regularization: 0.006604\n",
      "2019-04-10 00:05:53,178 root         INFO     Train Epoch: 148 [6144/8000 (77%)]\tTotal Loss: 0.108588\n",
      "Reconstruction: 0.102132, Regularization: 0.006455\n",
      "2019-04-10 00:05:53,242 root         INFO     Train Epoch: 148 [6656/8000 (83%)]\tTotal Loss: 0.137213\n",
      "Reconstruction: 0.124152, Regularization: 0.013061\n",
      "2019-04-10 00:05:53,306 root         INFO     Train Epoch: 148 [7168/8000 (90%)]\tTotal Loss: 0.108215\n",
      "Reconstruction: 0.100258, Regularization: 0.007956\n",
      "2019-04-10 00:05:53,370 root         INFO     Train Epoch: 148 [7680/8000 (96%)]\tTotal Loss: 0.102381\n",
      "Reconstruction: 0.094573, Regularization: 0.007809\n",
      "2019-04-10 00:05:53,426 root         INFO     ====> Epoch: 148 Average loss: 0.1124\n",
      "2019-04-10 00:05:53,450 root         INFO     Train Epoch: 149 [0/8000 (0%)]\tTotal Loss: 0.125298\n",
      "Reconstruction: 0.116114, Regularization: 0.009184\n",
      "2019-04-10 00:05:53,514 root         INFO     Train Epoch: 149 [512/8000 (6%)]\tTotal Loss: 0.114093\n",
      "Reconstruction: 0.105351, Regularization: 0.008742\n",
      "2019-04-10 00:05:53,578 root         INFO     Train Epoch: 149 [1024/8000 (13%)]\tTotal Loss: 0.109425\n",
      "Reconstruction: 0.098663, Regularization: 0.010762\n",
      "2019-04-10 00:05:53,641 root         INFO     Train Epoch: 149 [1536/8000 (19%)]\tTotal Loss: 0.106161\n",
      "Reconstruction: 0.097233, Regularization: 0.008928\n",
      "2019-04-10 00:05:53,705 root         INFO     Train Epoch: 149 [2048/8000 (26%)]\tTotal Loss: 0.108802\n",
      "Reconstruction: 0.101174, Regularization: 0.007628\n",
      "2019-04-10 00:05:53,768 root         INFO     Train Epoch: 149 [2560/8000 (32%)]\tTotal Loss: 0.107648\n",
      "Reconstruction: 0.098778, Regularization: 0.008870\n",
      "2019-04-10 00:05:53,832 root         INFO     Train Epoch: 149 [3072/8000 (38%)]\tTotal Loss: 0.121523\n",
      "Reconstruction: 0.112126, Regularization: 0.009397\n",
      "2019-04-10 00:05:53,896 root         INFO     Train Epoch: 149 [3584/8000 (45%)]\tTotal Loss: 0.115296\n",
      "Reconstruction: 0.107931, Regularization: 0.007365\n",
      "2019-04-10 00:05:53,959 root         INFO     Train Epoch: 149 [4096/8000 (51%)]\tTotal Loss: 0.118652\n",
      "Reconstruction: 0.109886, Regularization: 0.008766\n",
      "2019-04-10 00:05:54,021 root         INFO     Train Epoch: 149 [4608/8000 (58%)]\tTotal Loss: 0.120157\n",
      "Reconstruction: 0.108196, Regularization: 0.011961\n",
      "2019-04-10 00:05:54,083 root         INFO     Train Epoch: 149 [5120/8000 (64%)]\tTotal Loss: 0.114037\n",
      "Reconstruction: 0.104259, Regularization: 0.009778\n",
      "2019-04-10 00:05:54,146 root         INFO     Train Epoch: 149 [5632/8000 (70%)]\tTotal Loss: 0.109675\n",
      "Reconstruction: 0.102011, Regularization: 0.007664\n",
      "2019-04-10 00:05:54,208 root         INFO     Train Epoch: 149 [6144/8000 (77%)]\tTotal Loss: 0.114267\n",
      "Reconstruction: 0.104171, Regularization: 0.010096\n",
      "2019-04-10 00:05:54,270 root         INFO     Train Epoch: 149 [6656/8000 (83%)]\tTotal Loss: 0.103271\n",
      "Reconstruction: 0.096805, Regularization: 0.006466\n",
      "2019-04-10 00:05:54,332 root         INFO     Train Epoch: 149 [7168/8000 (90%)]\tTotal Loss: 0.115583\n",
      "Reconstruction: 0.106631, Regularization: 0.008953\n",
      "2019-04-10 00:05:54,394 root         INFO     Train Epoch: 149 [7680/8000 (96%)]\tTotal Loss: 0.113927\n",
      "Reconstruction: 0.104180, Regularization: 0.009747\n",
      "2019-04-10 00:05:54,447 root         INFO     ====> Epoch: 149 Average loss: 0.1126\n",
      "2019-04-10 00:05:54,471 root         INFO     Train Epoch: 150 [0/8000 (0%)]\tTotal Loss: 0.128458\n",
      "Reconstruction: 0.118099, Regularization: 0.010360\n",
      "2019-04-10 00:05:54,536 root         INFO     Train Epoch: 150 [512/8000 (6%)]\tTotal Loss: 0.121712\n",
      "Reconstruction: 0.110312, Regularization: 0.011400\n",
      "2019-04-10 00:05:54,599 root         INFO     Train Epoch: 150 [1024/8000 (13%)]\tTotal Loss: 0.108695\n",
      "Reconstruction: 0.101388, Regularization: 0.007307\n",
      "2019-04-10 00:05:54,663 root         INFO     Train Epoch: 150 [1536/8000 (19%)]\tTotal Loss: 0.107366\n",
      "Reconstruction: 0.098444, Regularization: 0.008922\n",
      "2019-04-10 00:05:54,726 root         INFO     Train Epoch: 150 [2048/8000 (26%)]\tTotal Loss: 0.104082\n",
      "Reconstruction: 0.098162, Regularization: 0.005919\n",
      "2019-04-10 00:05:54,790 root         INFO     Train Epoch: 150 [2560/8000 (32%)]\tTotal Loss: 0.108144\n",
      "Reconstruction: 0.102636, Regularization: 0.005508\n",
      "2019-04-10 00:05:54,853 root         INFO     Train Epoch: 150 [3072/8000 (38%)]\tTotal Loss: 0.101352\n",
      "Reconstruction: 0.093431, Regularization: 0.007921\n",
      "2019-04-10 00:05:54,916 root         INFO     Train Epoch: 150 [3584/8000 (45%)]\tTotal Loss: 0.130349\n",
      "Reconstruction: 0.118884, Regularization: 0.011465\n",
      "2019-04-10 00:05:54,980 root         INFO     Train Epoch: 150 [4096/8000 (51%)]\tTotal Loss: 0.110442\n",
      "Reconstruction: 0.102869, Regularization: 0.007573\n",
      "2019-04-10 00:05:55,044 root         INFO     Train Epoch: 150 [4608/8000 (58%)]\tTotal Loss: 0.108627\n",
      "Reconstruction: 0.101524, Regularization: 0.007103\n",
      "2019-04-10 00:05:55,108 root         INFO     Train Epoch: 150 [5120/8000 (64%)]\tTotal Loss: 0.110448\n",
      "Reconstruction: 0.102069, Regularization: 0.008378\n",
      "2019-04-10 00:05:55,171 root         INFO     Train Epoch: 150 [5632/8000 (70%)]\tTotal Loss: 0.099086\n",
      "Reconstruction: 0.090984, Regularization: 0.008102\n",
      "2019-04-10 00:05:55,235 root         INFO     Train Epoch: 150 [6144/8000 (77%)]\tTotal Loss: 0.114078\n",
      "Reconstruction: 0.105292, Regularization: 0.008785\n",
      "2019-04-10 00:05:55,299 root         INFO     Train Epoch: 150 [6656/8000 (83%)]\tTotal Loss: 0.109310\n",
      "Reconstruction: 0.100386, Regularization: 0.008924\n",
      "2019-04-10 00:05:55,363 root         INFO     Train Epoch: 150 [7168/8000 (90%)]\tTotal Loss: 0.115879\n",
      "Reconstruction: 0.106075, Regularization: 0.009804\n",
      "2019-04-10 00:05:55,426 root         INFO     Train Epoch: 150 [7680/8000 (96%)]\tTotal Loss: 0.124845\n",
      "Reconstruction: 0.115302, Regularization: 0.009543\n",
      "2019-04-10 00:05:55,480 root         INFO     ====> Epoch: 150 Average loss: 0.1125\n",
      "2019-04-10 00:05:55,504 root         INFO     Train Epoch: 151 [0/8000 (0%)]\tTotal Loss: 0.119753\n",
      "Reconstruction: 0.112236, Regularization: 0.007517\n",
      "2019-04-10 00:05:55,569 root         INFO     Train Epoch: 151 [512/8000 (6%)]\tTotal Loss: 0.104740\n",
      "Reconstruction: 0.098859, Regularization: 0.005881\n",
      "2019-04-10 00:05:55,633 root         INFO     Train Epoch: 151 [1024/8000 (13%)]\tTotal Loss: 0.117414\n",
      "Reconstruction: 0.107996, Regularization: 0.009417\n",
      "2019-04-10 00:05:55,698 root         INFO     Train Epoch: 151 [1536/8000 (19%)]\tTotal Loss: 0.117573\n",
      "Reconstruction: 0.107730, Regularization: 0.009843\n",
      "2019-04-10 00:05:55,763 root         INFO     Train Epoch: 151 [2048/8000 (26%)]\tTotal Loss: 0.116631\n",
      "Reconstruction: 0.107883, Regularization: 0.008748\n",
      "2019-04-10 00:05:55,827 root         INFO     Train Epoch: 151 [2560/8000 (32%)]\tTotal Loss: 0.108970\n",
      "Reconstruction: 0.101912, Regularization: 0.007058\n",
      "2019-04-10 00:05:55,891 root         INFO     Train Epoch: 151 [3072/8000 (38%)]\tTotal Loss: 0.111586\n",
      "Reconstruction: 0.102489, Regularization: 0.009097\n",
      "2019-04-10 00:05:55,955 root         INFO     Train Epoch: 151 [3584/8000 (45%)]\tTotal Loss: 0.112228\n",
      "Reconstruction: 0.104442, Regularization: 0.007786\n",
      "2019-04-10 00:05:56,019 root         INFO     Train Epoch: 151 [4096/8000 (51%)]\tTotal Loss: 0.120832\n",
      "Reconstruction: 0.108831, Regularization: 0.012001\n",
      "2019-04-10 00:05:56,083 root         INFO     Train Epoch: 151 [4608/8000 (58%)]\tTotal Loss: 0.121562\n",
      "Reconstruction: 0.111243, Regularization: 0.010319\n",
      "2019-04-10 00:05:56,147 root         INFO     Train Epoch: 151 [5120/8000 (64%)]\tTotal Loss: 0.114760\n",
      "Reconstruction: 0.105442, Regularization: 0.009318\n",
      "2019-04-10 00:05:56,211 root         INFO     Train Epoch: 151 [5632/8000 (70%)]\tTotal Loss: 0.115727\n",
      "Reconstruction: 0.106861, Regularization: 0.008866\n",
      "2019-04-10 00:05:56,275 root         INFO     Train Epoch: 151 [6144/8000 (77%)]\tTotal Loss: 0.112240\n",
      "Reconstruction: 0.103562, Regularization: 0.008678\n",
      "2019-04-10 00:05:56,340 root         INFO     Train Epoch: 151 [6656/8000 (83%)]\tTotal Loss: 0.116096\n",
      "Reconstruction: 0.105241, Regularization: 0.010855\n",
      "2019-04-10 00:05:56,404 root         INFO     Train Epoch: 151 [7168/8000 (90%)]\tTotal Loss: 0.103393\n",
      "Reconstruction: 0.094742, Regularization: 0.008650\n",
      "2019-04-10 00:05:56,467 root         INFO     Train Epoch: 151 [7680/8000 (96%)]\tTotal Loss: 0.116558\n",
      "Reconstruction: 0.107175, Regularization: 0.009383\n",
      "2019-04-10 00:05:56,522 root         INFO     ====> Epoch: 151 Average loss: 0.1123\n",
      "2019-04-10 00:05:56,545 root         INFO     Train Epoch: 152 [0/8000 (0%)]\tTotal Loss: 0.108969\n",
      "Reconstruction: 0.102913, Regularization: 0.006056\n",
      "2019-04-10 00:05:56,611 root         INFO     Train Epoch: 152 [512/8000 (6%)]\tTotal Loss: 0.122909\n",
      "Reconstruction: 0.113265, Regularization: 0.009644\n",
      "2019-04-10 00:05:56,674 root         INFO     Train Epoch: 152 [1024/8000 (13%)]\tTotal Loss: 0.108635\n",
      "Reconstruction: 0.099878, Regularization: 0.008758\n",
      "2019-04-10 00:05:56,738 root         INFO     Train Epoch: 152 [1536/8000 (19%)]\tTotal Loss: 0.102134\n",
      "Reconstruction: 0.094331, Regularization: 0.007803\n",
      "2019-04-10 00:05:56,801 root         INFO     Train Epoch: 152 [2048/8000 (26%)]\tTotal Loss: 0.115982\n",
      "Reconstruction: 0.105524, Regularization: 0.010458\n",
      "2019-04-10 00:05:56,865 root         INFO     Train Epoch: 152 [2560/8000 (32%)]\tTotal Loss: 0.110785\n",
      "Reconstruction: 0.102770, Regularization: 0.008015\n",
      "2019-04-10 00:05:56,930 root         INFO     Train Epoch: 152 [3072/8000 (38%)]\tTotal Loss: 0.112632\n",
      "Reconstruction: 0.102154, Regularization: 0.010478\n",
      "2019-04-10 00:05:56,993 root         INFO     Train Epoch: 152 [3584/8000 (45%)]\tTotal Loss: 0.111085\n",
      "Reconstruction: 0.103197, Regularization: 0.007888\n",
      "2019-04-10 00:05:57,057 root         INFO     Train Epoch: 152 [4096/8000 (51%)]\tTotal Loss: 0.114592\n",
      "Reconstruction: 0.103168, Regularization: 0.011424\n",
      "2019-04-10 00:05:57,121 root         INFO     Train Epoch: 152 [4608/8000 (58%)]\tTotal Loss: 0.098633\n",
      "Reconstruction: 0.093051, Regularization: 0.005582\n",
      "2019-04-10 00:05:57,185 root         INFO     Train Epoch: 152 [5120/8000 (64%)]\tTotal Loss: 0.107331\n",
      "Reconstruction: 0.098160, Regularization: 0.009172\n",
      "2019-04-10 00:05:57,248 root         INFO     Train Epoch: 152 [5632/8000 (70%)]\tTotal Loss: 0.113483\n",
      "Reconstruction: 0.104766, Regularization: 0.008717\n",
      "2019-04-10 00:05:57,310 root         INFO     Train Epoch: 152 [6144/8000 (77%)]\tTotal Loss: 0.114047\n",
      "Reconstruction: 0.101996, Regularization: 0.012051\n",
      "2019-04-10 00:05:57,373 root         INFO     Train Epoch: 152 [6656/8000 (83%)]\tTotal Loss: 0.111784\n",
      "Reconstruction: 0.104522, Regularization: 0.007262\n",
      "2019-04-10 00:05:57,436 root         INFO     Train Epoch: 152 [7168/8000 (90%)]\tTotal Loss: 0.126166\n",
      "Reconstruction: 0.114124, Regularization: 0.012042\n",
      "2019-04-10 00:05:57,498 root         INFO     Train Epoch: 152 [7680/8000 (96%)]\tTotal Loss: 0.100513\n",
      "Reconstruction: 0.091883, Regularization: 0.008630\n",
      "2019-04-10 00:05:57,552 root         INFO     ====> Epoch: 152 Average loss: 0.1126\n",
      "2019-04-10 00:05:57,577 root         INFO     Train Epoch: 153 [0/8000 (0%)]\tTotal Loss: 0.109737\n",
      "Reconstruction: 0.102196, Regularization: 0.007541\n",
      "2019-04-10 00:05:57,642 root         INFO     Train Epoch: 153 [512/8000 (6%)]\tTotal Loss: 0.116436\n",
      "Reconstruction: 0.106882, Regularization: 0.009554\n",
      "2019-04-10 00:05:57,707 root         INFO     Train Epoch: 153 [1024/8000 (13%)]\tTotal Loss: 0.112246\n",
      "Reconstruction: 0.103683, Regularization: 0.008563\n",
      "2019-04-10 00:05:57,771 root         INFO     Train Epoch: 153 [1536/8000 (19%)]\tTotal Loss: 0.113219\n",
      "Reconstruction: 0.106462, Regularization: 0.006757\n",
      "2019-04-10 00:05:57,835 root         INFO     Train Epoch: 153 [2048/8000 (26%)]\tTotal Loss: 0.111304\n",
      "Reconstruction: 0.103830, Regularization: 0.007474\n",
      "2019-04-10 00:05:57,899 root         INFO     Train Epoch: 153 [2560/8000 (32%)]\tTotal Loss: 0.120659\n",
      "Reconstruction: 0.111184, Regularization: 0.009475\n",
      "2019-04-10 00:05:57,963 root         INFO     Train Epoch: 153 [3072/8000 (38%)]\tTotal Loss: 0.108170\n",
      "Reconstruction: 0.100702, Regularization: 0.007468\n",
      "2019-04-10 00:05:58,027 root         INFO     Train Epoch: 153 [3584/8000 (45%)]\tTotal Loss: 0.109151\n",
      "Reconstruction: 0.098784, Regularization: 0.010367\n",
      "2019-04-10 00:05:58,091 root         INFO     Train Epoch: 153 [4096/8000 (51%)]\tTotal Loss: 0.122681\n",
      "Reconstruction: 0.112283, Regularization: 0.010398\n",
      "2019-04-10 00:05:58,155 root         INFO     Train Epoch: 153 [4608/8000 (58%)]\tTotal Loss: 0.114698\n",
      "Reconstruction: 0.105985, Regularization: 0.008713\n",
      "2019-04-10 00:05:58,219 root         INFO     Train Epoch: 153 [5120/8000 (64%)]\tTotal Loss: 0.101425\n",
      "Reconstruction: 0.094408, Regularization: 0.007017\n",
      "2019-04-10 00:05:58,283 root         INFO     Train Epoch: 153 [5632/8000 (70%)]\tTotal Loss: 0.106942\n",
      "Reconstruction: 0.098587, Regularization: 0.008355\n",
      "2019-04-10 00:05:58,347 root         INFO     Train Epoch: 153 [6144/8000 (77%)]\tTotal Loss: 0.119637\n",
      "Reconstruction: 0.110238, Regularization: 0.009400\n",
      "2019-04-10 00:05:58,411 root         INFO     Train Epoch: 153 [6656/8000 (83%)]\tTotal Loss: 0.118731\n",
      "Reconstruction: 0.109129, Regularization: 0.009602\n",
      "2019-04-10 00:05:58,475 root         INFO     Train Epoch: 153 [7168/8000 (90%)]\tTotal Loss: 0.110438\n",
      "Reconstruction: 0.100945, Regularization: 0.009493\n",
      "2019-04-10 00:05:58,538 root         INFO     Train Epoch: 153 [7680/8000 (96%)]\tTotal Loss: 0.114695\n",
      "Reconstruction: 0.105665, Regularization: 0.009030\n",
      "2019-04-10 00:05:58,593 root         INFO     ====> Epoch: 153 Average loss: 0.1129\n",
      "2019-04-10 00:05:58,616 root         INFO     Train Epoch: 154 [0/8000 (0%)]\tTotal Loss: 0.126004\n",
      "Reconstruction: 0.114681, Regularization: 0.011323\n",
      "2019-04-10 00:05:58,681 root         INFO     Train Epoch: 154 [512/8000 (6%)]\tTotal Loss: 0.107470\n",
      "Reconstruction: 0.099088, Regularization: 0.008382\n",
      "2019-04-10 00:05:58,746 root         INFO     Train Epoch: 154 [1024/8000 (13%)]\tTotal Loss: 0.101472\n",
      "Reconstruction: 0.093793, Regularization: 0.007680\n",
      "2019-04-10 00:05:58,811 root         INFO     Train Epoch: 154 [1536/8000 (19%)]\tTotal Loss: 0.110341\n",
      "Reconstruction: 0.101332, Regularization: 0.009009\n",
      "2019-04-10 00:05:58,875 root         INFO     Train Epoch: 154 [2048/8000 (26%)]\tTotal Loss: 0.112357\n",
      "Reconstruction: 0.103201, Regularization: 0.009156\n",
      "2019-04-10 00:05:58,939 root         INFO     Train Epoch: 154 [2560/8000 (32%)]\tTotal Loss: 0.119432\n",
      "Reconstruction: 0.110197, Regularization: 0.009234\n",
      "2019-04-10 00:05:59,003 root         INFO     Train Epoch: 154 [3072/8000 (38%)]\tTotal Loss: 0.107063\n",
      "Reconstruction: 0.097725, Regularization: 0.009337\n",
      "2019-04-10 00:05:59,068 root         INFO     Train Epoch: 154 [3584/8000 (45%)]\tTotal Loss: 0.104937\n",
      "Reconstruction: 0.097824, Regularization: 0.007113\n",
      "2019-04-10 00:05:59,131 root         INFO     Train Epoch: 154 [4096/8000 (51%)]\tTotal Loss: 0.111843\n",
      "Reconstruction: 0.103278, Regularization: 0.008564\n",
      "2019-04-10 00:05:59,195 root         INFO     Train Epoch: 154 [4608/8000 (58%)]\tTotal Loss: 0.116066\n",
      "Reconstruction: 0.104927, Regularization: 0.011140\n",
      "2019-04-10 00:05:59,259 root         INFO     Train Epoch: 154 [5120/8000 (64%)]\tTotal Loss: 0.108315\n",
      "Reconstruction: 0.101634, Regularization: 0.006682\n",
      "2019-04-10 00:05:59,323 root         INFO     Train Epoch: 154 [5632/8000 (70%)]\tTotal Loss: 0.113997\n",
      "Reconstruction: 0.105378, Regularization: 0.008618\n",
      "2019-04-10 00:05:59,388 root         INFO     Train Epoch: 154 [6144/8000 (77%)]\tTotal Loss: 0.119442\n",
      "Reconstruction: 0.112374, Regularization: 0.007068\n",
      "2019-04-10 00:05:59,451 root         INFO     Train Epoch: 154 [6656/8000 (83%)]\tTotal Loss: 0.107065\n",
      "Reconstruction: 0.098178, Regularization: 0.008888\n",
      "2019-04-10 00:05:59,514 root         INFO     Train Epoch: 154 [7168/8000 (90%)]\tTotal Loss: 0.112674\n",
      "Reconstruction: 0.103870, Regularization: 0.008804\n",
      "2019-04-10 00:05:59,577 root         INFO     Train Epoch: 154 [7680/8000 (96%)]\tTotal Loss: 0.110493\n",
      "Reconstruction: 0.102574, Regularization: 0.007919\n",
      "2019-04-10 00:05:59,631 root         INFO     ====> Epoch: 154 Average loss: 0.1125\n",
      "2019-04-10 00:05:59,655 root         INFO     Train Epoch: 155 [0/8000 (0%)]\tTotal Loss: 0.109146\n",
      "Reconstruction: 0.098843, Regularization: 0.010303\n",
      "2019-04-10 00:05:59,719 root         INFO     Train Epoch: 155 [512/8000 (6%)]\tTotal Loss: 0.108755\n",
      "Reconstruction: 0.101429, Regularization: 0.007326\n",
      "2019-04-10 00:05:59,783 root         INFO     Train Epoch: 155 [1024/8000 (13%)]\tTotal Loss: 0.114658\n",
      "Reconstruction: 0.104206, Regularization: 0.010451\n",
      "2019-04-10 00:05:59,847 root         INFO     Train Epoch: 155 [1536/8000 (19%)]\tTotal Loss: 0.111845\n",
      "Reconstruction: 0.102762, Regularization: 0.009083\n",
      "2019-04-10 00:05:59,911 root         INFO     Train Epoch: 155 [2048/8000 (26%)]\tTotal Loss: 0.103261\n",
      "Reconstruction: 0.094057, Regularization: 0.009204\n",
      "2019-04-10 00:05:59,974 root         INFO     Train Epoch: 155 [2560/8000 (32%)]\tTotal Loss: 0.110616\n",
      "Reconstruction: 0.100733, Regularization: 0.009882\n",
      "2019-04-10 00:06:00,038 root         INFO     Train Epoch: 155 [3072/8000 (38%)]\tTotal Loss: 0.116541\n",
      "Reconstruction: 0.108770, Regularization: 0.007770\n",
      "2019-04-10 00:06:00,102 root         INFO     Train Epoch: 155 [3584/8000 (45%)]\tTotal Loss: 0.117043\n",
      "Reconstruction: 0.102861, Regularization: 0.014182\n",
      "2019-04-10 00:06:00,166 root         INFO     Train Epoch: 155 [4096/8000 (51%)]\tTotal Loss: 0.113140\n",
      "Reconstruction: 0.104799, Regularization: 0.008341\n",
      "2019-04-10 00:06:00,230 root         INFO     Train Epoch: 155 [4608/8000 (58%)]\tTotal Loss: 0.112360\n",
      "Reconstruction: 0.105751, Regularization: 0.006609\n",
      "2019-04-10 00:06:00,294 root         INFO     Train Epoch: 155 [5120/8000 (64%)]\tTotal Loss: 0.118511\n",
      "Reconstruction: 0.108751, Regularization: 0.009760\n",
      "2019-04-10 00:06:00,358 root         INFO     Train Epoch: 155 [5632/8000 (70%)]\tTotal Loss: 0.106360\n",
      "Reconstruction: 0.096919, Regularization: 0.009442\n",
      "2019-04-10 00:06:00,422 root         INFO     Train Epoch: 155 [6144/8000 (77%)]\tTotal Loss: 0.117206\n",
      "Reconstruction: 0.108893, Regularization: 0.008313\n",
      "2019-04-10 00:06:00,485 root         INFO     Train Epoch: 155 [6656/8000 (83%)]\tTotal Loss: 0.110963\n",
      "Reconstruction: 0.101349, Regularization: 0.009614\n",
      "2019-04-10 00:06:00,549 root         INFO     Train Epoch: 155 [7168/8000 (90%)]\tTotal Loss: 0.114136\n",
      "Reconstruction: 0.104564, Regularization: 0.009572\n",
      "2019-04-10 00:06:00,614 root         INFO     Train Epoch: 155 [7680/8000 (96%)]\tTotal Loss: 0.109951\n",
      "Reconstruction: 0.099905, Regularization: 0.010046\n",
      "2019-04-10 00:06:00,668 root         INFO     ====> Epoch: 155 Average loss: 0.1125\n",
      "2019-04-10 00:06:00,692 root         INFO     Train Epoch: 156 [0/8000 (0%)]\tTotal Loss: 0.111540\n",
      "Reconstruction: 0.102344, Regularization: 0.009196\n",
      "2019-04-10 00:06:00,755 root         INFO     Train Epoch: 156 [512/8000 (6%)]\tTotal Loss: 0.108238\n",
      "Reconstruction: 0.099496, Regularization: 0.008742\n",
      "2019-04-10 00:06:00,818 root         INFO     Train Epoch: 156 [1024/8000 (13%)]\tTotal Loss: 0.106495\n",
      "Reconstruction: 0.098031, Regularization: 0.008464\n",
      "2019-04-10 00:06:00,881 root         INFO     Train Epoch: 156 [1536/8000 (19%)]\tTotal Loss: 0.117192\n",
      "Reconstruction: 0.108470, Regularization: 0.008722\n",
      "2019-04-10 00:06:00,945 root         INFO     Train Epoch: 156 [2048/8000 (26%)]\tTotal Loss: 0.101509\n",
      "Reconstruction: 0.094397, Regularization: 0.007112\n",
      "2019-04-10 00:06:01,008 root         INFO     Train Epoch: 156 [2560/8000 (32%)]\tTotal Loss: 0.097139\n",
      "Reconstruction: 0.090839, Regularization: 0.006300\n",
      "2019-04-10 00:06:01,071 root         INFO     Train Epoch: 156 [3072/8000 (38%)]\tTotal Loss: 0.111364\n",
      "Reconstruction: 0.100432, Regularization: 0.010932\n",
      "2019-04-10 00:06:01,135 root         INFO     Train Epoch: 156 [3584/8000 (45%)]\tTotal Loss: 0.097778\n",
      "Reconstruction: 0.088303, Regularization: 0.009475\n",
      "2019-04-10 00:06:01,200 root         INFO     Train Epoch: 156 [4096/8000 (51%)]\tTotal Loss: 0.122094\n",
      "Reconstruction: 0.112223, Regularization: 0.009872\n",
      "2019-04-10 00:06:01,264 root         INFO     Train Epoch: 156 [4608/8000 (58%)]\tTotal Loss: 0.122931\n",
      "Reconstruction: 0.109668, Regularization: 0.013263\n",
      "2019-04-10 00:06:01,329 root         INFO     Train Epoch: 156 [5120/8000 (64%)]\tTotal Loss: 0.118088\n",
      "Reconstruction: 0.108543, Regularization: 0.009545\n",
      "2019-04-10 00:06:01,394 root         INFO     Train Epoch: 156 [5632/8000 (70%)]\tTotal Loss: 0.118923\n",
      "Reconstruction: 0.107491, Regularization: 0.011431\n",
      "2019-04-10 00:06:01,459 root         INFO     Train Epoch: 156 [6144/8000 (77%)]\tTotal Loss: 0.121940\n",
      "Reconstruction: 0.110035, Regularization: 0.011905\n",
      "2019-04-10 00:06:01,524 root         INFO     Train Epoch: 156 [6656/8000 (83%)]\tTotal Loss: 0.110190\n",
      "Reconstruction: 0.102263, Regularization: 0.007927\n",
      "2019-04-10 00:06:01,588 root         INFO     Train Epoch: 156 [7168/8000 (90%)]\tTotal Loss: 0.102867\n",
      "Reconstruction: 0.094313, Regularization: 0.008554\n",
      "2019-04-10 00:06:01,651 root         INFO     Train Epoch: 156 [7680/8000 (96%)]\tTotal Loss: 0.123393\n",
      "Reconstruction: 0.112991, Regularization: 0.010402\n",
      "2019-04-10 00:06:01,706 root         INFO     ====> Epoch: 156 Average loss: 0.1124\n",
      "2019-04-10 00:06:01,730 root         INFO     Train Epoch: 157 [0/8000 (0%)]\tTotal Loss: 0.112727\n",
      "Reconstruction: 0.103440, Regularization: 0.009287\n",
      "2019-04-10 00:06:01,793 root         INFO     Train Epoch: 157 [512/8000 (6%)]\tTotal Loss: 0.114438\n",
      "Reconstruction: 0.103338, Regularization: 0.011100\n",
      "2019-04-10 00:06:01,857 root         INFO     Train Epoch: 157 [1024/8000 (13%)]\tTotal Loss: 0.104703\n",
      "Reconstruction: 0.097370, Regularization: 0.007333\n",
      "2019-04-10 00:06:01,921 root         INFO     Train Epoch: 157 [1536/8000 (19%)]\tTotal Loss: 0.103421\n",
      "Reconstruction: 0.096148, Regularization: 0.007273\n",
      "2019-04-10 00:06:01,983 root         INFO     Train Epoch: 157 [2048/8000 (26%)]\tTotal Loss: 0.113262\n",
      "Reconstruction: 0.102669, Regularization: 0.010593\n",
      "2019-04-10 00:06:02,046 root         INFO     Train Epoch: 157 [2560/8000 (32%)]\tTotal Loss: 0.106869\n",
      "Reconstruction: 0.098242, Regularization: 0.008627\n",
      "2019-04-10 00:06:02,108 root         INFO     Train Epoch: 157 [3072/8000 (38%)]\tTotal Loss: 0.105295\n",
      "Reconstruction: 0.097360, Regularization: 0.007935\n",
      "2019-04-10 00:06:02,172 root         INFO     Train Epoch: 157 [3584/8000 (45%)]\tTotal Loss: 0.113811\n",
      "Reconstruction: 0.102727, Regularization: 0.011084\n",
      "2019-04-10 00:06:02,234 root         INFO     Train Epoch: 157 [4096/8000 (51%)]\tTotal Loss: 0.125931\n",
      "Reconstruction: 0.112534, Regularization: 0.013397\n",
      "2019-04-10 00:06:02,297 root         INFO     Train Epoch: 157 [4608/8000 (58%)]\tTotal Loss: 0.116897\n",
      "Reconstruction: 0.106929, Regularization: 0.009967\n",
      "2019-04-10 00:06:02,359 root         INFO     Train Epoch: 157 [5120/8000 (64%)]\tTotal Loss: 0.101487\n",
      "Reconstruction: 0.092936, Regularization: 0.008551\n",
      "2019-04-10 00:06:02,422 root         INFO     Train Epoch: 157 [5632/8000 (70%)]\tTotal Loss: 0.118913\n",
      "Reconstruction: 0.110407, Regularization: 0.008506\n",
      "2019-04-10 00:06:02,485 root         INFO     Train Epoch: 157 [6144/8000 (77%)]\tTotal Loss: 0.099617\n",
      "Reconstruction: 0.090329, Regularization: 0.009287\n",
      "2019-04-10 00:06:02,547 root         INFO     Train Epoch: 157 [6656/8000 (83%)]\tTotal Loss: 0.112342\n",
      "Reconstruction: 0.104107, Regularization: 0.008234\n",
      "2019-04-10 00:06:02,610 root         INFO     Train Epoch: 157 [7168/8000 (90%)]\tTotal Loss: 0.109516\n",
      "Reconstruction: 0.100357, Regularization: 0.009159\n",
      "2019-04-10 00:06:02,673 root         INFO     Train Epoch: 157 [7680/8000 (96%)]\tTotal Loss: 0.126308\n",
      "Reconstruction: 0.114447, Regularization: 0.011861\n",
      "2019-04-10 00:06:02,727 root         INFO     ====> Epoch: 157 Average loss: 0.1122\n",
      "2019-04-10 00:06:02,751 root         INFO     Train Epoch: 158 [0/8000 (0%)]\tTotal Loss: 0.114301\n",
      "Reconstruction: 0.105046, Regularization: 0.009255\n",
      "2019-04-10 00:06:02,815 root         INFO     Train Epoch: 158 [512/8000 (6%)]\tTotal Loss: 0.116656\n",
      "Reconstruction: 0.106011, Regularization: 0.010645\n",
      "2019-04-10 00:06:02,878 root         INFO     Train Epoch: 158 [1024/8000 (13%)]\tTotal Loss: 0.097241\n",
      "Reconstruction: 0.089289, Regularization: 0.007952\n",
      "2019-04-10 00:06:02,942 root         INFO     Train Epoch: 158 [1536/8000 (19%)]\tTotal Loss: 0.119236\n",
      "Reconstruction: 0.109671, Regularization: 0.009565\n",
      "2019-04-10 00:06:03,005 root         INFO     Train Epoch: 158 [2048/8000 (26%)]\tTotal Loss: 0.117596\n",
      "Reconstruction: 0.106528, Regularization: 0.011069\n",
      "2019-04-10 00:06:03,069 root         INFO     Train Epoch: 158 [2560/8000 (32%)]\tTotal Loss: 0.117746\n",
      "Reconstruction: 0.106824, Regularization: 0.010922\n",
      "2019-04-10 00:06:03,133 root         INFO     Train Epoch: 158 [3072/8000 (38%)]\tTotal Loss: 0.104145\n",
      "Reconstruction: 0.095628, Regularization: 0.008517\n",
      "2019-04-10 00:06:03,196 root         INFO     Train Epoch: 158 [3584/8000 (45%)]\tTotal Loss: 0.108062\n",
      "Reconstruction: 0.099557, Regularization: 0.008505\n",
      "2019-04-10 00:06:03,259 root         INFO     Train Epoch: 158 [4096/8000 (51%)]\tTotal Loss: 0.101610\n",
      "Reconstruction: 0.093897, Regularization: 0.007713\n",
      "2019-04-10 00:06:03,323 root         INFO     Train Epoch: 158 [4608/8000 (58%)]\tTotal Loss: 0.103514\n",
      "Reconstruction: 0.094779, Regularization: 0.008735\n",
      "2019-04-10 00:06:03,386 root         INFO     Train Epoch: 158 [5120/8000 (64%)]\tTotal Loss: 0.103611\n",
      "Reconstruction: 0.096336, Regularization: 0.007275\n",
      "2019-04-10 00:06:03,450 root         INFO     Train Epoch: 158 [5632/8000 (70%)]\tTotal Loss: 0.106531\n",
      "Reconstruction: 0.097877, Regularization: 0.008653\n",
      "2019-04-10 00:06:03,514 root         INFO     Train Epoch: 158 [6144/8000 (77%)]\tTotal Loss: 0.097907\n",
      "Reconstruction: 0.090193, Regularization: 0.007714\n",
      "2019-04-10 00:06:03,576 root         INFO     Train Epoch: 158 [6656/8000 (83%)]\tTotal Loss: 0.113148\n",
      "Reconstruction: 0.106303, Regularization: 0.006845\n",
      "2019-04-10 00:06:03,639 root         INFO     Train Epoch: 158 [7168/8000 (90%)]\tTotal Loss: 0.103357\n",
      "Reconstruction: 0.095100, Regularization: 0.008257\n",
      "2019-04-10 00:06:03,703 root         INFO     Train Epoch: 158 [7680/8000 (96%)]\tTotal Loss: 0.113128\n",
      "Reconstruction: 0.103125, Regularization: 0.010003\n",
      "2019-04-10 00:06:03,757 root         INFO     ====> Epoch: 158 Average loss: 0.1130\n",
      "2019-04-10 00:06:03,780 root         INFO     Train Epoch: 159 [0/8000 (0%)]\tTotal Loss: 0.106407\n",
      "Reconstruction: 0.096828, Regularization: 0.009580\n",
      "2019-04-10 00:06:03,845 root         INFO     Train Epoch: 159 [512/8000 (6%)]\tTotal Loss: 0.111124\n",
      "Reconstruction: 0.101939, Regularization: 0.009185\n",
      "2019-04-10 00:06:03,908 root         INFO     Train Epoch: 159 [1024/8000 (13%)]\tTotal Loss: 0.114489\n",
      "Reconstruction: 0.101993, Regularization: 0.012496\n",
      "2019-04-10 00:06:03,971 root         INFO     Train Epoch: 159 [1536/8000 (19%)]\tTotal Loss: 0.101239\n",
      "Reconstruction: 0.092498, Regularization: 0.008741\n",
      "2019-04-10 00:06:04,034 root         INFO     Train Epoch: 159 [2048/8000 (26%)]\tTotal Loss: 0.117400\n",
      "Reconstruction: 0.108009, Regularization: 0.009391\n",
      "2019-04-10 00:06:04,098 root         INFO     Train Epoch: 159 [2560/8000 (32%)]\tTotal Loss: 0.107226\n",
      "Reconstruction: 0.100565, Regularization: 0.006661\n",
      "2019-04-10 00:06:04,161 root         INFO     Train Epoch: 159 [3072/8000 (38%)]\tTotal Loss: 0.106680\n",
      "Reconstruction: 0.099730, Regularization: 0.006950\n",
      "2019-04-10 00:06:04,225 root         INFO     Train Epoch: 159 [3584/8000 (45%)]\tTotal Loss: 0.114245\n",
      "Reconstruction: 0.106441, Regularization: 0.007804\n",
      "2019-04-10 00:06:04,288 root         INFO     Train Epoch: 159 [4096/8000 (51%)]\tTotal Loss: 0.112798\n",
      "Reconstruction: 0.101547, Regularization: 0.011251\n",
      "2019-04-10 00:06:04,351 root         INFO     Train Epoch: 159 [4608/8000 (58%)]\tTotal Loss: 0.119830\n",
      "Reconstruction: 0.110308, Regularization: 0.009522\n",
      "2019-04-10 00:06:04,414 root         INFO     Train Epoch: 159 [5120/8000 (64%)]\tTotal Loss: 0.110598\n",
      "Reconstruction: 0.102181, Regularization: 0.008417\n",
      "2019-04-10 00:06:04,477 root         INFO     Train Epoch: 159 [5632/8000 (70%)]\tTotal Loss: 0.111298\n",
      "Reconstruction: 0.102313, Regularization: 0.008986\n",
      "2019-04-10 00:06:04,541 root         INFO     Train Epoch: 159 [6144/8000 (77%)]\tTotal Loss: 0.110070\n",
      "Reconstruction: 0.100377, Regularization: 0.009693\n",
      "2019-04-10 00:06:04,605 root         INFO     Train Epoch: 159 [6656/8000 (83%)]\tTotal Loss: 0.098519\n",
      "Reconstruction: 0.090370, Regularization: 0.008149\n",
      "2019-04-10 00:06:04,669 root         INFO     Train Epoch: 159 [7168/8000 (90%)]\tTotal Loss: 0.114267\n",
      "Reconstruction: 0.106097, Regularization: 0.008170\n",
      "2019-04-10 00:06:04,733 root         INFO     Train Epoch: 159 [7680/8000 (96%)]\tTotal Loss: 0.125164\n",
      "Reconstruction: 0.115192, Regularization: 0.009972\n",
      "2019-04-10 00:06:04,786 root         INFO     ====> Epoch: 159 Average loss: 0.1122\n",
      "2019-04-10 00:06:04,810 root         INFO     Train Epoch: 160 [0/8000 (0%)]\tTotal Loss: 0.114424\n",
      "Reconstruction: 0.104281, Regularization: 0.010142\n",
      "2019-04-10 00:06:04,875 root         INFO     Train Epoch: 160 [512/8000 (6%)]\tTotal Loss: 0.108981\n",
      "Reconstruction: 0.100611, Regularization: 0.008371\n",
      "2019-04-10 00:06:04,939 root         INFO     Train Epoch: 160 [1024/8000 (13%)]\tTotal Loss: 0.111162\n",
      "Reconstruction: 0.102360, Regularization: 0.008802\n",
      "2019-04-10 00:06:05,002 root         INFO     Train Epoch: 160 [1536/8000 (19%)]\tTotal Loss: 0.118785\n",
      "Reconstruction: 0.110056, Regularization: 0.008729\n",
      "2019-04-10 00:06:05,065 root         INFO     Train Epoch: 160 [2048/8000 (26%)]\tTotal Loss: 0.106892\n",
      "Reconstruction: 0.097835, Regularization: 0.009057\n",
      "2019-04-10 00:06:05,128 root         INFO     Train Epoch: 160 [2560/8000 (32%)]\tTotal Loss: 0.106439\n",
      "Reconstruction: 0.096757, Regularization: 0.009682\n",
      "2019-04-10 00:06:05,192 root         INFO     Train Epoch: 160 [3072/8000 (38%)]\tTotal Loss: 0.116454\n",
      "Reconstruction: 0.109250, Regularization: 0.007205\n",
      "2019-04-10 00:06:05,254 root         INFO     Train Epoch: 160 [3584/8000 (45%)]\tTotal Loss: 0.105186\n",
      "Reconstruction: 0.097576, Regularization: 0.007610\n",
      "2019-04-10 00:06:05,315 root         INFO     Train Epoch: 160 [4096/8000 (51%)]\tTotal Loss: 0.113803\n",
      "Reconstruction: 0.103820, Regularization: 0.009983\n",
      "2019-04-10 00:06:05,378 root         INFO     Train Epoch: 160 [4608/8000 (58%)]\tTotal Loss: 0.100665\n",
      "Reconstruction: 0.090632, Regularization: 0.010034\n",
      "2019-04-10 00:06:05,441 root         INFO     Train Epoch: 160 [5120/8000 (64%)]\tTotal Loss: 0.117338\n",
      "Reconstruction: 0.105333, Regularization: 0.012004\n",
      "2019-04-10 00:06:05,505 root         INFO     Train Epoch: 160 [5632/8000 (70%)]\tTotal Loss: 0.119561\n",
      "Reconstruction: 0.107124, Regularization: 0.012438\n",
      "2019-04-10 00:06:05,568 root         INFO     Train Epoch: 160 [6144/8000 (77%)]\tTotal Loss: 0.125634\n",
      "Reconstruction: 0.115791, Regularization: 0.009843\n",
      "2019-04-10 00:06:05,631 root         INFO     Train Epoch: 160 [6656/8000 (83%)]\tTotal Loss: 0.115830\n",
      "Reconstruction: 0.107428, Regularization: 0.008402\n",
      "2019-04-10 00:06:05,694 root         INFO     Train Epoch: 160 [7168/8000 (90%)]\tTotal Loss: 0.115665\n",
      "Reconstruction: 0.105053, Regularization: 0.010612\n",
      "2019-04-10 00:06:05,756 root         INFO     Train Epoch: 160 [7680/8000 (96%)]\tTotal Loss: 0.110713\n",
      "Reconstruction: 0.102291, Regularization: 0.008422\n",
      "2019-04-10 00:06:05,811 root         INFO     ====> Epoch: 160 Average loss: 0.1118\n",
      "2019-04-10 00:06:05,835 root         INFO     Train Epoch: 161 [0/8000 (0%)]\tTotal Loss: 0.119671\n",
      "Reconstruction: 0.109660, Regularization: 0.010011\n",
      "2019-04-10 00:06:05,897 root         INFO     Train Epoch: 161 [512/8000 (6%)]\tTotal Loss: 0.123037\n",
      "Reconstruction: 0.113740, Regularization: 0.009297\n",
      "2019-04-10 00:06:05,959 root         INFO     Train Epoch: 161 [1024/8000 (13%)]\tTotal Loss: 0.109036\n",
      "Reconstruction: 0.099937, Regularization: 0.009099\n",
      "2019-04-10 00:06:06,020 root         INFO     Train Epoch: 161 [1536/8000 (19%)]\tTotal Loss: 0.121371\n",
      "Reconstruction: 0.110863, Regularization: 0.010507\n",
      "2019-04-10 00:06:06,082 root         INFO     Train Epoch: 161 [2048/8000 (26%)]\tTotal Loss: 0.108532\n",
      "Reconstruction: 0.100170, Regularization: 0.008361\n",
      "2019-04-10 00:06:06,143 root         INFO     Train Epoch: 161 [2560/8000 (32%)]\tTotal Loss: 0.126040\n",
      "Reconstruction: 0.115028, Regularization: 0.011012\n",
      "2019-04-10 00:06:06,205 root         INFO     Train Epoch: 161 [3072/8000 (38%)]\tTotal Loss: 0.115101\n",
      "Reconstruction: 0.103813, Regularization: 0.011287\n",
      "2019-04-10 00:06:06,266 root         INFO     Train Epoch: 161 [3584/8000 (45%)]\tTotal Loss: 0.113074\n",
      "Reconstruction: 0.104079, Regularization: 0.008995\n",
      "2019-04-10 00:06:06,328 root         INFO     Train Epoch: 161 [4096/8000 (51%)]\tTotal Loss: 0.111747\n",
      "Reconstruction: 0.102190, Regularization: 0.009557\n",
      "2019-04-10 00:06:06,389 root         INFO     Train Epoch: 161 [4608/8000 (58%)]\tTotal Loss: 0.110277\n",
      "Reconstruction: 0.099752, Regularization: 0.010526\n",
      "2019-04-10 00:06:06,449 root         INFO     Train Epoch: 161 [5120/8000 (64%)]\tTotal Loss: 0.108214\n",
      "Reconstruction: 0.099099, Regularization: 0.009115\n",
      "2019-04-10 00:06:06,510 root         INFO     Train Epoch: 161 [5632/8000 (70%)]\tTotal Loss: 0.117579\n",
      "Reconstruction: 0.105700, Regularization: 0.011879\n",
      "2019-04-10 00:06:06,572 root         INFO     Train Epoch: 161 [6144/8000 (77%)]\tTotal Loss: 0.108750\n",
      "Reconstruction: 0.099659, Regularization: 0.009091\n",
      "2019-04-10 00:06:06,633 root         INFO     Train Epoch: 161 [6656/8000 (83%)]\tTotal Loss: 0.117522\n",
      "Reconstruction: 0.105855, Regularization: 0.011666\n",
      "2019-04-10 00:06:06,694 root         INFO     Train Epoch: 161 [7168/8000 (90%)]\tTotal Loss: 0.108236\n",
      "Reconstruction: 0.096691, Regularization: 0.011545\n",
      "2019-04-10 00:06:06,754 root         INFO     Train Epoch: 161 [7680/8000 (96%)]\tTotal Loss: 0.113906\n",
      "Reconstruction: 0.105202, Regularization: 0.008704\n",
      "2019-04-10 00:06:06,807 root         INFO     ====> Epoch: 161 Average loss: 0.1124\n",
      "2019-04-10 00:06:06,831 root         INFO     Train Epoch: 162 [0/8000 (0%)]\tTotal Loss: 0.116851\n",
      "Reconstruction: 0.106005, Regularization: 0.010846\n",
      "2019-04-10 00:06:06,894 root         INFO     Train Epoch: 162 [512/8000 (6%)]\tTotal Loss: 0.114129\n",
      "Reconstruction: 0.103538, Regularization: 0.010591\n",
      "2019-04-10 00:06:06,956 root         INFO     Train Epoch: 162 [1024/8000 (13%)]\tTotal Loss: 0.110847\n",
      "Reconstruction: 0.100420, Regularization: 0.010427\n",
      "2019-04-10 00:06:07,018 root         INFO     Train Epoch: 162 [1536/8000 (19%)]\tTotal Loss: 0.104534\n",
      "Reconstruction: 0.095022, Regularization: 0.009511\n",
      "2019-04-10 00:06:07,080 root         INFO     Train Epoch: 162 [2048/8000 (26%)]\tTotal Loss: 0.112990\n",
      "Reconstruction: 0.104770, Regularization: 0.008219\n",
      "2019-04-10 00:06:07,142 root         INFO     Train Epoch: 162 [2560/8000 (32%)]\tTotal Loss: 0.126511\n",
      "Reconstruction: 0.115241, Regularization: 0.011270\n",
      "2019-04-10 00:06:07,204 root         INFO     Train Epoch: 162 [3072/8000 (38%)]\tTotal Loss: 0.112967\n",
      "Reconstruction: 0.103036, Regularization: 0.009931\n",
      "2019-04-10 00:06:07,267 root         INFO     Train Epoch: 162 [3584/8000 (45%)]\tTotal Loss: 0.104682\n",
      "Reconstruction: 0.095124, Regularization: 0.009559\n",
      "2019-04-10 00:06:07,329 root         INFO     Train Epoch: 162 [4096/8000 (51%)]\tTotal Loss: 0.117129\n",
      "Reconstruction: 0.107995, Regularization: 0.009134\n",
      "2019-04-10 00:06:07,391 root         INFO     Train Epoch: 162 [4608/8000 (58%)]\tTotal Loss: 0.110253\n",
      "Reconstruction: 0.100645, Regularization: 0.009608\n",
      "2019-04-10 00:06:07,453 root         INFO     Train Epoch: 162 [5120/8000 (64%)]\tTotal Loss: 0.116837\n",
      "Reconstruction: 0.107266, Regularization: 0.009571\n",
      "2019-04-10 00:06:07,515 root         INFO     Train Epoch: 162 [5632/8000 (70%)]\tTotal Loss: 0.122301\n",
      "Reconstruction: 0.109461, Regularization: 0.012840\n",
      "2019-04-10 00:06:07,577 root         INFO     Train Epoch: 162 [6144/8000 (77%)]\tTotal Loss: 0.101627\n",
      "Reconstruction: 0.094097, Regularization: 0.007529\n",
      "2019-04-10 00:06:07,639 root         INFO     Train Epoch: 162 [6656/8000 (83%)]\tTotal Loss: 0.108380\n",
      "Reconstruction: 0.099670, Regularization: 0.008710\n",
      "2019-04-10 00:06:07,702 root         INFO     Train Epoch: 162 [7168/8000 (90%)]\tTotal Loss: 0.116161\n",
      "Reconstruction: 0.107160, Regularization: 0.009001\n",
      "2019-04-10 00:06:07,764 root         INFO     Train Epoch: 162 [7680/8000 (96%)]\tTotal Loss: 0.108354\n",
      "Reconstruction: 0.099954, Regularization: 0.008400\n",
      "2019-04-10 00:06:07,817 root         INFO     ====> Epoch: 162 Average loss: 0.1123\n",
      "2019-04-10 00:06:07,841 root         INFO     Train Epoch: 163 [0/8000 (0%)]\tTotal Loss: 0.118722\n",
      "Reconstruction: 0.107325, Regularization: 0.011397\n",
      "2019-04-10 00:06:07,902 root         INFO     Train Epoch: 163 [512/8000 (6%)]\tTotal Loss: 0.116347\n",
      "Reconstruction: 0.107727, Regularization: 0.008619\n",
      "2019-04-10 00:06:07,963 root         INFO     Train Epoch: 163 [1024/8000 (13%)]\tTotal Loss: 0.109159\n",
      "Reconstruction: 0.098579, Regularization: 0.010580\n",
      "2019-04-10 00:06:08,024 root         INFO     Train Epoch: 163 [1536/8000 (19%)]\tTotal Loss: 0.108741\n",
      "Reconstruction: 0.099521, Regularization: 0.009221\n",
      "2019-04-10 00:06:08,085 root         INFO     Train Epoch: 163 [2048/8000 (26%)]\tTotal Loss: 0.110798\n",
      "Reconstruction: 0.102260, Regularization: 0.008537\n",
      "2019-04-10 00:06:08,145 root         INFO     Train Epoch: 163 [2560/8000 (32%)]\tTotal Loss: 0.112013\n",
      "Reconstruction: 0.100800, Regularization: 0.011213\n",
      "2019-04-10 00:06:08,206 root         INFO     Train Epoch: 163 [3072/8000 (38%)]\tTotal Loss: 0.107311\n",
      "Reconstruction: 0.099028, Regularization: 0.008283\n",
      "2019-04-10 00:06:08,267 root         INFO     Train Epoch: 163 [3584/8000 (45%)]\tTotal Loss: 0.112879\n",
      "Reconstruction: 0.102681, Regularization: 0.010198\n",
      "2019-04-10 00:06:08,327 root         INFO     Train Epoch: 163 [4096/8000 (51%)]\tTotal Loss: 0.120564\n",
      "Reconstruction: 0.109613, Regularization: 0.010951\n",
      "2019-04-10 00:06:08,388 root         INFO     Train Epoch: 163 [4608/8000 (58%)]\tTotal Loss: 0.108806\n",
      "Reconstruction: 0.097882, Regularization: 0.010924\n",
      "2019-04-10 00:06:08,449 root         INFO     Train Epoch: 163 [5120/8000 (64%)]\tTotal Loss: 0.119691\n",
      "Reconstruction: 0.107238, Regularization: 0.012453\n",
      "2019-04-10 00:06:08,509 root         INFO     Train Epoch: 163 [5632/8000 (70%)]\tTotal Loss: 0.123850\n",
      "Reconstruction: 0.113680, Regularization: 0.010170\n",
      "2019-04-10 00:06:08,570 root         INFO     Train Epoch: 163 [6144/8000 (77%)]\tTotal Loss: 0.099939\n",
      "Reconstruction: 0.092862, Regularization: 0.007078\n",
      "2019-04-10 00:06:08,630 root         INFO     Train Epoch: 163 [6656/8000 (83%)]\tTotal Loss: 0.103508\n",
      "Reconstruction: 0.094943, Regularization: 0.008565\n",
      "2019-04-10 00:06:08,691 root         INFO     Train Epoch: 163 [7168/8000 (90%)]\tTotal Loss: 0.121444\n",
      "Reconstruction: 0.110155, Regularization: 0.011289\n",
      "2019-04-10 00:06:08,752 root         INFO     Train Epoch: 163 [7680/8000 (96%)]\tTotal Loss: 0.107809\n",
      "Reconstruction: 0.101070, Regularization: 0.006739\n",
      "2019-04-10 00:06:08,805 root         INFO     ====> Epoch: 163 Average loss: 0.1122\n",
      "2019-04-10 00:06:08,829 root         INFO     Train Epoch: 164 [0/8000 (0%)]\tTotal Loss: 0.117940\n",
      "Reconstruction: 0.107680, Regularization: 0.010260\n",
      "2019-04-10 00:06:08,891 root         INFO     Train Epoch: 164 [512/8000 (6%)]\tTotal Loss: 0.114693\n",
      "Reconstruction: 0.104838, Regularization: 0.009854\n",
      "2019-04-10 00:06:08,954 root         INFO     Train Epoch: 164 [1024/8000 (13%)]\tTotal Loss: 0.107072\n",
      "Reconstruction: 0.098337, Regularization: 0.008736\n",
      "2019-04-10 00:06:09,016 root         INFO     Train Epoch: 164 [1536/8000 (19%)]\tTotal Loss: 0.111088\n",
      "Reconstruction: 0.102292, Regularization: 0.008796\n",
      "2019-04-10 00:06:09,079 root         INFO     Train Epoch: 164 [2048/8000 (26%)]\tTotal Loss: 0.111682\n",
      "Reconstruction: 0.101943, Regularization: 0.009739\n",
      "2019-04-10 00:06:09,141 root         INFO     Train Epoch: 164 [2560/8000 (32%)]\tTotal Loss: 0.110595\n",
      "Reconstruction: 0.101703, Regularization: 0.008892\n",
      "2019-04-10 00:06:09,203 root         INFO     Train Epoch: 164 [3072/8000 (38%)]\tTotal Loss: 0.101884\n",
      "Reconstruction: 0.093602, Regularization: 0.008282\n",
      "2019-04-10 00:06:09,265 root         INFO     Train Epoch: 164 [3584/8000 (45%)]\tTotal Loss: 0.116567\n",
      "Reconstruction: 0.106706, Regularization: 0.009862\n",
      "2019-04-10 00:06:09,328 root         INFO     Train Epoch: 164 [4096/8000 (51%)]\tTotal Loss: 0.103558\n",
      "Reconstruction: 0.095947, Regularization: 0.007611\n",
      "2019-04-10 00:06:09,390 root         INFO     Train Epoch: 164 [4608/8000 (58%)]\tTotal Loss: 0.106297\n",
      "Reconstruction: 0.097758, Regularization: 0.008539\n",
      "2019-04-10 00:06:09,453 root         INFO     Train Epoch: 164 [5120/8000 (64%)]\tTotal Loss: 0.108526\n",
      "Reconstruction: 0.098334, Regularization: 0.010192\n",
      "2019-04-10 00:06:09,515 root         INFO     Train Epoch: 164 [5632/8000 (70%)]\tTotal Loss: 0.110183\n",
      "Reconstruction: 0.101882, Regularization: 0.008301\n",
      "2019-04-10 00:06:09,577 root         INFO     Train Epoch: 164 [6144/8000 (77%)]\tTotal Loss: 0.116230\n",
      "Reconstruction: 0.106502, Regularization: 0.009728\n",
      "2019-04-10 00:06:09,640 root         INFO     Train Epoch: 164 [6656/8000 (83%)]\tTotal Loss: 0.121288\n",
      "Reconstruction: 0.106858, Regularization: 0.014430\n",
      "2019-04-10 00:06:09,702 root         INFO     Train Epoch: 164 [7168/8000 (90%)]\tTotal Loss: 0.109245\n",
      "Reconstruction: 0.098561, Regularization: 0.010683\n",
      "2019-04-10 00:06:09,764 root         INFO     Train Epoch: 164 [7680/8000 (96%)]\tTotal Loss: 0.099787\n",
      "Reconstruction: 0.091382, Regularization: 0.008405\n",
      "2019-04-10 00:06:09,818 root         INFO     ====> Epoch: 164 Average loss: 0.1123\n",
      "2019-04-10 00:06:09,842 root         INFO     Train Epoch: 165 [0/8000 (0%)]\tTotal Loss: 0.112315\n",
      "Reconstruction: 0.104103, Regularization: 0.008211\n",
      "2019-04-10 00:06:09,904 root         INFO     Train Epoch: 165 [512/8000 (6%)]\tTotal Loss: 0.108916\n",
      "Reconstruction: 0.100886, Regularization: 0.008030\n",
      "2019-04-10 00:06:09,967 root         INFO     Train Epoch: 165 [1024/8000 (13%)]\tTotal Loss: 0.113286\n",
      "Reconstruction: 0.103928, Regularization: 0.009358\n",
      "2019-04-10 00:06:10,029 root         INFO     Train Epoch: 165 [1536/8000 (19%)]\tTotal Loss: 0.120979\n",
      "Reconstruction: 0.109632, Regularization: 0.011347\n",
      "2019-04-10 00:06:10,091 root         INFO     Train Epoch: 165 [2048/8000 (26%)]\tTotal Loss: 0.112772\n",
      "Reconstruction: 0.103410, Regularization: 0.009363\n",
      "2019-04-10 00:06:10,154 root         INFO     Train Epoch: 165 [2560/8000 (32%)]\tTotal Loss: 0.101650\n",
      "Reconstruction: 0.093067, Regularization: 0.008583\n",
      "2019-04-10 00:06:10,216 root         INFO     Train Epoch: 165 [3072/8000 (38%)]\tTotal Loss: 0.102015\n",
      "Reconstruction: 0.092107, Regularization: 0.009907\n",
      "2019-04-10 00:06:10,278 root         INFO     Train Epoch: 165 [3584/8000 (45%)]\tTotal Loss: 0.108160\n",
      "Reconstruction: 0.099019, Regularization: 0.009141\n",
      "2019-04-10 00:06:10,341 root         INFO     Train Epoch: 165 [4096/8000 (51%)]\tTotal Loss: 0.113436\n",
      "Reconstruction: 0.102476, Regularization: 0.010960\n",
      "2019-04-10 00:06:10,403 root         INFO     Train Epoch: 165 [4608/8000 (58%)]\tTotal Loss: 0.117473\n",
      "Reconstruction: 0.106669, Regularization: 0.010804\n",
      "2019-04-10 00:06:10,464 root         INFO     Train Epoch: 165 [5120/8000 (64%)]\tTotal Loss: 0.121685\n",
      "Reconstruction: 0.111650, Regularization: 0.010034\n",
      "2019-04-10 00:06:10,527 root         INFO     Train Epoch: 165 [5632/8000 (70%)]\tTotal Loss: 0.104026\n",
      "Reconstruction: 0.096027, Regularization: 0.007998\n",
      "2019-04-10 00:06:10,590 root         INFO     Train Epoch: 165 [6144/8000 (77%)]\tTotal Loss: 0.113037\n",
      "Reconstruction: 0.101364, Regularization: 0.011673\n",
      "2019-04-10 00:06:10,652 root         INFO     Train Epoch: 165 [6656/8000 (83%)]\tTotal Loss: 0.107032\n",
      "Reconstruction: 0.099079, Regularization: 0.007953\n",
      "2019-04-10 00:06:10,715 root         INFO     Train Epoch: 165 [7168/8000 (90%)]\tTotal Loss: 0.102702\n",
      "Reconstruction: 0.094469, Regularization: 0.008233\n",
      "2019-04-10 00:06:10,778 root         INFO     Train Epoch: 165 [7680/8000 (96%)]\tTotal Loss: 0.113595\n",
      "Reconstruction: 0.102624, Regularization: 0.010971\n",
      "2019-04-10 00:06:10,832 root         INFO     ====> Epoch: 165 Average loss: 0.1127\n",
      "2019-04-10 00:06:10,856 root         INFO     Train Epoch: 166 [0/8000 (0%)]\tTotal Loss: 0.104162\n",
      "Reconstruction: 0.095503, Regularization: 0.008659\n",
      "2019-04-10 00:06:10,920 root         INFO     Train Epoch: 166 [512/8000 (6%)]\tTotal Loss: 0.114038\n",
      "Reconstruction: 0.103085, Regularization: 0.010953\n",
      "2019-04-10 00:06:10,982 root         INFO     Train Epoch: 166 [1024/8000 (13%)]\tTotal Loss: 0.103499\n",
      "Reconstruction: 0.094470, Regularization: 0.009029\n",
      "2019-04-10 00:06:11,044 root         INFO     Train Epoch: 166 [1536/8000 (19%)]\tTotal Loss: 0.105566\n",
      "Reconstruction: 0.095415, Regularization: 0.010150\n",
      "2019-04-10 00:06:11,105 root         INFO     Train Epoch: 166 [2048/8000 (26%)]\tTotal Loss: 0.122506\n",
      "Reconstruction: 0.110039, Regularization: 0.012467\n",
      "2019-04-10 00:06:11,166 root         INFO     Train Epoch: 166 [2560/8000 (32%)]\tTotal Loss: 0.113239\n",
      "Reconstruction: 0.105394, Regularization: 0.007844\n",
      "2019-04-10 00:06:11,226 root         INFO     Train Epoch: 166 [3072/8000 (38%)]\tTotal Loss: 0.107947\n",
      "Reconstruction: 0.099960, Regularization: 0.007986\n",
      "2019-04-10 00:06:11,288 root         INFO     Train Epoch: 166 [3584/8000 (45%)]\tTotal Loss: 0.105702\n",
      "Reconstruction: 0.094082, Regularization: 0.011620\n",
      "2019-04-10 00:06:11,355 root         INFO     Train Epoch: 166 [4096/8000 (51%)]\tTotal Loss: 0.109294\n",
      "Reconstruction: 0.099265, Regularization: 0.010029\n",
      "2019-04-10 00:06:11,418 root         INFO     Train Epoch: 166 [4608/8000 (58%)]\tTotal Loss: 0.108802\n",
      "Reconstruction: 0.098670, Regularization: 0.010132\n",
      "2019-04-10 00:06:11,482 root         INFO     Train Epoch: 166 [5120/8000 (64%)]\tTotal Loss: 0.116259\n",
      "Reconstruction: 0.103799, Regularization: 0.012460\n",
      "2019-04-10 00:06:11,546 root         INFO     Train Epoch: 166 [5632/8000 (70%)]\tTotal Loss: 0.110647\n",
      "Reconstruction: 0.101309, Regularization: 0.009338\n",
      "2019-04-10 00:06:11,609 root         INFO     Train Epoch: 166 [6144/8000 (77%)]\tTotal Loss: 0.108611\n",
      "Reconstruction: 0.099691, Regularization: 0.008919\n",
      "2019-04-10 00:06:11,674 root         INFO     Train Epoch: 166 [6656/8000 (83%)]\tTotal Loss: 0.110228\n",
      "Reconstruction: 0.100295, Regularization: 0.009932\n",
      "2019-04-10 00:06:11,739 root         INFO     Train Epoch: 166 [7168/8000 (90%)]\tTotal Loss: 0.120257\n",
      "Reconstruction: 0.109395, Regularization: 0.010862\n",
      "2019-04-10 00:06:11,803 root         INFO     Train Epoch: 166 [7680/8000 (96%)]\tTotal Loss: 0.106470\n",
      "Reconstruction: 0.097118, Regularization: 0.009352\n",
      "2019-04-10 00:06:11,857 root         INFO     ====> Epoch: 166 Average loss: 0.1121\n",
      "2019-04-10 00:06:11,881 root         INFO     Train Epoch: 167 [0/8000 (0%)]\tTotal Loss: 0.109161\n",
      "Reconstruction: 0.100055, Regularization: 0.009106\n",
      "2019-04-10 00:06:11,944 root         INFO     Train Epoch: 167 [512/8000 (6%)]\tTotal Loss: 0.108626\n",
      "Reconstruction: 0.098934, Regularization: 0.009692\n",
      "2019-04-10 00:06:12,007 root         INFO     Train Epoch: 167 [1024/8000 (13%)]\tTotal Loss: 0.111209\n",
      "Reconstruction: 0.102511, Regularization: 0.008698\n",
      "2019-04-10 00:06:12,070 root         INFO     Train Epoch: 167 [1536/8000 (19%)]\tTotal Loss: 0.105763\n",
      "Reconstruction: 0.096356, Regularization: 0.009408\n",
      "2019-04-10 00:06:12,133 root         INFO     Train Epoch: 167 [2048/8000 (26%)]\tTotal Loss: 0.106054\n",
      "Reconstruction: 0.097625, Regularization: 0.008429\n",
      "2019-04-10 00:06:12,196 root         INFO     Train Epoch: 167 [2560/8000 (32%)]\tTotal Loss: 0.099363\n",
      "Reconstruction: 0.090957, Regularization: 0.008406\n",
      "2019-04-10 00:06:12,259 root         INFO     Train Epoch: 167 [3072/8000 (38%)]\tTotal Loss: 0.107659\n",
      "Reconstruction: 0.097936, Regularization: 0.009723\n",
      "2019-04-10 00:06:12,322 root         INFO     Train Epoch: 167 [3584/8000 (45%)]\tTotal Loss: 0.119327\n",
      "Reconstruction: 0.108919, Regularization: 0.010408\n",
      "2019-04-10 00:06:12,385 root         INFO     Train Epoch: 167 [4096/8000 (51%)]\tTotal Loss: 0.120145\n",
      "Reconstruction: 0.109294, Regularization: 0.010852\n",
      "2019-04-10 00:06:12,449 root         INFO     Train Epoch: 167 [4608/8000 (58%)]\tTotal Loss: 0.111430\n",
      "Reconstruction: 0.101657, Regularization: 0.009773\n",
      "2019-04-10 00:06:12,512 root         INFO     Train Epoch: 167 [5120/8000 (64%)]\tTotal Loss: 0.109783\n",
      "Reconstruction: 0.100506, Regularization: 0.009276\n",
      "2019-04-10 00:06:12,575 root         INFO     Train Epoch: 167 [5632/8000 (70%)]\tTotal Loss: 0.116984\n",
      "Reconstruction: 0.106762, Regularization: 0.010222\n",
      "2019-04-10 00:06:12,639 root         INFO     Train Epoch: 167 [6144/8000 (77%)]\tTotal Loss: 0.123154\n",
      "Reconstruction: 0.110949, Regularization: 0.012205\n",
      "2019-04-10 00:06:12,702 root         INFO     Train Epoch: 167 [6656/8000 (83%)]\tTotal Loss: 0.120628\n",
      "Reconstruction: 0.110051, Regularization: 0.010577\n",
      "2019-04-10 00:06:12,765 root         INFO     Train Epoch: 167 [7168/8000 (90%)]\tTotal Loss: 0.115908\n",
      "Reconstruction: 0.103661, Regularization: 0.012247\n",
      "2019-04-10 00:06:12,828 root         INFO     Train Epoch: 167 [7680/8000 (96%)]\tTotal Loss: 0.115430\n",
      "Reconstruction: 0.104749, Regularization: 0.010681\n",
      "2019-04-10 00:06:12,882 root         INFO     ====> Epoch: 167 Average loss: 0.1123\n",
      "2019-04-10 00:06:12,906 root         INFO     Train Epoch: 168 [0/8000 (0%)]\tTotal Loss: 0.109776\n",
      "Reconstruction: 0.101589, Regularization: 0.008187\n",
      "2019-04-10 00:06:12,970 root         INFO     Train Epoch: 168 [512/8000 (6%)]\tTotal Loss: 0.117952\n",
      "Reconstruction: 0.107627, Regularization: 0.010326\n",
      "2019-04-10 00:06:13,033 root         INFO     Train Epoch: 168 [1024/8000 (13%)]\tTotal Loss: 0.100349\n",
      "Reconstruction: 0.092861, Regularization: 0.007488\n",
      "2019-04-10 00:06:13,097 root         INFO     Train Epoch: 168 [1536/8000 (19%)]\tTotal Loss: 0.126527\n",
      "Reconstruction: 0.114442, Regularization: 0.012084\n",
      "2019-04-10 00:06:13,160 root         INFO     Train Epoch: 168 [2048/8000 (26%)]\tTotal Loss: 0.110469\n",
      "Reconstruction: 0.101602, Regularization: 0.008867\n",
      "2019-04-10 00:06:13,224 root         INFO     Train Epoch: 168 [2560/8000 (32%)]\tTotal Loss: 0.117976\n",
      "Reconstruction: 0.106922, Regularization: 0.011054\n",
      "2019-04-10 00:06:13,288 root         INFO     Train Epoch: 168 [3072/8000 (38%)]\tTotal Loss: 0.116787\n",
      "Reconstruction: 0.105242, Regularization: 0.011545\n",
      "2019-04-10 00:06:13,351 root         INFO     Train Epoch: 168 [3584/8000 (45%)]\tTotal Loss: 0.116944\n",
      "Reconstruction: 0.106033, Regularization: 0.010911\n",
      "2019-04-10 00:06:13,415 root         INFO     Train Epoch: 168 [4096/8000 (51%)]\tTotal Loss: 0.107019\n",
      "Reconstruction: 0.098267, Regularization: 0.008752\n",
      "2019-04-10 00:06:13,478 root         INFO     Train Epoch: 168 [4608/8000 (58%)]\tTotal Loss: 0.107809\n",
      "Reconstruction: 0.096024, Regularization: 0.011786\n",
      "2019-04-10 00:06:13,541 root         INFO     Train Epoch: 168 [5120/8000 (64%)]\tTotal Loss: 0.104889\n",
      "Reconstruction: 0.097127, Regularization: 0.007762\n",
      "2019-04-10 00:06:13,605 root         INFO     Train Epoch: 168 [5632/8000 (70%)]\tTotal Loss: 0.126988\n",
      "Reconstruction: 0.112683, Regularization: 0.014305\n",
      "2019-04-10 00:06:13,669 root         INFO     Train Epoch: 168 [6144/8000 (77%)]\tTotal Loss: 0.109854\n",
      "Reconstruction: 0.097419, Regularization: 0.012435\n",
      "2019-04-10 00:06:13,732 root         INFO     Train Epoch: 168 [6656/8000 (83%)]\tTotal Loss: 0.110244\n",
      "Reconstruction: 0.098725, Regularization: 0.011520\n",
      "2019-04-10 00:06:13,794 root         INFO     Train Epoch: 168 [7168/8000 (90%)]\tTotal Loss: 0.113404\n",
      "Reconstruction: 0.102724, Regularization: 0.010679\n",
      "2019-04-10 00:06:13,856 root         INFO     Train Epoch: 168 [7680/8000 (96%)]\tTotal Loss: 0.119048\n",
      "Reconstruction: 0.105992, Regularization: 0.013056\n",
      "2019-04-10 00:06:13,909 root         INFO     ====> Epoch: 168 Average loss: 0.1121\n",
      "2019-04-10 00:06:13,933 root         INFO     Train Epoch: 169 [0/8000 (0%)]\tTotal Loss: 0.102440\n",
      "Reconstruction: 0.095732, Regularization: 0.006708\n",
      "2019-04-10 00:06:13,997 root         INFO     Train Epoch: 169 [512/8000 (6%)]\tTotal Loss: 0.112792\n",
      "Reconstruction: 0.103581, Regularization: 0.009211\n",
      "2019-04-10 00:06:14,060 root         INFO     Train Epoch: 169 [1024/8000 (13%)]\tTotal Loss: 0.105163\n",
      "Reconstruction: 0.095591, Regularization: 0.009572\n",
      "2019-04-10 00:06:14,123 root         INFO     Train Epoch: 169 [1536/8000 (19%)]\tTotal Loss: 0.107904\n",
      "Reconstruction: 0.096938, Regularization: 0.010966\n",
      "2019-04-10 00:06:14,186 root         INFO     Train Epoch: 169 [2048/8000 (26%)]\tTotal Loss: 0.112503\n",
      "Reconstruction: 0.101968, Regularization: 0.010534\n",
      "2019-04-10 00:06:14,248 root         INFO     Train Epoch: 169 [2560/8000 (32%)]\tTotal Loss: 0.110004\n",
      "Reconstruction: 0.100756, Regularization: 0.009248\n",
      "2019-04-10 00:06:14,312 root         INFO     Train Epoch: 169 [3072/8000 (38%)]\tTotal Loss: 0.116155\n",
      "Reconstruction: 0.104614, Regularization: 0.011540\n",
      "2019-04-10 00:06:14,376 root         INFO     Train Epoch: 169 [3584/8000 (45%)]\tTotal Loss: 0.107078\n",
      "Reconstruction: 0.096642, Regularization: 0.010435\n",
      "2019-04-10 00:06:14,439 root         INFO     Train Epoch: 169 [4096/8000 (51%)]\tTotal Loss: 0.116870\n",
      "Reconstruction: 0.104449, Regularization: 0.012420\n",
      "2019-04-10 00:06:14,503 root         INFO     Train Epoch: 169 [4608/8000 (58%)]\tTotal Loss: 0.105023\n",
      "Reconstruction: 0.093795, Regularization: 0.011228\n",
      "2019-04-10 00:06:14,566 root         INFO     Train Epoch: 169 [5120/8000 (64%)]\tTotal Loss: 0.121937\n",
      "Reconstruction: 0.108017, Regularization: 0.013920\n",
      "2019-04-10 00:06:14,630 root         INFO     Train Epoch: 169 [5632/8000 (70%)]\tTotal Loss: 0.112953\n",
      "Reconstruction: 0.103888, Regularization: 0.009065\n",
      "2019-04-10 00:06:14,692 root         INFO     Train Epoch: 169 [6144/8000 (77%)]\tTotal Loss: 0.111415\n",
      "Reconstruction: 0.102408, Regularization: 0.009008\n",
      "2019-04-10 00:06:14,755 root         INFO     Train Epoch: 169 [6656/8000 (83%)]\tTotal Loss: 0.109932\n",
      "Reconstruction: 0.102123, Regularization: 0.007809\n",
      "2019-04-10 00:06:14,817 root         INFO     Train Epoch: 169 [7168/8000 (90%)]\tTotal Loss: 0.120965\n",
      "Reconstruction: 0.105521, Regularization: 0.015444\n",
      "2019-04-10 00:06:14,879 root         INFO     Train Epoch: 169 [7680/8000 (96%)]\tTotal Loss: 0.106304\n",
      "Reconstruction: 0.096316, Regularization: 0.009988\n",
      "2019-04-10 00:06:14,933 root         INFO     ====> Epoch: 169 Average loss: 0.1122\n",
      "2019-04-10 00:06:14,957 root         INFO     Train Epoch: 170 [0/8000 (0%)]\tTotal Loss: 0.107501\n",
      "Reconstruction: 0.098868, Regularization: 0.008633\n",
      "2019-04-10 00:06:15,021 root         INFO     Train Epoch: 170 [512/8000 (6%)]\tTotal Loss: 0.108354\n",
      "Reconstruction: 0.096913, Regularization: 0.011441\n",
      "2019-04-10 00:06:15,084 root         INFO     Train Epoch: 170 [1024/8000 (13%)]\tTotal Loss: 0.107634\n",
      "Reconstruction: 0.097739, Regularization: 0.009895\n",
      "2019-04-10 00:06:15,147 root         INFO     Train Epoch: 170 [1536/8000 (19%)]\tTotal Loss: 0.113483\n",
      "Reconstruction: 0.102858, Regularization: 0.010625\n",
      "2019-04-10 00:06:15,211 root         INFO     Train Epoch: 170 [2048/8000 (26%)]\tTotal Loss: 0.111393\n",
      "Reconstruction: 0.101288, Regularization: 0.010106\n",
      "2019-04-10 00:06:15,274 root         INFO     Train Epoch: 170 [2560/8000 (32%)]\tTotal Loss: 0.116341\n",
      "Reconstruction: 0.105639, Regularization: 0.010702\n",
      "2019-04-10 00:06:15,338 root         INFO     Train Epoch: 170 [3072/8000 (38%)]\tTotal Loss: 0.119685\n",
      "Reconstruction: 0.108529, Regularization: 0.011155\n",
      "2019-04-10 00:06:15,402 root         INFO     Train Epoch: 170 [3584/8000 (45%)]\tTotal Loss: 0.110402\n",
      "Reconstruction: 0.099015, Regularization: 0.011387\n",
      "2019-04-10 00:06:15,466 root         INFO     Train Epoch: 170 [4096/8000 (51%)]\tTotal Loss: 0.105749\n",
      "Reconstruction: 0.096756, Regularization: 0.008993\n",
      "2019-04-10 00:06:15,529 root         INFO     Train Epoch: 170 [4608/8000 (58%)]\tTotal Loss: 0.111350\n",
      "Reconstruction: 0.101043, Regularization: 0.010306\n",
      "2019-04-10 00:06:15,592 root         INFO     Train Epoch: 170 [5120/8000 (64%)]\tTotal Loss: 0.113068\n",
      "Reconstruction: 0.102330, Regularization: 0.010737\n",
      "2019-04-10 00:06:15,655 root         INFO     Train Epoch: 170 [5632/8000 (70%)]\tTotal Loss: 0.112811\n",
      "Reconstruction: 0.102222, Regularization: 0.010589\n",
      "2019-04-10 00:06:15,719 root         INFO     Train Epoch: 170 [6144/8000 (77%)]\tTotal Loss: 0.110813\n",
      "Reconstruction: 0.101729, Regularization: 0.009084\n",
      "2019-04-10 00:06:15,783 root         INFO     Train Epoch: 170 [6656/8000 (83%)]\tTotal Loss: 0.107767\n",
      "Reconstruction: 0.097019, Regularization: 0.010748\n",
      "2019-04-10 00:06:15,846 root         INFO     Train Epoch: 170 [7168/8000 (90%)]\tTotal Loss: 0.116148\n",
      "Reconstruction: 0.105835, Regularization: 0.010313\n",
      "2019-04-10 00:06:15,910 root         INFO     Train Epoch: 170 [7680/8000 (96%)]\tTotal Loss: 0.118501\n",
      "Reconstruction: 0.105820, Regularization: 0.012682\n",
      "2019-04-10 00:06:15,964 root         INFO     ====> Epoch: 170 Average loss: 0.1119\n",
      "2019-04-10 00:06:15,988 root         INFO     Train Epoch: 171 [0/8000 (0%)]\tTotal Loss: 0.120471\n",
      "Reconstruction: 0.108944, Regularization: 0.011527\n",
      "2019-04-10 00:06:16,051 root         INFO     Train Epoch: 171 [512/8000 (6%)]\tTotal Loss: 0.102719\n",
      "Reconstruction: 0.092486, Regularization: 0.010233\n",
      "2019-04-10 00:06:16,115 root         INFO     Train Epoch: 171 [1024/8000 (13%)]\tTotal Loss: 0.115891\n",
      "Reconstruction: 0.104566, Regularization: 0.011326\n",
      "2019-04-10 00:06:16,179 root         INFO     Train Epoch: 171 [1536/8000 (19%)]\tTotal Loss: 0.119891\n",
      "Reconstruction: 0.108149, Regularization: 0.011742\n",
      "2019-04-10 00:06:16,243 root         INFO     Train Epoch: 171 [2048/8000 (26%)]\tTotal Loss: 0.110295\n",
      "Reconstruction: 0.097718, Regularization: 0.012576\n",
      "2019-04-10 00:06:16,306 root         INFO     Train Epoch: 171 [2560/8000 (32%)]\tTotal Loss: 0.127821\n",
      "Reconstruction: 0.113989, Regularization: 0.013833\n",
      "2019-04-10 00:06:16,371 root         INFO     Train Epoch: 171 [3072/8000 (38%)]\tTotal Loss: 0.104787\n",
      "Reconstruction: 0.094707, Regularization: 0.010080\n",
      "2019-04-10 00:06:16,435 root         INFO     Train Epoch: 171 [3584/8000 (45%)]\tTotal Loss: 0.109187\n",
      "Reconstruction: 0.097080, Regularization: 0.012106\n",
      "2019-04-10 00:06:16,499 root         INFO     Train Epoch: 171 [4096/8000 (51%)]\tTotal Loss: 0.116463\n",
      "Reconstruction: 0.103766, Regularization: 0.012697\n",
      "2019-04-10 00:06:16,563 root         INFO     Train Epoch: 171 [4608/8000 (58%)]\tTotal Loss: 0.114049\n",
      "Reconstruction: 0.103490, Regularization: 0.010559\n",
      "2019-04-10 00:06:16,627 root         INFO     Train Epoch: 171 [5120/8000 (64%)]\tTotal Loss: 0.107805\n",
      "Reconstruction: 0.098599, Regularization: 0.009206\n",
      "2019-04-10 00:06:16,691 root         INFO     Train Epoch: 171 [5632/8000 (70%)]\tTotal Loss: 0.115519\n",
      "Reconstruction: 0.106424, Regularization: 0.009096\n",
      "2019-04-10 00:06:16,755 root         INFO     Train Epoch: 171 [6144/8000 (77%)]\tTotal Loss: 0.115968\n",
      "Reconstruction: 0.105290, Regularization: 0.010678\n",
      "2019-04-10 00:06:16,819 root         INFO     Train Epoch: 171 [6656/8000 (83%)]\tTotal Loss: 0.107130\n",
      "Reconstruction: 0.095939, Regularization: 0.011190\n",
      "2019-04-10 00:06:16,883 root         INFO     Train Epoch: 171 [7168/8000 (90%)]\tTotal Loss: 0.118763\n",
      "Reconstruction: 0.108178, Regularization: 0.010585\n",
      "2019-04-10 00:06:16,947 root         INFO     Train Epoch: 171 [7680/8000 (96%)]\tTotal Loss: 0.103310\n",
      "Reconstruction: 0.095395, Regularization: 0.007914\n",
      "2019-04-10 00:06:17,002 root         INFO     ====> Epoch: 171 Average loss: 0.1123\n",
      "2019-04-10 00:06:17,026 root         INFO     Train Epoch: 172 [0/8000 (0%)]\tTotal Loss: 0.106757\n",
      "Reconstruction: 0.096532, Regularization: 0.010225\n",
      "2019-04-10 00:06:17,090 root         INFO     Train Epoch: 172 [512/8000 (6%)]\tTotal Loss: 0.121221\n",
      "Reconstruction: 0.110581, Regularization: 0.010641\n",
      "2019-04-10 00:06:17,155 root         INFO     Train Epoch: 172 [1024/8000 (13%)]\tTotal Loss: 0.114356\n",
      "Reconstruction: 0.104350, Regularization: 0.010005\n",
      "2019-04-10 00:06:17,218 root         INFO     Train Epoch: 172 [1536/8000 (19%)]\tTotal Loss: 0.115520\n",
      "Reconstruction: 0.105211, Regularization: 0.010309\n",
      "2019-04-10 00:06:17,282 root         INFO     Train Epoch: 172 [2048/8000 (26%)]\tTotal Loss: 0.114337\n",
      "Reconstruction: 0.101998, Regularization: 0.012339\n",
      "2019-04-10 00:06:17,346 root         INFO     Train Epoch: 172 [2560/8000 (32%)]\tTotal Loss: 0.142997\n",
      "Reconstruction: 0.130919, Regularization: 0.012078\n",
      "2019-04-10 00:06:17,410 root         INFO     Train Epoch: 172 [3072/8000 (38%)]\tTotal Loss: 0.113011\n",
      "Reconstruction: 0.103671, Regularization: 0.009340\n",
      "2019-04-10 00:06:17,474 root         INFO     Train Epoch: 172 [3584/8000 (45%)]\tTotal Loss: 0.114460\n",
      "Reconstruction: 0.103451, Regularization: 0.011009\n",
      "2019-04-10 00:06:17,537 root         INFO     Train Epoch: 172 [4096/8000 (51%)]\tTotal Loss: 0.106823\n",
      "Reconstruction: 0.097810, Regularization: 0.009013\n",
      "2019-04-10 00:06:17,601 root         INFO     Train Epoch: 172 [4608/8000 (58%)]\tTotal Loss: 0.103021\n",
      "Reconstruction: 0.092591, Regularization: 0.010431\n",
      "2019-04-10 00:06:17,663 root         INFO     Train Epoch: 172 [5120/8000 (64%)]\tTotal Loss: 0.111761\n",
      "Reconstruction: 0.100548, Regularization: 0.011213\n",
      "2019-04-10 00:06:17,725 root         INFO     Train Epoch: 172 [5632/8000 (70%)]\tTotal Loss: 0.103343\n",
      "Reconstruction: 0.096274, Regularization: 0.007070\n",
      "2019-04-10 00:06:17,786 root         INFO     Train Epoch: 172 [6144/8000 (77%)]\tTotal Loss: 0.101367\n",
      "Reconstruction: 0.093446, Regularization: 0.007920\n",
      "2019-04-10 00:06:17,847 root         INFO     Train Epoch: 172 [6656/8000 (83%)]\tTotal Loss: 0.109354\n",
      "Reconstruction: 0.100021, Regularization: 0.009333\n",
      "2019-04-10 00:06:17,910 root         INFO     Train Epoch: 172 [7168/8000 (90%)]\tTotal Loss: 0.105954\n",
      "Reconstruction: 0.095229, Regularization: 0.010725\n",
      "2019-04-10 00:06:17,973 root         INFO     Train Epoch: 172 [7680/8000 (96%)]\tTotal Loss: 0.116062\n",
      "Reconstruction: 0.105708, Regularization: 0.010355\n",
      "2019-04-10 00:06:18,027 root         INFO     ====> Epoch: 172 Average loss: 0.1125\n",
      "2019-04-10 00:06:18,051 root         INFO     Train Epoch: 173 [0/8000 (0%)]\tTotal Loss: 0.121426\n",
      "Reconstruction: 0.108339, Regularization: 0.013087\n",
      "2019-04-10 00:06:18,113 root         INFO     Train Epoch: 173 [512/8000 (6%)]\tTotal Loss: 0.108449\n",
      "Reconstruction: 0.097892, Regularization: 0.010557\n",
      "2019-04-10 00:06:18,174 root         INFO     Train Epoch: 173 [1024/8000 (13%)]\tTotal Loss: 0.117539\n",
      "Reconstruction: 0.105325, Regularization: 0.012214\n",
      "2019-04-10 00:06:18,236 root         INFO     Train Epoch: 173 [1536/8000 (19%)]\tTotal Loss: 0.109625\n",
      "Reconstruction: 0.099240, Regularization: 0.010385\n",
      "2019-04-10 00:06:18,297 root         INFO     Train Epoch: 173 [2048/8000 (26%)]\tTotal Loss: 0.118279\n",
      "Reconstruction: 0.103799, Regularization: 0.014480\n",
      "2019-04-10 00:06:18,359 root         INFO     Train Epoch: 173 [2560/8000 (32%)]\tTotal Loss: 0.114737\n",
      "Reconstruction: 0.104137, Regularization: 0.010600\n",
      "2019-04-10 00:06:18,421 root         INFO     Train Epoch: 173 [3072/8000 (38%)]\tTotal Loss: 0.114995\n",
      "Reconstruction: 0.102825, Regularization: 0.012170\n",
      "2019-04-10 00:06:18,482 root         INFO     Train Epoch: 173 [3584/8000 (45%)]\tTotal Loss: 0.106375\n",
      "Reconstruction: 0.095239, Regularization: 0.011136\n",
      "2019-04-10 00:06:18,544 root         INFO     Train Epoch: 173 [4096/8000 (51%)]\tTotal Loss: 0.099464\n",
      "Reconstruction: 0.090980, Regularization: 0.008484\n",
      "2019-04-10 00:06:18,606 root         INFO     Train Epoch: 173 [4608/8000 (58%)]\tTotal Loss: 0.113446\n",
      "Reconstruction: 0.101043, Regularization: 0.012403\n",
      "2019-04-10 00:06:18,668 root         INFO     Train Epoch: 173 [5120/8000 (64%)]\tTotal Loss: 0.102307\n",
      "Reconstruction: 0.093541, Regularization: 0.008766\n",
      "2019-04-10 00:06:18,730 root         INFO     Train Epoch: 173 [5632/8000 (70%)]\tTotal Loss: 0.115964\n",
      "Reconstruction: 0.104577, Regularization: 0.011387\n",
      "2019-04-10 00:06:18,792 root         INFO     Train Epoch: 173 [6144/8000 (77%)]\tTotal Loss: 0.107788\n",
      "Reconstruction: 0.100411, Regularization: 0.007376\n",
      "2019-04-10 00:06:18,854 root         INFO     Train Epoch: 173 [6656/8000 (83%)]\tTotal Loss: 0.117996\n",
      "Reconstruction: 0.106803, Regularization: 0.011194\n",
      "2019-04-10 00:06:18,916 root         INFO     Train Epoch: 173 [7168/8000 (90%)]\tTotal Loss: 0.110670\n",
      "Reconstruction: 0.100554, Regularization: 0.010116\n",
      "2019-04-10 00:06:18,977 root         INFO     Train Epoch: 173 [7680/8000 (96%)]\tTotal Loss: 0.107678\n",
      "Reconstruction: 0.096933, Regularization: 0.010745\n",
      "2019-04-10 00:06:19,029 root         INFO     ====> Epoch: 173 Average loss: 0.1119\n",
      "2019-04-10 00:06:19,053 root         INFO     Train Epoch: 174 [0/8000 (0%)]\tTotal Loss: 0.107947\n",
      "Reconstruction: 0.097998, Regularization: 0.009948\n",
      "2019-04-10 00:06:19,115 root         INFO     Train Epoch: 174 [512/8000 (6%)]\tTotal Loss: 0.109674\n",
      "Reconstruction: 0.101029, Regularization: 0.008645\n",
      "2019-04-10 00:06:19,177 root         INFO     Train Epoch: 174 [1024/8000 (13%)]\tTotal Loss: 0.112780\n",
      "Reconstruction: 0.101485, Regularization: 0.011294\n",
      "2019-04-10 00:06:19,238 root         INFO     Train Epoch: 174 [1536/8000 (19%)]\tTotal Loss: 0.117513\n",
      "Reconstruction: 0.104401, Regularization: 0.013113\n",
      "2019-04-10 00:06:19,300 root         INFO     Train Epoch: 174 [2048/8000 (26%)]\tTotal Loss: 0.112391\n",
      "Reconstruction: 0.102487, Regularization: 0.009904\n",
      "2019-04-10 00:06:19,361 root         INFO     Train Epoch: 174 [2560/8000 (32%)]\tTotal Loss: 0.113634\n",
      "Reconstruction: 0.102256, Regularization: 0.011378\n",
      "2019-04-10 00:06:19,423 root         INFO     Train Epoch: 174 [3072/8000 (38%)]\tTotal Loss: 0.104726\n",
      "Reconstruction: 0.095726, Regularization: 0.009000\n",
      "2019-04-10 00:06:19,484 root         INFO     Train Epoch: 174 [3584/8000 (45%)]\tTotal Loss: 0.103804\n",
      "Reconstruction: 0.094048, Regularization: 0.009757\n",
      "2019-04-10 00:06:19,546 root         INFO     Train Epoch: 174 [4096/8000 (51%)]\tTotal Loss: 0.121239\n",
      "Reconstruction: 0.108713, Regularization: 0.012526\n",
      "2019-04-10 00:06:19,607 root         INFO     Train Epoch: 174 [4608/8000 (58%)]\tTotal Loss: 0.122115\n",
      "Reconstruction: 0.109789, Regularization: 0.012327\n",
      "2019-04-10 00:06:19,669 root         INFO     Train Epoch: 174 [5120/8000 (64%)]\tTotal Loss: 0.112516\n",
      "Reconstruction: 0.100194, Regularization: 0.012322\n",
      "2019-04-10 00:06:19,730 root         INFO     Train Epoch: 174 [5632/8000 (70%)]\tTotal Loss: 0.105684\n",
      "Reconstruction: 0.096792, Regularization: 0.008892\n",
      "2019-04-10 00:06:19,792 root         INFO     Train Epoch: 174 [6144/8000 (77%)]\tTotal Loss: 0.118439\n",
      "Reconstruction: 0.105976, Regularization: 0.012463\n",
      "2019-04-10 00:06:19,853 root         INFO     Train Epoch: 174 [6656/8000 (83%)]\tTotal Loss: 0.104210\n",
      "Reconstruction: 0.094847, Regularization: 0.009363\n",
      "2019-04-10 00:06:19,914 root         INFO     Train Epoch: 174 [7168/8000 (90%)]\tTotal Loss: 0.119786\n",
      "Reconstruction: 0.105551, Regularization: 0.014235\n",
      "2019-04-10 00:06:19,976 root         INFO     Train Epoch: 174 [7680/8000 (96%)]\tTotal Loss: 0.109430\n",
      "Reconstruction: 0.098997, Regularization: 0.010433\n",
      "2019-04-10 00:06:20,028 root         INFO     ====> Epoch: 174 Average loss: 0.1123\n",
      "2019-04-10 00:06:20,052 root         INFO     Train Epoch: 175 [0/8000 (0%)]\tTotal Loss: 0.110792\n",
      "Reconstruction: 0.100152, Regularization: 0.010640\n",
      "2019-04-10 00:06:20,113 root         INFO     Train Epoch: 175 [512/8000 (6%)]\tTotal Loss: 0.111962\n",
      "Reconstruction: 0.101258, Regularization: 0.010704\n",
      "2019-04-10 00:06:20,175 root         INFO     Train Epoch: 175 [1024/8000 (13%)]\tTotal Loss: 0.116506\n",
      "Reconstruction: 0.105263, Regularization: 0.011243\n",
      "2019-04-10 00:06:20,237 root         INFO     Train Epoch: 175 [1536/8000 (19%)]\tTotal Loss: 0.109820\n",
      "Reconstruction: 0.099619, Regularization: 0.010200\n",
      "2019-04-10 00:06:20,299 root         INFO     Train Epoch: 175 [2048/8000 (26%)]\tTotal Loss: 0.127397\n",
      "Reconstruction: 0.111128, Regularization: 0.016269\n",
      "2019-04-10 00:06:20,360 root         INFO     Train Epoch: 175 [2560/8000 (32%)]\tTotal Loss: 0.117348\n",
      "Reconstruction: 0.104355, Regularization: 0.012994\n",
      "2019-04-10 00:06:20,423 root         INFO     Train Epoch: 175 [3072/8000 (38%)]\tTotal Loss: 0.109330\n",
      "Reconstruction: 0.098446, Regularization: 0.010884\n",
      "2019-04-10 00:06:20,484 root         INFO     Train Epoch: 175 [3584/8000 (45%)]\tTotal Loss: 0.120594\n",
      "Reconstruction: 0.108205, Regularization: 0.012389\n",
      "2019-04-10 00:06:20,546 root         INFO     Train Epoch: 175 [4096/8000 (51%)]\tTotal Loss: 0.118634\n",
      "Reconstruction: 0.105880, Regularization: 0.012754\n",
      "2019-04-10 00:06:20,608 root         INFO     Train Epoch: 175 [4608/8000 (58%)]\tTotal Loss: 0.116169\n",
      "Reconstruction: 0.105007, Regularization: 0.011162\n",
      "2019-04-10 00:06:20,671 root         INFO     Train Epoch: 175 [5120/8000 (64%)]\tTotal Loss: 0.112026\n",
      "Reconstruction: 0.100866, Regularization: 0.011160\n",
      "2019-04-10 00:06:20,733 root         INFO     Train Epoch: 175 [5632/8000 (70%)]\tTotal Loss: 0.114522\n",
      "Reconstruction: 0.104050, Regularization: 0.010472\n",
      "2019-04-10 00:06:20,796 root         INFO     Train Epoch: 175 [6144/8000 (77%)]\tTotal Loss: 0.119792\n",
      "Reconstruction: 0.107846, Regularization: 0.011946\n",
      "2019-04-10 00:06:20,858 root         INFO     Train Epoch: 175 [6656/8000 (83%)]\tTotal Loss: 0.108313\n",
      "Reconstruction: 0.099675, Regularization: 0.008638\n",
      "2019-04-10 00:06:20,921 root         INFO     Train Epoch: 175 [7168/8000 (90%)]\tTotal Loss: 0.119537\n",
      "Reconstruction: 0.108260, Regularization: 0.011276\n",
      "2019-04-10 00:06:20,984 root         INFO     Train Epoch: 175 [7680/8000 (96%)]\tTotal Loss: 0.109242\n",
      "Reconstruction: 0.098859, Regularization: 0.010384\n",
      "2019-04-10 00:06:21,037 root         INFO     ====> Epoch: 175 Average loss: 0.1119\n",
      "2019-04-10 00:06:21,060 root         INFO     Train Epoch: 176 [0/8000 (0%)]\tTotal Loss: 0.112973\n",
      "Reconstruction: 0.098933, Regularization: 0.014040\n",
      "2019-04-10 00:06:21,122 root         INFO     Train Epoch: 176 [512/8000 (6%)]\tTotal Loss: 0.108442\n",
      "Reconstruction: 0.096874, Regularization: 0.011569\n",
      "2019-04-10 00:06:21,183 root         INFO     Train Epoch: 176 [1024/8000 (13%)]\tTotal Loss: 0.121413\n",
      "Reconstruction: 0.109813, Regularization: 0.011599\n",
      "2019-04-10 00:06:21,244 root         INFO     Train Epoch: 176 [1536/8000 (19%)]\tTotal Loss: 0.126166\n",
      "Reconstruction: 0.108973, Regularization: 0.017193\n",
      "2019-04-10 00:06:21,305 root         INFO     Train Epoch: 176 [2048/8000 (26%)]\tTotal Loss: 0.106597\n",
      "Reconstruction: 0.098222, Regularization: 0.008375\n",
      "2019-04-10 00:06:21,367 root         INFO     Train Epoch: 176 [2560/8000 (32%)]\tTotal Loss: 0.110902\n",
      "Reconstruction: 0.100561, Regularization: 0.010341\n",
      "2019-04-10 00:06:21,428 root         INFO     Train Epoch: 176 [3072/8000 (38%)]\tTotal Loss: 0.111982\n",
      "Reconstruction: 0.099303, Regularization: 0.012679\n",
      "2019-04-10 00:06:21,488 root         INFO     Train Epoch: 176 [3584/8000 (45%)]\tTotal Loss: 0.105105\n",
      "Reconstruction: 0.095531, Regularization: 0.009574\n",
      "2019-04-10 00:06:21,550 root         INFO     Train Epoch: 176 [4096/8000 (51%)]\tTotal Loss: 0.110249\n",
      "Reconstruction: 0.100101, Regularization: 0.010148\n",
      "2019-04-10 00:06:21,614 root         INFO     Train Epoch: 176 [4608/8000 (58%)]\tTotal Loss: 0.108523\n",
      "Reconstruction: 0.096665, Regularization: 0.011858\n",
      "2019-04-10 00:06:21,677 root         INFO     Train Epoch: 176 [5120/8000 (64%)]\tTotal Loss: 0.113878\n",
      "Reconstruction: 0.104964, Regularization: 0.008914\n",
      "2019-04-10 00:06:21,740 root         INFO     Train Epoch: 176 [5632/8000 (70%)]\tTotal Loss: 0.109980\n",
      "Reconstruction: 0.098231, Regularization: 0.011749\n",
      "2019-04-10 00:06:21,802 root         INFO     Train Epoch: 176 [6144/8000 (77%)]\tTotal Loss: 0.117155\n",
      "Reconstruction: 0.103398, Regularization: 0.013757\n",
      "2019-04-10 00:06:21,864 root         INFO     Train Epoch: 176 [6656/8000 (83%)]\tTotal Loss: 0.120809\n",
      "Reconstruction: 0.109471, Regularization: 0.011339\n",
      "2019-04-10 00:06:21,926 root         INFO     Train Epoch: 176 [7168/8000 (90%)]\tTotal Loss: 0.107280\n",
      "Reconstruction: 0.094296, Regularization: 0.012984\n",
      "2019-04-10 00:06:21,988 root         INFO     Train Epoch: 176 [7680/8000 (96%)]\tTotal Loss: 0.108582\n",
      "Reconstruction: 0.099505, Regularization: 0.009077\n",
      "2019-04-10 00:06:22,041 root         INFO     ====> Epoch: 176 Average loss: 0.1121\n",
      "2019-04-10 00:06:22,066 root         INFO     Train Epoch: 177 [0/8000 (0%)]\tTotal Loss: 0.103861\n",
      "Reconstruction: 0.093405, Regularization: 0.010457\n",
      "2019-04-10 00:06:22,129 root         INFO     Train Epoch: 177 [512/8000 (6%)]\tTotal Loss: 0.112093\n",
      "Reconstruction: 0.098134, Regularization: 0.013959\n",
      "2019-04-10 00:06:22,192 root         INFO     Train Epoch: 177 [1024/8000 (13%)]\tTotal Loss: 0.121146\n",
      "Reconstruction: 0.110964, Regularization: 0.010182\n",
      "2019-04-10 00:06:22,255 root         INFO     Train Epoch: 177 [1536/8000 (19%)]\tTotal Loss: 0.116210\n",
      "Reconstruction: 0.105260, Regularization: 0.010950\n",
      "2019-04-10 00:06:22,318 root         INFO     Train Epoch: 177 [2048/8000 (26%)]\tTotal Loss: 0.114061\n",
      "Reconstruction: 0.102854, Regularization: 0.011207\n",
      "2019-04-10 00:06:22,381 root         INFO     Train Epoch: 177 [2560/8000 (32%)]\tTotal Loss: 0.109778\n",
      "Reconstruction: 0.098703, Regularization: 0.011075\n",
      "2019-04-10 00:06:22,444 root         INFO     Train Epoch: 177 [3072/8000 (38%)]\tTotal Loss: 0.125835\n",
      "Reconstruction: 0.110348, Regularization: 0.015487\n",
      "2019-04-10 00:06:22,507 root         INFO     Train Epoch: 177 [3584/8000 (45%)]\tTotal Loss: 0.112413\n",
      "Reconstruction: 0.100114, Regularization: 0.012299\n",
      "2019-04-10 00:06:22,570 root         INFO     Train Epoch: 177 [4096/8000 (51%)]\tTotal Loss: 0.120477\n",
      "Reconstruction: 0.107605, Regularization: 0.012872\n",
      "2019-04-10 00:06:22,632 root         INFO     Train Epoch: 177 [4608/8000 (58%)]\tTotal Loss: 0.109071\n",
      "Reconstruction: 0.100785, Regularization: 0.008286\n",
      "2019-04-10 00:06:22,695 root         INFO     Train Epoch: 177 [5120/8000 (64%)]\tTotal Loss: 0.108542\n",
      "Reconstruction: 0.099120, Regularization: 0.009423\n",
      "2019-04-10 00:06:22,758 root         INFO     Train Epoch: 177 [5632/8000 (70%)]\tTotal Loss: 0.117087\n",
      "Reconstruction: 0.104932, Regularization: 0.012155\n",
      "2019-04-10 00:06:22,821 root         INFO     Train Epoch: 177 [6144/8000 (77%)]\tTotal Loss: 0.118681\n",
      "Reconstruction: 0.106725, Regularization: 0.011957\n",
      "2019-04-10 00:06:22,884 root         INFO     Train Epoch: 177 [6656/8000 (83%)]\tTotal Loss: 0.113176\n",
      "Reconstruction: 0.101851, Regularization: 0.011325\n",
      "2019-04-10 00:06:22,948 root         INFO     Train Epoch: 177 [7168/8000 (90%)]\tTotal Loss: 0.114293\n",
      "Reconstruction: 0.102625, Regularization: 0.011668\n",
      "2019-04-10 00:06:23,010 root         INFO     Train Epoch: 177 [7680/8000 (96%)]\tTotal Loss: 0.108847\n",
      "Reconstruction: 0.099273, Regularization: 0.009573\n",
      "2019-04-10 00:06:23,063 root         INFO     ====> Epoch: 177 Average loss: 0.1123\n",
      "2019-04-10 00:06:23,088 root         INFO     Train Epoch: 178 [0/8000 (0%)]\tTotal Loss: 0.107795\n",
      "Reconstruction: 0.096590, Regularization: 0.011205\n",
      "2019-04-10 00:06:23,151 root         INFO     Train Epoch: 178 [512/8000 (6%)]\tTotal Loss: 0.117707\n",
      "Reconstruction: 0.105223, Regularization: 0.012485\n",
      "2019-04-10 00:06:23,214 root         INFO     Train Epoch: 178 [1024/8000 (13%)]\tTotal Loss: 0.110731\n",
      "Reconstruction: 0.100137, Regularization: 0.010594\n",
      "2019-04-10 00:06:23,277 root         INFO     Train Epoch: 178 [1536/8000 (19%)]\tTotal Loss: 0.121703\n",
      "Reconstruction: 0.107987, Regularization: 0.013716\n",
      "2019-04-10 00:06:23,341 root         INFO     Train Epoch: 178 [2048/8000 (26%)]\tTotal Loss: 0.106347\n",
      "Reconstruction: 0.095780, Regularization: 0.010567\n",
      "2019-04-10 00:06:23,404 root         INFO     Train Epoch: 178 [2560/8000 (32%)]\tTotal Loss: 0.128232\n",
      "Reconstruction: 0.114059, Regularization: 0.014174\n",
      "2019-04-10 00:06:23,467 root         INFO     Train Epoch: 178 [3072/8000 (38%)]\tTotal Loss: 0.107219\n",
      "Reconstruction: 0.099782, Regularization: 0.007437\n",
      "2019-04-10 00:06:23,531 root         INFO     Train Epoch: 178 [3584/8000 (45%)]\tTotal Loss: 0.102178\n",
      "Reconstruction: 0.093364, Regularization: 0.008814\n",
      "2019-04-10 00:06:23,594 root         INFO     Train Epoch: 178 [4096/8000 (51%)]\tTotal Loss: 0.103063\n",
      "Reconstruction: 0.092593, Regularization: 0.010470\n",
      "2019-04-10 00:06:23,657 root         INFO     Train Epoch: 178 [4608/8000 (58%)]\tTotal Loss: 0.108718\n",
      "Reconstruction: 0.098678, Regularization: 0.010040\n",
      "2019-04-10 00:06:23,721 root         INFO     Train Epoch: 178 [5120/8000 (64%)]\tTotal Loss: 0.106405\n",
      "Reconstruction: 0.096574, Regularization: 0.009831\n",
      "2019-04-10 00:06:23,784 root         INFO     Train Epoch: 178 [5632/8000 (70%)]\tTotal Loss: 0.109698\n",
      "Reconstruction: 0.100729, Regularization: 0.008969\n",
      "2019-04-10 00:06:23,847 root         INFO     Train Epoch: 178 [6144/8000 (77%)]\tTotal Loss: 0.111236\n",
      "Reconstruction: 0.099970, Regularization: 0.011266\n",
      "2019-04-10 00:06:23,910 root         INFO     Train Epoch: 178 [6656/8000 (83%)]\tTotal Loss: 0.104335\n",
      "Reconstruction: 0.093801, Regularization: 0.010534\n",
      "2019-04-10 00:06:23,974 root         INFO     Train Epoch: 178 [7168/8000 (90%)]\tTotal Loss: 0.123176\n",
      "Reconstruction: 0.109286, Regularization: 0.013891\n",
      "2019-04-10 00:06:24,037 root         INFO     Train Epoch: 178 [7680/8000 (96%)]\tTotal Loss: 0.113426\n",
      "Reconstruction: 0.102900, Regularization: 0.010525\n",
      "2019-04-10 00:06:24,091 root         INFO     ====> Epoch: 178 Average loss: 0.1123\n",
      "2019-04-10 00:06:24,115 root         INFO     Train Epoch: 179 [0/8000 (0%)]\tTotal Loss: 0.118547\n",
      "Reconstruction: 0.106606, Regularization: 0.011941\n",
      "2019-04-10 00:06:24,179 root         INFO     Train Epoch: 179 [512/8000 (6%)]\tTotal Loss: 0.114603\n",
      "Reconstruction: 0.103916, Regularization: 0.010687\n",
      "2019-04-10 00:06:24,242 root         INFO     Train Epoch: 179 [1024/8000 (13%)]\tTotal Loss: 0.107581\n",
      "Reconstruction: 0.097022, Regularization: 0.010559\n",
      "2019-04-10 00:06:24,305 root         INFO     Train Epoch: 179 [1536/8000 (19%)]\tTotal Loss: 0.116303\n",
      "Reconstruction: 0.106177, Regularization: 0.010126\n",
      "2019-04-10 00:06:24,368 root         INFO     Train Epoch: 179 [2048/8000 (26%)]\tTotal Loss: 0.111728\n",
      "Reconstruction: 0.101161, Regularization: 0.010567\n",
      "2019-04-10 00:06:24,431 root         INFO     Train Epoch: 179 [2560/8000 (32%)]\tTotal Loss: 0.100054\n",
      "Reconstruction: 0.091509, Regularization: 0.008545\n",
      "2019-04-10 00:06:24,494 root         INFO     Train Epoch: 179 [3072/8000 (38%)]\tTotal Loss: 0.112654\n",
      "Reconstruction: 0.100885, Regularization: 0.011769\n",
      "2019-04-10 00:06:24,557 root         INFO     Train Epoch: 179 [3584/8000 (45%)]\tTotal Loss: 0.121439\n",
      "Reconstruction: 0.107396, Regularization: 0.014042\n",
      "2019-04-10 00:06:24,621 root         INFO     Train Epoch: 179 [4096/8000 (51%)]\tTotal Loss: 0.121629\n",
      "Reconstruction: 0.107604, Regularization: 0.014025\n",
      "2019-04-10 00:06:24,684 root         INFO     Train Epoch: 179 [4608/8000 (58%)]\tTotal Loss: 0.112910\n",
      "Reconstruction: 0.099837, Regularization: 0.013073\n",
      "2019-04-10 00:06:24,747 root         INFO     Train Epoch: 179 [5120/8000 (64%)]\tTotal Loss: 0.112437\n",
      "Reconstruction: 0.100888, Regularization: 0.011549\n",
      "2019-04-10 00:06:24,811 root         INFO     Train Epoch: 179 [5632/8000 (70%)]\tTotal Loss: 0.111787\n",
      "Reconstruction: 0.101015, Regularization: 0.010773\n",
      "2019-04-10 00:06:24,874 root         INFO     Train Epoch: 179 [6144/8000 (77%)]\tTotal Loss: 0.109863\n",
      "Reconstruction: 0.098211, Regularization: 0.011651\n",
      "2019-04-10 00:06:24,937 root         INFO     Train Epoch: 179 [6656/8000 (83%)]\tTotal Loss: 0.098769\n",
      "Reconstruction: 0.088570, Regularization: 0.010198\n",
      "2019-04-10 00:06:25,000 root         INFO     Train Epoch: 179 [7168/8000 (90%)]\tTotal Loss: 0.113856\n",
      "Reconstruction: 0.102290, Regularization: 0.011566\n",
      "2019-04-10 00:06:25,064 root         INFO     Train Epoch: 179 [7680/8000 (96%)]\tTotal Loss: 0.110771\n",
      "Reconstruction: 0.100529, Regularization: 0.010243\n",
      "2019-04-10 00:06:25,117 root         INFO     ====> Epoch: 179 Average loss: 0.1122\n",
      "2019-04-10 00:06:25,142 root         INFO     Train Epoch: 180 [0/8000 (0%)]\tTotal Loss: 0.114427\n",
      "Reconstruction: 0.102852, Regularization: 0.011575\n",
      "2019-04-10 00:06:25,206 root         INFO     Train Epoch: 180 [512/8000 (6%)]\tTotal Loss: 0.119349\n",
      "Reconstruction: 0.107939, Regularization: 0.011410\n",
      "2019-04-10 00:06:25,269 root         INFO     Train Epoch: 180 [1024/8000 (13%)]\tTotal Loss: 0.106695\n",
      "Reconstruction: 0.093680, Regularization: 0.013015\n",
      "2019-04-10 00:06:25,334 root         INFO     Train Epoch: 180 [1536/8000 (19%)]\tTotal Loss: 0.107735\n",
      "Reconstruction: 0.097400, Regularization: 0.010335\n",
      "2019-04-10 00:06:25,397 root         INFO     Train Epoch: 180 [2048/8000 (26%)]\tTotal Loss: 0.116188\n",
      "Reconstruction: 0.102289, Regularization: 0.013898\n",
      "2019-04-10 00:06:25,460 root         INFO     Train Epoch: 180 [2560/8000 (32%)]\tTotal Loss: 0.109440\n",
      "Reconstruction: 0.096705, Regularization: 0.012735\n",
      "2019-04-10 00:06:25,523 root         INFO     Train Epoch: 180 [3072/8000 (38%)]\tTotal Loss: 0.109204\n",
      "Reconstruction: 0.100444, Regularization: 0.008759\n",
      "2019-04-10 00:06:25,585 root         INFO     Train Epoch: 180 [3584/8000 (45%)]\tTotal Loss: 0.117606\n",
      "Reconstruction: 0.105208, Regularization: 0.012398\n",
      "2019-04-10 00:06:25,648 root         INFO     Train Epoch: 180 [4096/8000 (51%)]\tTotal Loss: 0.113112\n",
      "Reconstruction: 0.102309, Regularization: 0.010803\n",
      "2019-04-10 00:06:25,710 root         INFO     Train Epoch: 180 [4608/8000 (58%)]\tTotal Loss: 0.114327\n",
      "Reconstruction: 0.102776, Regularization: 0.011552\n",
      "2019-04-10 00:06:25,773 root         INFO     Train Epoch: 180 [5120/8000 (64%)]\tTotal Loss: 0.128585\n",
      "Reconstruction: 0.116813, Regularization: 0.011772\n",
      "2019-04-10 00:06:25,835 root         INFO     Train Epoch: 180 [5632/8000 (70%)]\tTotal Loss: 0.114444\n",
      "Reconstruction: 0.102900, Regularization: 0.011544\n",
      "2019-04-10 00:06:25,897 root         INFO     Train Epoch: 180 [6144/8000 (77%)]\tTotal Loss: 0.104876\n",
      "Reconstruction: 0.092847, Regularization: 0.012028\n",
      "2019-04-10 00:06:25,960 root         INFO     Train Epoch: 180 [6656/8000 (83%)]\tTotal Loss: 0.111870\n",
      "Reconstruction: 0.102378, Regularization: 0.009491\n",
      "2019-04-10 00:06:26,022 root         INFO     Train Epoch: 180 [7168/8000 (90%)]\tTotal Loss: 0.115970\n",
      "Reconstruction: 0.105137, Regularization: 0.010833\n",
      "2019-04-10 00:06:26,085 root         INFO     Train Epoch: 180 [7680/8000 (96%)]\tTotal Loss: 0.111387\n",
      "Reconstruction: 0.100031, Regularization: 0.011355\n",
      "2019-04-10 00:06:26,138 root         INFO     ====> Epoch: 180 Average loss: 0.1122\n",
      "2019-04-10 00:06:26,162 root         INFO     Train Epoch: 181 [0/8000 (0%)]\tTotal Loss: 0.115296\n",
      "Reconstruction: 0.103472, Regularization: 0.011824\n",
      "2019-04-10 00:06:26,223 root         INFO     Train Epoch: 181 [512/8000 (6%)]\tTotal Loss: 0.104017\n",
      "Reconstruction: 0.094860, Regularization: 0.009157\n",
      "2019-04-10 00:06:26,284 root         INFO     Train Epoch: 181 [1024/8000 (13%)]\tTotal Loss: 0.105450\n",
      "Reconstruction: 0.095982, Regularization: 0.009467\n",
      "2019-04-10 00:06:26,348 root         INFO     Train Epoch: 181 [1536/8000 (19%)]\tTotal Loss: 0.119075\n",
      "Reconstruction: 0.108136, Regularization: 0.010940\n",
      "2019-04-10 00:06:26,410 root         INFO     Train Epoch: 181 [2048/8000 (26%)]\tTotal Loss: 0.111850\n",
      "Reconstruction: 0.099318, Regularization: 0.012531\n",
      "2019-04-10 00:06:26,472 root         INFO     Train Epoch: 181 [2560/8000 (32%)]\tTotal Loss: 0.107710\n",
      "Reconstruction: 0.097613, Regularization: 0.010097\n",
      "2019-04-10 00:06:26,534 root         INFO     Train Epoch: 181 [3072/8000 (38%)]\tTotal Loss: 0.112101\n",
      "Reconstruction: 0.099886, Regularization: 0.012215\n",
      "2019-04-10 00:06:26,596 root         INFO     Train Epoch: 181 [3584/8000 (45%)]\tTotal Loss: 0.118649\n",
      "Reconstruction: 0.105638, Regularization: 0.013011\n",
      "2019-04-10 00:06:26,657 root         INFO     Train Epoch: 181 [4096/8000 (51%)]\tTotal Loss: 0.115686\n",
      "Reconstruction: 0.102239, Regularization: 0.013447\n",
      "2019-04-10 00:06:26,718 root         INFO     Train Epoch: 181 [4608/8000 (58%)]\tTotal Loss: 0.109317\n",
      "Reconstruction: 0.097605, Regularization: 0.011712\n",
      "2019-04-10 00:06:26,779 root         INFO     Train Epoch: 181 [5120/8000 (64%)]\tTotal Loss: 0.108441\n",
      "Reconstruction: 0.098405, Regularization: 0.010036\n",
      "2019-04-10 00:06:26,840 root         INFO     Train Epoch: 181 [5632/8000 (70%)]\tTotal Loss: 0.107478\n",
      "Reconstruction: 0.097070, Regularization: 0.010408\n",
      "2019-04-10 00:06:26,901 root         INFO     Train Epoch: 181 [6144/8000 (77%)]\tTotal Loss: 0.112111\n",
      "Reconstruction: 0.102308, Regularization: 0.009803\n",
      "2019-04-10 00:06:26,962 root         INFO     Train Epoch: 181 [6656/8000 (83%)]\tTotal Loss: 0.109387\n",
      "Reconstruction: 0.097656, Regularization: 0.011731\n",
      "2019-04-10 00:06:27,023 root         INFO     Train Epoch: 181 [7168/8000 (90%)]\tTotal Loss: 0.119509\n",
      "Reconstruction: 0.108179, Regularization: 0.011331\n",
      "2019-04-10 00:06:27,084 root         INFO     Train Epoch: 181 [7680/8000 (96%)]\tTotal Loss: 0.107379\n",
      "Reconstruction: 0.097437, Regularization: 0.009942\n",
      "2019-04-10 00:06:27,136 root         INFO     ====> Epoch: 181 Average loss: 0.1122\n",
      "2019-04-10 00:06:27,160 root         INFO     Train Epoch: 182 [0/8000 (0%)]\tTotal Loss: 0.115304\n",
      "Reconstruction: 0.102707, Regularization: 0.012597\n",
      "2019-04-10 00:06:27,222 root         INFO     Train Epoch: 182 [512/8000 (6%)]\tTotal Loss: 0.107337\n",
      "Reconstruction: 0.094559, Regularization: 0.012777\n",
      "2019-04-10 00:06:27,283 root         INFO     Train Epoch: 182 [1024/8000 (13%)]\tTotal Loss: 0.114162\n",
      "Reconstruction: 0.101770, Regularization: 0.012392\n",
      "2019-04-10 00:06:27,346 root         INFO     Train Epoch: 182 [1536/8000 (19%)]\tTotal Loss: 0.102002\n",
      "Reconstruction: 0.092613, Regularization: 0.009388\n",
      "2019-04-10 00:06:27,408 root         INFO     Train Epoch: 182 [2048/8000 (26%)]\tTotal Loss: 0.110377\n",
      "Reconstruction: 0.100217, Regularization: 0.010161\n",
      "2019-04-10 00:06:27,470 root         INFO     Train Epoch: 182 [2560/8000 (32%)]\tTotal Loss: 0.109288\n",
      "Reconstruction: 0.098330, Regularization: 0.010958\n",
      "2019-04-10 00:06:27,531 root         INFO     Train Epoch: 182 [3072/8000 (38%)]\tTotal Loss: 0.105987\n",
      "Reconstruction: 0.094106, Regularization: 0.011881\n",
      "2019-04-10 00:06:27,593 root         INFO     Train Epoch: 182 [3584/8000 (45%)]\tTotal Loss: 0.111672\n",
      "Reconstruction: 0.099229, Regularization: 0.012443\n",
      "2019-04-10 00:06:27,655 root         INFO     Train Epoch: 182 [4096/8000 (51%)]\tTotal Loss: 0.110448\n",
      "Reconstruction: 0.097735, Regularization: 0.012712\n",
      "2019-04-10 00:06:27,716 root         INFO     Train Epoch: 182 [4608/8000 (58%)]\tTotal Loss: 0.116508\n",
      "Reconstruction: 0.102606, Regularization: 0.013902\n",
      "2019-04-10 00:06:27,777 root         INFO     Train Epoch: 182 [5120/8000 (64%)]\tTotal Loss: 0.114901\n",
      "Reconstruction: 0.105870, Regularization: 0.009031\n",
      "2019-04-10 00:06:27,839 root         INFO     Train Epoch: 182 [5632/8000 (70%)]\tTotal Loss: 0.122046\n",
      "Reconstruction: 0.108213, Regularization: 0.013833\n",
      "2019-04-10 00:06:27,900 root         INFO     Train Epoch: 182 [6144/8000 (77%)]\tTotal Loss: 0.109856\n",
      "Reconstruction: 0.099186, Regularization: 0.010670\n",
      "2019-04-10 00:06:27,962 root         INFO     Train Epoch: 182 [6656/8000 (83%)]\tTotal Loss: 0.102988\n",
      "Reconstruction: 0.091125, Regularization: 0.011863\n",
      "2019-04-10 00:06:28,023 root         INFO     Train Epoch: 182 [7168/8000 (90%)]\tTotal Loss: 0.113988\n",
      "Reconstruction: 0.102093, Regularization: 0.011895\n",
      "2019-04-10 00:06:28,084 root         INFO     Train Epoch: 182 [7680/8000 (96%)]\tTotal Loss: 0.102676\n",
      "Reconstruction: 0.093410, Regularization: 0.009267\n",
      "2019-04-10 00:06:28,137 root         INFO     ====> Epoch: 182 Average loss: 0.1116\n",
      "2019-04-10 00:06:28,161 root         INFO     Train Epoch: 183 [0/8000 (0%)]\tTotal Loss: 0.109887\n",
      "Reconstruction: 0.100684, Regularization: 0.009203\n",
      "2019-04-10 00:06:28,223 root         INFO     Train Epoch: 183 [512/8000 (6%)]\tTotal Loss: 0.112498\n",
      "Reconstruction: 0.101417, Regularization: 0.011080\n",
      "2019-04-10 00:06:28,284 root         INFO     Train Epoch: 183 [1024/8000 (13%)]\tTotal Loss: 0.113674\n",
      "Reconstruction: 0.102084, Regularization: 0.011590\n",
      "2019-04-10 00:06:28,345 root         INFO     Train Epoch: 183 [1536/8000 (19%)]\tTotal Loss: 0.110075\n",
      "Reconstruction: 0.096588, Regularization: 0.013487\n",
      "2019-04-10 00:06:28,407 root         INFO     Train Epoch: 183 [2048/8000 (26%)]\tTotal Loss: 0.112923\n",
      "Reconstruction: 0.099020, Regularization: 0.013903\n",
      "2019-04-10 00:06:28,469 root         INFO     Train Epoch: 183 [2560/8000 (32%)]\tTotal Loss: 0.104968\n",
      "Reconstruction: 0.096953, Regularization: 0.008015\n",
      "2019-04-10 00:06:28,531 root         INFO     Train Epoch: 183 [3072/8000 (38%)]\tTotal Loss: 0.118563\n",
      "Reconstruction: 0.107146, Regularization: 0.011416\n",
      "2019-04-10 00:06:28,592 root         INFO     Train Epoch: 183 [3584/8000 (45%)]\tTotal Loss: 0.117971\n",
      "Reconstruction: 0.105685, Regularization: 0.012286\n",
      "2019-04-10 00:06:28,653 root         INFO     Train Epoch: 183 [4096/8000 (51%)]\tTotal Loss: 0.110811\n",
      "Reconstruction: 0.099350, Regularization: 0.011460\n",
      "2019-04-10 00:06:28,716 root         INFO     Train Epoch: 183 [4608/8000 (58%)]\tTotal Loss: 0.115955\n",
      "Reconstruction: 0.107346, Regularization: 0.008609\n",
      "2019-04-10 00:06:28,778 root         INFO     Train Epoch: 183 [5120/8000 (64%)]\tTotal Loss: 0.119542\n",
      "Reconstruction: 0.108272, Regularization: 0.011271\n",
      "2019-04-10 00:06:28,840 root         INFO     Train Epoch: 183 [5632/8000 (70%)]\tTotal Loss: 0.099740\n",
      "Reconstruction: 0.089705, Regularization: 0.010036\n",
      "2019-04-10 00:06:28,902 root         INFO     Train Epoch: 183 [6144/8000 (77%)]\tTotal Loss: 0.106991\n",
      "Reconstruction: 0.097253, Regularization: 0.009738\n",
      "2019-04-10 00:06:28,964 root         INFO     Train Epoch: 183 [6656/8000 (83%)]\tTotal Loss: 0.116946\n",
      "Reconstruction: 0.105242, Regularization: 0.011704\n",
      "2019-04-10 00:06:29,026 root         INFO     Train Epoch: 183 [7168/8000 (90%)]\tTotal Loss: 0.120013\n",
      "Reconstruction: 0.106979, Regularization: 0.013035\n",
      "2019-04-10 00:06:29,088 root         INFO     Train Epoch: 183 [7680/8000 (96%)]\tTotal Loss: 0.102931\n",
      "Reconstruction: 0.093590, Regularization: 0.009341\n",
      "2019-04-10 00:06:29,141 root         INFO     ====> Epoch: 183 Average loss: 0.1117\n",
      "2019-04-10 00:06:29,165 root         INFO     Train Epoch: 184 [0/8000 (0%)]\tTotal Loss: 0.115282\n",
      "Reconstruction: 0.103102, Regularization: 0.012180\n",
      "2019-04-10 00:06:29,228 root         INFO     Train Epoch: 184 [512/8000 (6%)]\tTotal Loss: 0.106950\n",
      "Reconstruction: 0.096909, Regularization: 0.010040\n",
      "2019-04-10 00:06:29,290 root         INFO     Train Epoch: 184 [1024/8000 (13%)]\tTotal Loss: 0.108493\n",
      "Reconstruction: 0.098665, Regularization: 0.009828\n",
      "2019-04-10 00:06:29,353 root         INFO     Train Epoch: 184 [1536/8000 (19%)]\tTotal Loss: 0.111852\n",
      "Reconstruction: 0.099252, Regularization: 0.012600\n",
      "2019-04-10 00:06:29,415 root         INFO     Train Epoch: 184 [2048/8000 (26%)]\tTotal Loss: 0.101792\n",
      "Reconstruction: 0.091616, Regularization: 0.010176\n",
      "2019-04-10 00:06:29,478 root         INFO     Train Epoch: 184 [2560/8000 (32%)]\tTotal Loss: 0.107598\n",
      "Reconstruction: 0.093556, Regularization: 0.014042\n",
      "2019-04-10 00:06:29,540 root         INFO     Train Epoch: 184 [3072/8000 (38%)]\tTotal Loss: 0.108850\n",
      "Reconstruction: 0.100471, Regularization: 0.008380\n",
      "2019-04-10 00:06:29,602 root         INFO     Train Epoch: 184 [3584/8000 (45%)]\tTotal Loss: 0.115395\n",
      "Reconstruction: 0.101527, Regularization: 0.013868\n",
      "2019-04-10 00:06:29,665 root         INFO     Train Epoch: 184 [4096/8000 (51%)]\tTotal Loss: 0.105824\n",
      "Reconstruction: 0.095837, Regularization: 0.009986\n",
      "2019-04-10 00:06:29,726 root         INFO     Train Epoch: 184 [4608/8000 (58%)]\tTotal Loss: 0.116067\n",
      "Reconstruction: 0.102483, Regularization: 0.013584\n",
      "2019-04-10 00:06:29,788 root         INFO     Train Epoch: 184 [5120/8000 (64%)]\tTotal Loss: 0.113384\n",
      "Reconstruction: 0.102673, Regularization: 0.010712\n",
      "2019-04-10 00:06:29,850 root         INFO     Train Epoch: 184 [5632/8000 (70%)]\tTotal Loss: 0.106415\n",
      "Reconstruction: 0.095916, Regularization: 0.010499\n",
      "2019-04-10 00:06:29,912 root         INFO     Train Epoch: 184 [6144/8000 (77%)]\tTotal Loss: 0.109597\n",
      "Reconstruction: 0.097360, Regularization: 0.012237\n",
      "2019-04-10 00:06:29,974 root         INFO     Train Epoch: 184 [6656/8000 (83%)]\tTotal Loss: 0.108221\n",
      "Reconstruction: 0.097398, Regularization: 0.010823\n",
      "2019-04-10 00:06:30,036 root         INFO     Train Epoch: 184 [7168/8000 (90%)]\tTotal Loss: 0.100518\n",
      "Reconstruction: 0.089753, Regularization: 0.010765\n",
      "2019-04-10 00:06:30,098 root         INFO     Train Epoch: 184 [7680/8000 (96%)]\tTotal Loss: 0.113212\n",
      "Reconstruction: 0.098661, Regularization: 0.014551\n",
      "2019-04-10 00:06:30,152 root         INFO     ====> Epoch: 184 Average loss: 0.1120\n",
      "2019-04-10 00:06:30,176 root         INFO     Train Epoch: 185 [0/8000 (0%)]\tTotal Loss: 0.102632\n",
      "Reconstruction: 0.091234, Regularization: 0.011398\n",
      "2019-04-10 00:06:30,239 root         INFO     Train Epoch: 185 [512/8000 (6%)]\tTotal Loss: 0.113277\n",
      "Reconstruction: 0.102272, Regularization: 0.011005\n",
      "2019-04-10 00:06:30,302 root         INFO     Train Epoch: 185 [1024/8000 (13%)]\tTotal Loss: 0.110020\n",
      "Reconstruction: 0.098055, Regularization: 0.011966\n",
      "2019-04-10 00:06:30,365 root         INFO     Train Epoch: 185 [1536/8000 (19%)]\tTotal Loss: 0.117558\n",
      "Reconstruction: 0.106282, Regularization: 0.011276\n",
      "2019-04-10 00:06:30,427 root         INFO     Train Epoch: 185 [2048/8000 (26%)]\tTotal Loss: 0.114004\n",
      "Reconstruction: 0.100729, Regularization: 0.013275\n",
      "2019-04-10 00:06:30,490 root         INFO     Train Epoch: 185 [2560/8000 (32%)]\tTotal Loss: 0.110826\n",
      "Reconstruction: 0.096133, Regularization: 0.014693\n",
      "2019-04-10 00:06:30,553 root         INFO     Train Epoch: 185 [3072/8000 (38%)]\tTotal Loss: 0.127114\n",
      "Reconstruction: 0.115384, Regularization: 0.011730\n",
      "2019-04-10 00:06:30,615 root         INFO     Train Epoch: 185 [3584/8000 (45%)]\tTotal Loss: 0.111273\n",
      "Reconstruction: 0.100680, Regularization: 0.010593\n",
      "2019-04-10 00:06:30,677 root         INFO     Train Epoch: 185 [4096/8000 (51%)]\tTotal Loss: 0.113982\n",
      "Reconstruction: 0.102393, Regularization: 0.011590\n",
      "2019-04-10 00:06:30,739 root         INFO     Train Epoch: 185 [4608/8000 (58%)]\tTotal Loss: 0.103987\n",
      "Reconstruction: 0.092029, Regularization: 0.011958\n",
      "2019-04-10 00:06:30,802 root         INFO     Train Epoch: 185 [5120/8000 (64%)]\tTotal Loss: 0.109262\n",
      "Reconstruction: 0.098323, Regularization: 0.010940\n",
      "2019-04-10 00:06:30,863 root         INFO     Train Epoch: 185 [5632/8000 (70%)]\tTotal Loss: 0.117063\n",
      "Reconstruction: 0.106245, Regularization: 0.010819\n",
      "2019-04-10 00:06:30,926 root         INFO     Train Epoch: 185 [6144/8000 (77%)]\tTotal Loss: 0.104660\n",
      "Reconstruction: 0.096186, Regularization: 0.008474\n",
      "2019-04-10 00:06:30,988 root         INFO     Train Epoch: 185 [6656/8000 (83%)]\tTotal Loss: 0.109509\n",
      "Reconstruction: 0.098772, Regularization: 0.010737\n",
      "2019-04-10 00:06:31,050 root         INFO     Train Epoch: 185 [7168/8000 (90%)]\tTotal Loss: 0.117912\n",
      "Reconstruction: 0.105842, Regularization: 0.012071\n",
      "2019-04-10 00:06:31,113 root         INFO     Train Epoch: 185 [7680/8000 (96%)]\tTotal Loss: 0.121886\n",
      "Reconstruction: 0.109016, Regularization: 0.012869\n",
      "2019-04-10 00:06:31,168 root         INFO     ====> Epoch: 185 Average loss: 0.1114\n",
      "2019-04-10 00:06:31,192 root         INFO     Train Epoch: 186 [0/8000 (0%)]\tTotal Loss: 0.109121\n",
      "Reconstruction: 0.098300, Regularization: 0.010820\n",
      "2019-04-10 00:06:31,254 root         INFO     Train Epoch: 186 [512/8000 (6%)]\tTotal Loss: 0.116338\n",
      "Reconstruction: 0.105049, Regularization: 0.011289\n",
      "2019-04-10 00:06:31,316 root         INFO     Train Epoch: 186 [1024/8000 (13%)]\tTotal Loss: 0.109477\n",
      "Reconstruction: 0.098000, Regularization: 0.011477\n",
      "2019-04-10 00:06:31,379 root         INFO     Train Epoch: 186 [1536/8000 (19%)]\tTotal Loss: 0.105559\n",
      "Reconstruction: 0.096391, Regularization: 0.009168\n",
      "2019-04-10 00:06:31,441 root         INFO     Train Epoch: 186 [2048/8000 (26%)]\tTotal Loss: 0.107056\n",
      "Reconstruction: 0.096895, Regularization: 0.010161\n",
      "2019-04-10 00:06:31,503 root         INFO     Train Epoch: 186 [2560/8000 (32%)]\tTotal Loss: 0.106657\n",
      "Reconstruction: 0.097152, Regularization: 0.009505\n",
      "2019-04-10 00:06:31,565 root         INFO     Train Epoch: 186 [3072/8000 (38%)]\tTotal Loss: 0.106638\n",
      "Reconstruction: 0.097755, Regularization: 0.008882\n",
      "2019-04-10 00:06:31,628 root         INFO     Train Epoch: 186 [3584/8000 (45%)]\tTotal Loss: 0.133646\n",
      "Reconstruction: 0.119811, Regularization: 0.013835\n",
      "2019-04-10 00:06:31,690 root         INFO     Train Epoch: 186 [4096/8000 (51%)]\tTotal Loss: 0.107524\n",
      "Reconstruction: 0.096170, Regularization: 0.011354\n",
      "2019-04-10 00:06:31,752 root         INFO     Train Epoch: 186 [4608/8000 (58%)]\tTotal Loss: 0.122206\n",
      "Reconstruction: 0.110085, Regularization: 0.012121\n",
      "2019-04-10 00:06:31,815 root         INFO     Train Epoch: 186 [5120/8000 (64%)]\tTotal Loss: 0.100325\n",
      "Reconstruction: 0.090604, Regularization: 0.009721\n",
      "2019-04-10 00:06:31,877 root         INFO     Train Epoch: 186 [5632/8000 (70%)]\tTotal Loss: 0.113139\n",
      "Reconstruction: 0.102319, Regularization: 0.010820\n",
      "2019-04-10 00:06:31,940 root         INFO     Train Epoch: 186 [6144/8000 (77%)]\tTotal Loss: 0.132898\n",
      "Reconstruction: 0.114457, Regularization: 0.018441\n",
      "2019-04-10 00:06:32,002 root         INFO     Train Epoch: 186 [6656/8000 (83%)]\tTotal Loss: 0.114699\n",
      "Reconstruction: 0.102920, Regularization: 0.011780\n",
      "2019-04-10 00:06:32,064 root         INFO     Train Epoch: 186 [7168/8000 (90%)]\tTotal Loss: 0.111188\n",
      "Reconstruction: 0.098687, Regularization: 0.012501\n",
      "2019-04-10 00:06:32,126 root         INFO     Train Epoch: 186 [7680/8000 (96%)]\tTotal Loss: 0.109334\n",
      "Reconstruction: 0.099453, Regularization: 0.009880\n",
      "2019-04-10 00:06:32,179 root         INFO     ====> Epoch: 186 Average loss: 0.1116\n",
      "2019-04-10 00:06:32,203 root         INFO     Train Epoch: 187 [0/8000 (0%)]\tTotal Loss: 0.111509\n",
      "Reconstruction: 0.100375, Regularization: 0.011134\n",
      "2019-04-10 00:06:32,266 root         INFO     Train Epoch: 187 [512/8000 (6%)]\tTotal Loss: 0.115094\n",
      "Reconstruction: 0.105030, Regularization: 0.010065\n",
      "2019-04-10 00:06:32,328 root         INFO     Train Epoch: 187 [1024/8000 (13%)]\tTotal Loss: 0.119171\n",
      "Reconstruction: 0.106097, Regularization: 0.013074\n",
      "2019-04-10 00:06:32,390 root         INFO     Train Epoch: 187 [1536/8000 (19%)]\tTotal Loss: 0.126630\n",
      "Reconstruction: 0.111156, Regularization: 0.015474\n",
      "2019-04-10 00:06:32,452 root         INFO     Train Epoch: 187 [2048/8000 (26%)]\tTotal Loss: 0.107568\n",
      "Reconstruction: 0.098470, Regularization: 0.009097\n",
      "2019-04-10 00:06:32,514 root         INFO     Train Epoch: 187 [2560/8000 (32%)]\tTotal Loss: 0.110502\n",
      "Reconstruction: 0.099351, Regularization: 0.011152\n",
      "2019-04-10 00:06:32,576 root         INFO     Train Epoch: 187 [3072/8000 (38%)]\tTotal Loss: 0.110486\n",
      "Reconstruction: 0.100602, Regularization: 0.009883\n",
      "2019-04-10 00:06:32,639 root         INFO     Train Epoch: 187 [3584/8000 (45%)]\tTotal Loss: 0.121481\n",
      "Reconstruction: 0.108490, Regularization: 0.012991\n",
      "2019-04-10 00:06:32,701 root         INFO     Train Epoch: 187 [4096/8000 (51%)]\tTotal Loss: 0.114300\n",
      "Reconstruction: 0.101746, Regularization: 0.012555\n",
      "2019-04-10 00:06:32,765 root         INFO     Train Epoch: 187 [4608/8000 (58%)]\tTotal Loss: 0.118243\n",
      "Reconstruction: 0.102250, Regularization: 0.015994\n",
      "2019-04-10 00:06:32,827 root         INFO     Train Epoch: 187 [5120/8000 (64%)]\tTotal Loss: 0.115703\n",
      "Reconstruction: 0.103161, Regularization: 0.012542\n",
      "2019-04-10 00:06:32,889 root         INFO     Train Epoch: 187 [5632/8000 (70%)]\tTotal Loss: 0.110982\n",
      "Reconstruction: 0.100665, Regularization: 0.010317\n",
      "2019-04-10 00:06:32,951 root         INFO     Train Epoch: 187 [6144/8000 (77%)]\tTotal Loss: 0.096907\n",
      "Reconstruction: 0.089240, Regularization: 0.007667\n",
      "2019-04-10 00:06:33,013 root         INFO     Train Epoch: 187 [6656/8000 (83%)]\tTotal Loss: 0.104026\n",
      "Reconstruction: 0.093873, Regularization: 0.010153\n",
      "2019-04-10 00:06:33,075 root         INFO     Train Epoch: 187 [7168/8000 (90%)]\tTotal Loss: 0.114867\n",
      "Reconstruction: 0.101920, Regularization: 0.012947\n",
      "2019-04-10 00:06:33,137 root         INFO     Train Epoch: 187 [7680/8000 (96%)]\tTotal Loss: 0.116734\n",
      "Reconstruction: 0.103293, Regularization: 0.013441\n",
      "2019-04-10 00:06:33,191 root         INFO     ====> Epoch: 187 Average loss: 0.1113\n",
      "2019-04-10 00:06:33,215 root         INFO     Train Epoch: 188 [0/8000 (0%)]\tTotal Loss: 0.117157\n",
      "Reconstruction: 0.103594, Regularization: 0.013564\n",
      "2019-04-10 00:06:33,279 root         INFO     Train Epoch: 188 [512/8000 (6%)]\tTotal Loss: 0.103040\n",
      "Reconstruction: 0.093104, Regularization: 0.009935\n",
      "2019-04-10 00:06:33,342 root         INFO     Train Epoch: 188 [1024/8000 (13%)]\tTotal Loss: 0.113553\n",
      "Reconstruction: 0.097893, Regularization: 0.015660\n",
      "2019-04-10 00:06:33,406 root         INFO     Train Epoch: 188 [1536/8000 (19%)]\tTotal Loss: 0.110985\n",
      "Reconstruction: 0.098675, Regularization: 0.012309\n",
      "2019-04-10 00:06:33,469 root         INFO     Train Epoch: 188 [2048/8000 (26%)]\tTotal Loss: 0.123275\n",
      "Reconstruction: 0.112342, Regularization: 0.010933\n",
      "2019-04-10 00:06:33,532 root         INFO     Train Epoch: 188 [2560/8000 (32%)]\tTotal Loss: 0.115304\n",
      "Reconstruction: 0.101035, Regularization: 0.014269\n",
      "2019-04-10 00:06:33,595 root         INFO     Train Epoch: 188 [3072/8000 (38%)]\tTotal Loss: 0.109332\n",
      "Reconstruction: 0.097474, Regularization: 0.011858\n",
      "2019-04-10 00:06:33,659 root         INFO     Train Epoch: 188 [3584/8000 (45%)]\tTotal Loss: 0.111088\n",
      "Reconstruction: 0.100935, Regularization: 0.010153\n",
      "2019-04-10 00:06:33,722 root         INFO     Train Epoch: 188 [4096/8000 (51%)]\tTotal Loss: 0.104139\n",
      "Reconstruction: 0.093767, Regularization: 0.010371\n",
      "2019-04-10 00:06:33,785 root         INFO     Train Epoch: 188 [4608/8000 (58%)]\tTotal Loss: 0.109409\n",
      "Reconstruction: 0.099630, Regularization: 0.009779\n",
      "2019-04-10 00:06:33,849 root         INFO     Train Epoch: 188 [5120/8000 (64%)]\tTotal Loss: 0.115452\n",
      "Reconstruction: 0.103289, Regularization: 0.012163\n",
      "2019-04-10 00:06:33,912 root         INFO     Train Epoch: 188 [5632/8000 (70%)]\tTotal Loss: 0.094983\n",
      "Reconstruction: 0.085924, Regularization: 0.009060\n",
      "2019-04-10 00:06:33,975 root         INFO     Train Epoch: 188 [6144/8000 (77%)]\tTotal Loss: 0.125340\n",
      "Reconstruction: 0.112245, Regularization: 0.013095\n",
      "2019-04-10 00:06:34,039 root         INFO     Train Epoch: 188 [6656/8000 (83%)]\tTotal Loss: 0.107269\n",
      "Reconstruction: 0.096354, Regularization: 0.010915\n",
      "2019-04-10 00:06:34,102 root         INFO     Train Epoch: 188 [7168/8000 (90%)]\tTotal Loss: 0.117037\n",
      "Reconstruction: 0.102937, Regularization: 0.014100\n",
      "2019-04-10 00:06:34,165 root         INFO     Train Epoch: 188 [7680/8000 (96%)]\tTotal Loss: 0.100997\n",
      "Reconstruction: 0.092184, Regularization: 0.008813\n",
      "2019-04-10 00:06:34,219 root         INFO     ====> Epoch: 188 Average loss: 0.1107\n",
      "2019-04-10 00:06:34,243 root         INFO     Train Epoch: 189 [0/8000 (0%)]\tTotal Loss: 0.132101\n",
      "Reconstruction: 0.112550, Regularization: 0.019551\n",
      "2019-04-10 00:06:34,307 root         INFO     Train Epoch: 189 [512/8000 (6%)]\tTotal Loss: 0.104757\n",
      "Reconstruction: 0.093822, Regularization: 0.010935\n",
      "2019-04-10 00:06:34,370 root         INFO     Train Epoch: 189 [1024/8000 (13%)]\tTotal Loss: 0.104288\n",
      "Reconstruction: 0.095156, Regularization: 0.009132\n",
      "2019-04-10 00:06:34,434 root         INFO     Train Epoch: 189 [1536/8000 (19%)]\tTotal Loss: 0.124126\n",
      "Reconstruction: 0.106950, Regularization: 0.017176\n",
      "2019-04-10 00:06:34,497 root         INFO     Train Epoch: 189 [2048/8000 (26%)]\tTotal Loss: 0.110313\n",
      "Reconstruction: 0.100566, Regularization: 0.009747\n",
      "2019-04-10 00:06:34,560 root         INFO     Train Epoch: 189 [2560/8000 (32%)]\tTotal Loss: 0.131553\n",
      "Reconstruction: 0.114288, Regularization: 0.017265\n",
      "2019-04-10 00:06:34,624 root         INFO     Train Epoch: 189 [3072/8000 (38%)]\tTotal Loss: 0.120556\n",
      "Reconstruction: 0.105313, Regularization: 0.015243\n",
      "2019-04-10 00:06:34,687 root         INFO     Train Epoch: 189 [3584/8000 (45%)]\tTotal Loss: 0.122585\n",
      "Reconstruction: 0.108547, Regularization: 0.014039\n",
      "2019-04-10 00:06:34,751 root         INFO     Train Epoch: 189 [4096/8000 (51%)]\tTotal Loss: 0.108193\n",
      "Reconstruction: 0.097776, Regularization: 0.010417\n",
      "2019-04-10 00:06:34,814 root         INFO     Train Epoch: 189 [4608/8000 (58%)]\tTotal Loss: 0.114188\n",
      "Reconstruction: 0.100228, Regularization: 0.013960\n",
      "2019-04-10 00:06:34,878 root         INFO     Train Epoch: 189 [5120/8000 (64%)]\tTotal Loss: 0.102807\n",
      "Reconstruction: 0.091422, Regularization: 0.011385\n",
      "2019-04-10 00:06:34,941 root         INFO     Train Epoch: 189 [5632/8000 (70%)]\tTotal Loss: 0.108289\n",
      "Reconstruction: 0.095930, Regularization: 0.012359\n",
      "2019-04-10 00:06:35,005 root         INFO     Train Epoch: 189 [6144/8000 (77%)]\tTotal Loss: 0.124772\n",
      "Reconstruction: 0.114638, Regularization: 0.010134\n",
      "2019-04-10 00:06:35,068 root         INFO     Train Epoch: 189 [6656/8000 (83%)]\tTotal Loss: 0.107714\n",
      "Reconstruction: 0.096982, Regularization: 0.010732\n",
      "2019-04-10 00:06:35,132 root         INFO     Train Epoch: 189 [7168/8000 (90%)]\tTotal Loss: 0.110808\n",
      "Reconstruction: 0.098942, Regularization: 0.011866\n",
      "2019-04-10 00:06:35,195 root         INFO     Train Epoch: 189 [7680/8000 (96%)]\tTotal Loss: 0.109639\n",
      "Reconstruction: 0.098914, Regularization: 0.010724\n",
      "2019-04-10 00:06:35,249 root         INFO     ====> Epoch: 189 Average loss: 0.1116\n",
      "2019-04-10 00:06:35,273 root         INFO     Train Epoch: 190 [0/8000 (0%)]\tTotal Loss: 0.111078\n",
      "Reconstruction: 0.102426, Regularization: 0.008653\n",
      "2019-04-10 00:06:35,337 root         INFO     Train Epoch: 190 [512/8000 (6%)]\tTotal Loss: 0.104987\n",
      "Reconstruction: 0.094709, Regularization: 0.010278\n",
      "2019-04-10 00:06:35,401 root         INFO     Train Epoch: 190 [1024/8000 (13%)]\tTotal Loss: 0.118572\n",
      "Reconstruction: 0.105204, Regularization: 0.013369\n",
      "2019-04-10 00:06:35,465 root         INFO     Train Epoch: 190 [1536/8000 (19%)]\tTotal Loss: 0.110471\n",
      "Reconstruction: 0.097816, Regularization: 0.012654\n",
      "2019-04-10 00:06:35,529 root         INFO     Train Epoch: 190 [2048/8000 (26%)]\tTotal Loss: 0.112674\n",
      "Reconstruction: 0.101929, Regularization: 0.010745\n",
      "2019-04-10 00:06:35,593 root         INFO     Train Epoch: 190 [2560/8000 (32%)]\tTotal Loss: 0.108195\n",
      "Reconstruction: 0.098116, Regularization: 0.010080\n",
      "2019-04-10 00:06:35,657 root         INFO     Train Epoch: 190 [3072/8000 (38%)]\tTotal Loss: 0.112678\n",
      "Reconstruction: 0.100305, Regularization: 0.012372\n",
      "2019-04-10 00:06:35,721 root         INFO     Train Epoch: 190 [3584/8000 (45%)]\tTotal Loss: 0.102716\n",
      "Reconstruction: 0.093588, Regularization: 0.009128\n",
      "2019-04-10 00:06:35,785 root         INFO     Train Epoch: 190 [4096/8000 (51%)]\tTotal Loss: 0.112104\n",
      "Reconstruction: 0.100157, Regularization: 0.011947\n",
      "2019-04-10 00:06:35,848 root         INFO     Train Epoch: 190 [4608/8000 (58%)]\tTotal Loss: 0.120239\n",
      "Reconstruction: 0.108284, Regularization: 0.011954\n",
      "2019-04-10 00:06:35,913 root         INFO     Train Epoch: 190 [5120/8000 (64%)]\tTotal Loss: 0.113017\n",
      "Reconstruction: 0.101846, Regularization: 0.011170\n",
      "2019-04-10 00:06:35,976 root         INFO     Train Epoch: 190 [5632/8000 (70%)]\tTotal Loss: 0.109514\n",
      "Reconstruction: 0.096694, Regularization: 0.012820\n",
      "2019-04-10 00:06:36,039 root         INFO     Train Epoch: 190 [6144/8000 (77%)]\tTotal Loss: 0.111110\n",
      "Reconstruction: 0.098649, Regularization: 0.012461\n",
      "2019-04-10 00:06:36,101 root         INFO     Train Epoch: 190 [6656/8000 (83%)]\tTotal Loss: 0.112680\n",
      "Reconstruction: 0.099049, Regularization: 0.013631\n",
      "2019-04-10 00:06:36,163 root         INFO     Train Epoch: 190 [7168/8000 (90%)]\tTotal Loss: 0.114856\n",
      "Reconstruction: 0.104244, Regularization: 0.010612\n",
      "2019-04-10 00:06:36,226 root         INFO     Train Epoch: 190 [7680/8000 (96%)]\tTotal Loss: 0.111108\n",
      "Reconstruction: 0.096794, Regularization: 0.014313\n",
      "2019-04-10 00:06:36,280 root         INFO     ====> Epoch: 190 Average loss: 0.1115\n",
      "2019-04-10 00:06:36,303 root         INFO     Train Epoch: 191 [0/8000 (0%)]\tTotal Loss: 0.114554\n",
      "Reconstruction: 0.100414, Regularization: 0.014139\n",
      "2019-04-10 00:06:36,367 root         INFO     Train Epoch: 191 [512/8000 (6%)]\tTotal Loss: 0.111015\n",
      "Reconstruction: 0.100967, Regularization: 0.010048\n",
      "2019-04-10 00:06:36,431 root         INFO     Train Epoch: 191 [1024/8000 (13%)]\tTotal Loss: 0.113553\n",
      "Reconstruction: 0.098848, Regularization: 0.014704\n",
      "2019-04-10 00:06:36,494 root         INFO     Train Epoch: 191 [1536/8000 (19%)]\tTotal Loss: 0.118177\n",
      "Reconstruction: 0.106001, Regularization: 0.012176\n",
      "2019-04-10 00:06:36,557 root         INFO     Train Epoch: 191 [2048/8000 (26%)]\tTotal Loss: 0.108889\n",
      "Reconstruction: 0.099942, Regularization: 0.008947\n",
      "2019-04-10 00:06:36,619 root         INFO     Train Epoch: 191 [2560/8000 (32%)]\tTotal Loss: 0.105992\n",
      "Reconstruction: 0.094577, Regularization: 0.011415\n",
      "2019-04-10 00:06:36,682 root         INFO     Train Epoch: 191 [3072/8000 (38%)]\tTotal Loss: 0.108408\n",
      "Reconstruction: 0.097894, Regularization: 0.010514\n",
      "2019-04-10 00:06:36,743 root         INFO     Train Epoch: 191 [3584/8000 (45%)]\tTotal Loss: 0.108128\n",
      "Reconstruction: 0.095390, Regularization: 0.012738\n",
      "2019-04-10 00:06:36,805 root         INFO     Train Epoch: 191 [4096/8000 (51%)]\tTotal Loss: 0.105848\n",
      "Reconstruction: 0.094596, Regularization: 0.011252\n",
      "2019-04-10 00:06:36,866 root         INFO     Train Epoch: 191 [4608/8000 (58%)]\tTotal Loss: 0.113793\n",
      "Reconstruction: 0.102507, Regularization: 0.011286\n",
      "2019-04-10 00:06:36,928 root         INFO     Train Epoch: 191 [5120/8000 (64%)]\tTotal Loss: 0.116977\n",
      "Reconstruction: 0.105430, Regularization: 0.011547\n",
      "2019-04-10 00:06:36,990 root         INFO     Train Epoch: 191 [5632/8000 (70%)]\tTotal Loss: 0.111547\n",
      "Reconstruction: 0.095939, Regularization: 0.015608\n",
      "2019-04-10 00:06:37,051 root         INFO     Train Epoch: 191 [6144/8000 (77%)]\tTotal Loss: 0.105230\n",
      "Reconstruction: 0.092302, Regularization: 0.012928\n",
      "2019-04-10 00:06:37,113 root         INFO     Train Epoch: 191 [6656/8000 (83%)]\tTotal Loss: 0.112529\n",
      "Reconstruction: 0.098422, Regularization: 0.014107\n",
      "2019-04-10 00:06:37,175 root         INFO     Train Epoch: 191 [7168/8000 (90%)]\tTotal Loss: 0.129050\n",
      "Reconstruction: 0.109274, Regularization: 0.019776\n",
      "2019-04-10 00:06:37,236 root         INFO     Train Epoch: 191 [7680/8000 (96%)]\tTotal Loss: 0.112806\n",
      "Reconstruction: 0.101462, Regularization: 0.011344\n",
      "2019-04-10 00:06:37,289 root         INFO     ====> Epoch: 191 Average loss: 0.1118\n",
      "2019-04-10 00:06:37,313 root         INFO     Train Epoch: 192 [0/8000 (0%)]\tTotal Loss: 0.104312\n",
      "Reconstruction: 0.093292, Regularization: 0.011021\n",
      "2019-04-10 00:06:37,376 root         INFO     Train Epoch: 192 [512/8000 (6%)]\tTotal Loss: 0.111905\n",
      "Reconstruction: 0.098247, Regularization: 0.013658\n",
      "2019-04-10 00:06:37,439 root         INFO     Train Epoch: 192 [1024/8000 (13%)]\tTotal Loss: 0.130371\n",
      "Reconstruction: 0.117568, Regularization: 0.012803\n",
      "2019-04-10 00:06:37,502 root         INFO     Train Epoch: 192 [1536/8000 (19%)]\tTotal Loss: 0.117695\n",
      "Reconstruction: 0.105772, Regularization: 0.011924\n",
      "2019-04-10 00:06:37,565 root         INFO     Train Epoch: 192 [2048/8000 (26%)]\tTotal Loss: 0.102155\n",
      "Reconstruction: 0.090099, Regularization: 0.012056\n",
      "2019-04-10 00:06:37,627 root         INFO     Train Epoch: 192 [2560/8000 (32%)]\tTotal Loss: 0.117337\n",
      "Reconstruction: 0.103780, Regularization: 0.013557\n",
      "2019-04-10 00:06:37,690 root         INFO     Train Epoch: 192 [3072/8000 (38%)]\tTotal Loss: 0.108207\n",
      "Reconstruction: 0.096278, Regularization: 0.011929\n",
      "2019-04-10 00:06:37,752 root         INFO     Train Epoch: 192 [3584/8000 (45%)]\tTotal Loss: 0.113359\n",
      "Reconstruction: 0.103071, Regularization: 0.010289\n",
      "2019-04-10 00:06:37,815 root         INFO     Train Epoch: 192 [4096/8000 (51%)]\tTotal Loss: 0.118250\n",
      "Reconstruction: 0.102947, Regularization: 0.015303\n",
      "2019-04-10 00:06:37,878 root         INFO     Train Epoch: 192 [4608/8000 (58%)]\tTotal Loss: 0.111618\n",
      "Reconstruction: 0.098882, Regularization: 0.012737\n",
      "2019-04-10 00:06:37,940 root         INFO     Train Epoch: 192 [5120/8000 (64%)]\tTotal Loss: 0.105113\n",
      "Reconstruction: 0.095150, Regularization: 0.009963\n",
      "2019-04-10 00:06:38,003 root         INFO     Train Epoch: 192 [5632/8000 (70%)]\tTotal Loss: 0.117779\n",
      "Reconstruction: 0.105803, Regularization: 0.011976\n",
      "2019-04-10 00:06:38,066 root         INFO     Train Epoch: 192 [6144/8000 (77%)]\tTotal Loss: 0.107439\n",
      "Reconstruction: 0.093114, Regularization: 0.014325\n",
      "2019-04-10 00:06:38,128 root         INFO     Train Epoch: 192 [6656/8000 (83%)]\tTotal Loss: 0.110230\n",
      "Reconstruction: 0.097232, Regularization: 0.012998\n",
      "2019-04-10 00:06:38,190 root         INFO     Train Epoch: 192 [7168/8000 (90%)]\tTotal Loss: 0.106330\n",
      "Reconstruction: 0.095094, Regularization: 0.011236\n",
      "2019-04-10 00:06:38,253 root         INFO     Train Epoch: 192 [7680/8000 (96%)]\tTotal Loss: 0.122017\n",
      "Reconstruction: 0.110091, Regularization: 0.011926\n",
      "2019-04-10 00:06:38,308 root         INFO     ====> Epoch: 192 Average loss: 0.1118\n",
      "2019-04-10 00:06:38,332 root         INFO     Train Epoch: 193 [0/8000 (0%)]\tTotal Loss: 0.102600\n",
      "Reconstruction: 0.092440, Regularization: 0.010160\n",
      "2019-04-10 00:06:38,398 root         INFO     Train Epoch: 193 [512/8000 (6%)]\tTotal Loss: 0.116664\n",
      "Reconstruction: 0.101300, Regularization: 0.015364\n",
      "2019-04-10 00:06:38,461 root         INFO     Train Epoch: 193 [1024/8000 (13%)]\tTotal Loss: 0.105490\n",
      "Reconstruction: 0.094828, Regularization: 0.010662\n",
      "2019-04-10 00:06:38,525 root         INFO     Train Epoch: 193 [1536/8000 (19%)]\tTotal Loss: 0.107392\n",
      "Reconstruction: 0.097156, Regularization: 0.010236\n",
      "2019-04-10 00:06:38,587 root         INFO     Train Epoch: 193 [2048/8000 (26%)]\tTotal Loss: 0.097547\n",
      "Reconstruction: 0.088287, Regularization: 0.009260\n",
      "2019-04-10 00:06:38,650 root         INFO     Train Epoch: 193 [2560/8000 (32%)]\tTotal Loss: 0.118161\n",
      "Reconstruction: 0.105284, Regularization: 0.012877\n",
      "2019-04-10 00:06:38,712 root         INFO     Train Epoch: 193 [3072/8000 (38%)]\tTotal Loss: 0.109812\n",
      "Reconstruction: 0.094993, Regularization: 0.014819\n",
      "2019-04-10 00:06:38,774 root         INFO     Train Epoch: 193 [3584/8000 (45%)]\tTotal Loss: 0.113186\n",
      "Reconstruction: 0.099636, Regularization: 0.013550\n",
      "2019-04-10 00:06:38,837 root         INFO     Train Epoch: 193 [4096/8000 (51%)]\tTotal Loss: 0.121116\n",
      "Reconstruction: 0.107669, Regularization: 0.013447\n",
      "2019-04-10 00:06:38,900 root         INFO     Train Epoch: 193 [4608/8000 (58%)]\tTotal Loss: 0.102816\n",
      "Reconstruction: 0.089946, Regularization: 0.012870\n",
      "2019-04-10 00:06:38,962 root         INFO     Train Epoch: 193 [5120/8000 (64%)]\tTotal Loss: 0.119470\n",
      "Reconstruction: 0.104636, Regularization: 0.014835\n",
      "2019-04-10 00:06:39,024 root         INFO     Train Epoch: 193 [5632/8000 (70%)]\tTotal Loss: 0.120722\n",
      "Reconstruction: 0.109103, Regularization: 0.011619\n",
      "2019-04-10 00:06:39,087 root         INFO     Train Epoch: 193 [6144/8000 (77%)]\tTotal Loss: 0.117290\n",
      "Reconstruction: 0.104906, Regularization: 0.012384\n",
      "2019-04-10 00:06:39,149 root         INFO     Train Epoch: 193 [6656/8000 (83%)]\tTotal Loss: 0.116916\n",
      "Reconstruction: 0.103757, Regularization: 0.013159\n",
      "2019-04-10 00:06:39,212 root         INFO     Train Epoch: 193 [7168/8000 (90%)]\tTotal Loss: 0.111972\n",
      "Reconstruction: 0.099206, Regularization: 0.012766\n",
      "2019-04-10 00:06:39,275 root         INFO     Train Epoch: 193 [7680/8000 (96%)]\tTotal Loss: 0.112190\n",
      "Reconstruction: 0.100663, Regularization: 0.011527\n",
      "2019-04-10 00:06:39,329 root         INFO     ====> Epoch: 193 Average loss: 0.1119\n",
      "2019-04-10 00:06:39,353 root         INFO     Train Epoch: 194 [0/8000 (0%)]\tTotal Loss: 0.113285\n",
      "Reconstruction: 0.102872, Regularization: 0.010414\n",
      "2019-04-10 00:06:39,417 root         INFO     Train Epoch: 194 [512/8000 (6%)]\tTotal Loss: 0.112119\n",
      "Reconstruction: 0.099251, Regularization: 0.012868\n",
      "2019-04-10 00:06:39,480 root         INFO     Train Epoch: 194 [1024/8000 (13%)]\tTotal Loss: 0.107294\n",
      "Reconstruction: 0.094254, Regularization: 0.013041\n",
      "2019-04-10 00:06:39,544 root         INFO     Train Epoch: 194 [1536/8000 (19%)]\tTotal Loss: 0.105288\n",
      "Reconstruction: 0.093669, Regularization: 0.011619\n",
      "2019-04-10 00:06:39,607 root         INFO     Train Epoch: 194 [2048/8000 (26%)]\tTotal Loss: 0.102829\n",
      "Reconstruction: 0.091615, Regularization: 0.011214\n",
      "2019-04-10 00:06:39,671 root         INFO     Train Epoch: 194 [2560/8000 (32%)]\tTotal Loss: 0.110132\n",
      "Reconstruction: 0.097423, Regularization: 0.012709\n",
      "2019-04-10 00:06:39,735 root         INFO     Train Epoch: 194 [3072/8000 (38%)]\tTotal Loss: 0.122070\n",
      "Reconstruction: 0.106948, Regularization: 0.015122\n",
      "2019-04-10 00:06:39,799 root         INFO     Train Epoch: 194 [3584/8000 (45%)]\tTotal Loss: 0.113766\n",
      "Reconstruction: 0.098408, Regularization: 0.015358\n",
      "2019-04-10 00:06:39,863 root         INFO     Train Epoch: 194 [4096/8000 (51%)]\tTotal Loss: 0.115814\n",
      "Reconstruction: 0.102428, Regularization: 0.013386\n",
      "2019-04-10 00:06:39,927 root         INFO     Train Epoch: 194 [4608/8000 (58%)]\tTotal Loss: 0.108545\n",
      "Reconstruction: 0.098471, Regularization: 0.010074\n",
      "2019-04-10 00:06:39,991 root         INFO     Train Epoch: 194 [5120/8000 (64%)]\tTotal Loss: 0.111576\n",
      "Reconstruction: 0.099536, Regularization: 0.012039\n",
      "2019-04-10 00:06:40,055 root         INFO     Train Epoch: 194 [5632/8000 (70%)]\tTotal Loss: 0.105999\n",
      "Reconstruction: 0.094111, Regularization: 0.011888\n",
      "2019-04-10 00:06:40,119 root         INFO     Train Epoch: 194 [6144/8000 (77%)]\tTotal Loss: 0.115696\n",
      "Reconstruction: 0.101541, Regularization: 0.014154\n",
      "2019-04-10 00:06:40,184 root         INFO     Train Epoch: 194 [6656/8000 (83%)]\tTotal Loss: 0.111865\n",
      "Reconstruction: 0.100470, Regularization: 0.011395\n",
      "2019-04-10 00:06:40,248 root         INFO     Train Epoch: 194 [7168/8000 (90%)]\tTotal Loss: 0.109182\n",
      "Reconstruction: 0.098231, Regularization: 0.010951\n",
      "2019-04-10 00:06:40,311 root         INFO     Train Epoch: 194 [7680/8000 (96%)]\tTotal Loss: 0.114861\n",
      "Reconstruction: 0.101711, Regularization: 0.013150\n",
      "2019-04-10 00:06:40,366 root         INFO     ====> Epoch: 194 Average loss: 0.1113\n",
      "2019-04-10 00:06:40,389 root         INFO     Train Epoch: 195 [0/8000 (0%)]\tTotal Loss: 0.111241\n",
      "Reconstruction: 0.099083, Regularization: 0.012158\n",
      "2019-04-10 00:06:40,452 root         INFO     Train Epoch: 195 [512/8000 (6%)]\tTotal Loss: 0.109991\n",
      "Reconstruction: 0.094591, Regularization: 0.015400\n",
      "2019-04-10 00:06:40,515 root         INFO     Train Epoch: 195 [1024/8000 (13%)]\tTotal Loss: 0.106625\n",
      "Reconstruction: 0.095815, Regularization: 0.010810\n",
      "2019-04-10 00:06:40,578 root         INFO     Train Epoch: 195 [1536/8000 (19%)]\tTotal Loss: 0.107470\n",
      "Reconstruction: 0.097347, Regularization: 0.010123\n",
      "2019-04-10 00:06:40,642 root         INFO     Train Epoch: 195 [2048/8000 (26%)]\tTotal Loss: 0.104332\n",
      "Reconstruction: 0.092784, Regularization: 0.011548\n",
      "2019-04-10 00:06:40,706 root         INFO     Train Epoch: 195 [2560/8000 (32%)]\tTotal Loss: 0.100764\n",
      "Reconstruction: 0.089818, Regularization: 0.010946\n",
      "2019-04-10 00:06:40,769 root         INFO     Train Epoch: 195 [3072/8000 (38%)]\tTotal Loss: 0.108397\n",
      "Reconstruction: 0.097123, Regularization: 0.011274\n",
      "2019-04-10 00:06:40,833 root         INFO     Train Epoch: 195 [3584/8000 (45%)]\tTotal Loss: 0.102473\n",
      "Reconstruction: 0.094037, Regularization: 0.008436\n",
      "2019-04-10 00:06:40,897 root         INFO     Train Epoch: 195 [4096/8000 (51%)]\tTotal Loss: 0.108555\n",
      "Reconstruction: 0.098322, Regularization: 0.010233\n",
      "2019-04-10 00:06:40,961 root         INFO     Train Epoch: 195 [4608/8000 (58%)]\tTotal Loss: 0.104380\n",
      "Reconstruction: 0.092597, Regularization: 0.011783\n",
      "2019-04-10 00:06:41,024 root         INFO     Train Epoch: 195 [5120/8000 (64%)]\tTotal Loss: 0.107813\n",
      "Reconstruction: 0.096995, Regularization: 0.010819\n",
      "2019-04-10 00:06:41,087 root         INFO     Train Epoch: 195 [5632/8000 (70%)]\tTotal Loss: 0.116229\n",
      "Reconstruction: 0.100501, Regularization: 0.015728\n",
      "2019-04-10 00:06:41,149 root         INFO     Train Epoch: 195 [6144/8000 (77%)]\tTotal Loss: 0.118440\n",
      "Reconstruction: 0.103343, Regularization: 0.015097\n",
      "2019-04-10 00:06:41,213 root         INFO     Train Epoch: 195 [6656/8000 (83%)]\tTotal Loss: 0.126573\n",
      "Reconstruction: 0.111443, Regularization: 0.015130\n",
      "2019-04-10 00:06:41,275 root         INFO     Train Epoch: 195 [7168/8000 (90%)]\tTotal Loss: 0.107875\n",
      "Reconstruction: 0.096235, Regularization: 0.011639\n",
      "2019-04-10 00:06:41,340 root         INFO     Train Epoch: 195 [7680/8000 (96%)]\tTotal Loss: 0.112754\n",
      "Reconstruction: 0.100736, Regularization: 0.012018\n",
      "2019-04-10 00:06:41,395 root         INFO     ====> Epoch: 195 Average loss: 0.1113\n",
      "2019-04-10 00:06:41,418 root         INFO     Train Epoch: 196 [0/8000 (0%)]\tTotal Loss: 0.112035\n",
      "Reconstruction: 0.101980, Regularization: 0.010055\n",
      "2019-04-10 00:06:41,483 root         INFO     Train Epoch: 196 [512/8000 (6%)]\tTotal Loss: 0.103852\n",
      "Reconstruction: 0.092591, Regularization: 0.011262\n",
      "2019-04-10 00:06:41,547 root         INFO     Train Epoch: 196 [1024/8000 (13%)]\tTotal Loss: 0.110317\n",
      "Reconstruction: 0.099229, Regularization: 0.011089\n",
      "2019-04-10 00:06:41,610 root         INFO     Train Epoch: 196 [1536/8000 (19%)]\tTotal Loss: 0.110690\n",
      "Reconstruction: 0.101493, Regularization: 0.009197\n",
      "2019-04-10 00:06:41,673 root         INFO     Train Epoch: 196 [2048/8000 (26%)]\tTotal Loss: 0.111220\n",
      "Reconstruction: 0.098231, Regularization: 0.012989\n",
      "2019-04-10 00:06:41,736 root         INFO     Train Epoch: 196 [2560/8000 (32%)]\tTotal Loss: 0.114333\n",
      "Reconstruction: 0.098271, Regularization: 0.016062\n",
      "2019-04-10 00:06:41,797 root         INFO     Train Epoch: 196 [3072/8000 (38%)]\tTotal Loss: 0.117141\n",
      "Reconstruction: 0.102309, Regularization: 0.014833\n",
      "2019-04-10 00:06:41,859 root         INFO     Train Epoch: 196 [3584/8000 (45%)]\tTotal Loss: 0.108339\n",
      "Reconstruction: 0.096641, Regularization: 0.011698\n",
      "2019-04-10 00:06:41,920 root         INFO     Train Epoch: 196 [4096/8000 (51%)]\tTotal Loss: 0.103654\n",
      "Reconstruction: 0.091280, Regularization: 0.012374\n",
      "2019-04-10 00:06:41,982 root         INFO     Train Epoch: 196 [4608/8000 (58%)]\tTotal Loss: 0.104916\n",
      "Reconstruction: 0.094020, Regularization: 0.010896\n",
      "2019-04-10 00:06:42,044 root         INFO     Train Epoch: 196 [5120/8000 (64%)]\tTotal Loss: 0.115774\n",
      "Reconstruction: 0.101561, Regularization: 0.014213\n",
      "2019-04-10 00:06:42,106 root         INFO     Train Epoch: 196 [5632/8000 (70%)]\tTotal Loss: 0.103850\n",
      "Reconstruction: 0.092181, Regularization: 0.011669\n",
      "2019-04-10 00:06:42,168 root         INFO     Train Epoch: 196 [6144/8000 (77%)]\tTotal Loss: 0.103015\n",
      "Reconstruction: 0.092961, Regularization: 0.010054\n",
      "2019-04-10 00:06:42,230 root         INFO     Train Epoch: 196 [6656/8000 (83%)]\tTotal Loss: 0.114315\n",
      "Reconstruction: 0.100985, Regularization: 0.013330\n",
      "2019-04-10 00:06:42,292 root         INFO     Train Epoch: 196 [7168/8000 (90%)]\tTotal Loss: 0.118170\n",
      "Reconstruction: 0.105516, Regularization: 0.012654\n",
      "2019-04-10 00:06:42,354 root         INFO     Train Epoch: 196 [7680/8000 (96%)]\tTotal Loss: 0.105709\n",
      "Reconstruction: 0.093908, Regularization: 0.011801\n",
      "2019-04-10 00:06:42,407 root         INFO     ====> Epoch: 196 Average loss: 0.1116\n",
      "2019-04-10 00:06:42,431 root         INFO     Train Epoch: 197 [0/8000 (0%)]\tTotal Loss: 0.102013\n",
      "Reconstruction: 0.090716, Regularization: 0.011297\n",
      "2019-04-10 00:06:42,496 root         INFO     Train Epoch: 197 [512/8000 (6%)]\tTotal Loss: 0.107807\n",
      "Reconstruction: 0.094591, Regularization: 0.013216\n",
      "2019-04-10 00:06:42,560 root         INFO     Train Epoch: 197 [1024/8000 (13%)]\tTotal Loss: 0.126325\n",
      "Reconstruction: 0.112925, Regularization: 0.013401\n",
      "2019-04-10 00:06:42,623 root         INFO     Train Epoch: 197 [1536/8000 (19%)]\tTotal Loss: 0.114472\n",
      "Reconstruction: 0.097777, Regularization: 0.016695\n",
      "2019-04-10 00:06:42,686 root         INFO     Train Epoch: 197 [2048/8000 (26%)]\tTotal Loss: 0.104604\n",
      "Reconstruction: 0.091145, Regularization: 0.013459\n",
      "2019-04-10 00:06:42,749 root         INFO     Train Epoch: 197 [2560/8000 (32%)]\tTotal Loss: 0.111286\n",
      "Reconstruction: 0.097770, Regularization: 0.013516\n",
      "2019-04-10 00:06:42,812 root         INFO     Train Epoch: 197 [3072/8000 (38%)]\tTotal Loss: 0.113864\n",
      "Reconstruction: 0.101087, Regularization: 0.012776\n",
      "2019-04-10 00:06:42,875 root         INFO     Train Epoch: 197 [3584/8000 (45%)]\tTotal Loss: 0.108076\n",
      "Reconstruction: 0.097098, Regularization: 0.010977\n",
      "2019-04-10 00:06:42,937 root         INFO     Train Epoch: 197 [4096/8000 (51%)]\tTotal Loss: 0.108990\n",
      "Reconstruction: 0.097308, Regularization: 0.011681\n",
      "2019-04-10 00:06:43,000 root         INFO     Train Epoch: 197 [4608/8000 (58%)]\tTotal Loss: 0.108416\n",
      "Reconstruction: 0.097848, Regularization: 0.010568\n",
      "2019-04-10 00:06:43,063 root         INFO     Train Epoch: 197 [5120/8000 (64%)]\tTotal Loss: 0.109531\n",
      "Reconstruction: 0.097353, Regularization: 0.012178\n",
      "2019-04-10 00:06:43,126 root         INFO     Train Epoch: 197 [5632/8000 (70%)]\tTotal Loss: 0.118812\n",
      "Reconstruction: 0.104439, Regularization: 0.014373\n",
      "2019-04-10 00:06:43,189 root         INFO     Train Epoch: 197 [6144/8000 (77%)]\tTotal Loss: 0.106283\n",
      "Reconstruction: 0.094680, Regularization: 0.011604\n",
      "2019-04-10 00:06:43,252 root         INFO     Train Epoch: 197 [6656/8000 (83%)]\tTotal Loss: 0.116550\n",
      "Reconstruction: 0.104315, Regularization: 0.012235\n",
      "2019-04-10 00:06:43,314 root         INFO     Train Epoch: 197 [7168/8000 (90%)]\tTotal Loss: 0.104114\n",
      "Reconstruction: 0.093143, Regularization: 0.010971\n",
      "2019-04-10 00:06:43,377 root         INFO     Train Epoch: 197 [7680/8000 (96%)]\tTotal Loss: 0.116539\n",
      "Reconstruction: 0.102090, Regularization: 0.014450\n",
      "2019-04-10 00:06:43,431 root         INFO     ====> Epoch: 197 Average loss: 0.1114\n",
      "2019-04-10 00:06:43,455 root         INFO     Train Epoch: 198 [0/8000 (0%)]\tTotal Loss: 0.108571\n",
      "Reconstruction: 0.095697, Regularization: 0.012874\n",
      "2019-04-10 00:06:43,520 root         INFO     Train Epoch: 198 [512/8000 (6%)]\tTotal Loss: 0.102106\n",
      "Reconstruction: 0.090416, Regularization: 0.011689\n",
      "2019-04-10 00:06:43,583 root         INFO     Train Epoch: 198 [1024/8000 (13%)]\tTotal Loss: 0.104411\n",
      "Reconstruction: 0.093459, Regularization: 0.010952\n",
      "2019-04-10 00:06:43,647 root         INFO     Train Epoch: 198 [1536/8000 (19%)]\tTotal Loss: 0.109108\n",
      "Reconstruction: 0.097959, Regularization: 0.011149\n",
      "2019-04-10 00:06:43,711 root         INFO     Train Epoch: 198 [2048/8000 (26%)]\tTotal Loss: 0.105742\n",
      "Reconstruction: 0.094362, Regularization: 0.011380\n",
      "2019-04-10 00:06:43,773 root         INFO     Train Epoch: 198 [2560/8000 (32%)]\tTotal Loss: 0.111441\n",
      "Reconstruction: 0.098072, Regularization: 0.013369\n",
      "2019-04-10 00:06:43,835 root         INFO     Train Epoch: 198 [3072/8000 (38%)]\tTotal Loss: 0.108577\n",
      "Reconstruction: 0.096075, Regularization: 0.012501\n",
      "2019-04-10 00:06:43,899 root         INFO     Train Epoch: 198 [3584/8000 (45%)]\tTotal Loss: 0.099548\n",
      "Reconstruction: 0.090277, Regularization: 0.009271\n",
      "2019-04-10 00:06:43,963 root         INFO     Train Epoch: 198 [4096/8000 (51%)]\tTotal Loss: 0.123134\n",
      "Reconstruction: 0.106608, Regularization: 0.016526\n",
      "2019-04-10 00:06:44,025 root         INFO     Train Epoch: 198 [4608/8000 (58%)]\tTotal Loss: 0.118984\n",
      "Reconstruction: 0.107928, Regularization: 0.011056\n",
      "2019-04-10 00:06:44,087 root         INFO     Train Epoch: 198 [5120/8000 (64%)]\tTotal Loss: 0.110871\n",
      "Reconstruction: 0.098376, Regularization: 0.012495\n",
      "2019-04-10 00:06:44,150 root         INFO     Train Epoch: 198 [5632/8000 (70%)]\tTotal Loss: 0.118977\n",
      "Reconstruction: 0.101624, Regularization: 0.017353\n",
      "2019-04-10 00:06:44,212 root         INFO     Train Epoch: 198 [6144/8000 (77%)]\tTotal Loss: 0.096985\n",
      "Reconstruction: 0.088736, Regularization: 0.008249\n",
      "2019-04-10 00:06:44,274 root         INFO     Train Epoch: 198 [6656/8000 (83%)]\tTotal Loss: 0.110586\n",
      "Reconstruction: 0.097830, Regularization: 0.012756\n",
      "2019-04-10 00:06:44,337 root         INFO     Train Epoch: 198 [7168/8000 (90%)]\tTotal Loss: 0.121566\n",
      "Reconstruction: 0.108842, Regularization: 0.012725\n",
      "2019-04-10 00:06:44,399 root         INFO     Train Epoch: 198 [7680/8000 (96%)]\tTotal Loss: 0.105063\n",
      "Reconstruction: 0.094733, Regularization: 0.010330\n",
      "2019-04-10 00:06:44,453 root         INFO     ====> Epoch: 198 Average loss: 0.1115\n",
      "2019-04-10 00:06:44,477 root         INFO     Train Epoch: 199 [0/8000 (0%)]\tTotal Loss: 0.113222\n",
      "Reconstruction: 0.100557, Regularization: 0.012665\n",
      "2019-04-10 00:06:44,541 root         INFO     Train Epoch: 199 [512/8000 (6%)]\tTotal Loss: 0.110832\n",
      "Reconstruction: 0.096227, Regularization: 0.014605\n",
      "2019-04-10 00:06:44,604 root         INFO     Train Epoch: 199 [1024/8000 (13%)]\tTotal Loss: 0.105272\n",
      "Reconstruction: 0.094041, Regularization: 0.011231\n",
      "2019-04-10 00:06:44,667 root         INFO     Train Epoch: 199 [1536/8000 (19%)]\tTotal Loss: 0.117048\n",
      "Reconstruction: 0.105148, Regularization: 0.011900\n",
      "2019-04-10 00:06:44,729 root         INFO     Train Epoch: 199 [2048/8000 (26%)]\tTotal Loss: 0.114484\n",
      "Reconstruction: 0.100584, Regularization: 0.013899\n",
      "2019-04-10 00:06:44,791 root         INFO     Train Epoch: 199 [2560/8000 (32%)]\tTotal Loss: 0.109190\n",
      "Reconstruction: 0.095076, Regularization: 0.014114\n",
      "2019-04-10 00:06:44,853 root         INFO     Train Epoch: 199 [3072/8000 (38%)]\tTotal Loss: 0.121169\n",
      "Reconstruction: 0.107455, Regularization: 0.013714\n",
      "2019-04-10 00:06:44,915 root         INFO     Train Epoch: 199 [3584/8000 (45%)]\tTotal Loss: 0.112114\n",
      "Reconstruction: 0.099300, Regularization: 0.012814\n",
      "2019-04-10 00:06:44,978 root         INFO     Train Epoch: 199 [4096/8000 (51%)]\tTotal Loss: 0.107875\n",
      "Reconstruction: 0.095717, Regularization: 0.012158\n",
      "2019-04-10 00:06:45,040 root         INFO     Train Epoch: 199 [4608/8000 (58%)]\tTotal Loss: 0.119647\n",
      "Reconstruction: 0.105266, Regularization: 0.014381\n",
      "2019-04-10 00:06:45,102 root         INFO     Train Epoch: 199 [5120/8000 (64%)]\tTotal Loss: 0.124504\n",
      "Reconstruction: 0.107906, Regularization: 0.016597\n",
      "2019-04-10 00:06:45,164 root         INFO     Train Epoch: 199 [5632/8000 (70%)]\tTotal Loss: 0.104370\n",
      "Reconstruction: 0.090991, Regularization: 0.013380\n",
      "2019-04-10 00:06:45,226 root         INFO     Train Epoch: 199 [6144/8000 (77%)]\tTotal Loss: 0.112552\n",
      "Reconstruction: 0.100437, Regularization: 0.012115\n",
      "2019-04-10 00:06:45,289 root         INFO     Train Epoch: 199 [6656/8000 (83%)]\tTotal Loss: 0.106073\n",
      "Reconstruction: 0.092097, Regularization: 0.013976\n",
      "2019-04-10 00:06:45,351 root         INFO     Train Epoch: 199 [7168/8000 (90%)]\tTotal Loss: 0.096648\n",
      "Reconstruction: 0.084004, Regularization: 0.012644\n",
      "2019-04-10 00:06:45,415 root         INFO     Train Epoch: 199 [7680/8000 (96%)]\tTotal Loss: 0.127934\n",
      "Reconstruction: 0.109484, Regularization: 0.018450\n",
      "2019-04-10 00:06:45,470 root         INFO     ====> Epoch: 199 Average loss: 0.1114\n",
      "2019-04-10 00:06:45,478 luigi-interface INFO     [pid 26441] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26441) done      TrainVAE()\n",
      "2019-04-10 00:06:45,478 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 00:06:45,479 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   DONE\n",
      "2019-04-10 00:06:45,479 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 00:06:45,479 luigi-interface DEBUG    Pending tasks: 1\n",
      "2019-04-10 00:06:45,479 luigi-interface INFO     [pid 26441] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26441) running   RunAll()\n",
      "2019-04-10 00:06:45,479 luigi-interface INFO     [pid 26441] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26441) done      RunAll()\n",
      "2019-04-10 00:06:45,480 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-10 00:06:45,480 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   DONE\n",
      "2019-04-10 00:06:45,480 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-10 00:06:45,480 luigi-interface DEBUG    Done\n",
      "2019-04-10 00:06:45,480 luigi-interface DEBUG    There are no more tasks to run at this time\n",
      "2019-04-10 00:06:45,480 luigi-interface INFO     Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26441) was stopped. Shutting down Keep-Alive thread\n",
      "2019-04-10 00:06:45,481 luigi-interface INFO     \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 MakeDataSet()\n",
      "* 3 ran successfully:\n",
      "    - 1 RunAll()\n",
      "    - 1 TrainVAE()\n",
      "    - 1 TrainVEM()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "\n",
      "\n",
      "-- Log file: logs2019-04-09 23:56:24.021666.txt\n",
      "\n",
      "2019-04-09 23:56:24,021 root         INFO     start\n",
      "2019-04-09 23:56:24,036 luigi        INFO     logging configured by default settings\n",
      "2019-04-09 23:56:24,066 luigi-interface DEBUG    Checking if RunAll() is complete\n",
      "2019-04-09 23:56:24,066 luigi-interface DEBUG    Checking if TrainVAE() is complete\n",
      "2019-04-09 23:56:24,067 luigi-interface DEBUG    Checking if TrainVEM() is complete\n",
      "2019-04-09 23:56:24,067 luigi-interface INFO     Informed scheduler that task   RunAll__99914b932b   has status   PENDING\n",
      "2019-04-09 23:56:24,068 luigi-interface DEBUG    Checking if MakeDataSet() is complete\n",
      "2019-04-09 23:56:24,068 luigi-interface INFO     Informed scheduler that task   TrainVEM__99914b932b   has status   PENDING\n",
      "2019-04-09 23:56:24,068 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   PENDING\n",
      "2019-04-09 23:56:24,069 luigi-interface INFO     Informed scheduler that task   TrainVAE__99914b932b   has status   PENDING\n",
      "2019-04-09 23:56:24,069 luigi-interface INFO     Done scheduling tasks\n",
      "2019-04-09 23:56:24,069 luigi-interface INFO     Running Worker with 1 processes\n",
      "2019-04-09 23:56:24,069 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-09 23:56:24,070 luigi-interface DEBUG    Pending tasks: 4\n",
      "2019-04-09 23:56:24,070 luigi-interface INFO     [pid 26188] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26188) running   MakeDataSet()\n",
      "2019-04-09 23:56:24,070 root         INFO     Configuration:\n",
      "2019-04-09 23:56:24,070 root         INFO     DATA_DIM = 2\n",
      "2019-04-09 23:56:24,070 root         INFO     LATENT_DIM = 1\n",
      "2019-04-09 23:56:24,070 root         INFO     N_DECODER_LAYERS = 5\n",
      "2019-04-09 23:56:24,070 root         INFO     NONLINEARITY=False\n",
      "2019-04-09 23:56:24,070 root         INFO     WITH_BIASX=True\n",
      "2019-04-09 23:56:24,070 root         INFO     WITH_LOGVARX=True\n",
      "2019-04-09 23:56:24,070 root         INFO     WITH_BIASZ=True\n",
      "2019-04-09 23:56:24,071 root         INFO     WITH_LOGVARZ=True\n",
      "2019-04-09 23:56:24,071 root         INFO     N_SAMPLES=10000\n",
      "2019-04-09 23:56:24,071 root         INFO     W_TRUE:\n",
      "2019-04-09 23:56:24,071 root         INFO     {0: [[0.6], [-0.7]], 1: [[0.1, -0.1], [-0.1, 0.1]], 2: [[0.1, -0.1], [-0.1, 0.1]], 3: [[0.1, -0.1], [-0.1, 0.1]], 4: [[50.0, 0.0], [0.0, -40.0]], 5: [[0.0, 0.0], [0.0, 0.0]]}\n",
      "2019-04-09 23:56:24,071 root         INFO     B_TRUE:\n",
      "2019-04-09 23:56:24,071 root         INFO     {0: [0.0, -0.1], 1: [0.1, 0.0], 2: [0.1, 0.0], 3: [0.1, 0.0], 4: [2.0, 2.4], 5: [0.0, 0.0]}\n",
      "2019-04-09 23:56:27,890 root         INFO     Values of true 'decoder' parameters:\n",
      "2019-04-09 23:56:27,890 root         INFO     layers.0.weight\n",
      "2019-04-09 23:56:27,890 root         INFO     tensor([[ 0.6000],\n",
      "        [-0.7000]], device='cuda:0')\n",
      "2019-04-09 23:56:27,911 root         INFO     layers.0.bias\n",
      "2019-04-09 23:56:27,911 root         INFO     tensor([ 0.0000, -0.1000], device='cuda:0')\n",
      "2019-04-09 23:56:27,912 root         INFO     layers.1.weight\n",
      "2019-04-09 23:56:27,912 root         INFO     tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0')\n",
      "2019-04-09 23:56:27,913 root         INFO     layers.1.bias\n",
      "2019-04-09 23:56:27,913 root         INFO     tensor([0.1000, 0.0000], device='cuda:0')\n",
      "2019-04-09 23:56:27,914 root         INFO     layers.2.weight\n",
      "2019-04-09 23:56:27,914 root         INFO     tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0')\n",
      "2019-04-09 23:56:27,915 root         INFO     layers.2.bias\n",
      "2019-04-09 23:56:27,915 root         INFO     tensor([0.1000, 0.0000], device='cuda:0')\n",
      "2019-04-09 23:56:27,916 root         INFO     layers.3.weight\n",
      "2019-04-09 23:56:27,916 root         INFO     tensor([[ 0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000]], device='cuda:0')\n",
      "2019-04-09 23:56:27,917 root         INFO     layers.3.bias\n",
      "2019-04-09 23:56:27,917 root         INFO     tensor([0.1000, 0.0000], device='cuda:0')\n",
      "2019-04-09 23:56:27,918 root         INFO     layers.4.weight\n",
      "2019-04-09 23:56:27,918 root         INFO     tensor([[ 50.,   0.],\n",
      "        [  0., -40.]], device='cuda:0')\n",
      "2019-04-09 23:56:27,919 root         INFO     layers.4.bias\n",
      "2019-04-09 23:56:27,919 root         INFO     tensor([2.0000, 2.4000], device='cuda:0')\n",
      "2019-04-09 23:56:27,920 root         INFO     layers.5.weight\n",
      "2019-04-09 23:56:27,920 root         INFO     tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "2019-04-09 23:56:27,921 root         INFO     layers.5.bias\n",
      "2019-04-09 23:56:27,921 root         INFO     tensor([0., 0.], device='cuda:0')\n",
      "2019-04-09 23:56:28,030 luigi-interface INFO     [pid 26188] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26188) done      MakeDataSet()\n",
      "2019-04-09 23:56:28,031 luigi-interface DEBUG    1 running tasks, waiting for next task to finish\n",
      "2019-04-09 23:56:28,031 luigi-interface INFO     Informed scheduler that task   MakeDataSet__99914b932b   has status   DONE\n",
      "2019-04-09 23:56:28,031 luigi-interface DEBUG    Asking scheduler for work...\n",
      "2019-04-09 23:56:28,031 luigi-interface DEBUG    Pending tasks: 3\n",
      "2019-04-09 23:56:28,031 luigi-interface INFO     [pid 26188] Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26188) running   TrainVEM()\n",
      "2019-04-09 23:56:28,033 root         INFO     --Dataset tensor: (10000, 2)\n",
      "2019-04-09 23:56:28,034 root         INFO     -- Train tensor: (8000, 2)\n",
      "2019-04-09 23:56:28,035 root         INFO     Values of VEM's decoder parameters before training:\n",
      "2019-04-09 23:56:28,035 root         INFO     layers.0.weight\n",
      "2019-04-09 23:56:28,035 root         INFO     tensor([[-0.0526],\n",
      "        [-0.2078]], device='cuda:0')\n",
      "2019-04-09 23:56:28,036 root         INFO     layers.0.bias\n",
      "2019-04-09 23:56:28,036 root         INFO     tensor([ 0.3363, -0.4224], device='cuda:0')\n",
      "2019-04-09 23:56:28,037 root         INFO     layers.1.weight\n",
      "2019-04-09 23:56:28,037 root         INFO     tensor([[ 0.2904, -0.4362],\n",
      "        [-0.2126,  0.5412]], device='cuda:0')\n",
      "2019-04-09 23:56:28,038 root         INFO     layers.1.bias\n",
      "2019-04-09 23:56:28,039 root         INFO     tensor([ 0.0693, -0.5593], device='cuda:0')\n",
      "2019-04-09 23:56:28,040 root         INFO     layers.2.weight\n",
      "2019-04-09 23:56:28,040 root         INFO     tensor([[ 0.4248, -0.4699],\n",
      "        [-0.0456, -0.0020]], device='cuda:0')\n",
      "2019-04-09 23:56:28,041 root         INFO     layers.2.bias\n",
      "2019-04-09 23:56:28,041 root         INFO     tensor([-0.3116,  0.3165], device='cuda:0')\n",
      "2019-04-09 23:56:28,042 root         INFO     layers.3.weight\n",
      "2019-04-09 23:56:28,042 root         INFO     tensor([[-0.0699,  0.3861],\n",
      "        [ 0.3593, -0.5530]], device='cuda:0')\n",
      "2019-04-09 23:56:28,043 root         INFO     layers.3.bias\n",
      "2019-04-09 23:56:28,043 root         INFO     tensor([0.5281, 0.0176], device='cuda:0')\n",
      "2019-04-09 23:56:28,044 root         INFO     layers.4.weight\n",
      "2019-04-09 23:56:28,044 root         INFO     tensor([[ 0.5583,  0.4401],\n",
      "        [-0.2079, -0.1400]], device='cuda:0')\n",
      "2019-04-09 23:56:28,045 root         INFO     layers.4.bias\n",
      "2019-04-09 23:56:28,045 root         INFO     tensor([-0.2586, -0.3348], device='cuda:0')\n",
      "2019-04-09 23:56:28,046 root         INFO     layers.5.weight\n",
      "2019-04-09 23:56:28,047 root         INFO     tensor([[0.5477, 0.6593],\n",
      "        [0.3130, 0.6352]], device='cuda:0')\n",
      "2019-04-09 23:56:28,048 root         INFO     layers.5.bias\n",
      "2019-04-09 23:56:28,048 root         INFO     tensor([0.2400, 0.1101], device='cuda:0')\n",
      "2019-04-09 23:56:28,138 root         INFO     Train Epoch: 0 [0/8000 (0%)]\tTotal Loss: 1.990688\n",
      "Reconstruction: 1.669326, Regularization: 0.203393, Discriminator: 0.056259; Generator: 0.061711,\n",
      "D(x): 0.263, D(G(z)): 0.197\n",
      "2019-04-09 23:56:28,258 root         INFO     Train Epoch: 0 [512/8000 (6%)]\tTotal Loss: 1.624819\n",
      "Reconstruction: 1.318619, Regularization: 0.197834, Discriminator: 0.064163; Generator: 0.044202,\n",
      "D(x): 0.251, D(G(z)): 0.336\n",
      "2019-04-09 23:56:28,366 root         INFO     Train Epoch: 0 [1024/8000 (13%)]\tTotal Loss: 1.655145\n",
      "Reconstruction: 1.228851, Regularization: 0.302140, Discriminator: 0.066870; Generator: 0.057284,\n",
      "D(x): 0.204, D(G(z)): 0.275\n",
      "2019-04-09 23:56:28,475 root         INFO     Train Epoch: 0 [1536/8000 (19%)]\tTotal Loss: 1.912854\n",
      "Reconstruction: 1.545566, Regularization: 0.246770, Discriminator: 0.065354; Generator: 0.055164,\n",
      "D(x): 0.222, D(G(z)): 0.268\n",
      "2019-04-09 23:56:28,583 root         INFO     Train Epoch: 0 [2048/8000 (26%)]\tTotal Loss: 1.491618\n",
      "Reconstruction: 1.211009, Regularization: 0.170633, Discriminator: 0.060301; Generator: 0.049676,\n",
      "D(x): 0.256, D(G(z)): 0.294\n",
      "2019-04-09 23:56:28,692 root         INFO     Train Epoch: 0 [2560/8000 (32%)]\tTotal Loss: 1.741854\n",
      "Reconstruction: 1.468260, Regularization: 0.162938, Discriminator: 0.059942; Generator: 0.050714,\n",
      "D(x): 0.258, D(G(z)): 0.294\n",
      "2019-04-09 23:56:28,800 root         INFO     Train Epoch: 0 [3072/8000 (38%)]\tTotal Loss: 2.191812\n",
      "Reconstruction: 1.847228, Regularization: 0.225959, Discriminator: 0.059922; Generator: 0.058703,\n",
      "D(x): 0.246, D(G(z)): 0.246\n",
      "2019-04-09 23:56:28,910 root         INFO     Train Epoch: 0 [3584/8000 (45%)]\tTotal Loss: 2.356915\n",
      "Reconstruction: 1.969259, Regularization: 0.270635, Discriminator: 0.058922; Generator: 0.058099,\n",
      "D(x): 0.258, D(G(z)): 0.248\n",
      "2019-04-09 23:56:29,019 root         INFO     Train Epoch: 0 [4096/8000 (51%)]\tTotal Loss: 2.083876\n",
      "Reconstruction: 1.502390, Regularization: 0.467635, Discriminator: 0.059519; Generator: 0.054332,\n",
      "D(x): 0.245, D(G(z)): 0.227\n",
      "2019-04-09 23:56:29,128 root         INFO     Train Epoch: 0 [4608/8000 (58%)]\tTotal Loss: 2.397284\n",
      "Reconstruction: 2.018603, Regularization: 0.268413, Discriminator: 0.061470; Generator: 0.048797,\n",
      "D(x): 0.261, D(G(z)): 0.305\n",
      "2019-04-09 23:56:29,236 root         INFO     Train Epoch: 0 [5120/8000 (64%)]\tTotal Loss: 2.395184\n",
      "Reconstruction: 1.655430, Regularization: 0.633861, Discriminator: 0.059978; Generator: 0.045914,\n",
      "D(x): 0.259, D(G(z)): 0.297\n",
      "2019-04-09 23:56:29,345 root         INFO     Train Epoch: 0 [5632/8000 (70%)]\tTotal Loss: 1.655143\n",
      "Reconstruction: 1.394217, Regularization: 0.156950, Discriminator: 0.054453; Generator: 0.049523,\n",
      "D(x): 0.300, D(G(z)): 0.296\n",
      "2019-04-09 23:56:29,454 root         INFO     Train Epoch: 0 [6144/8000 (77%)]\tTotal Loss: 1.614264\n",
      "Reconstruction: 1.334512, Regularization: 0.176497, Discriminator: 0.058474; Generator: 0.044782,\n",
      "D(x): 0.276, D(G(z)): 0.323\n",
      "2019-04-09 23:56:29,564 root         INFO     Train Epoch: 0 [6656/8000 (83%)]\tTotal Loss: 2.440052\n",
      "Reconstruction: 1.886141, Regularization: 0.456900, Discriminator: 0.054890; Generator: 0.042121,\n",
      "D(x): 0.325, D(G(z)): 0.341\n",
      "2019-04-09 23:56:29,674 root         INFO     Train Epoch: 0 [7168/8000 (90%)]\tTotal Loss: 1.968928\n",
      "Reconstruction: 1.538057, Regularization: 0.322618, Discriminator: 0.060679; Generator: 0.047574,\n",
      "D(x): 0.249, D(G(z)): 0.303\n",
      "2019-04-09 23:56:29,783 root         INFO     Train Epoch: 0 [7680/8000 (96%)]\tTotal Loss: 1.644799\n",
      "Reconstruction: 1.181405, Regularization: 0.358191, Discriminator: 0.055782; Generator: 0.049421,\n",
      "D(x): 0.303, D(G(z)): 0.320\n",
      "2019-04-09 23:56:29,863 root         INFO     ====> Epoch: 0 Average loss: 54.9475\n",
      "2019-04-09 23:56:29,890 root         INFO     Train Epoch: 1 [0/8000 (0%)]\tTotal Loss: 2.065754\n",
      "Reconstruction: 1.714571, Regularization: 0.254070, Discriminator: 0.053164; Generator: 0.043950,\n",
      "D(x): 0.319, D(G(z)): 0.307\n",
      "2019-04-09 23:56:30,001 root         INFO     Train Epoch: 1 [512/8000 (6%)]\tTotal Loss: 332.130554\n",
      "Reconstruction: 331.666107, Regularization: 0.366714, Discriminator: 0.050835; Generator: 0.046909,\n",
      "D(x): 0.349, D(G(z)): 0.290\n",
      "2019-04-09 23:56:30,112 root         INFO     Train Epoch: 1 [1024/8000 (13%)]\tTotal Loss: 3.768329\n",
      "Reconstruction: 3.365359, Regularization: 0.308525, Discriminator: 0.049909; Generator: 0.044537,\n",
      "D(x): 0.339, D(G(z)): 0.299\n",
      "2019-04-09 23:56:30,224 root         INFO     Train Epoch: 1 [1536/8000 (19%)]\tTotal Loss: 1.718879\n",
      "Reconstruction: 1.483924, Regularization: 0.139202, Discriminator: 0.044265; Generator: 0.051490,\n",
      "D(x): 0.362, D(G(z)): 0.245\n",
      "2019-04-09 23:56:30,335 root         INFO     Train Epoch: 1 [2048/8000 (26%)]\tTotal Loss: 1.555435\n",
      "Reconstruction: 1.324286, Regularization: 0.134747, Discriminator: 0.048880; Generator: 0.047522,\n",
      "D(x): 0.345, D(G(z)): 0.298\n",
      "2019-04-09 23:56:30,447 root         INFO     Train Epoch: 1 [2560/8000 (32%)]\tTotal Loss: 2.093114\n",
      "Reconstruction: 1.862962, Regularization: 0.138774, Discriminator: 0.047462; Generator: 0.043916,\n",
      "D(x): 0.371, D(G(z)): 0.315\n",
      "2019-04-09 23:56:30,558 root         INFO     Train Epoch: 1 [3072/8000 (38%)]\tTotal Loss: 1.578318\n",
      "Reconstruction: 1.304710, Regularization: 0.181388, Discriminator: 0.051560; Generator: 0.040660,\n",
      "D(x): 0.364, D(G(z)): 0.358\n",
      "2019-04-09 23:56:30,667 root         INFO     Train Epoch: 1 [3584/8000 (45%)]\tTotal Loss: 1.796010\n",
      "Reconstruction: 1.410722, Regularization: 0.291899, Discriminator: 0.052895; Generator: 0.040495,\n",
      "D(x): 0.327, D(G(z)): 0.336\n",
      "2019-04-09 23:56:30,774 root         INFO     Train Epoch: 1 [4096/8000 (51%)]\tTotal Loss: 4.460275\n",
      "Reconstruction: 3.463312, Regularization: 0.901726, Discriminator: 0.052681; Generator: 0.042557,\n",
      "D(x): 0.326, D(G(z)): 0.312\n",
      "2019-04-09 23:56:30,881 root         INFO     Train Epoch: 1 [4608/8000 (58%)]\tTotal Loss: 2.170061\n",
      "Reconstruction: 1.835484, Regularization: 0.235542, Discriminator: 0.047782; Generator: 0.051253,\n",
      "D(x): 0.352, D(G(z)): 0.280\n",
      "2019-04-09 23:56:30,989 root         INFO     Train Epoch: 1 [5120/8000 (64%)]\tTotal Loss: 1.794173\n",
      "Reconstruction: 1.466591, Regularization: 0.240780, Discriminator: 0.053421; Generator: 0.033382,\n",
      "D(x): 0.373, D(G(z)): 0.409\n",
      "2019-04-09 23:56:31,097 root         INFO     Train Epoch: 1 [5632/8000 (70%)]\tTotal Loss: 1.520652\n",
      "Reconstruction: 1.295243, Regularization: 0.137446, Discriminator: 0.046835; Generator: 0.041128,\n",
      "D(x): 0.397, D(G(z)): 0.349\n",
      "2019-04-09 23:56:31,204 root         INFO     Train Epoch: 1 [6144/8000 (77%)]\tTotal Loss: 1.684649\n",
      "Reconstruction: 1.387137, Regularization: 0.212190, Discriminator: 0.043183; Generator: 0.042139,\n",
      "D(x): 0.393, D(G(z)): 0.300\n",
      "2019-04-09 23:56:31,311 root         INFO     Train Epoch: 1 [6656/8000 (83%)]\tTotal Loss: 1.815170\n",
      "Reconstruction: 1.465186, Regularization: 0.262966, Discriminator: 0.048547; Generator: 0.038471,\n",
      "D(x): 0.417, D(G(z)): 0.390\n",
      "2019-04-09 23:56:31,418 root         INFO     Train Epoch: 1 [7168/8000 (90%)]\tTotal Loss: 2.035432\n",
      "Reconstruction: 1.759441, Regularization: 0.185302, Discriminator: 0.044245; Generator: 0.046443,\n",
      "D(x): 0.368, D(G(z)): 0.276\n",
      "2019-04-09 23:56:31,525 root         INFO     Train Epoch: 1 [7680/8000 (96%)]\tTotal Loss: 1.625159\n",
      "Reconstruction: 1.321116, Regularization: 0.215698, Discriminator: 0.045295; Generator: 0.043051,\n",
      "D(x): 0.427, D(G(z)): 0.344\n",
      "2019-04-09 23:56:31,604 root         INFO     ====> Epoch: 1 Average loss: 140.2451\n",
      "2019-04-09 23:56:31,632 root         INFO     Train Epoch: 2 [0/8000 (0%)]\tTotal Loss: 1.588735\n",
      "Reconstruction: 1.383449, Regularization: 0.124826, Discriminator: 0.039668; Generator: 0.040791,\n",
      "D(x): 0.479, D(G(z)): 0.334\n",
      "2019-04-09 23:56:31,741 root         INFO     Train Epoch: 2 [512/8000 (6%)]\tTotal Loss: 2.461249\n",
      "Reconstruction: 1.404761, Regularization: 0.966607, Discriminator: 0.039549; Generator: 0.050333,\n",
      "D(x): 0.427, D(G(z)): 0.247\n",
      "2019-04-09 23:56:31,850 root         INFO     Train Epoch: 2 [1024/8000 (13%)]\tTotal Loss: 4.908595\n",
      "Reconstruction: 4.172683, Regularization: 0.646251, Discriminator: 0.044734; Generator: 0.044926,\n",
      "D(x): 0.415, D(G(z)): 0.302\n",
      "2019-04-09 23:56:31,959 root         INFO     Train Epoch: 2 [1536/8000 (19%)]\tTotal Loss: 1.587966\n",
      "Reconstruction: 1.378684, Regularization: 0.125971, Discriminator: 0.034823; Generator: 0.048488,\n",
      "D(x): 0.501, D(G(z)): 0.272\n",
      "2019-04-09 23:56:32,067 root         INFO     Train Epoch: 2 [2048/8000 (26%)]\tTotal Loss: 1.613159\n",
      "Reconstruction: 1.267628, Regularization: 0.258321, Discriminator: 0.045936; Generator: 0.041275,\n",
      "D(x): 0.390, D(G(z)): 0.332\n",
      "2019-04-09 23:56:32,176 root         INFO     Train Epoch: 2 [2560/8000 (32%)]\tTotal Loss: 1.819925\n",
      "Reconstruction: 1.523584, Regularization: 0.217576, Discriminator: 0.045218; Generator: 0.033546,\n",
      "D(x): 0.462, D(G(z)): 0.404\n",
      "2019-04-09 23:56:32,285 root         INFO     Train Epoch: 2 [3072/8000 (38%)]\tTotal Loss: 1.501240\n",
      "Reconstruction: 1.268707, Regularization: 0.155643, Discriminator: 0.040243; Generator: 0.036647,\n",
      "D(x): 0.520, D(G(z)): 0.387\n",
      "2019-04-09 23:56:32,393 root         INFO     Train Epoch: 2 [3584/8000 (45%)]\tTotal Loss: 1.988606\n",
      "Reconstruction: 1.734639, Regularization: 0.174845, Discriminator: 0.032511; Generator: 0.046610,\n",
      "D(x): 0.541, D(G(z)): 0.278\n",
      "2019-04-09 23:56:32,502 root         INFO     Train Epoch: 2 [4096/8000 (51%)]\tTotal Loss: 1.575237\n",
      "Reconstruction: 1.338907, Regularization: 0.160457, Discriminator: 0.036336; Generator: 0.039537,\n",
      "D(x): 0.529, D(G(z)): 0.339\n",
      "2019-04-09 23:56:32,611 root         INFO     Train Epoch: 2 [4608/8000 (58%)]\tTotal Loss: 1.966835\n",
      "Reconstruction: 1.670186, Regularization: 0.218511, Discriminator: 0.037835; Generator: 0.040304,\n",
      "D(x): 0.519, D(G(z)): 0.334\n",
      "2019-04-09 23:56:32,720 root         INFO     Train Epoch: 2 [5120/8000 (64%)]\tTotal Loss: 1.588713\n",
      "Reconstruction: 1.377805, Regularization: 0.139191, Discriminator: 0.036545; Generator: 0.035172,\n",
      "D(x): 0.541, D(G(z)): 0.373\n",
      "2019-04-09 23:56:32,828 root         INFO     Train Epoch: 2 [5632/8000 (70%)]\tTotal Loss: 1.624697\n",
      "Reconstruction: 1.287921, Regularization: 0.252085, Discriminator: 0.038965; Generator: 0.045726,\n",
      "D(x): 0.465, D(G(z)): 0.294\n",
      "2019-04-09 23:56:32,937 root         INFO     Train Epoch: 2 [6144/8000 (77%)]\tTotal Loss: 3814.913818\n",
      "Reconstruction: 3814.304688, Regularization: 0.534627, Discriminator: 0.039039; Generator: 0.035517,\n",
      "D(x): 0.528, D(G(z)): 0.370\n",
      "2019-04-09 23:56:33,045 root         INFO     Train Epoch: 2 [6656/8000 (83%)]\tTotal Loss: 1.972117\n",
      "Reconstruction: 1.589818, Regularization: 0.302748, Discriminator: 0.037948; Generator: 0.041603,\n",
      "D(x): 0.515, D(G(z)): 0.335\n",
      "2019-04-09 23:56:33,154 root         INFO     Train Epoch: 2 [7168/8000 (90%)]\tTotal Loss: 28.905569\n",
      "Reconstruction: 28.513056, Regularization: 0.318903, Discriminator: 0.038110; Generator: 0.035500,\n",
      "D(x): 0.528, D(G(z)): 0.363\n",
      "2019-04-09 23:56:33,263 root         INFO     Train Epoch: 2 [7680/8000 (96%)]\tTotal Loss: 2.221935\n",
      "Reconstruction: 1.903444, Regularization: 0.245884, Discriminator: 0.038717; Generator: 0.033890,\n",
      "D(x): 0.551, D(G(z)): 0.383\n",
      "2019-04-09 23:56:33,342 root         INFO     ====> Epoch: 2 Average loss: 21.4772\n",
      "2019-04-09 23:56:33,369 root         INFO     Train Epoch: 3 [0/8000 (0%)]\tTotal Loss: 1.526305\n",
      "Reconstruction: 1.353692, Regularization: 0.100531, Discriminator: 0.030442; Generator: 0.041640,\n",
      "D(x): 0.571, D(G(z)): 0.296\n",
      "2019-04-09 23:56:33,479 root         INFO     Train Epoch: 3 [512/8000 (6%)]\tTotal Loss: 1.379846\n",
      "Reconstruction: 1.166922, Regularization: 0.133895, Discriminator: 0.036814; Generator: 0.042215,\n",
      "D(x): 0.510, D(G(z)): 0.330\n",
      "2019-04-09 23:56:33,588 root         INFO     Train Epoch: 3 [1024/8000 (13%)]\tTotal Loss: 1.498610\n",
      "Reconstruction: 1.234252, Regularization: 0.188891, Discriminator: 0.036224; Generator: 0.039242,\n",
      "D(x): 0.531, D(G(z)): 0.334\n",
      "2019-04-09 23:56:33,697 root         INFO     Train Epoch: 3 [1536/8000 (19%)]\tTotal Loss: 2.587605\n",
      "Reconstruction: 2.170743, Regularization: 0.334406, Discriminator: 0.040094; Generator: 0.042363,\n",
      "D(x): 0.471, D(G(z)): 0.318\n",
      "2019-04-09 23:56:33,805 root         INFO     Train Epoch: 3 [2048/8000 (26%)]\tTotal Loss: 1.675088\n",
      "Reconstruction: 1.462643, Regularization: 0.137985, Discriminator: 0.029522; Generator: 0.044939,\n",
      "D(x): 0.598, D(G(z)): 0.281\n",
      "2019-04-09 23:56:33,914 root         INFO     Train Epoch: 3 [2560/8000 (32%)]\tTotal Loss: 1.459438\n",
      "Reconstruction: 1.228006, Regularization: 0.159392, Discriminator: 0.033570; Generator: 0.038469,\n",
      "D(x): 0.560, D(G(z)): 0.341\n",
      "2019-04-09 23:56:34,023 root         INFO     Train Epoch: 3 [3072/8000 (38%)]\tTotal Loss: 1.490946\n",
      "Reconstruction: 1.259533, Regularization: 0.158141, Discriminator: 0.036968; Generator: 0.036304,\n",
      "D(x): 0.559, D(G(z)): 0.377\n",
      "2019-04-09 23:56:34,130 root         INFO     Train Epoch: 3 [3584/8000 (45%)]\tTotal Loss: 1.557141\n",
      "Reconstruction: 1.356636, Regularization: 0.127096, Discriminator: 0.031276; Generator: 0.042133,\n",
      "D(x): 0.615, D(G(z)): 0.329\n",
      "2019-04-09 23:56:34,236 root         INFO     Train Epoch: 3 [4096/8000 (51%)]\tTotal Loss: 1.778527\n",
      "Reconstruction: 1.424880, Regularization: 0.281464, Discriminator: 0.034490; Generator: 0.037694,\n",
      "D(x): 0.611, D(G(z)): 0.369\n",
      "2019-04-09 23:56:34,342 root         INFO     Train Epoch: 3 [4608/8000 (58%)]\tTotal Loss: 1.699332\n",
      "Reconstruction: 1.459033, Regularization: 0.170604, Discriminator: 0.031541; Generator: 0.038153,\n",
      "D(x): 0.625, D(G(z)): 0.351\n",
      "2019-04-09 23:56:34,448 root         INFO     Train Epoch: 3 [5120/8000 (64%)]\tTotal Loss: 2.041695\n",
      "Reconstruction: 1.597092, Regularization: 0.373435, Discriminator: 0.036922; Generator: 0.034246,\n",
      "D(x): 0.578, D(G(z)): 0.390\n",
      "2019-04-09 23:56:34,554 root         INFO     Train Epoch: 3 [5632/8000 (70%)]\tTotal Loss: 1.734078\n",
      "Reconstruction: 1.331289, Regularization: 0.332681, Discriminator: 0.037797; Generator: 0.032311,\n",
      "D(x): 0.559, D(G(z)): 0.403\n",
      "2019-04-09 23:56:34,660 root         INFO     Train Epoch: 3 [6144/8000 (77%)]\tTotal Loss: 1.471719\n",
      "Reconstruction: 1.236895, Regularization: 0.164057, Discriminator: 0.032773; Generator: 0.037993,\n",
      "D(x): 0.618, D(G(z)): 0.360\n",
      "2019-04-09 23:56:34,766 root         INFO     Train Epoch: 3 [6656/8000 (83%)]\tTotal Loss: 1.452584\n",
      "Reconstruction: 1.198478, Regularization: 0.182877, Discriminator: 0.031375; Generator: 0.039854,\n",
      "D(x): 0.616, D(G(z)): 0.338\n",
      "2019-04-09 23:56:34,873 root         INFO     Train Epoch: 3 [7168/8000 (90%)]\tTotal Loss: 1.535767\n",
      "Reconstruction: 1.306613, Regularization: 0.156602, Discriminator: 0.031797; Generator: 0.040755,\n",
      "D(x): 0.623, D(G(z)): 0.345\n",
      "2019-04-09 23:56:34,980 root         INFO     Train Epoch: 3 [7680/8000 (96%)]\tTotal Loss: 3.005650\n",
      "Reconstruction: 2.583974, Regularization: 0.355070, Discriminator: 0.031271; Generator: 0.035334,\n",
      "D(x): 0.659, D(G(z)): 0.376\n",
      "2019-04-09 23:56:35,058 root         INFO     ====> Epoch: 3 Average loss: 2.9256\n",
      "2019-04-09 23:56:35,086 root         INFO     Train Epoch: 4 [0/8000 (0%)]\tTotal Loss: 1.789068\n",
      "Reconstruction: 1.575146, Regularization: 0.149069, Discriminator: 0.025373; Generator: 0.039479,\n",
      "D(x): 0.702, D(G(z)): 0.320\n",
      "2019-04-09 23:56:35,194 root         INFO     Train Epoch: 4 [512/8000 (6%)]\tTotal Loss: 1.615171\n",
      "Reconstruction: 1.412697, Regularization: 0.139177, Discriminator: 0.029865; Generator: 0.033432,\n",
      "D(x): 0.682, D(G(z)): 0.386\n",
      "2019-04-09 23:56:35,301 root         INFO     Train Epoch: 4 [1024/8000 (13%)]\tTotal Loss: 1.389338\n",
      "Reconstruction: 1.180217, Regularization: 0.142586, Discriminator: 0.030160; Generator: 0.036376,\n",
      "D(x): 0.635, D(G(z)): 0.354\n",
      "2019-04-09 23:56:35,409 root         INFO     Train Epoch: 4 [1536/8000 (19%)]\tTotal Loss: 5.216094\n",
      "Reconstruction: 4.668324, Regularization: 0.482637, Discriminator: 0.031501; Generator: 0.033633,\n",
      "D(x): 0.671, D(G(z)): 0.392\n",
      "2019-04-09 23:56:35,517 root         INFO     Train Epoch: 4 [2048/8000 (26%)]\tTotal Loss: 1.675103\n",
      "Reconstruction: 1.375027, Regularization: 0.236546, Discriminator: 0.026332; Generator: 0.037198,\n",
      "D(x): 0.708, D(G(z)): 0.341\n",
      "2019-04-09 23:56:35,626 root         INFO     Train Epoch: 4 [2560/8000 (32%)]\tTotal Loss: 1.641174\n",
      "Reconstruction: 1.442117, Regularization: 0.133230, Discriminator: 0.032392; Generator: 0.033435,\n",
      "D(x): 0.694, D(G(z)): 0.420\n",
      "2019-04-09 23:56:35,735 root         INFO     Train Epoch: 4 [3072/8000 (38%)]\tTotal Loss: 1.746655\n",
      "Reconstruction: 1.333827, Regularization: 0.344339, Discriminator: 0.035414; Generator: 0.033073,\n",
      "D(x): 0.641, D(G(z)): 0.411\n",
      "2019-04-09 23:56:35,843 root         INFO     Train Epoch: 4 [3584/8000 (45%)]\tTotal Loss: 1.883313\n",
      "Reconstruction: 1.578766, Regularization: 0.239151, Discriminator: 0.027746; Generator: 0.037651,\n",
      "D(x): 0.699, D(G(z)): 0.365\n",
      "2019-04-09 23:56:35,951 root         INFO     Train Epoch: 4 [4096/8000 (51%)]\tTotal Loss: 1.737890\n",
      "Reconstruction: 1.439638, Regularization: 0.232912, Discriminator: 0.032216; Generator: 0.033124,\n",
      "D(x): 0.672, D(G(z)): 0.397\n",
      "2019-04-09 23:56:36,059 root         INFO     Train Epoch: 4 [4608/8000 (58%)]\tTotal Loss: 1.819023\n",
      "Reconstruction: 1.542062, Regularization: 0.211378, Discriminator: 0.027384; Generator: 0.038198,\n",
      "D(x): 0.674, D(G(z)): 0.334\n",
      "2019-04-09 23:56:36,167 root         INFO     Train Epoch: 4 [5120/8000 (64%)]\tTotal Loss: 1.739210\n",
      "Reconstruction: 1.535954, Regularization: 0.140300, Discriminator: 0.031709; Generator: 0.031247,\n",
      "D(x): 0.713, D(G(z)): 0.424\n",
      "2019-04-09 23:56:36,276 root         INFO     Train Epoch: 4 [5632/8000 (70%)]\tTotal Loss: 1.646433\n",
      "Reconstruction: 1.380309, Regularization: 0.202059, Discriminator: 0.024297; Generator: 0.039768,\n",
      "D(x): 0.724, D(G(z)): 0.322\n",
      "2019-04-09 23:56:36,383 root         INFO     Train Epoch: 4 [6144/8000 (77%)]\tTotal Loss: 1.656913\n",
      "Reconstruction: 1.436267, Regularization: 0.156808, Discriminator: 0.028333; Generator: 0.035505,\n",
      "D(x): 0.739, D(G(z)): 0.385\n",
      "2019-04-09 23:56:36,489 root         INFO     Train Epoch: 4 [6656/8000 (83%)]\tTotal Loss: 1.420493\n",
      "Reconstruction: 1.213093, Regularization: 0.141584, Discriminator: 0.028607; Generator: 0.037209,\n",
      "D(x): 0.676, D(G(z)): 0.354\n",
      "2019-04-09 23:56:36,595 root         INFO     Train Epoch: 4 [7168/8000 (90%)]\tTotal Loss: 1.498974\n",
      "Reconstruction: 1.243621, Regularization: 0.192484, Discriminator: 0.029498; Generator: 0.033371,\n",
      "D(x): 0.711, D(G(z)): 0.396\n",
      "2019-04-09 23:56:36,701 root         INFO     Train Epoch: 4 [7680/8000 (96%)]\tTotal Loss: 1.583854\n",
      "Reconstruction: 1.315641, Regularization: 0.203957, Discriminator: 0.025444; Generator: 0.038811,\n",
      "D(x): 0.724, D(G(z)): 0.341\n",
      "2019-04-09 23:56:36,778 root         INFO     ====> Epoch: 4 Average loss: 39.1477\n",
      "2019-04-09 23:56:36,805 root         INFO     Train Epoch: 5 [0/8000 (0%)]\tTotal Loss: 1.501662\n",
      "Reconstruction: 1.223356, Regularization: 0.215827, Discriminator: 0.030674; Generator: 0.031805,\n",
      "D(x): 0.710, D(G(z)): 0.410\n",
      "2019-04-09 23:56:36,912 root         INFO     Train Epoch: 5 [512/8000 (6%)]\tTotal Loss: 1.608510\n",
      "Reconstruction: 1.386291, Regularization: 0.159402, Discriminator: 0.026768; Generator: 0.036049,\n",
      "D(x): 0.737, D(G(z)): 0.374\n",
      "2019-04-09 23:56:37,020 root         INFO     Train Epoch: 5 [1024/8000 (13%)]\tTotal Loss: 1.624130\n",
      "Reconstruction: 1.323320, Regularization: 0.237282, Discriminator: 0.028333; Generator: 0.035196,\n",
      "D(x): 0.726, D(G(z)): 0.377\n",
      "2019-04-09 23:56:37,128 root         INFO     Train Epoch: 5 [1536/8000 (19%)]\tTotal Loss: 1.859639\n",
      "Reconstruction: 1.390548, Regularization: 0.403660, Discriminator: 0.028427; Generator: 0.037003,\n",
      "D(x): 0.706, D(G(z)): 0.364\n",
      "2019-04-09 23:56:37,235 root         INFO     Train Epoch: 5 [2048/8000 (26%)]\tTotal Loss: 1.913852\n",
      "Reconstruction: 1.685413, Regularization: 0.169186, Discriminator: 0.027531; Generator: 0.031722,\n",
      "D(x): 0.792, D(G(z)): 0.420\n",
      "2019-04-09 23:56:37,342 root         INFO     Train Epoch: 5 [2560/8000 (32%)]\tTotal Loss: 1.700515\n",
      "Reconstruction: 1.434433, Regularization: 0.205208, Discriminator: 0.025404; Generator: 0.035469,\n",
      "D(x): 0.781, D(G(z)): 0.385\n",
      "2019-04-09 23:56:37,451 root         INFO     Train Epoch: 5 [3072/8000 (38%)]\tTotal Loss: 1.428733\n",
      "Reconstruction: 1.234230, Regularization: 0.131083, Discriminator: 0.021991; Generator: 0.041430,\n",
      "D(x): 0.756, D(G(z)): 0.306\n",
      "2019-04-09 23:56:37,558 root         INFO     Train Epoch: 5 [3584/8000 (45%)]\tTotal Loss: 1.447007\n",
      "Reconstruction: 1.268984, Regularization: 0.119996, Discriminator: 0.025924; Generator: 0.032104,\n",
      "D(x): 0.801, D(G(z)): 0.412\n",
      "2019-04-09 23:56:37,665 root         INFO     Train Epoch: 5 [4096/8000 (51%)]\tTotal Loss: 1.427812\n",
      "Reconstruction: 1.266942, Regularization: 0.101138, Discriminator: 0.025186; Generator: 0.034545,\n",
      "D(x): 0.810, D(G(z)): 0.393\n",
      "2019-04-09 23:56:37,772 root         INFO     Train Epoch: 5 [4608/8000 (58%)]\tTotal Loss: 1.651293\n",
      "Reconstruction: 1.325788, Regularization: 0.266490, Discriminator: 0.026932; Generator: 0.032083,\n",
      "D(x): 0.741, D(G(z)): 0.386\n",
      "2019-04-09 23:56:37,878 root         INFO     Train Epoch: 5 [5120/8000 (64%)]\tTotal Loss: 1.574745\n",
      "Reconstruction: 1.325287, Regularization: 0.191046, Discriminator: 0.027398; Generator: 0.031015,\n",
      "D(x): 0.781, D(G(z)): 0.420\n",
      "2019-04-09 23:56:37,988 root         INFO     Train Epoch: 5 [5632/8000 (70%)]\tTotal Loss: 1.651543\n",
      "Reconstruction: 1.465569, Regularization: 0.127045, Discriminator: 0.023191; Generator: 0.035738,\n",
      "D(x): 0.828, D(G(z)): 0.365\n",
      "2019-04-09 23:56:38,096 root         INFO     Train Epoch: 5 [6144/8000 (77%)]\tTotal Loss: 1.477932\n",
      "Reconstruction: 1.249635, Regularization: 0.167337, Discriminator: 0.023879; Generator: 0.037081,\n",
      "D(x): 0.761, D(G(z)): 0.351\n",
      "2019-04-09 23:56:38,205 root         INFO     Train Epoch: 5 [6656/8000 (83%)]\tTotal Loss: 1.395162\n",
      "Reconstruction: 1.160832, Regularization: 0.175715, Discriminator: 0.021866; Generator: 0.036749,\n",
      "D(x): 0.808, D(G(z)): 0.349\n",
      "2019-04-09 23:56:38,313 root         INFO     Train Epoch: 5 [7168/8000 (90%)]\tTotal Loss: 2.093519\n",
      "Reconstruction: 1.563771, Regularization: 0.470306, Discriminator: 0.029317; Generator: 0.030126,\n",
      "D(x): 0.761, D(G(z)): 0.428\n",
      "2019-04-09 23:56:38,422 root         INFO     Train Epoch: 5 [7680/8000 (96%)]\tTotal Loss: 1.450665\n",
      "Reconstruction: 1.247699, Regularization: 0.143186, Discriminator: 0.019627; Generator: 0.040153,\n",
      "D(x): 0.815, D(G(z)): 0.309\n",
      "2019-04-09 23:56:38,502 root         INFO     ====> Epoch: 5 Average loss: 2.3389\n",
      "2019-04-09 23:56:38,529 root         INFO     Train Epoch: 6 [0/8000 (0%)]\tTotal Loss: 1.696990\n",
      "Reconstruction: 1.422348, Regularization: 0.215499, Discriminator: 0.026094; Generator: 0.033048,\n",
      "D(x): 0.767, D(G(z)): 0.383\n",
      "2019-04-09 23:56:38,638 root         INFO     Train Epoch: 6 [512/8000 (6%)]\tTotal Loss: 1.795038\n",
      "Reconstruction: 1.378292, Regularization: 0.353987, Discriminator: 0.029208; Generator: 0.033551,\n",
      "D(x): 0.761, D(G(z)): 0.414\n",
      "2019-04-09 23:56:38,747 root         INFO     Train Epoch: 6 [1024/8000 (13%)]\tTotal Loss: 2.439158\n",
      "Reconstruction: 1.647327, Regularization: 0.727095, Discriminator: 0.029749; Generator: 0.034987,\n",
      "D(x): 0.718, D(G(z)): 0.391\n",
      "2019-04-09 23:56:38,855 root         INFO     Train Epoch: 6 [1536/8000 (19%)]\tTotal Loss: 2.030977\n",
      "Reconstruction: 1.478409, Regularization: 0.490456, Discriminator: 0.029153; Generator: 0.032959,\n",
      "D(x): 0.723, D(G(z)): 0.394\n",
      "2019-04-09 23:56:38,964 root         INFO     Train Epoch: 6 [2048/8000 (26%)]\tTotal Loss: 1.624785\n",
      "Reconstruction: 1.289620, Regularization: 0.273831, Discriminator: 0.023766; Generator: 0.037567,\n",
      "D(x): 0.751, D(G(z)): 0.337\n",
      "2019-04-09 23:56:39,071 root         INFO     Train Epoch: 6 [2560/8000 (32%)]\tTotal Loss: 1.919639\n",
      "Reconstruction: 1.502447, Regularization: 0.353347, Discriminator: 0.027495; Generator: 0.036350,\n",
      "D(x): 0.764, D(G(z)): 0.378\n",
      "2019-04-09 23:56:39,179 root         INFO     Train Epoch: 6 [3072/8000 (38%)]\tTotal Loss: 1.586316\n",
      "Reconstruction: 1.320010, Regularization: 0.210852, Discriminator: 0.024329; Generator: 0.031125,\n",
      "D(x): 0.822, D(G(z)): 0.405\n",
      "2019-04-09 23:56:39,287 root         INFO     Train Epoch: 6 [3584/8000 (45%)]\tTotal Loss: 1.748013\n",
      "Reconstruction: 1.469058, Regularization: 0.220181, Discriminator: 0.024363; Generator: 0.034411,\n",
      "D(x): 0.793, D(G(z)): 0.376\n",
      "2019-04-09 23:56:39,395 root         INFO     Train Epoch: 6 [4096/8000 (51%)]\tTotal Loss: 1.575164\n",
      "Reconstruction: 1.428677, Regularization: 0.091434, Discriminator: 0.021775; Generator: 0.033277,\n",
      "D(x): 0.880, D(G(z)): 0.397\n",
      "2019-04-09 23:56:39,503 root         INFO     Train Epoch: 6 [4608/8000 (58%)]\tTotal Loss: 1.411844\n",
      "Reconstruction: 1.216275, Regularization: 0.137847, Discriminator: 0.022361; Generator: 0.035361,\n",
      "D(x): 0.831, D(G(z)): 0.372\n",
      "2019-04-09 23:56:39,612 root         INFO     Train Epoch: 6 [5120/8000 (64%)]\tTotal Loss: 3.067523\n",
      "Reconstruction: 2.632092, Regularization: 0.377342, Discriminator: 0.023519; Generator: 0.034569,\n",
      "D(x): 0.815, D(G(z)): 0.374\n",
      "2019-04-09 23:56:39,719 root         INFO     Train Epoch: 6 [5632/8000 (70%)]\tTotal Loss: 1.607256\n",
      "Reconstruction: 1.248060, Regularization: 0.297462, Discriminator: 0.022912; Generator: 0.038823,\n",
      "D(x): 0.799, D(G(z)): 0.345\n",
      "2019-04-09 23:56:39,827 root         INFO     Train Epoch: 6 [6144/8000 (77%)]\tTotal Loss: 1.498035\n",
      "Reconstruction: 1.250858, Regularization: 0.188507, Discriminator: 0.019939; Generator: 0.038732,\n",
      "D(x): 0.830, D(G(z)): 0.329\n",
      "2019-04-09 23:56:39,935 root         INFO     Train Epoch: 6 [6656/8000 (83%)]\tTotal Loss: 1.771176\n",
      "Reconstruction: 1.357732, Regularization: 0.352549, Discriminator: 0.021684; Generator: 0.039210,\n",
      "D(x): 0.818, D(G(z)): 0.344\n",
      "2019-04-09 23:56:40,043 root         INFO     Train Epoch: 6 [7168/8000 (90%)]\tTotal Loss: 1.479267\n",
      "Reconstruction: 1.235536, Regularization: 0.182942, Discriminator: 0.025261; Generator: 0.035527,\n",
      "D(x): 0.801, D(G(z)): 0.381\n",
      "2019-04-09 23:56:40,151 root         INFO     Train Epoch: 6 [7680/8000 (96%)]\tTotal Loss: 1.613051\n",
      "Reconstruction: 1.344814, Regularization: 0.205836, Discriminator: 0.018635; Generator: 0.043765,\n",
      "D(x): 0.840, D(G(z)): 0.304\n",
      "2019-04-09 23:56:40,230 root         INFO     ====> Epoch: 6 Average loss: 1.9319\n",
      "2019-04-09 23:56:40,257 root         INFO     Train Epoch: 7 [0/8000 (0%)]\tTotal Loss: 1.730209\n",
      "Reconstruction: 1.287645, Regularization: 0.379549, Discriminator: 0.021821; Generator: 0.041193,\n",
      "D(x): 0.785, D(G(z)): 0.325\n",
      "2019-04-09 23:56:40,365 root         INFO     Train Epoch: 7 [512/8000 (6%)]\tTotal Loss: 1.474890\n",
      "Reconstruction: 1.283612, Regularization: 0.129554, Discriminator: 0.021303; Generator: 0.040420,\n",
      "D(x): 0.829, D(G(z)): 0.336\n",
      "2019-04-09 23:56:40,474 root         INFO     Train Epoch: 7 [1024/8000 (13%)]\tTotal Loss: 1.699181\n",
      "Reconstruction: 1.403993, Regularization: 0.233301, Discriminator: 0.023947; Generator: 0.037941,\n",
      "D(x): 0.784, D(G(z)): 0.349\n",
      "2019-04-09 23:56:40,583 root         INFO     Train Epoch: 7 [1536/8000 (19%)]\tTotal Loss: 1.516524\n",
      "Reconstruction: 1.223533, Regularization: 0.230879, Discriminator: 0.021981; Generator: 0.040132,\n",
      "D(x): 0.789, D(G(z)): 0.328\n",
      "2019-04-09 23:56:40,691 root         INFO     Train Epoch: 7 [2048/8000 (26%)]\tTotal Loss: 2.519198\n",
      "Reconstruction: 2.066422, Regularization: 0.390562, Discriminator: 0.023178; Generator: 0.039035,\n",
      "D(x): 0.776, D(G(z)): 0.327\n",
      "2019-04-09 23:56:40,800 root         INFO     Train Epoch: 7 [2560/8000 (32%)]\tTotal Loss: 1.717609\n",
      "Reconstruction: 1.437921, Regularization: 0.219348, Discriminator: 0.022321; Generator: 0.038019,\n",
      "D(x): 0.803, D(G(z)): 0.339\n",
      "2019-04-09 23:56:40,908 root         INFO     Train Epoch: 7 [3072/8000 (38%)]\tTotal Loss: 1.905750\n",
      "Reconstruction: 1.649619, Regularization: 0.195623, Discriminator: 0.019187; Generator: 0.041320,\n",
      "D(x): 0.828, D(G(z)): 0.313\n",
      "2019-04-09 23:56:41,015 root         INFO     Train Epoch: 7 [3584/8000 (45%)]\tTotal Loss: 1.679858\n",
      "Reconstruction: 1.303400, Regularization: 0.315820, Discriminator: 0.024493; Generator: 0.036146,\n",
      "D(x): 0.816, D(G(z)): 0.370\n",
      "2019-04-09 23:56:41,123 root         INFO     Train Epoch: 7 [4096/8000 (51%)]\tTotal Loss: 2.141476\n",
      "Reconstruction: 1.757437, Regularization: 0.325768, Discriminator: 0.021120; Generator: 0.037151,\n",
      "D(x): 0.827, D(G(z)): 0.345\n",
      "2019-04-09 23:56:41,231 root         INFO     Train Epoch: 7 [4608/8000 (58%)]\tTotal Loss: 1.506384\n",
      "Reconstruction: 1.245839, Regularization: 0.200920, Discriminator: 0.021935; Generator: 0.037690,\n",
      "D(x): 0.863, D(G(z)): 0.363\n",
      "2019-04-09 23:56:41,340 root         INFO     Train Epoch: 7 [5120/8000 (64%)]\tTotal Loss: 2.153474\n",
      "Reconstruction: 1.690112, Regularization: 0.402757, Discriminator: 0.019636; Generator: 0.040970,\n",
      "D(x): 0.819, D(G(z)): 0.307\n",
      "2019-04-09 23:56:41,449 root         INFO     Train Epoch: 7 [5632/8000 (70%)]\tTotal Loss: 2.442940\n",
      "Reconstruction: 1.618057, Regularization: 0.765093, Discriminator: 0.023745; Generator: 0.036045,\n",
      "D(x): 0.799, D(G(z)): 0.354\n",
      "2019-04-09 23:56:41,558 root         INFO     Train Epoch: 7 [6144/8000 (77%)]\tTotal Loss: 2.013590\n",
      "Reconstruction: 1.426416, Regularization: 0.528376, Discriminator: 0.018686; Generator: 0.040112,\n",
      "D(x): 0.872, D(G(z)): 0.326\n",
      "2019-04-09 23:56:41,668 root         INFO     Train Epoch: 7 [6656/8000 (83%)]\tTotal Loss: 1.549333\n",
      "Reconstruction: 1.168936, Regularization: 0.317393, Discriminator: 0.021038; Generator: 0.041966,\n",
      "D(x): 0.819, D(G(z)): 0.323\n",
      "2019-04-09 23:56:41,778 root         INFO     Train Epoch: 7 [7168/8000 (90%)]\tTotal Loss: 2.579311\n",
      "Reconstruction: 1.555383, Regularization: 0.962621, Discriminator: 0.019580; Generator: 0.041726,\n",
      "D(x): 0.858, D(G(z)): 0.329\n",
      "2019-04-09 23:56:41,888 root         INFO     Train Epoch: 7 [7680/8000 (96%)]\tTotal Loss: 1.847908\n",
      "Reconstruction: 1.413610, Regularization: 0.379909, Discriminator: 0.021116; Generator: 0.033273,\n",
      "D(x): 0.841, D(G(z)): 0.368\n",
      "2019-04-09 23:56:41,969 root         INFO     ====> Epoch: 7 Average loss: 1.8006\n",
      "2019-04-09 23:56:41,996 root         INFO     Train Epoch: 8 [0/8000 (0%)]\tTotal Loss: 1.483434\n",
      "Reconstruction: 1.269515, Regularization: 0.153508, Discriminator: 0.018883; Generator: 0.041528,\n",
      "D(x): 0.821, D(G(z)): 0.304\n",
      "2019-04-09 23:56:42,108 root         INFO     Train Epoch: 8 [512/8000 (6%)]\tTotal Loss: 1.769373\n",
      "Reconstruction: 1.517588, Regularization: 0.194845, Discriminator: 0.018886; Generator: 0.038054,\n",
      "D(x): 0.861, D(G(z)): 0.334\n",
      "2019-04-09 23:56:42,219 root         INFO     Train Epoch: 8 [1024/8000 (13%)]\tTotal Loss: 1.537488\n",
      "Reconstruction: 1.247050, Regularization: 0.231578, Discriminator: 0.017374; Generator: 0.041487,\n",
      "D(x): 0.852, D(G(z)): 0.299\n",
      "2019-04-09 23:56:42,330 root         INFO     Train Epoch: 8 [1536/8000 (19%)]\tTotal Loss: 1.377329\n",
      "Reconstruction: 1.186607, Regularization: 0.131381, Discriminator: 0.019821; Generator: 0.039519,\n",
      "D(x): 0.828, D(G(z)): 0.322\n",
      "2019-04-09 23:56:42,441 root         INFO     Train Epoch: 8 [2048/8000 (26%)]\tTotal Loss: 2.643383\n",
      "Reconstruction: 1.794798, Regularization: 0.788315, Discriminator: 0.017915; Generator: 0.042355,\n",
      "D(x): 0.840, D(G(z)): 0.293\n",
      "2019-04-09 23:56:42,552 root         INFO     Train Epoch: 8 [2560/8000 (32%)]\tTotal Loss: 1.686712\n",
      "Reconstruction: 1.377317, Regularization: 0.251456, Discriminator: 0.019844; Generator: 0.038095,\n",
      "D(x): 0.835, D(G(z)): 0.326\n",
      "2019-04-09 23:56:42,663 root         INFO     Train Epoch: 8 [3072/8000 (38%)]\tTotal Loss: 1.627136\n",
      "Reconstruction: 1.281648, Regularization: 0.284018, Discriminator: 0.016574; Generator: 0.044897,\n",
      "D(x): 0.857, D(G(z)): 0.282\n",
      "2019-04-09 23:56:42,774 root         INFO     Train Epoch: 8 [3584/8000 (45%)]\tTotal Loss: 1.578250\n",
      "Reconstruction: 1.402458, Regularization: 0.115319, Discriminator: 0.015635; Generator: 0.044838,\n",
      "D(x): 0.906, D(G(z)): 0.287\n",
      "2019-04-09 23:56:42,885 root         INFO     Train Epoch: 8 [4096/8000 (51%)]\tTotal Loss: 2.388305\n",
      "Reconstruction: 1.566802, Regularization: 0.761328, Discriminator: 0.022232; Generator: 0.037944,\n",
      "D(x): 0.823, D(G(z)): 0.348\n",
      "2019-04-09 23:56:42,995 root         INFO     Train Epoch: 8 [4608/8000 (58%)]\tTotal Loss: 1.850419\n",
      "Reconstruction: 1.533777, Regularization: 0.257733, Discriminator: 0.017354; Generator: 0.041555,\n",
      "D(x): 0.892, D(G(z)): 0.319\n",
      "2019-04-09 23:56:43,106 root         INFO     Train Epoch: 8 [5120/8000 (64%)]\tTotal Loss: 1.689117\n",
      "Reconstruction: 1.333444, Regularization: 0.293357, Discriminator: 0.017708; Generator: 0.044608,\n",
      "D(x): 0.877, D(G(z)): 0.310\n",
      "2019-04-09 23:56:43,217 root         INFO     Train Epoch: 8 [5632/8000 (70%)]\tTotal Loss: 2.015880\n",
      "Reconstruction: 1.551828, Regularization: 0.400662, Discriminator: 0.020242; Generator: 0.043149,\n",
      "D(x): 0.813, D(G(z)): 0.308\n",
      "2019-04-09 23:56:43,327 root         INFO     Train Epoch: 8 [6144/8000 (77%)]\tTotal Loss: 1.492984\n",
      "Reconstruction: 1.283955, Regularization: 0.151427, Discriminator: 0.017302; Generator: 0.040299,\n",
      "D(x): 0.880, D(G(z)): 0.316\n",
      "2019-04-09 23:56:43,438 root         INFO     Train Epoch: 8 [6656/8000 (83%)]\tTotal Loss: 2.259302\n",
      "Reconstruction: 1.982016, Regularization: 0.217491, Discriminator: 0.015560; Generator: 0.044235,\n",
      "D(x): 0.900, D(G(z)): 0.295\n",
      "2019-04-09 23:56:43,549 root         INFO     Train Epoch: 8 [7168/8000 (90%)]\tTotal Loss: 2.100173\n",
      "Reconstruction: 1.729268, Regularization: 0.310297, Discriminator: 0.018975; Generator: 0.041634,\n",
      "D(x): 0.844, D(G(z)): 0.314\n",
      "2019-04-09 23:56:43,659 root         INFO     Train Epoch: 8 [7680/8000 (96%)]\tTotal Loss: 1.857148\n",
      "Reconstruction: 1.594260, Regularization: 0.202293, Discriminator: 0.016890; Generator: 0.043705,\n",
      "D(x): 0.892, D(G(z)): 0.313\n",
      "2019-04-09 23:56:43,740 root         INFO     ====> Epoch: 8 Average loss: 1.9225\n",
      "2019-04-09 23:56:43,767 root         INFO     Train Epoch: 9 [0/8000 (0%)]\tTotal Loss: 1.933936\n",
      "Reconstruction: 1.695395, Regularization: 0.180389, Discriminator: 0.016947; Generator: 0.041204,\n",
      "D(x): 0.874, D(G(z)): 0.301\n",
      "2019-04-09 23:56:43,880 root         INFO     Train Epoch: 9 [512/8000 (6%)]\tTotal Loss: 1.843019\n",
      "Reconstruction: 1.612097, Regularization: 0.168634, Discriminator: 0.015199; Generator: 0.047089,\n",
      "D(x): 0.900, D(G(z)): 0.285\n",
      "2019-04-09 23:56:43,990 root         INFO     Train Epoch: 9 [1024/8000 (13%)]\tTotal Loss: 1.803779\n",
      "Reconstruction: 1.430146, Regularization: 0.313956, Discriminator: 0.018782; Generator: 0.040895,\n",
      "D(x): 0.844, D(G(z)): 0.306\n",
      "2019-04-09 23:56:44,101 root         INFO     Train Epoch: 9 [1536/8000 (19%)]\tTotal Loss: 1.659956\n",
      "Reconstruction: 1.389536, Regularization: 0.207314, Discriminator: 0.015321; Generator: 0.047785,\n",
      "D(x): 0.881, D(G(z)): 0.272\n",
      "2019-04-09 23:56:44,210 root         INFO     Train Epoch: 9 [2048/8000 (26%)]\tTotal Loss: 3.252057\n",
      "Reconstruction: 1.692990, Regularization: 1.499277, Discriminator: 0.013440; Generator: 0.046350,\n",
      "D(x): 0.907, D(G(z)): 0.265\n",
      "2019-04-09 23:56:44,318 root         INFO     Train Epoch: 9 [2560/8000 (32%)]\tTotal Loss: 1.927242\n",
      "Reconstruction: 1.669375, Regularization: 0.199569, Discriminator: 0.014029; Generator: 0.044269,\n",
      "D(x): 0.918, D(G(z)): 0.281\n",
      "2019-04-09 23:56:44,428 root         INFO     Train Epoch: 9 [3072/8000 (38%)]\tTotal Loss: 1.775023\n",
      "Reconstruction: 1.436397, Regularization: 0.274745, Discriminator: 0.015365; Generator: 0.048517,\n",
      "D(x): 0.857, D(G(z)): 0.246\n",
      "2019-04-09 23:56:44,537 root         INFO     Train Epoch: 9 [3584/8000 (45%)]\tTotal Loss: 2.150807\n",
      "Reconstruction: 1.757948, Regularization: 0.332409, Discriminator: 0.015693; Generator: 0.044757,\n",
      "D(x): 0.889, D(G(z)): 0.286\n",
      "2019-04-09 23:56:44,645 root         INFO     Train Epoch: 9 [4096/8000 (51%)]\tTotal Loss: 1.484145\n",
      "Reconstruction: 1.230078, Regularization: 0.193216, Discriminator: 0.017418; Generator: 0.043433,\n",
      "D(x): 0.865, D(G(z)): 0.298\n",
      "2019-04-09 23:56:44,754 root         INFO     Train Epoch: 9 [4608/8000 (58%)]\tTotal Loss: 3.350528\n",
      "Reconstruction: 2.699772, Regularization: 0.586379, Discriminator: 0.017005; Generator: 0.047374,\n",
      "D(x): 0.889, D(G(z)): 0.280\n",
      "2019-04-09 23:56:44,863 root         INFO     Train Epoch: 9 [5120/8000 (64%)]\tTotal Loss: 1.707605\n",
      "Reconstruction: 1.429688, Regularization: 0.209019, Discriminator: 0.012784; Generator: 0.056114,\n",
      "D(x): 0.883, D(G(z)): 0.215\n",
      "2019-04-09 23:56:44,971 root         INFO     Train Epoch: 9 [5632/8000 (70%)]\tTotal Loss: 2.003741\n",
      "Reconstruction: 1.553811, Regularization: 0.387625, Discriminator: 0.013899; Generator: 0.048407,\n",
      "D(x): 0.880, D(G(z)): 0.248\n",
      "2019-04-09 23:56:45,079 root         INFO     Train Epoch: 9 [6144/8000 (77%)]\tTotal Loss: 1.595999\n",
      "Reconstruction: 1.397616, Regularization: 0.138179, Discriminator: 0.015285; Generator: 0.044918,\n",
      "D(x): 0.897, D(G(z)): 0.289\n",
      "2019-04-09 23:56:45,188 root         INFO     Train Epoch: 9 [6656/8000 (83%)]\tTotal Loss: 1.735807\n",
      "Reconstruction: 1.444190, Regularization: 0.230595, Discriminator: 0.017350; Generator: 0.043672,\n",
      "D(x): 0.872, D(G(z)): 0.301\n",
      "2019-04-09 23:56:45,297 root         INFO     Train Epoch: 9 [7168/8000 (90%)]\tTotal Loss: 2.299118\n",
      "Reconstruction: 1.898466, Regularization: 0.342850, Discriminator: 0.014543; Generator: 0.043260,\n",
      "D(x): 0.897, D(G(z)): 0.279\n",
      "2019-04-09 23:56:45,406 root         INFO     Train Epoch: 9 [7680/8000 (96%)]\tTotal Loss: 1.898182\n",
      "Reconstruction: 1.604060, Regularization: 0.228713, Discriminator: 0.013582; Generator: 0.051826,\n",
      "D(x): 0.911, D(G(z)): 0.256\n",
      "2019-04-09 23:56:45,486 root         INFO     ====> Epoch: 9 Average loss: 2.3601\n",
      "2019-04-09 23:56:45,513 root         INFO     Train Epoch: 10 [0/8000 (0%)]\tTotal Loss: 1.524064\n",
      "Reconstruction: 1.280290, Regularization: 0.182879, Discriminator: 0.015018; Generator: 0.045876,\n",
      "D(x): 0.881, D(G(z)): 0.271\n",
      "2019-04-09 23:56:45,625 root         INFO     Train Epoch: 10 [512/8000 (6%)]\tTotal Loss: 2.037281\n",
      "Reconstruction: 1.773524, Regularization: 0.205094, Discriminator: 0.015153; Generator: 0.043510,\n",
      "D(x): 0.894, D(G(z)): 0.292\n",
      "2019-04-09 23:56:45,735 root         INFO     Train Epoch: 10 [1024/8000 (13%)]\tTotal Loss: 1.767463\n",
      "Reconstruction: 1.513179, Regularization: 0.189714, Discriminator: 0.012054; Generator: 0.052517,\n",
      "D(x): 0.917, D(G(z)): 0.233\n",
      "2019-04-09 23:56:45,845 root         INFO     Train Epoch: 10 [1536/8000 (19%)]\tTotal Loss: 1.852831\n",
      "Reconstruction: 1.530046, Regularization: 0.261810, Discriminator: 0.012189; Generator: 0.048785,\n",
      "D(x): 0.919, D(G(z)): 0.243\n",
      "2019-04-09 23:56:45,955 root         INFO     Train Epoch: 10 [2048/8000 (26%)]\tTotal Loss: 1.583315\n",
      "Reconstruction: 1.402003, Regularization: 0.118350, Discriminator: 0.011008; Generator: 0.051954,\n",
      "D(x): 0.923, D(G(z)): 0.223\n",
      "2019-04-09 23:56:46,065 root         INFO     Train Epoch: 10 [2560/8000 (32%)]\tTotal Loss: 2.151776\n",
      "Reconstruction: 1.706205, Regularization: 0.385645, Discriminator: 0.013572; Generator: 0.046354,\n",
      "D(x): 0.910, D(G(z)): 0.268\n",
      "2019-04-09 23:56:46,175 root         INFO     Train Epoch: 10 [3072/8000 (38%)]\tTotal Loss: 1.322916\n",
      "Reconstruction: 1.132830, Regularization: 0.128248, Discriminator: 0.012829; Generator: 0.049009,\n",
      "D(x): 0.912, D(G(z)): 0.253\n",
      "2019-04-09 23:56:46,285 root         INFO     Train Epoch: 10 [3584/8000 (45%)]\tTotal Loss: 1.948674\n",
      "Reconstruction: 1.747093, Regularization: 0.138472, Discriminator: 0.013907; Generator: 0.049202,\n",
      "D(x): 0.914, D(G(z)): 0.255\n",
      "2019-04-09 23:56:46,395 root         INFO     Train Epoch: 10 [4096/8000 (51%)]\tTotal Loss: 1.837927\n",
      "Reconstruction: 1.592814, Regularization: 0.181987, Discriminator: 0.012468; Generator: 0.050659,\n",
      "D(x): 0.896, D(G(z)): 0.229\n",
      "2019-04-09 23:56:46,504 root         INFO     Train Epoch: 10 [4608/8000 (58%)]\tTotal Loss: 1.982230\n",
      "Reconstruction: 1.763630, Regularization: 0.161113, Discriminator: 0.015358; Generator: 0.042128,\n",
      "D(x): 0.933, D(G(z)): 0.312\n",
      "2019-04-09 23:56:46,613 root         INFO     Train Epoch: 10 [5120/8000 (64%)]\tTotal Loss: 1.825347\n",
      "Reconstruction: 1.516142, Regularization: 0.246625, Discriminator: 0.010442; Generator: 0.052137,\n",
      "D(x): 0.922, D(G(z)): 0.210\n",
      "2019-04-09 23:56:46,722 root         INFO     Train Epoch: 10 [5632/8000 (70%)]\tTotal Loss: 2.893113\n",
      "Reconstruction: 2.029893, Regularization: 0.802908, Discriminator: 0.012687; Generator: 0.047625,\n",
      "D(x): 0.897, D(G(z)): 0.244\n",
      "2019-04-09 23:56:46,831 root         INFO     Train Epoch: 10 [6144/8000 (77%)]\tTotal Loss: 2.815832\n",
      "Reconstruction: 2.319904, Regularization: 0.431091, Discriminator: 0.015278; Generator: 0.049559,\n",
      "D(x): 0.871, D(G(z)): 0.256\n",
      "2019-04-09 23:56:46,940 root         INFO     Train Epoch: 10 [6656/8000 (83%)]\tTotal Loss: 2.896575\n",
      "Reconstruction: 2.489029, Regularization: 0.342536, Discriminator: 0.011915; Generator: 0.053095,\n",
      "D(x): 0.905, D(G(z)): 0.221\n",
      "2019-04-09 23:56:47,049 root         INFO     Train Epoch: 10 [7168/8000 (90%)]\tTotal Loss: 2.151717\n",
      "Reconstruction: 1.752032, Regularization: 0.334824, Discriminator: 0.010826; Generator: 0.054036,\n",
      "D(x): 0.931, D(G(z)): 0.222\n",
      "2019-04-09 23:56:47,157 root         INFO     Train Epoch: 10 [7680/8000 (96%)]\tTotal Loss: 2.519321\n",
      "Reconstruction: 2.100776, Regularization: 0.359334, Discriminator: 0.014313; Generator: 0.044899,\n",
      "D(x): 0.905, D(G(z)): 0.278\n",
      "2019-04-09 23:56:47,236 root         INFO     ====> Epoch: 10 Average loss: 5.1710\n",
      "2019-04-09 23:56:47,263 root         INFO     Train Epoch: 11 [0/8000 (0%)]\tTotal Loss: 2.317814\n",
      "Reconstruction: 1.893189, Regularization: 0.356106, Discriminator: 0.010851; Generator: 0.057667,\n",
      "D(x): 0.878, D(G(z)): 0.181\n",
      "2019-04-09 23:56:47,375 root         INFO     Train Epoch: 11 [512/8000 (6%)]\tTotal Loss: 1.694211\n",
      "Reconstruction: 1.493998, Regularization: 0.134377, Discriminator: 0.010123; Generator: 0.055712,\n",
      "D(x): 0.920, D(G(z)): 0.201\n",
      "2019-04-09 23:56:47,485 root         INFO     Train Epoch: 11 [1024/8000 (13%)]\tTotal Loss: 1.869845\n",
      "Reconstruction: 1.676934, Regularization: 0.132420, Discriminator: 0.010857; Generator: 0.049634,\n",
      "D(x): 0.940, D(G(z)): 0.236\n",
      "2019-04-09 23:56:47,594 root         INFO     Train Epoch: 11 [1536/8000 (19%)]\tTotal Loss: 12.480996\n",
      "Reconstruction: 12.183100, Regularization: 0.236722, Discriminator: 0.012259; Generator: 0.048916,\n",
      "D(x): 0.906, D(G(z)): 0.236\n",
      "2019-04-09 23:56:47,703 root         INFO     Train Epoch: 11 [2048/8000 (26%)]\tTotal Loss: 1.952992\n",
      "Reconstruction: 1.714041, Regularization: 0.176506, Discriminator: 0.010694; Generator: 0.051751,\n",
      "D(x): 0.912, D(G(z)): 0.212\n",
      "2019-04-09 23:56:47,812 root         INFO     Train Epoch: 11 [2560/8000 (32%)]\tTotal Loss: 2.242863\n",
      "Reconstruction: 1.642544, Regularization: 0.537492, Discriminator: 0.010828; Generator: 0.051998,\n",
      "D(x): 0.914, D(G(z)): 0.217\n",
      "2019-04-09 23:56:47,921 root         INFO     Train Epoch: 11 [3072/8000 (38%)]\tTotal Loss: 4.554789\n",
      "Reconstruction: 3.764421, Regularization: 0.726836, Discriminator: 0.010085; Generator: 0.053448,\n",
      "D(x): 0.925, D(G(z)): 0.207\n",
      "2019-04-09 23:56:48,032 root         INFO     Train Epoch: 11 [3584/8000 (45%)]\tTotal Loss: 2.517943\n",
      "Reconstruction: 2.212128, Regularization: 0.237365, Discriminator: 0.010420; Generator: 0.058030,\n",
      "D(x): 0.903, D(G(z)): 0.186\n",
      "2019-04-09 23:56:48,144 root         INFO     Train Epoch: 11 [4096/8000 (51%)]\tTotal Loss: 1.688606\n",
      "Reconstruction: 1.518474, Regularization: 0.109922, Discriminator: 0.011169; Generator: 0.049041,\n",
      "D(x): 0.921, D(G(z)): 0.227\n",
      "2019-04-09 23:56:48,255 root         INFO     Train Epoch: 11 [4608/8000 (58%)]\tTotal Loss: 1.945765\n",
      "Reconstruction: 1.692282, Regularization: 0.190213, Discriminator: 0.011467; Generator: 0.051804,\n",
      "D(x): 0.902, D(G(z)): 0.217\n",
      "2019-04-09 23:56:48,366 root         INFO     Train Epoch: 11 [5120/8000 (64%)]\tTotal Loss: 1.993718\n",
      "Reconstruction: 1.781809, Regularization: 0.148576, Discriminator: 0.011138; Generator: 0.052195,\n",
      "D(x): 0.910, D(G(z)): 0.215\n",
      "2019-04-09 23:56:48,476 root         INFO     Train Epoch: 11 [5632/8000 (70%)]\tTotal Loss: 11.985268\n",
      "Reconstruction: 11.614804, Regularization: 0.309780, Discriminator: 0.010687; Generator: 0.049997,\n",
      "D(x): 0.923, D(G(z)): 0.222\n",
      "2019-04-09 23:56:48,586 root         INFO     Train Epoch: 11 [6144/8000 (77%)]\tTotal Loss: 1.976598\n",
      "Reconstruction: 1.494973, Regularization: 0.415039, Discriminator: 0.009520; Generator: 0.057066,\n",
      "D(x): 0.914, D(G(z)): 0.184\n",
      "2019-04-09 23:56:48,697 root         INFO     Train Epoch: 11 [6656/8000 (83%)]\tTotal Loss: 3.751858\n",
      "Reconstruction: 3.460594, Regularization: 0.226703, Discriminator: 0.009624; Generator: 0.054936,\n",
      "D(x): 0.929, D(G(z)): 0.195\n",
      "2019-04-09 23:56:48,807 root         INFO     Train Epoch: 11 [7168/8000 (90%)]\tTotal Loss: 4.441367\n",
      "Reconstruction: 4.165646, Regularization: 0.208011, Discriminator: 0.009853; Generator: 0.057856,\n",
      "D(x): 0.902, D(G(z)): 0.177\n",
      "2019-04-09 23:56:48,916 root         INFO     Train Epoch: 11 [7680/8000 (96%)]\tTotal Loss: 1.944575\n",
      "Reconstruction: 1.626973, Regularization: 0.252871, Discriminator: 0.010394; Generator: 0.054337,\n",
      "D(x): 0.931, D(G(z)): 0.213\n",
      "2019-04-09 23:56:48,996 root         INFO     ====> Epoch: 11 Average loss: 8.9531\n",
      "2019-04-09 23:56:49,024 root         INFO     Train Epoch: 12 [0/8000 (0%)]\tTotal Loss: 2.505531\n",
      "Reconstruction: 2.272855, Regularization: 0.169504, Discriminator: 0.009005; Generator: 0.054168,\n",
      "D(x): 0.931, D(G(z)): 0.188\n",
      "2019-04-09 23:56:49,135 root         INFO     Train Epoch: 12 [512/8000 (6%)]\tTotal Loss: 4.580524\n",
      "Reconstruction: 3.871854, Regularization: 0.643009, Discriminator: 0.009204; Generator: 0.056457,\n",
      "D(x): 0.934, D(G(z)): 0.192\n",
      "2019-04-09 23:56:49,246 root         INFO     Train Epoch: 12 [1024/8000 (13%)]\tTotal Loss: 2.426898\n",
      "Reconstruction: 2.173368, Regularization: 0.182901, Discriminator: 0.007098; Generator: 0.063531,\n",
      "D(x): 0.941, D(G(z)): 0.148\n",
      "2019-04-09 23:56:49,356 root         INFO     Train Epoch: 12 [1536/8000 (19%)]\tTotal Loss: 1.963347\n",
      "Reconstruction: 1.712731, Regularization: 0.182662, Discriminator: 0.008550; Generator: 0.059403,\n",
      "D(x): 0.923, D(G(z)): 0.166\n",
      "2019-04-09 23:56:49,466 root         INFO     Train Epoch: 12 [2048/8000 (26%)]\tTotal Loss: 2.249247\n",
      "Reconstruction: 1.927508, Regularization: 0.260583, Discriminator: 0.009135; Generator: 0.052020,\n",
      "D(x): 0.948, D(G(z)): 0.206\n",
      "2019-04-09 23:56:49,575 root         INFO     Train Epoch: 12 [2560/8000 (32%)]\tTotal Loss: 1.855768\n",
      "Reconstruction: 1.581002, Regularization: 0.208014, Discriminator: 0.008461; Generator: 0.058290,\n",
      "D(x): 0.933, D(G(z)): 0.174\n",
      "2019-04-09 23:56:49,683 root         INFO     Train Epoch: 12 [3072/8000 (38%)]\tTotal Loss: 2.660201\n",
      "Reconstruction: 2.383689, Regularization: 0.209699, Discriminator: 0.007078; Generator: 0.059735,\n",
      "D(x): 0.958, D(G(z)): 0.164\n",
      "2019-04-09 23:56:49,791 root         INFO     Train Epoch: 12 [3584/8000 (45%)]\tTotal Loss: 2.880131\n",
      "Reconstruction: 2.428166, Regularization: 0.384579, Discriminator: 0.007580; Generator: 0.059805,\n",
      "D(x): 0.962, D(G(z)): 0.178\n",
      "2019-04-09 23:56:49,900 root         INFO     Train Epoch: 12 [4096/8000 (51%)]\tTotal Loss: 18991.238281\n",
      "Reconstruction: 18989.892578, Regularization: 1.278844, Discriminator: 0.008081; Generator: 0.057381,\n",
      "D(x): 0.937, D(G(z)): 0.171\n",
      "2019-04-09 23:56:50,008 root         INFO     Train Epoch: 12 [4608/8000 (58%)]\tTotal Loss: 10.393563\n",
      "Reconstruction: 10.037867, Regularization: 0.288296, Discriminator: 0.007068; Generator: 0.060334,\n",
      "D(x): 0.954, D(G(z)): 0.160\n",
      "2019-04-09 23:56:50,116 root         INFO     Train Epoch: 12 [5120/8000 (64%)]\tTotal Loss: 2.462568\n",
      "Reconstruction: 2.058523, Regularization: 0.336773, Discriminator: 0.008424; Generator: 0.058848,\n",
      "D(x): 0.930, D(G(z)): 0.170\n",
      "2019-04-09 23:56:50,224 root         INFO     Train Epoch: 12 [5632/8000 (70%)]\tTotal Loss: 1.906154\n",
      "Reconstruction: 1.571204, Regularization: 0.270312, Discriminator: 0.008413; Generator: 0.056224,\n",
      "D(x): 0.945, D(G(z)): 0.185\n",
      "2019-04-09 23:56:50,333 root         INFO     Train Epoch: 12 [6144/8000 (77%)]\tTotal Loss: 1.980548\n",
      "Reconstruction: 1.632612, Regularization: 0.276459, Discriminator: 0.007045; Generator: 0.064432,\n",
      "D(x): 0.931, D(G(z)): 0.138\n",
      "2019-04-09 23:56:50,440 root         INFO     Train Epoch: 12 [6656/8000 (83%)]\tTotal Loss: 3.033243\n",
      "Reconstruction: 2.760910, Regularization: 0.203006, Discriminator: 0.006832; Generator: 0.062496,\n",
      "D(x): 0.951, D(G(z)): 0.150\n",
      "2019-04-09 23:56:50,547 root         INFO     Train Epoch: 12 [7168/8000 (90%)]\tTotal Loss: 2.890050\n",
      "Reconstruction: 2.569585, Regularization: 0.251606, Discriminator: 0.008170; Generator: 0.060689,\n",
      "D(x): 0.927, D(G(z)): 0.158\n",
      "2019-04-09 23:56:50,655 root         INFO     Train Epoch: 12 [7680/8000 (96%)]\tTotal Loss: 6.056471\n",
      "Reconstruction: 5.473882, Regularization: 0.513400, Discriminator: 0.006996; Generator: 0.062193,\n",
      "D(x): 0.943, D(G(z)): 0.148\n",
      "2019-04-09 23:56:50,735 root         INFO     ====> Epoch: 12 Average loss: 819.7865\n",
      "2019-04-09 23:56:50,762 root         INFO     Train Epoch: 13 [0/8000 (0%)]\tTotal Loss: 6.829357\n",
      "Reconstruction: 6.506642, Regularization: 0.250731, Discriminator: 0.006193; Generator: 0.065791,\n",
      "D(x): 0.951, D(G(z)): 0.135\n",
      "2019-04-09 23:56:50,874 root         INFO     Train Epoch: 13 [512/8000 (6%)]\tTotal Loss: 2.907015\n",
      "Reconstruction: 2.560496, Regularization: 0.273710, Discriminator: 0.006174; Generator: 0.066635,\n",
      "D(x): 0.947, D(G(z)): 0.130\n",
      "2019-04-09 23:56:50,985 root         INFO     Train Epoch: 13 [1024/8000 (13%)]\tTotal Loss: 4.649789\n",
      "Reconstruction: 4.330743, Regularization: 0.247860, Discriminator: 0.007024; Generator: 0.064162,\n",
      "D(x): 0.938, D(G(z)): 0.144\n",
      "2019-04-09 23:56:51,097 root         INFO     Train Epoch: 13 [1536/8000 (19%)]\tTotal Loss: 2.029166\n",
      "Reconstruction: 1.837226, Regularization: 0.119924, Discriminator: 0.007198; Generator: 0.064819,\n",
      "D(x): 0.927, D(G(z)): 0.136\n",
      "2019-04-09 23:56:51,208 root         INFO     Train Epoch: 13 [2048/8000 (26%)]\tTotal Loss: 1.681198\n",
      "Reconstruction: 1.515396, Regularization: 0.097138, Discriminator: 0.007011; Generator: 0.061652,\n",
      "D(x): 0.943, D(G(z)): 0.148\n",
      "2019-04-09 23:56:51,319 root         INFO     Train Epoch: 13 [2560/8000 (32%)]\tTotal Loss: 11.787499\n",
      "Reconstruction: 11.364071, Regularization: 0.353048, Discriminator: 0.006918; Generator: 0.063463,\n",
      "D(x): 0.938, D(G(z)): 0.141\n",
      "2019-04-09 23:56:51,430 root         INFO     Train Epoch: 13 [3072/8000 (38%)]\tTotal Loss: 4.560582\n",
      "Reconstruction: 4.080457, Regularization: 0.407790, Discriminator: 0.007165; Generator: 0.065169,\n",
      "D(x): 0.932, D(G(z)): 0.132\n",
      "2019-04-09 23:56:51,541 root         INFO     Train Epoch: 13 [3584/8000 (45%)]\tTotal Loss: 2.653806\n",
      "Reconstruction: 2.399246, Regularization: 0.183105, Discriminator: 0.005770; Generator: 0.065685,\n",
      "D(x): 0.955, D(G(z)): 0.128\n",
      "2019-04-09 23:56:51,653 root         INFO     Train Epoch: 13 [4096/8000 (51%)]\tTotal Loss: 34.305412\n",
      "Reconstruction: 34.079006, Regularization: 0.151486, Discriminator: 0.006671; Generator: 0.068250,\n",
      "D(x): 0.930, D(G(z)): 0.123\n",
      "2019-04-09 23:56:51,763 root         INFO     Train Epoch: 13 [4608/8000 (58%)]\tTotal Loss: 7.114954\n",
      "Reconstruction: 6.896148, Regularization: 0.145054, Discriminator: 0.005275; Generator: 0.068477,\n",
      "D(x): 0.959, D(G(z)): 0.118\n",
      "2019-04-09 23:56:51,868 root         INFO     Train Epoch: 13 [5120/8000 (64%)]\tTotal Loss: 2.326792\n",
      "Reconstruction: 2.121194, Regularization: 0.130349, Discriminator: 0.005817; Generator: 0.069431,\n",
      "D(x): 0.945, D(G(z)): 0.118\n",
      "2019-04-09 23:56:51,973 root         INFO     Train Epoch: 13 [5632/8000 (70%)]\tTotal Loss: 12.682895\n",
      "Reconstruction: 12.218901, Regularization: 0.389576, Discriminator: 0.005971; Generator: 0.068447,\n",
      "D(x): 0.941, D(G(z)): 0.119\n",
      "2019-04-09 23:56:52,078 root         INFO     Train Epoch: 13 [6144/8000 (77%)]\tTotal Loss: 24011.261719\n",
      "Reconstruction: 24010.585938, Regularization: 0.598653, Discriminator: 0.004916; Generator: 0.071457,\n",
      "D(x): 0.960, D(G(z)): 0.108\n",
      "2019-04-09 23:56:52,186 root         INFO     Train Epoch: 13 [6656/8000 (83%)]\tTotal Loss: 3289.359863\n",
      "Reconstruction: 3288.411621, Regularization: 0.871228, Discriminator: 0.005509; Generator: 0.071429,\n",
      "D(x): 0.942, D(G(z)): 0.107\n",
      "2019-04-09 23:56:52,294 root         INFO     Train Epoch: 13 [7168/8000 (90%)]\tTotal Loss: 325.321472\n",
      "Reconstruction: 324.931519, Regularization: 0.313964, Discriminator: 0.004970; Generator: 0.071021,\n",
      "D(x): 0.959, D(G(z)): 0.108\n",
      "2019-04-09 23:56:52,403 root         INFO     Train Epoch: 13 [7680/8000 (96%)]\tTotal Loss: 6.666081\n",
      "Reconstruction: 6.359587, Regularization: 0.228755, Discriminator: 0.006334; Generator: 0.071405,\n",
      "D(x): 0.927, D(G(z)): 0.112\n",
      "2019-04-09 23:56:52,484 root         INFO     ====> Epoch: 13 Average loss: 180731740.5782\n",
      "2019-04-09 23:56:52,510 root         INFO     Train Epoch: 14 [0/8000 (0%)]\tTotal Loss: 1.575136\n",
      "Reconstruction: 1.294246, Regularization: 0.200892, Discriminator: 0.005738; Generator: 0.074261,\n",
      "D(x): 0.924, D(G(z)): 0.097\n",
      "2019-04-09 23:56:52,619 root         INFO     Train Epoch: 14 [512/8000 (6%)]\tTotal Loss: 4.100451\n",
      "Reconstruction: 3.756438, Regularization: 0.265631, Discriminator: 0.004755; Generator: 0.073628,\n",
      "D(x): 0.958, D(G(z)): 0.102\n",
      "2019-04-09 23:56:52,727 root         INFO     Train Epoch: 14 [1024/8000 (13%)]\tTotal Loss: 21.999414\n",
      "Reconstruction: 21.734949, Regularization: 0.187654, Discriminator: 0.005526; Generator: 0.071285,\n",
      "D(x): 0.944, D(G(z)): 0.109\n",
      "2019-04-09 23:56:52,835 root         INFO     Train Epoch: 14 [1536/8000 (19%)]\tTotal Loss: 6.187638\n",
      "Reconstruction: 5.924358, Regularization: 0.184774, Discriminator: 0.005228; Generator: 0.073279,\n",
      "D(x): 0.943, D(G(z)): 0.101\n",
      "2019-04-09 23:56:52,943 root         INFO     Train Epoch: 14 [2048/8000 (26%)]\tTotal Loss: 2.470510\n",
      "Reconstruction: 2.323057, Regularization: 0.071392, Discriminator: 0.004645; Generator: 0.071416,\n",
      "D(x): 0.966, D(G(z)): 0.107\n",
      "2019-04-09 23:56:53,051 root         INFO     Train Epoch: 14 [2560/8000 (32%)]\tTotal Loss: 180.057281\n",
      "Reconstruction: 179.738770, Regularization: 0.240130, Discriminator: 0.004076; Generator: 0.074305,\n",
      "D(x): 0.974, D(G(z)): 0.098\n",
      "2019-04-09 23:56:53,159 root         INFO     Train Epoch: 14 [3072/8000 (38%)]\tTotal Loss: 8.441469\n",
      "Reconstruction: 8.076963, Regularization: 0.288740, Discriminator: 0.005914; Generator: 0.069852,\n",
      "D(x): 0.941, D(G(z)): 0.115\n",
      "2019-04-09 23:56:53,267 root         INFO     Train Epoch: 14 [3584/8000 (45%)]\tTotal Loss: 21.041758\n",
      "Reconstruction: 20.264778, Regularization: 0.699023, Discriminator: 0.004571; Generator: 0.073386,\n",
      "D(x): 0.965, D(G(z)): 0.103\n",
      "2019-04-09 23:56:53,375 root         INFO     Train Epoch: 14 [4096/8000 (51%)]\tTotal Loss: 3.022159\n",
      "Reconstruction: 2.716441, Regularization: 0.226936, Discriminator: 0.005504; Generator: 0.073278,\n",
      "D(x): 0.941, D(G(z)): 0.103\n",
      "2019-04-09 23:56:53,483 root         INFO     Train Epoch: 14 [4608/8000 (58%)]\tTotal Loss: 2.762718\n",
      "Reconstruction: 2.569721, Regularization: 0.111820, Discriminator: 0.004788; Generator: 0.076389,\n",
      "D(x): 0.946, D(G(z)): 0.089\n",
      "2019-04-09 23:56:53,591 root         INFO     Train Epoch: 14 [5120/8000 (64%)]\tTotal Loss: 3.812033\n",
      "Reconstruction: 3.600995, Regularization: 0.131216, Discriminator: 0.004052; Generator: 0.075770,\n",
      "D(x): 0.970, D(G(z)): 0.094\n",
      "2019-04-09 23:56:53,699 root         INFO     Train Epoch: 14 [5632/8000 (70%)]\tTotal Loss: 5165.478516\n",
      "Reconstruction: 5165.166016, Regularization: 0.232918, Discriminator: 0.004820; Generator: 0.074703,\n",
      "D(x): 0.952, D(G(z)): 0.097\n",
      "2019-04-09 23:56:53,808 root         INFO     Train Epoch: 14 [6144/8000 (77%)]\tTotal Loss: 6.151014\n",
      "Reconstruction: 5.793443, Regularization: 0.277126, Discriminator: 0.004225; Generator: 0.076221,\n",
      "D(x): 0.961, D(G(z)): 0.089\n",
      "2019-04-09 23:56:53,916 root         INFO     Train Epoch: 14 [6656/8000 (83%)]\tTotal Loss: 8.842331\n",
      "Reconstruction: 8.645065, Regularization: 0.117290, Discriminator: 0.004962; Generator: 0.075014,\n",
      "D(x): 0.950, D(G(z)): 0.096\n",
      "2019-04-09 23:56:54,024 root         INFO     Train Epoch: 14 [7168/8000 (90%)]\tTotal Loss: 3.200033\n",
      "Reconstruction: 2.981887, Regularization: 0.138028, Discriminator: 0.004073; Generator: 0.076045,\n",
      "D(x): 0.969, D(G(z)): 0.092\n",
      "2019-04-09 23:56:54,132 root         INFO     Train Epoch: 14 [7680/8000 (96%)]\tTotal Loss: 3.735596\n",
      "Reconstruction: 3.537262, Regularization: 0.116139, Discriminator: 0.003601; Generator: 0.078593,\n",
      "D(x): 0.975, D(G(z)): 0.085\n",
      "2019-04-09 23:56:54,211 root         INFO     ====> Epoch: 14 Average loss: 866988.9379\n",
      "2019-04-09 23:56:54,238 root         INFO     Train Epoch: 15 [0/8000 (0%)]\tTotal Loss: 3852.153320\n",
      "Reconstruction: 3851.766846, Regularization: 0.305066, Discriminator: 0.003867; Generator: 0.077425,\n",
      "D(x): 0.971, D(G(z)): 0.088\n",
      "2019-04-09 23:56:54,348 root         INFO     Train Epoch: 15 [512/8000 (6%)]\tTotal Loss: 2.321685\n",
      "Reconstruction: 2.066370, Regularization: 0.173360, Discriminator: 0.004967; Generator: 0.076987,\n",
      "D(x): 0.941, D(G(z)): 0.090\n",
      "2019-04-09 23:56:54,456 root         INFO     Train Epoch: 15 [1024/8000 (13%)]\tTotal Loss: 9529.434570\n",
      "Reconstruction: 9529.167969, Regularization: 0.185426, Discriminator: 0.004454; Generator: 0.077060,\n",
      "D(x): 0.957, D(G(z)): 0.091\n",
      "2019-04-09 23:56:54,565 root         INFO     Train Epoch: 15 [1536/8000 (19%)]\tTotal Loss: 2.948166\n",
      "Reconstruction: 2.732551, Regularization: 0.135134, Discriminator: 0.004745; Generator: 0.075736,\n",
      "D(x): 0.951, D(G(z)): 0.093\n",
      "2019-04-09 23:56:54,674 root         INFO     Train Epoch: 15 [2048/8000 (26%)]\tTotal Loss: 213.820908\n",
      "Reconstruction: 213.564072, Regularization: 0.173869, Discriminator: 0.004492; Generator: 0.078476,\n",
      "D(x): 0.951, D(G(z)): 0.087\n",
      "2019-04-09 23:56:54,783 root         INFO     Train Epoch: 15 [2560/8000 (32%)]\tTotal Loss: 10774319104.000000\n",
      "Reconstruction: 10774319104.000000, Regularization: 0.390040, Discriminator: 0.004190; Generator: 0.076608,\n",
      "D(x): 0.964, D(G(z)): 0.091\n",
      "2019-04-09 23:56:54,892 root         INFO     Train Epoch: 15 [3072/8000 (38%)]\tTotal Loss: 1.485780\n",
      "Reconstruction: 1.297109, Regularization: 0.107113, Discriminator: 0.004563; Generator: 0.076994,\n",
      "D(x): 0.950, D(G(z)): 0.088\n",
      "2019-04-09 23:56:55,001 root         INFO     Train Epoch: 15 [3584/8000 (45%)]\tTotal Loss: 1.567540\n",
      "Reconstruction: 1.303034, Regularization: 0.180016, Discriminator: 0.005273; Generator: 0.079218,\n",
      "D(x): 0.934, D(G(z)): 0.086\n",
      "2019-04-09 23:56:55,110 root         INFO     Train Epoch: 15 [4096/8000 (51%)]\tTotal Loss: 7.055700\n",
      "Reconstruction: 6.826791, Regularization: 0.147148, Discriminator: 0.003976; Generator: 0.077785,\n",
      "D(x): 0.967, D(G(z)): 0.088\n",
      "2019-04-09 23:56:55,219 root         INFO     Train Epoch: 15 [4608/8000 (58%)]\tTotal Loss: 139.933533\n",
      "Reconstruction: 139.647720, Regularization: 0.204555, Discriminator: 0.005273; Generator: 0.075980,\n",
      "D(x): 0.937, D(G(z)): 0.093\n",
      "2019-04-09 23:56:55,328 root         INFO     Train Epoch: 15 [5120/8000 (64%)]\tTotal Loss: 28.182322\n",
      "Reconstruction: 27.976501, Regularization: 0.123051, Discriminator: 0.004093; Generator: 0.078676,\n",
      "D(x): 0.961, D(G(z)): 0.085\n",
      "2019-04-09 23:56:55,436 root         INFO     Train Epoch: 15 [5632/8000 (70%)]\tTotal Loss: 1586314.625000\n",
      "Reconstruction: 1586314.250000, Regularization: 0.302170, Discriminator: 0.004252; Generator: 0.076064,\n",
      "D(x): 0.966, D(G(z)): 0.094\n",
      "2019-04-09 23:56:55,545 root         INFO     Train Epoch: 15 [6144/8000 (77%)]\tTotal Loss: 41.471760\n",
      "Reconstruction: 41.141727, Regularization: 0.244978, Discriminator: 0.004368; Generator: 0.080684,\n",
      "D(x): 0.949, D(G(z)): 0.082\n",
      "2019-04-09 23:56:55,654 root         INFO     Train Epoch: 15 [6656/8000 (83%)]\tTotal Loss: 187.227859\n",
      "Reconstruction: 186.908600, Regularization: 0.235449, Discriminator: 0.003692; Generator: 0.080120,\n",
      "D(x): 0.974, D(G(z)): 0.087\n",
      "2019-04-09 23:56:55,763 root         INFO     Train Epoch: 15 [7168/8000 (90%)]\tTotal Loss: 469.493195\n",
      "Reconstruction: 469.186676, Regularization: 0.220807, Discriminator: 0.003931; Generator: 0.081792,\n",
      "D(x): 0.961, D(G(z)): 0.077\n",
      "2019-04-09 23:56:55,871 root         INFO     Train Epoch: 15 [7680/8000 (96%)]\tTotal Loss: 51.310932\n",
      "Reconstruction: 49.970116, Regularization: 1.259139, Discriminator: 0.003788; Generator: 0.077888,\n",
      "D(x): 0.977, D(G(z)): 0.091\n",
      "2019-04-09 23:56:55,951 root         INFO     ====> Epoch: 15 Average loss: 317534163.3162\n",
      "2019-04-09 23:56:55,978 root         INFO     Train Epoch: 16 [0/8000 (0%)]\tTotal Loss: 261.034485\n",
      "Reconstruction: 260.728577, Regularization: 0.220846, Discriminator: 0.003551; Generator: 0.081503,\n",
      "D(x): 0.972, D(G(z)): 0.081\n",
      "2019-04-09 23:56:56,087 root         INFO     Train Epoch: 16 [512/8000 (6%)]\tTotal Loss: 13845289861382144.000000\n",
      "Reconstruction: 13845289861382144.000000, Regularization: 0.992014, Discriminator: 0.004733; Generator: 0.073748,\n",
      "D(x): 0.964, D(G(z)): 0.103\n",
      "2019-04-09 23:56:56,195 root         INFO     Train Epoch: 16 [1024/8000 (13%)]\tTotal Loss: 53.496586\n",
      "Reconstruction: 53.166225, Regularization: 0.246227, Discriminator: 0.003387; Generator: 0.080748,\n",
      "D(x): 0.980, D(G(z)): 0.083\n",
      "2019-04-09 23:56:56,304 root         INFO     Train Epoch: 16 [1536/8000 (19%)]\tTotal Loss: 770.297058\n",
      "Reconstruction: 770.095825, Regularization: 0.120586, Discriminator: 0.006606; Generator: 0.074022,\n",
      "D(x): 0.925, D(G(z)): 0.103\n",
      "2019-04-09 23:56:56,412 root         INFO     Train Epoch: 16 [2048/8000 (26%)]\tTotal Loss: 3.683698\n",
      "Reconstruction: 3.473487, Regularization: 0.124068, Discriminator: 0.003645; Generator: 0.082499,\n",
      "D(x): 0.968, D(G(z)): 0.079\n",
      "2019-04-09 23:56:56,520 root         INFO     Train Epoch: 16 [2560/8000 (32%)]\tTotal Loss: 774.221008\n",
      "Reconstruction: 773.670532, Regularization: 0.463231, Discriminator: 0.003926; Generator: 0.083312,\n",
      "D(x): 0.959, D(G(z)): 0.073\n",
      "2019-04-09 23:56:56,629 root         INFO     Train Epoch: 16 [3072/8000 (38%)]\tTotal Loss: 73904440.000000\n",
      "Reconstruction: 73904440.000000, Regularization: 0.440986, Discriminator: 0.004043; Generator: 0.079997,\n",
      "D(x): 0.968, D(G(z)): 0.088\n",
      "2019-04-09 23:56:56,736 root         INFO     Train Epoch: 16 [3584/8000 (45%)]\tTotal Loss: 23821969408.000000\n",
      "Reconstruction: 23821969408.000000, Regularization: 0.389611, Discriminator: 0.003985; Generator: 0.079820,\n",
      "D(x): 0.967, D(G(z)): 0.087\n",
      "2019-04-09 23:56:56,844 root         INFO     Train Epoch: 16 [4096/8000 (51%)]\tTotal Loss: 2175.945068\n",
      "Reconstruction: 2175.563721, Regularization: 0.289468, Discriminator: 0.005680; Generator: 0.086212,\n",
      "D(x): 0.916, D(G(z)): 0.071\n",
      "2019-04-09 23:56:56,954 root         INFO     Train Epoch: 16 [4608/8000 (58%)]\tTotal Loss: 9531.382812\n",
      "Reconstruction: 9530.974609, Regularization: 0.324481, Discriminator: 0.004838; Generator: 0.079192,\n",
      "D(x): 0.948, D(G(z)): 0.093\n",
      "2019-04-09 23:56:57,065 root         INFO     Train Epoch: 16 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.003938; Generator: 0.080260,\n",
      "D(x): 0.968, D(G(z)): 0.087\n",
      "2019-04-09 23:56:57,176 root         INFO     Train Epoch: 16 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004380; Generator: 0.077654,\n",
      "D(x): 0.961, D(G(z)): 0.093\n",
      "2019-04-09 23:56:57,288 root         INFO     Train Epoch: 16 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004367; Generator: 0.078700,\n",
      "D(x): 0.968, D(G(z)): 0.095\n",
      "2019-04-09 23:56:57,398 root         INFO     Train Epoch: 16 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005026; Generator: 0.074986,\n",
      "D(x): 0.960, D(G(z)): 0.109\n",
      "2019-04-09 23:56:57,510 root         INFO     Train Epoch: 16 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004141; Generator: 0.078783,\n",
      "D(x): 0.962, D(G(z)): 0.088\n",
      "2019-04-09 23:56:57,621 root         INFO     Train Epoch: 16 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004498; Generator: 0.079888,\n",
      "D(x): 0.961, D(G(z)): 0.094\n",
      "2019-04-09 23:56:57,703 root         INFO     ====> Epoch: 16 Average loss: nan\n",
      "2019-04-09 23:56:57,730 root         INFO     Train Epoch: 17 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004801; Generator: 0.079351,\n",
      "D(x): 0.950, D(G(z)): 0.090\n",
      "2019-04-09 23:56:57,840 root         INFO     Train Epoch: 17 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.003825; Generator: 0.078038,\n",
      "D(x): 0.981, D(G(z)): 0.096\n",
      "2019-04-09 23:56:57,949 root         INFO     Train Epoch: 17 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.006163; Generator: 0.072404,\n",
      "D(x): 0.943, D(G(z)): 0.118\n",
      "2019-04-09 23:56:58,059 root         INFO     Train Epoch: 17 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004883; Generator: 0.077935,\n",
      "D(x): 0.962, D(G(z)): 0.106\n",
      "2019-04-09 23:56:58,169 root         INFO     Train Epoch: 17 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004131; Generator: 0.078042,\n",
      "D(x): 0.965, D(G(z)): 0.091\n",
      "2019-04-09 23:56:58,278 root         INFO     Train Epoch: 17 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004250; Generator: 0.082155,\n",
      "D(x): 0.968, D(G(z)): 0.094\n",
      "2019-04-09 23:56:58,388 root         INFO     Train Epoch: 17 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005079; Generator: 0.083517,\n",
      "D(x): 0.937, D(G(z)): 0.084\n",
      "2019-04-09 23:56:58,498 root         INFO     Train Epoch: 17 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005214; Generator: 0.076475,\n",
      "D(x): 0.967, D(G(z)): 0.117\n",
      "2019-04-09 23:56:58,609 root         INFO     Train Epoch: 17 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004572; Generator: 0.079038,\n",
      "D(x): 0.962, D(G(z)): 0.099\n",
      "2019-04-09 23:56:58,719 root         INFO     Train Epoch: 17 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005334; Generator: 0.072722,\n",
      "D(x): 0.967, D(G(z)): 0.123\n",
      "2019-04-09 23:56:58,829 root         INFO     Train Epoch: 17 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005073; Generator: 0.076144,\n",
      "D(x): 0.956, D(G(z)): 0.104\n",
      "2019-04-09 23:56:58,941 root         INFO     Train Epoch: 17 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005548; Generator: 0.077588,\n",
      "D(x): 0.944, D(G(z)): 0.108\n",
      "2019-04-09 23:56:59,053 root         INFO     Train Epoch: 17 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.004896; Generator: 0.079540,\n",
      "D(x): 0.966, D(G(z)): 0.109\n",
      "2019-04-09 23:56:59,165 root         INFO     Train Epoch: 17 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.006888; Generator: 0.073015,\n",
      "D(x): 0.943, D(G(z)): 0.134\n",
      "2019-04-09 23:56:59,275 root         INFO     Train Epoch: 17 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.006358; Generator: 0.073569,\n",
      "D(x): 0.935, D(G(z)): 0.117\n",
      "2019-04-09 23:56:59,385 root         INFO     Train Epoch: 17 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.003932; Generator: 0.083323,\n",
      "D(x): 0.975, D(G(z)): 0.092\n",
      "2019-04-09 23:56:59,466 root         INFO     ====> Epoch: 17 Average loss: nan\n",
      "2019-04-09 23:56:59,493 root         INFO     Train Epoch: 18 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005322; Generator: 0.078633,\n",
      "D(x): 0.953, D(G(z)): 0.107\n",
      "2019-04-09 23:56:59,604 root         INFO     Train Epoch: 18 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.005174; Generator: 0.082875,\n",
      "D(x): 0.951, D(G(z)): 0.097\n",
      "2019-04-09 23:56:59,715 root         INFO     Train Epoch: 18 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.009773; Generator: 0.073107,\n",
      "D(x): 0.911, D(G(z)): 0.144\n",
      "2019-04-09 23:56:59,826 root         INFO     Train Epoch: 18 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.007345; Generator: 0.073721,\n",
      "D(x): 0.938, D(G(z)): 0.138\n",
      "2019-04-09 23:56:59,936 root         INFO     Train Epoch: 18 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.008505; Generator: 0.067293,\n",
      "D(x): 0.941, D(G(z)): 0.175\n",
      "2019-04-09 23:57:00,047 root         INFO     Train Epoch: 18 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.006323; Generator: 0.081617,\n",
      "D(x): 0.939, D(G(z)): 0.113\n",
      "2019-04-09 23:57:00,158 root         INFO     Train Epoch: 18 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.007680; Generator: 0.065341,\n",
      "D(x): 0.945, D(G(z)): 0.156\n",
      "2019-04-09 23:57:00,268 root         INFO     Train Epoch: 18 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.009289; Generator: 0.073591,\n",
      "D(x): 0.923, D(G(z)): 0.162\n",
      "2019-04-09 23:57:00,379 root         INFO     Train Epoch: 18 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.008803; Generator: 0.068223,\n",
      "D(x): 0.927, D(G(z)): 0.159\n",
      "2019-04-09 23:57:00,489 root         INFO     Train Epoch: 18 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.009562; Generator: 0.071066,\n",
      "D(x): 0.929, D(G(z)): 0.169\n",
      "2019-04-09 23:57:00,600 root         INFO     Train Epoch: 18 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.010562; Generator: 0.063901,\n",
      "D(x): 0.912, D(G(z)): 0.186\n",
      "2019-04-09 23:57:00,711 root         INFO     Train Epoch: 18 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.007761; Generator: 0.075848,\n",
      "D(x): 0.934, D(G(z)): 0.146\n",
      "2019-04-09 23:57:00,821 root         INFO     Train Epoch: 18 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.009183; Generator: 0.074667,\n",
      "D(x): 0.910, D(G(z)): 0.153\n",
      "2019-04-09 23:57:00,931 root         INFO     Train Epoch: 18 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.009651; Generator: 0.064856,\n",
      "D(x): 0.922, D(G(z)): 0.170\n",
      "2019-04-09 23:57:01,038 root         INFO     Train Epoch: 18 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.013470; Generator: 0.067612,\n",
      "D(x): 0.916, D(G(z)): 0.225\n",
      "2019-04-09 23:57:01,145 root         INFO     Train Epoch: 18 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.011784; Generator: 0.073882,\n",
      "D(x): 0.916, D(G(z)): 0.196\n",
      "2019-04-09 23:57:01,224 root         INFO     ====> Epoch: 18 Average loss: nan\n",
      "2019-04-09 23:57:01,251 root         INFO     Train Epoch: 19 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.011363; Generator: 0.072948,\n",
      "D(x): 0.940, D(G(z)): 0.210\n",
      "2019-04-09 23:57:01,362 root         INFO     Train Epoch: 19 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.012060; Generator: 0.067264,\n",
      "D(x): 0.927, D(G(z)): 0.202\n",
      "2019-04-09 23:57:01,472 root         INFO     Train Epoch: 19 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.012181; Generator: 0.072359,\n",
      "D(x): 0.927, D(G(z)): 0.215\n",
      "2019-04-09 23:57:01,583 root         INFO     Train Epoch: 19 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.010472; Generator: 0.071831,\n",
      "D(x): 0.907, D(G(z)): 0.169\n",
      "2019-04-09 23:57:01,694 root         INFO     Train Epoch: 19 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.018985; Generator: 0.070388,\n",
      "D(x): 0.855, D(G(z)): 0.226\n",
      "2019-04-09 23:57:01,804 root         INFO     Train Epoch: 19 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.011984; Generator: 0.070941,\n",
      "D(x): 0.907, D(G(z)): 0.202\n",
      "2019-04-09 23:57:01,914 root         INFO     Train Epoch: 19 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.013129; Generator: 0.070502,\n",
      "D(x): 0.875, D(G(z)): 0.195\n",
      "2019-04-09 23:57:02,025 root         INFO     Train Epoch: 19 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.010816; Generator: 0.069091,\n",
      "D(x): 0.920, D(G(z)): 0.182\n",
      "2019-04-09 23:57:02,137 root         INFO     Train Epoch: 19 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.018539; Generator: 0.071713,\n",
      "D(x): 0.866, D(G(z)): 0.235\n",
      "2019-04-09 23:57:02,247 root         INFO     Train Epoch: 19 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015608; Generator: 0.081213,\n",
      "D(x): 0.852, D(G(z)): 0.193\n",
      "2019-04-09 23:57:02,357 root         INFO     Train Epoch: 19 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.013527; Generator: 0.080903,\n",
      "D(x): 0.844, D(G(z)): 0.159\n",
      "2019-04-09 23:57:02,466 root         INFO     Train Epoch: 19 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014362; Generator: 0.064110,\n",
      "D(x): 0.866, D(G(z)): 0.220\n",
      "2019-04-09 23:57:02,576 root         INFO     Train Epoch: 19 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.012688; Generator: 0.078236,\n",
      "D(x): 0.905, D(G(z)): 0.190\n",
      "2019-04-09 23:57:02,686 root         INFO     Train Epoch: 19 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.020753; Generator: 0.080338,\n",
      "D(x): 0.811, D(G(z)): 0.216\n",
      "2019-04-09 23:57:02,795 root         INFO     Train Epoch: 19 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.018490; Generator: 0.068015,\n",
      "D(x): 0.820, D(G(z)): 0.214\n",
      "2019-04-09 23:57:02,904 root         INFO     Train Epoch: 19 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014578; Generator: 0.074564,\n",
      "D(x): 0.909, D(G(z)): 0.243\n",
      "2019-04-09 23:57:02,985 root         INFO     ====> Epoch: 19 Average loss: nan\n",
      "2019-04-09 23:57:03,012 root         INFO     Train Epoch: 20 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014109; Generator: 0.089943,\n",
      "D(x): 0.863, D(G(z)): 0.186\n",
      "2019-04-09 23:57:03,124 root         INFO     Train Epoch: 20 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.010400; Generator: 0.091042,\n",
      "D(x): 0.899, D(G(z)): 0.160\n",
      "2019-04-09 23:57:03,234 root         INFO     Train Epoch: 20 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015418; Generator: 0.081660,\n",
      "D(x): 0.849, D(G(z)): 0.172\n",
      "2019-04-09 23:57:03,344 root         INFO     Train Epoch: 20 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.017787; Generator: 0.068289,\n",
      "D(x): 0.888, D(G(z)): 0.270\n",
      "2019-04-09 23:57:03,455 root         INFO     Train Epoch: 20 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.016512; Generator: 0.068438,\n",
      "D(x): 0.866, D(G(z)): 0.241\n",
      "2019-04-09 23:57:03,565 root         INFO     Train Epoch: 20 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.009518; Generator: 0.089459,\n",
      "D(x): 0.880, D(G(z)): 0.118\n",
      "2019-04-09 23:57:03,676 root         INFO     Train Epoch: 20 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.013290; Generator: 0.085758,\n",
      "D(x): 0.862, D(G(z)): 0.154\n",
      "2019-04-09 23:57:03,786 root         INFO     Train Epoch: 20 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014771; Generator: 0.070193,\n",
      "D(x): 0.888, D(G(z)): 0.214\n",
      "2019-04-09 23:57:03,896 root         INFO     Train Epoch: 20 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.020909; Generator: 0.058828,\n",
      "D(x): 0.855, D(G(z)): 0.298\n",
      "2019-04-09 23:57:04,007 root         INFO     Train Epoch: 20 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.017953; Generator: 0.069386,\n",
      "D(x): 0.869, D(G(z)): 0.250\n",
      "2019-04-09 23:57:04,116 root         INFO     Train Epoch: 20 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034037; Generator: 0.045363,\n",
      "D(x): 0.848, D(G(z)): 0.424\n",
      "2019-04-09 23:57:04,224 root         INFO     Train Epoch: 20 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.018670; Generator: 0.083401,\n",
      "D(x): 0.823, D(G(z)): 0.223\n",
      "2019-04-09 23:57:04,336 root         INFO     Train Epoch: 20 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015797; Generator: 0.082643,\n",
      "D(x): 0.805, D(G(z)): 0.160\n",
      "2019-04-09 23:57:04,445 root         INFO     Train Epoch: 20 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014861; Generator: 0.087807,\n",
      "D(x): 0.842, D(G(z)): 0.184\n",
      "2019-04-09 23:57:04,554 root         INFO     Train Epoch: 20 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.013798; Generator: 0.090252,\n",
      "D(x): 0.859, D(G(z)): 0.180\n",
      "2019-04-09 23:57:04,662 root         INFO     Train Epoch: 20 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.021747; Generator: 0.075196,\n",
      "D(x): 0.832, D(G(z)): 0.265\n",
      "2019-04-09 23:57:04,742 root         INFO     ====> Epoch: 20 Average loss: nan\n",
      "2019-04-09 23:57:04,769 root         INFO     Train Epoch: 21 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.016510; Generator: 0.071403,\n",
      "D(x): 0.839, D(G(z)): 0.214\n",
      "2019-04-09 23:57:04,881 root         INFO     Train Epoch: 21 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027696; Generator: 0.067347,\n",
      "D(x): 0.851, D(G(z)): 0.307\n",
      "2019-04-09 23:57:04,992 root         INFO     Train Epoch: 21 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.012475; Generator: 0.080834,\n",
      "D(x): 0.859, D(G(z)): 0.174\n",
      "2019-04-09 23:57:05,103 root         INFO     Train Epoch: 21 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015381; Generator: 0.067674,\n",
      "D(x): 0.862, D(G(z)): 0.218\n",
      "2019-04-09 23:57:05,214 root         INFO     Train Epoch: 21 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.023285; Generator: 0.083283,\n",
      "D(x): 0.818, D(G(z)): 0.268\n",
      "2019-04-09 23:57:05,326 root         INFO     Train Epoch: 21 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015865; Generator: 0.087906,\n",
      "D(x): 0.814, D(G(z)): 0.163\n",
      "2019-04-09 23:57:05,438 root         INFO     Train Epoch: 21 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015086; Generator: 0.070608,\n",
      "D(x): 0.841, D(G(z)): 0.214\n",
      "2019-04-09 23:57:05,551 root         INFO     Train Epoch: 21 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.017536; Generator: 0.075601,\n",
      "D(x): 0.838, D(G(z)): 0.233\n",
      "2019-04-09 23:57:05,664 root         INFO     Train Epoch: 21 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.025472; Generator: 0.067717,\n",
      "D(x): 0.802, D(G(z)): 0.294\n",
      "2019-04-09 23:57:05,777 root         INFO     Train Epoch: 21 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015661; Generator: 0.084311,\n",
      "D(x): 0.823, D(G(z)): 0.194\n",
      "2019-04-09 23:57:05,889 root         INFO     Train Epoch: 21 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.017937; Generator: 0.080123,\n",
      "D(x): 0.872, D(G(z)): 0.225\n",
      "2019-04-09 23:57:06,002 root         INFO     Train Epoch: 21 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014844; Generator: 0.069631,\n",
      "D(x): 0.870, D(G(z)): 0.236\n",
      "2019-04-09 23:57:06,113 root         INFO     Train Epoch: 21 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.024164; Generator: 0.081553,\n",
      "D(x): 0.870, D(G(z)): 0.290\n",
      "2019-04-09 23:57:06,223 root         INFO     Train Epoch: 21 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014942; Generator: 0.096461,\n",
      "D(x): 0.849, D(G(z)): 0.163\n",
      "2019-04-09 23:57:06,334 root         INFO     Train Epoch: 21 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.017879; Generator: 0.089544,\n",
      "D(x): 0.806, D(G(z)): 0.191\n",
      "2019-04-09 23:57:06,444 root         INFO     Train Epoch: 21 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028237; Generator: 0.068796,\n",
      "D(x): 0.763, D(G(z)): 0.261\n",
      "2019-04-09 23:57:06,524 root         INFO     ====> Epoch: 21 Average loss: nan\n",
      "2019-04-09 23:57:06,551 root         INFO     Train Epoch: 22 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.022524; Generator: 0.076639,\n",
      "D(x): 0.809, D(G(z)): 0.224\n",
      "2019-04-09 23:57:06,663 root         INFO     Train Epoch: 22 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.017731; Generator: 0.091686,\n",
      "D(x): 0.831, D(G(z)): 0.177\n",
      "2019-04-09 23:57:06,773 root         INFO     Train Epoch: 22 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.018403; Generator: 0.101023,\n",
      "D(x): 0.797, D(G(z)): 0.196\n",
      "2019-04-09 23:57:06,884 root         INFO     Train Epoch: 22 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.015476; Generator: 0.085029,\n",
      "D(x): 0.827, D(G(z)): 0.195\n",
      "2019-04-09 23:57:06,995 root         INFO     Train Epoch: 22 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.022767; Generator: 0.082857,\n",
      "D(x): 0.754, D(G(z)): 0.221\n",
      "2019-04-09 23:57:07,109 root         INFO     Train Epoch: 22 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.020142; Generator: 0.069253,\n",
      "D(x): 0.827, D(G(z)): 0.268\n",
      "2019-04-09 23:57:07,223 root         INFO     Train Epoch: 22 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.014465; Generator: 0.084734,\n",
      "D(x): 0.850, D(G(z)): 0.176\n",
      "2019-04-09 23:57:07,337 root         INFO     Train Epoch: 22 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.022527; Generator: 0.081927,\n",
      "D(x): 0.789, D(G(z)): 0.252\n",
      "2019-04-09 23:57:07,450 root         INFO     Train Epoch: 22 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.021061; Generator: 0.100372,\n",
      "D(x): 0.796, D(G(z)): 0.209\n",
      "2019-04-09 23:57:07,563 root         INFO     Train Epoch: 22 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.024111; Generator: 0.070094,\n",
      "D(x): 0.813, D(G(z)): 0.278\n",
      "2019-04-09 23:57:07,675 root         INFO     Train Epoch: 22 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028166; Generator: 0.077355,\n",
      "D(x): 0.737, D(G(z)): 0.262\n",
      "2019-04-09 23:57:07,787 root         INFO     Train Epoch: 22 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.025509; Generator: 0.085740,\n",
      "D(x): 0.779, D(G(z)): 0.287\n",
      "2019-04-09 23:57:07,900 root         INFO     Train Epoch: 22 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.025814; Generator: 0.080445,\n",
      "D(x): 0.763, D(G(z)): 0.259\n",
      "2019-04-09 23:57:08,013 root         INFO     Train Epoch: 22 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.023605; Generator: 0.069351,\n",
      "D(x): 0.820, D(G(z)): 0.287\n",
      "2019-04-09 23:57:08,123 root         INFO     Train Epoch: 22 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026787; Generator: 0.085480,\n",
      "D(x): 0.727, D(G(z)): 0.219\n",
      "2019-04-09 23:57:08,234 root         INFO     Train Epoch: 22 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.021875; Generator: 0.073265,\n",
      "D(x): 0.793, D(G(z)): 0.288\n",
      "2019-04-09 23:57:08,314 root         INFO     ====> Epoch: 22 Average loss: nan\n",
      "2019-04-09 23:57:08,341 root         INFO     Train Epoch: 23 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026383; Generator: 0.073439,\n",
      "D(x): 0.788, D(G(z)): 0.260\n",
      "2019-04-09 23:57:08,454 root         INFO     Train Epoch: 23 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.022568; Generator: 0.087283,\n",
      "D(x): 0.772, D(G(z)): 0.199\n",
      "2019-04-09 23:57:08,566 root         INFO     Train Epoch: 23 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031416; Generator: 0.072575,\n",
      "D(x): 0.761, D(G(z)): 0.319\n",
      "2019-04-09 23:57:08,681 root         INFO     Train Epoch: 23 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.029662; Generator: 0.070621,\n",
      "D(x): 0.764, D(G(z)): 0.331\n",
      "2019-04-09 23:57:08,793 root         INFO     Train Epoch: 23 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.022734; Generator: 0.080746,\n",
      "D(x): 0.738, D(G(z)): 0.224\n",
      "2019-04-09 23:57:08,906 root         INFO     Train Epoch: 23 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.022425; Generator: 0.087200,\n",
      "D(x): 0.777, D(G(z)): 0.242\n",
      "2019-04-09 23:57:09,018 root         INFO     Train Epoch: 23 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030653; Generator: 0.084484,\n",
      "D(x): 0.737, D(G(z)): 0.278\n",
      "2019-04-09 23:57:09,130 root         INFO     Train Epoch: 23 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031409; Generator: 0.074784,\n",
      "D(x): 0.758, D(G(z)): 0.296\n",
      "2019-04-09 23:57:09,243 root         INFO     Train Epoch: 23 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.023470; Generator: 0.074928,\n",
      "D(x): 0.764, D(G(z)): 0.275\n",
      "2019-04-09 23:57:09,355 root         INFO     Train Epoch: 23 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028273; Generator: 0.072153,\n",
      "D(x): 0.745, D(G(z)): 0.281\n",
      "2019-04-09 23:57:09,465 root         INFO     Train Epoch: 23 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036064; Generator: 0.085589,\n",
      "D(x): 0.682, D(G(z)): 0.287\n",
      "2019-04-09 23:57:09,573 root         INFO     Train Epoch: 23 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032786; Generator: 0.071574,\n",
      "D(x): 0.695, D(G(z)): 0.292\n",
      "2019-04-09 23:57:09,681 root         INFO     Train Epoch: 23 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028863; Generator: 0.073308,\n",
      "D(x): 0.717, D(G(z)): 0.236\n",
      "2019-04-09 23:57:09,788 root         INFO     Train Epoch: 23 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.016982; Generator: 0.087479,\n",
      "D(x): 0.780, D(G(z)): 0.185\n",
      "2019-04-09 23:57:09,896 root         INFO     Train Epoch: 23 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.025296; Generator: 0.077733,\n",
      "D(x): 0.723, D(G(z)): 0.235\n",
      "2019-04-09 23:57:10,003 root         INFO     Train Epoch: 23 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.019563; Generator: 0.092495,\n",
      "D(x): 0.791, D(G(z)): 0.228\n",
      "2019-04-09 23:57:10,082 root         INFO     ====> Epoch: 23 Average loss: nan\n",
      "2019-04-09 23:57:10,108 root         INFO     Train Epoch: 24 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039647; Generator: 0.065486,\n",
      "D(x): 0.648, D(G(z)): 0.310\n",
      "2019-04-09 23:57:10,220 root         INFO     Train Epoch: 24 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039406; Generator: 0.059647,\n",
      "D(x): 0.725, D(G(z)): 0.355\n",
      "2019-04-09 23:57:10,330 root         INFO     Train Epoch: 24 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028364; Generator: 0.071679,\n",
      "D(x): 0.712, D(G(z)): 0.224\n",
      "2019-04-09 23:57:10,441 root         INFO     Train Epoch: 24 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032717; Generator: 0.067570,\n",
      "D(x): 0.669, D(G(z)): 0.295\n",
      "2019-04-09 23:57:10,552 root         INFO     Train Epoch: 24 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032000; Generator: 0.077812,\n",
      "D(x): 0.768, D(G(z)): 0.331\n",
      "2019-04-09 23:57:10,662 root         INFO     Train Epoch: 24 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031491; Generator: 0.062669,\n",
      "D(x): 0.739, D(G(z)): 0.291\n",
      "2019-04-09 23:57:10,773 root         INFO     Train Epoch: 24 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035794; Generator: 0.066846,\n",
      "D(x): 0.712, D(G(z)): 0.304\n",
      "2019-04-09 23:57:10,884 root         INFO     Train Epoch: 24 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031034; Generator: 0.065238,\n",
      "D(x): 0.677, D(G(z)): 0.290\n",
      "2019-04-09 23:57:10,995 root         INFO     Train Epoch: 24 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036667; Generator: 0.053515,\n",
      "D(x): 0.699, D(G(z)): 0.367\n",
      "2019-04-09 23:57:11,105 root         INFO     Train Epoch: 24 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033100; Generator: 0.061587,\n",
      "D(x): 0.673, D(G(z)): 0.325\n",
      "2019-04-09 23:57:11,216 root         INFO     Train Epoch: 24 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030674; Generator: 0.060563,\n",
      "D(x): 0.688, D(G(z)): 0.291\n",
      "2019-04-09 23:57:11,328 root         INFO     Train Epoch: 24 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.032472; Generator: 0.074310,\n",
      "D(x): 0.588, D(G(z)): 0.213\n",
      "2019-04-09 23:57:11,439 root         INFO     Train Epoch: 24 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.031198; Generator: 0.073846,\n",
      "D(x): 0.718, D(G(z)): 0.252\n",
      "2019-04-09 23:57:11,549 root         INFO     Train Epoch: 24 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026776; Generator: 0.077225,\n",
      "D(x): 0.699, D(G(z)): 0.263\n",
      "2019-04-09 23:57:11,659 root         INFO     Train Epoch: 24 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037889; Generator: 0.047374,\n",
      "D(x): 0.667, D(G(z)): 0.374\n",
      "2019-04-09 23:57:11,769 root         INFO     Train Epoch: 24 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035946; Generator: 0.054884,\n",
      "D(x): 0.670, D(G(z)): 0.350\n",
      "2019-04-09 23:57:11,849 root         INFO     ====> Epoch: 24 Average loss: nan\n",
      "2019-04-09 23:57:11,876 root         INFO     Train Epoch: 25 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.026878; Generator: 0.073799,\n",
      "D(x): 0.688, D(G(z)): 0.228\n",
      "2019-04-09 23:57:11,987 root         INFO     Train Epoch: 25 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027434; Generator: 0.080060,\n",
      "D(x): 0.661, D(G(z)): 0.232\n",
      "2019-04-09 23:57:12,096 root         INFO     Train Epoch: 25 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.027798; Generator: 0.063985,\n",
      "D(x): 0.622, D(G(z)): 0.244\n",
      "2019-04-09 23:57:12,204 root         INFO     Train Epoch: 25 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039050; Generator: 0.054793,\n",
      "D(x): 0.638, D(G(z)): 0.374\n",
      "2019-04-09 23:57:12,313 root         INFO     Train Epoch: 25 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036013; Generator: 0.055371,\n",
      "D(x): 0.668, D(G(z)): 0.348\n",
      "2019-04-09 23:57:12,422 root         INFO     Train Epoch: 25 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040034; Generator: 0.052725,\n",
      "D(x): 0.605, D(G(z)): 0.364\n",
      "2019-04-09 23:57:12,530 root         INFO     Train Epoch: 25 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033188; Generator: 0.069364,\n",
      "D(x): 0.644, D(G(z)): 0.299\n",
      "2019-04-09 23:57:12,639 root         INFO     Train Epoch: 25 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041551; Generator: 0.055255,\n",
      "D(x): 0.658, D(G(z)): 0.389\n",
      "2019-04-09 23:57:12,749 root         INFO     Train Epoch: 25 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035118; Generator: 0.063128,\n",
      "D(x): 0.617, D(G(z)): 0.292\n",
      "2019-04-09 23:57:12,859 root         INFO     Train Epoch: 25 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035447; Generator: 0.077024,\n",
      "D(x): 0.549, D(G(z)): 0.211\n",
      "2019-04-09 23:57:12,969 root         INFO     Train Epoch: 25 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036207; Generator: 0.056499,\n",
      "D(x): 0.577, D(G(z)): 0.298\n",
      "2019-04-09 23:57:13,079 root         INFO     Train Epoch: 25 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037451; Generator: 0.045119,\n",
      "D(x): 0.628, D(G(z)): 0.364\n",
      "2019-04-09 23:57:13,189 root         INFO     Train Epoch: 25 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038955; Generator: 0.049606,\n",
      "D(x): 0.718, D(G(z)): 0.413\n",
      "2019-04-09 23:57:13,298 root         INFO     Train Epoch: 25 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.043945; Generator: 0.054872,\n",
      "D(x): 0.547, D(G(z)): 0.365\n",
      "2019-04-09 23:57:13,408 root         INFO     Train Epoch: 25 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046978; Generator: 0.043661,\n",
      "D(x): 0.571, D(G(z)): 0.416\n",
      "2019-04-09 23:57:13,517 root         INFO     Train Epoch: 25 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.037897; Generator: 0.046848,\n",
      "D(x): 0.629, D(G(z)): 0.368\n",
      "2019-04-09 23:57:13,597 root         INFO     ====> Epoch: 25 Average loss: nan\n",
      "2019-04-09 23:57:13,625 root         INFO     Train Epoch: 26 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.033474; Generator: 0.057934,\n",
      "D(x): 0.621, D(G(z)): 0.320\n",
      "2019-04-09 23:57:13,735 root         INFO     Train Epoch: 26 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.030323; Generator: 0.059831,\n",
      "D(x): 0.591, D(G(z)): 0.232\n",
      "2019-04-09 23:57:13,844 root         INFO     Train Epoch: 26 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041542; Generator: 0.051062,\n",
      "D(x): 0.581, D(G(z)): 0.375\n",
      "2019-04-09 23:57:13,954 root         INFO     Train Epoch: 26 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044734; Generator: 0.047846,\n",
      "D(x): 0.586, D(G(z)): 0.410\n",
      "2019-04-09 23:57:14,063 root         INFO     Train Epoch: 26 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040449; Generator: 0.047723,\n",
      "D(x): 0.591, D(G(z)): 0.362\n",
      "2019-04-09 23:57:14,172 root         INFO     Train Epoch: 26 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.028661; Generator: 0.064016,\n",
      "D(x): 0.660, D(G(z)): 0.277\n",
      "2019-04-09 23:57:14,282 root         INFO     Train Epoch: 26 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040773; Generator: 0.049426,\n",
      "D(x): 0.584, D(G(z)): 0.371\n",
      "2019-04-09 23:57:14,392 root         INFO     Train Epoch: 26 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034392; Generator: 0.066603,\n",
      "D(x): 0.553, D(G(z)): 0.262\n",
      "2019-04-09 23:57:14,503 root         INFO     Train Epoch: 26 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.035045; Generator: 0.062495,\n",
      "D(x): 0.620, D(G(z)): 0.268\n",
      "2019-04-09 23:57:14,613 root         INFO     Train Epoch: 26 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.045396; Generator: 0.059991,\n",
      "D(x): 0.457, D(G(z)): 0.323\n",
      "2019-04-09 23:57:14,723 root         INFO     Train Epoch: 26 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044935; Generator: 0.033187,\n",
      "D(x): 0.567, D(G(z)): 0.436\n",
      "2019-04-09 23:57:14,834 root         INFO     Train Epoch: 26 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.034877; Generator: 0.058159,\n",
      "D(x): 0.539, D(G(z)): 0.289\n",
      "2019-04-09 23:57:14,944 root         INFO     Train Epoch: 26 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041401; Generator: 0.051199,\n",
      "D(x): 0.507, D(G(z)): 0.328\n",
      "2019-04-09 23:57:15,053 root         INFO     Train Epoch: 26 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046750; Generator: 0.029651,\n",
      "D(x): 0.560, D(G(z)): 0.489\n",
      "2019-04-09 23:57:15,163 root         INFO     Train Epoch: 26 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040939; Generator: 0.053775,\n",
      "D(x): 0.581, D(G(z)): 0.357\n",
      "2019-04-09 23:57:15,272 root         INFO     Train Epoch: 26 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.040563; Generator: 0.041792,\n",
      "D(x): 0.581, D(G(z)): 0.413\n",
      "2019-04-09 23:57:15,352 root         INFO     ====> Epoch: 26 Average loss: nan\n",
      "2019-04-09 23:57:15,379 root         INFO     Train Epoch: 27 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036606; Generator: 0.053649,\n",
      "D(x): 0.558, D(G(z)): 0.320\n",
      "2019-04-09 23:57:15,491 root         INFO     Train Epoch: 27 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.036314; Generator: 0.037578,\n",
      "D(x): 0.613, D(G(z)): 0.394\n",
      "2019-04-09 23:57:15,603 root         INFO     Train Epoch: 27 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.045791; Generator: 0.042168,\n",
      "D(x): 0.498, D(G(z)): 0.375\n",
      "2019-04-09 23:57:15,713 root         INFO     Train Epoch: 27 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041381; Generator: 0.041370,\n",
      "D(x): 0.536, D(G(z)): 0.379\n",
      "2019-04-09 23:57:15,825 root         INFO     Train Epoch: 27 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.051879; Generator: 0.040929,\n",
      "D(x): 0.551, D(G(z)): 0.457\n",
      "2019-04-09 23:57:15,934 root         INFO     Train Epoch: 27 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.045223; Generator: 0.046766,\n",
      "D(x): 0.545, D(G(z)): 0.401\n",
      "2019-04-09 23:57:16,043 root         INFO     Train Epoch: 27 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.056319; Generator: 0.042965,\n",
      "D(x): 0.464, D(G(z)): 0.467\n",
      "2019-04-09 23:57:16,153 root         INFO     Train Epoch: 27 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.045991; Generator: 0.042441,\n",
      "D(x): 0.544, D(G(z)): 0.424\n",
      "2019-04-09 23:57:16,261 root         INFO     Train Epoch: 27 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042006; Generator: 0.042531,\n",
      "D(x): 0.509, D(G(z)): 0.368\n",
      "2019-04-09 23:57:16,369 root         INFO     Train Epoch: 27 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.049480; Generator: 0.037318,\n",
      "D(x): 0.510, D(G(z)): 0.448\n",
      "2019-04-09 23:57:16,477 root         INFO     Train Epoch: 27 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044482; Generator: 0.038222,\n",
      "D(x): 0.576, D(G(z)): 0.440\n",
      "2019-04-09 23:57:16,585 root         INFO     Train Epoch: 27 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.053436; Generator: 0.032359,\n",
      "D(x): 0.492, D(G(z)): 0.484\n",
      "2019-04-09 23:57:16,694 root         INFO     Train Epoch: 27 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041222; Generator: 0.037314,\n",
      "D(x): 0.524, D(G(z)): 0.386\n",
      "2019-04-09 23:57:16,803 root         INFO     Train Epoch: 27 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.051081; Generator: 0.036546,\n",
      "D(x): 0.492, D(G(z)): 0.452\n",
      "2019-04-09 23:57:16,911 root         INFO     Train Epoch: 27 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.049120; Generator: 0.039992,\n",
      "D(x): 0.478, D(G(z)): 0.410\n",
      "2019-04-09 23:57:17,023 root         INFO     Train Epoch: 27 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.041413; Generator: 0.034845,\n",
      "D(x): 0.518, D(G(z)): 0.406\n",
      "2019-04-09 23:57:17,104 root         INFO     ====> Epoch: 27 Average loss: nan\n",
      "2019-04-09 23:57:17,132 root         INFO     Train Epoch: 28 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.043526; Generator: 0.041605,\n",
      "D(x): 0.518, D(G(z)): 0.404\n",
      "2019-04-09 23:57:17,245 root         INFO     Train Epoch: 28 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044852; Generator: 0.043378,\n",
      "D(x): 0.505, D(G(z)): 0.399\n",
      "2019-04-09 23:57:17,359 root         INFO     Train Epoch: 28 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046316; Generator: 0.043993,\n",
      "D(x): 0.543, D(G(z)): 0.382\n",
      "2019-04-09 23:57:17,473 root         INFO     Train Epoch: 28 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.050387; Generator: 0.030518,\n",
      "D(x): 0.514, D(G(z)): 0.494\n",
      "2019-04-09 23:57:17,586 root         INFO     Train Epoch: 28 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.038400; Generator: 0.043812,\n",
      "D(x): 0.526, D(G(z)): 0.354\n",
      "2019-04-09 23:57:17,700 root         INFO     Train Epoch: 28 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046999; Generator: 0.033151,\n",
      "D(x): 0.538, D(G(z)): 0.434\n",
      "2019-04-09 23:57:17,813 root         INFO     Train Epoch: 28 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.043134; Generator: 0.034101,\n",
      "D(x): 0.517, D(G(z)): 0.422\n",
      "2019-04-09 23:57:17,925 root         INFO     Train Epoch: 28 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.043591; Generator: 0.041462,\n",
      "D(x): 0.509, D(G(z)): 0.391\n",
      "2019-04-09 23:57:18,038 root         INFO     Train Epoch: 28 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044612; Generator: 0.038345,\n",
      "D(x): 0.527, D(G(z)): 0.410\n",
      "2019-04-09 23:57:18,151 root         INFO     Train Epoch: 28 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.047349; Generator: 0.037154,\n",
      "D(x): 0.525, D(G(z)): 0.436\n",
      "2019-04-09 23:57:18,264 root         INFO     Train Epoch: 28 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039733; Generator: 0.045231,\n",
      "D(x): 0.484, D(G(z)): 0.324\n",
      "2019-04-09 23:57:18,377 root         INFO     Train Epoch: 28 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.039329; Generator: 0.045626,\n",
      "D(x): 0.510, D(G(z)): 0.346\n",
      "2019-04-09 23:57:18,490 root         INFO     Train Epoch: 28 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042340; Generator: 0.046198,\n",
      "D(x): 0.498, D(G(z)): 0.356\n",
      "2019-04-09 23:57:18,603 root         INFO     Train Epoch: 28 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.048659; Generator: 0.036781,\n",
      "D(x): 0.485, D(G(z)): 0.453\n",
      "2019-04-09 23:57:18,716 root         INFO     Train Epoch: 28 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.043524; Generator: 0.040968,\n",
      "D(x): 0.479, D(G(z)): 0.389\n",
      "2019-04-09 23:57:18,829 root         INFO     Train Epoch: 28 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044356; Generator: 0.043716,\n",
      "D(x): 0.440, D(G(z)): 0.361\n",
      "2019-04-09 23:57:18,911 root         INFO     ====> Epoch: 28 Average loss: nan\n",
      "2019-04-09 23:57:18,938 root         INFO     Train Epoch: 29 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.052123; Generator: 0.028464,\n",
      "D(x): 0.508, D(G(z)): 0.494\n",
      "2019-04-09 23:57:19,052 root         INFO     Train Epoch: 29 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.052821; Generator: 0.031628,\n",
      "D(x): 0.490, D(G(z)): 0.484\n",
      "2019-04-09 23:57:19,165 root         INFO     Train Epoch: 29 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.047602; Generator: 0.037306,\n",
      "D(x): 0.462, D(G(z)): 0.439\n",
      "2019-04-09 23:57:19,276 root         INFO     Train Epoch: 29 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046779; Generator: 0.035604,\n",
      "D(x): 0.498, D(G(z)): 0.428\n",
      "2019-04-09 23:57:19,388 root         INFO     Train Epoch: 29 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.053665; Generator: 0.028107,\n",
      "D(x): 0.490, D(G(z)): 0.488\n",
      "2019-04-09 23:57:19,499 root         INFO     Train Epoch: 29 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.052422; Generator: 0.036885,\n",
      "D(x): 0.443, D(G(z)): 0.449\n",
      "2019-04-09 23:57:19,610 root         INFO     Train Epoch: 29 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.050168; Generator: 0.030480,\n",
      "D(x): 0.447, D(G(z)): 0.469\n",
      "2019-04-09 23:57:19,722 root         INFO     Train Epoch: 29 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.054753; Generator: 0.027017,\n",
      "D(x): 0.489, D(G(z)): 0.523\n",
      "2019-04-09 23:57:19,833 root         INFO     Train Epoch: 29 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.054718; Generator: 0.029202,\n",
      "D(x): 0.407, D(G(z)): 0.478\n",
      "2019-04-09 23:57:19,944 root         INFO     Train Epoch: 29 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046554; Generator: 0.035192,\n",
      "D(x): 0.477, D(G(z)): 0.420\n",
      "2019-04-09 23:57:20,055 root         INFO     Train Epoch: 29 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.054735; Generator: 0.027883,\n",
      "D(x): 0.477, D(G(z)): 0.516\n",
      "2019-04-09 23:57:20,167 root         INFO     Train Epoch: 29 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.042819; Generator: 0.040813,\n",
      "D(x): 0.462, D(G(z)): 0.366\n",
      "2019-04-09 23:57:20,278 root         INFO     Train Epoch: 29 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.048393; Generator: 0.027121,\n",
      "D(x): 0.448, D(G(z)): 0.467\n",
      "2019-04-09 23:57:20,389 root         INFO     Train Epoch: 29 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.050514; Generator: 0.029676,\n",
      "D(x): 0.461, D(G(z)): 0.471\n",
      "2019-04-09 23:57:20,500 root         INFO     Train Epoch: 29 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.051630; Generator: 0.033477,\n",
      "D(x): 0.447, D(G(z)): 0.457\n",
      "2019-04-09 23:57:20,612 root         INFO     Train Epoch: 29 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.053611; Generator: 0.030061,\n",
      "D(x): 0.404, D(G(z)): 0.464\n",
      "2019-04-09 23:57:20,693 root         INFO     ====> Epoch: 29 Average loss: nan\n",
      "2019-04-09 23:57:20,720 root         INFO     Train Epoch: 30 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.057011; Generator: 0.025251,\n",
      "D(x): 0.431, D(G(z)): 0.514\n",
      "2019-04-09 23:57:20,834 root         INFO     Train Epoch: 30 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.048463; Generator: 0.024142,\n",
      "D(x): 0.502, D(G(z)): 0.514\n",
      "2019-04-09 23:57:20,946 root         INFO     Train Epoch: 30 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046362; Generator: 0.038739,\n",
      "D(x): 0.472, D(G(z)): 0.391\n",
      "2019-04-09 23:57:21,059 root         INFO     Train Epoch: 30 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.055203; Generator: 0.028166,\n",
      "D(x): 0.430, D(G(z)): 0.490\n",
      "2019-04-09 23:57:21,172 root         INFO     Train Epoch: 30 [2048/8000 (26%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.046238; Generator: 0.032214,\n",
      "D(x): 0.481, D(G(z)): 0.427\n",
      "2019-04-09 23:57:21,285 root         INFO     Train Epoch: 30 [2560/8000 (32%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.050659; Generator: 0.030681,\n",
      "D(x): 0.444, D(G(z)): 0.450\n",
      "2019-04-09 23:57:21,397 root         INFO     Train Epoch: 30 [3072/8000 (38%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.045124; Generator: 0.033707,\n",
      "D(x): 0.467, D(G(z)): 0.423\n",
      "2019-04-09 23:57:21,510 root         INFO     Train Epoch: 30 [3584/8000 (45%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.054106; Generator: 0.030949,\n",
      "D(x): 0.427, D(G(z)): 0.450\n",
      "2019-04-09 23:57:21,623 root         INFO     Train Epoch: 30 [4096/8000 (51%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.051192; Generator: 0.026114,\n",
      "D(x): 0.456, D(G(z)): 0.489\n",
      "2019-04-09 23:57:21,735 root         INFO     Train Epoch: 30 [4608/8000 (58%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.054239; Generator: 0.024579,\n",
      "D(x): 0.412, D(G(z)): 0.506\n",
      "2019-04-09 23:57:21,848 root         INFO     Train Epoch: 30 [5120/8000 (64%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.044088; Generator: 0.036963,\n",
      "D(x): 0.438, D(G(z)): 0.376\n",
      "2019-04-09 23:57:21,961 root         INFO     Train Epoch: 30 [5632/8000 (70%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.055214; Generator: 0.022502,\n",
      "D(x): 0.451, D(G(z)): 0.547\n",
      "2019-04-09 23:57:22,074 root         INFO     Train Epoch: 30 [6144/8000 (77%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.050503; Generator: 0.031271,\n",
      "D(x): 0.478, D(G(z)): 0.480\n",
      "2019-04-09 23:57:22,187 root         INFO     Train Epoch: 30 [6656/8000 (83%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.050820; Generator: 0.030628,\n",
      "D(x): 0.443, D(G(z)): 0.458\n",
      "2019-04-09 23:57:22,299 root         INFO     Train Epoch: 30 [7168/8000 (90%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.052978; Generator: 0.028431,\n",
      "D(x): 0.441, D(G(z)): 0.467\n",
      "2019-04-09 23:57:22,408 root         INFO     Train Epoch: 30 [7680/8000 (96%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.053276; Generator: 0.027398,\n",
      "D(x): 0.439, D(G(z)): 0.500\n",
      "2019-04-09 23:57:22,489 root         INFO     ====> Epoch: 30 Average loss: nan\n",
      "2019-04-09 23:57:22,516 root         INFO     Train Epoch: 31 [0/8000 (0%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.047777; Generator: 0.028987,\n",
      "D(x): 0.453, D(G(z)): 0.460\n",
      "2019-04-09 23:57:22,628 root         INFO     Train Epoch: 31 [512/8000 (6%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.054730; Generator: 0.029839,\n",
      "D(x): 0.431, D(G(z)): 0.471\n",
      "2019-04-09 23:57:22,739 root         INFO     Train Epoch: 31 [1024/8000 (13%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.045791; Generator: 0.033164,\n",
      "D(x): 0.422, D(G(z)): 0.396\n",
      "2019-04-09 23:57:22,849 root         INFO     Train Epoch: 31 [1536/8000 (19%)]\tTotal Loss: nan\n",
      "Reconstruction: nan, Regularization: nan, Discriminator: 0.048742; Generator: 0.028561,\n",
      "D(x): 0.412, D(G(z)): 0.441\n",
      "2019-04-09 23:57:22,978 luigi-interface INFO     Worker Worker(salt=447342177, workers=1, host=gne, username=nina, pid=26188) was stopped. Shutting down Keep-Alive thread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = 'logs'\n",
    "logs = []\n",
    "for filename in os.listdir(OUTPUT):\n",
    "    if re.search(pattern, filename):\n",
    "        logs.append(filename)\n",
    "\n",
    "print('Found %d log files.' % len(logs))\n",
    "        \n",
    "for filename in logs:\n",
    "    path = os.path.join(OUTPUT, filename)\n",
    "    print('\\n-- Log file: %s\\n' % filename)\n",
    "    with open(path, 'r') as f:\n",
    "        message = f.read()\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
